{"files":[{"patch":"@@ -1321,0 +1321,17 @@\n+  void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,\n+                      FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n+                      FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3);\n+  void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                    FloatRegister p, FloatRegister z, FloatRegister t1);\n+  void ghash_processBlocks_wide(address p, Register state, Register subkeyH,\n+                                Register data, Register blocks, int unrolls);\n+  void ghash_modmul (FloatRegister result,\n+                     FloatRegister result_lo, FloatRegister result_hi, FloatRegister b,\n+                     FloatRegister a, FloatRegister vzr, FloatRegister a1_xor_a0, FloatRegister p,\n+                     FloatRegister t1, FloatRegister t2, FloatRegister t3);\n+\n+  void aesenc_loadkeys(Register key, Register keylen);\n+  void aesecb_encrypt(Register from, Register to, Register keylen,\n+                      FloatRegister data = v0, int unrolls = 1);\n+  void aesecb_decrypt(Register from, Register to, Register key, Register keylen);\n+  void aes_round(FloatRegister input, FloatRegister subkey);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2967,0 +2967,259 @@\n+  \/\/ CTR AES crypt.\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/   c_rarg3   - counter vector byte array address\n+  \/\/   c_rarg4   - input length\n+  \/\/   c_rarg5   - saved encryptedCounter start\n+  \/\/   c_rarg6   - saved used length\n+  \/\/\n+  \/\/ Output:\n+  \/\/   r0       - input length\n+  \/\/\n+  address generate_counterMode_AESCrypt() {\n+    const Register in = c_rarg0;\n+    const Register out = c_rarg1;\n+    const Register key = c_rarg2;\n+    const Register counter = c_rarg3;\n+    const Register saved_len = c_rarg4, len = r10;\n+    const Register saved_encrypted_ctr = c_rarg5;\n+    const Register used_ptr = c_rarg6, used = r12;\n+\n+    const Register offset = r7;\n+    const Register keylen = r11;\n+\n+    const unsigned char block_size = 16;\n+    const int bulk_width = 4;\n+    \/\/ NB: bulk_width can be 4 or 8. 8 gives slightly faster\n+    \/\/ performance with larger data sizes, but it also means that the\n+    \/\/ fast path isn't used until you have at least 8 blocks, and up\n+    \/\/ to 127 bytes of data will be executed on the slow path. For\n+    \/\/ that reason, and also so as not to blow away too much icache, 4\n+    \/\/ blocks seems like a sensible compromise.\n+\n+    \/\/ Algorithm:\n+    \/\/\n+    \/\/    if (len == 0) {\n+    \/\/        goto DONE;\n+    \/\/    }\n+    \/\/    int result = len;\n+    \/\/    do {\n+    \/\/        if (used >= blockSize) {\n+    \/\/            if (len >= bulk_width * blockSize) {\n+    \/\/                CTR_large_block();\n+    \/\/                if (len == 0)\n+    \/\/                    goto DONE;\n+    \/\/            }\n+    \/\/            for (;;) {\n+    \/\/                16ByteVector v0 = counter;\n+    \/\/                embeddedCipher.encryptBlock(v0, 0, encryptedCounter, 0);\n+    \/\/                used = 0;\n+    \/\/                if (len < blockSize)\n+    \/\/                    break;    \/* goto NEXT *\/\n+    \/\/                16ByteVector v1 = load16Bytes(in, offset);\n+    \/\/                v1 = v1 ^ encryptedCounter;\n+    \/\/                store16Bytes(out, offset);\n+    \/\/                used = blockSize;\n+    \/\/                offset += blockSize;\n+    \/\/                len -= blockSize;\n+    \/\/                if (len == 0)\n+    \/\/                    goto DONE;\n+    \/\/            }\n+    \/\/        }\n+    \/\/      NEXT:\n+    \/\/        out[outOff++] = (byte)(in[inOff++] ^ encryptedCounter[used++]);\n+    \/\/        len--;\n+    \/\/    } while (len != 0);\n+    \/\/  DONE:\n+    \/\/    return result;\n+    \/\/\n+    \/\/ CTR_large_block()\n+    \/\/    Wide bulk encryption of whole blocks.\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+    const address start = __ pc();\n+    __ enter();\n+\n+    Label DONE, CTR_large_block, large_block_return;\n+    __ ldrw(used, Address(used_ptr));\n+    __ cbzw(saved_len, DONE);\n+\n+    __ mov(len, saved_len);\n+    __ mov(offset, 0);\n+\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ aesenc_loadkeys(key, keylen);\n+\n+    {\n+      Label L_CTR_loop, NEXT;\n+\n+      __ bind(L_CTR_loop);\n+\n+      __ cmp(used, block_size);\n+      __ br(__ LO, NEXT);\n+\n+      \/\/ Maybe we have a lot of data\n+      __ subsw(rscratch1, len, bulk_width * block_size);\n+      __ br(__ HS, CTR_large_block);\n+      __ BIND(large_block_return);\n+      __ cbzw(len, DONE);\n+\n+      \/\/ Setup the counter\n+      __ movi(v4, __ T4S, 0);\n+      __ movi(v5, __ T4S, 1);\n+      __ ins(v4, __ S, v5, 3, 3); \/\/ v4 contains { 0, 0, 0, 1 }\n+\n+      __ ld1(v0, __ T16B, counter); \/\/ Load the counter into v0\n+      __ rev32(v16, __ T16B, v0);\n+      __ addv(v16, __ T4S, v16, v4);\n+      __ rev32(v16, __ T16B, v16);\n+      __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+\n+      {\n+        \/\/ We have fewer than bulk_width blocks of data left. Encrypt\n+        \/\/ them one by one until there is less than a full block\n+        \/\/ remaining, being careful to save both the encrypted counter\n+        \/\/ and the counter.\n+\n+        Label inner_loop;\n+        __ bind(inner_loop);\n+        \/\/ Counter to encrypt is in v0\n+        __ aesecb_encrypt(noreg, noreg, keylen);\n+        __ st1(v0, __ T16B, saved_encrypted_ctr);\n+\n+        \/\/ Do we have a remaining full block?\n+\n+        __ mov(used, 0);\n+        __ cmp(len, block_size);\n+        __ br(__ LO, NEXT);\n+\n+        \/\/ Yes, we have a full block\n+        __ ldrq(v1, Address(in, offset));\n+        __ eor(v1, __ T16B, v1, v0);\n+        __ strq(v1, Address(out, offset));\n+        __ mov(used, block_size);\n+        __ add(offset, offset, block_size);\n+\n+        __ subw(len, len, block_size);\n+        __ cbzw(len, DONE);\n+\n+        \/\/ Increment the counter, store it back\n+        __ orr(v0, __ T16B, v16, v16);\n+        __ rev32(v16, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v4);\n+        __ rev32(v16, __ T16B, v16);\n+        __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+\n+        __ b(inner_loop);\n+      }\n+\n+      __ BIND(NEXT);\n+\n+      \/\/ Encrypt a single byte, and loop.\n+      \/\/ We expect this to be a rare event.\n+      __ ldrb(rscratch1, Address(in, offset));\n+      __ ldrb(rscratch2, Address(saved_encrypted_ctr, used));\n+      __ eor(rscratch1, rscratch1, rscratch2);\n+      __ strb(rscratch1, Address(out, offset));\n+      __ add(offset, offset, 1);\n+      __ add(used, used, 1);\n+      __ subw(len, len,1);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    __ bind(DONE);\n+    __ strw(used, Address(used_ptr));\n+    __ mov(r0, saved_len);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(lr);\n+\n+    \/\/ Bulk encryption\n+\n+    __ BIND (CTR_large_block);\n+    assert(bulk_width == 4 || bulk_width == 8, \"must be\");\n+\n+    if (bulk_width == 8) {\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+    }\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+    RegSet saved_regs = (RegSet::of(in, out, offset)\n+                         + RegSet::of(saved_encrypted_ctr, used_ptr, len));\n+    __ push(saved_regs, sp);\n+    __ andr(len, len, -16 * bulk_width);  \/\/ 8\/4 encryptions, 16 bytes per encryption\n+    __ add(in, in, offset);\n+    __ add(out, out, offset);\n+\n+    \/\/ Keys should already be loaded into the correct registers\n+\n+    __ ld1(v0, __ T16B, counter); \/\/ v0 contains the first counter\n+    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+\n+    \/\/ AES\/CTR loop\n+    {\n+      Label L_CTR_loop;\n+      __ BIND(L_CTR_loop);\n+\n+      \/\/ Setup the counters\n+      __ movi(v8, __ T4S, 0);\n+      __ movi(v9, __ T4S, 1);\n+      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+\n+      for (FloatRegister f = v0; f < v0 + bulk_width; f++) {\n+        __ rev32(f, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v8);\n+      }\n+\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ Encrypt the counters\n+      __ aesecb_encrypt(noreg, noreg, keylen, v0, bulk_width);\n+\n+      if (bulk_width == 8) {\n+        __ ld1(v12, v13, v14, v15, __ T16B, __ post(in, 4 * 16));\n+      }\n+\n+      \/\/ XOR the encrypted counters with the inputs\n+      for (int i = 0; i < bulk_width; i++) {\n+        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+      }\n+\n+      \/\/ Write the encrypted data\n+      __ st1(v0, v1, v2, v3, __ T16B, __ post(out, 4 * 16));\n+      if (bulk_width == 8) {\n+        __ st1(v4, v5, v6, v7, __ T16B, __ post(out, 4 * 16));\n+      }\n+\n+      __ subw(len, len, 16 * bulk_width);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    \/\/ Save the counter back where it goes\n+    __ rev32(v16, __ T16B, v16);\n+    __ st1(v16, __ T16B, counter);\n+\n+    __ pop(saved_regs, sp);\n+\n+    __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+    if (bulk_width == 8) {\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+    }\n+\n+    __ andr(rscratch1, len, -16 * bulk_width);\n+    __ sub(len, len, rscratch1);\n+    __ add(offset, offset, rscratch1);\n+    __ mov(used, 16);\n+    __ strw(used, Address(used_ptr));\n+    __ b(large_block_return);\n+\n+    return start;\n+  }\n+\n@@ -5877,0 +6136,49 @@\n+  address generate_ghash_processBlocks_wide() {\n+    address small = generate_ghash_processBlocks();\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks_wide\");\n+    __ align(wordSize * 2);\n+    address p = __ pc();\n+    __ emit_int64(0x87);  \/\/ The low-order bits of the field\n+                          \/\/ polynomial (i.e. p = z^7+z^2+z+1)\n+                          \/\/ repeated in the low and high parts of a\n+                          \/\/ 128-bit vector\n+    __ emit_int64(0x87);\n+\n+    __ align(CodeEntryAlignment);\n+    address start = __ pc();\n+\n+    Register state   = c_rarg0;\n+    Register subkeyH = c_rarg1;\n+    Register data    = c_rarg2;\n+    Register blocks  = c_rarg3;\n+\n+    const int unroll = 4;\n+\n+    __ cmp(blocks, (unsigned char)(unroll * 2));\n+    __ br(__ LT, small);\n+\n+    if (unroll > 1) {\n+    \/\/ Save state before entering routine\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+    }\n+\n+    __ ghash_processBlocks_wide(p, state, subkeyH, data, blocks, unroll);\n+\n+    if (unroll > 1) {\n+      \/\/ And restore state\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+    }\n+\n+    __ cmp(blocks, zr);\n+    __ br(__ GT, small);\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -7114,1 +7422,5 @@\n-      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+      if (UseAESCTRIntrinsics) {\n+        StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks_wide();\n+      } else {\n+        StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+      }\n@@ -7133,0 +7445,4 @@\n+    if (UseAESCTRIntrinsics) {\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":317,"deletions":1,"binary":false,"changes":318,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  code_size2 = 28000           \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 32000           \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -240,0 +240,3 @@\n+    if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    }\n@@ -249,5 +252,4 @@\n-  }\n-\n-  if (UseAESCTRIntrinsics) {\n-    warning(\"AES\/CTR intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    if (UseAESCTRIntrinsics) {\n+      warning(\"AES\/CTR intrinsics are not available on this CPU\");\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"}]}