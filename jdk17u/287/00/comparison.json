{"files":[{"patch":"@@ -310,1 +310,2 @@\n-        test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\" ; then\n+        test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\" || \\\n+        test \"x$OPENJDK_TARGET_CPU\" = \"xppc64le\"; then\n","filename":"make\/autoconf\/jvm-features.m4","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -152,0 +152,1 @@\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/shenandoah\/shenandoah_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -154,0 +154,2 @@\n+  __ block_comment(\"nmethod_entry_barrier (nmethod_entry_barrier) {\");\n+\n@@ -170,0 +172,2 @@\n+\n+  __ block_comment(\"} nmethod_entry_barrier (nmethod_entry_barrier)\");\n@@ -180,0 +184,2 @@\n+  __ block_comment(\"c2i_entry_barrier (c2i_entry_barrier) {\");\n+\n@@ -210,0 +216,2 @@\n+\n+  __ block_comment(\"} c2i_entry_barrier (c2i_entry_barrier)\");\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shared\/barrierSetAssembler_ppc.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,162 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp\"\n+\n+#define __ masm->masm()->\n+\n+void LIR_OpShenandoahCompareAndSwap::emit_code(LIR_Assembler *masm) {\n+  __ block_comment(\"LIR_OpShenandoahCompareAndSwap (shenandaohgc) {\");\n+\n+  Register addr = _addr->as_register_lo();\n+  Register new_val = _new_value->as_register();\n+  Register cmp_val = _cmp_value->as_register();\n+  Register tmp1 = _tmp1->as_register();\n+  Register tmp2 = _tmp2->as_register();\n+  Register result = result_opr()->as_register();\n+\n+  if (ShenandoahIUBarrier) {\n+    ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), new_val, tmp1, tmp2,\n+                                                  MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS);\n+  }\n+\n+  if (UseCompressedOops) {\n+    __ encode_heap_oop(cmp_val, cmp_val);\n+    __ encode_heap_oop(new_val, new_val);\n+  }\n+\n+  \/\/ Due to the memory barriers emitted in ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved,\n+  \/\/ there is no need to specify stronger memory semantics.\n+  ShenandoahBarrierSet::assembler()->cmpxchg_oop(masm->masm(), addr, cmp_val, new_val, tmp1, tmp2,\n+                                                 false, result);\n+\n+  if (UseCompressedOops) {\n+    __ decode_heap_oop(cmp_val);\n+    __ decode_heap_oop(new_val);\n+  }\n+\n+  __ block_comment(\"} LIR_OpShenandoahCompareAndSwap (shenandaohgc)\");\n+}\n+\n+#undef __\n+\n+#ifdef ASSERT\n+#define __ gen->lir(__FILE__, __LINE__)->\n+#else\n+#define __ gen->lir()->\n+#endif\n+\n+LIR_Opr ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess &access, LIRItem &cmp_value, LIRItem &new_value) {\n+  BasicType bt = access.type();\n+\n+  if (access.is_oop()) {\n+    LIRGenerator* gen = access.gen();\n+\n+    if (ShenandoahCASBarrier) {\n+      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        __ membar();\n+      } else {\n+        __ membar_release();\n+      }\n+    }\n+\n+    if (ShenandoahSATBBarrier) {\n+      pre_barrier(gen, access.access_emit_info(), access.decorators(), access.resolved_addr(),\n+                  LIR_OprFact::illegalOpr);\n+    }\n+\n+    if (ShenandoahCASBarrier) {\n+      cmp_value.load_item();\n+      new_value.load_item();\n+\n+      LIR_Opr t1 = gen->new_register(T_OBJECT);\n+      LIR_Opr t2 = gen->new_register(T_OBJECT);\n+      LIR_Opr addr = access.resolved_addr()->as_address_ptr()->base();\n+      LIR_Opr result = gen->new_register(T_INT);\n+\n+      __ append(new LIR_OpShenandoahCompareAndSwap(addr, cmp_value.result(), new_value.result(), t1, t2, result));\n+\n+      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        __ membar_acquire();\n+      } else {\n+        __ membar();\n+      }\n+\n+      return result;\n+    }\n+  }\n+\n+  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+}\n+\n+LIR_Opr ShenandoahBarrierSetC1::atomic_xchg_at_resolved(LIRAccess &access, LIRItem &value) {\n+  LIRGenerator* gen = access.gen();\n+  BasicType type = access.type();\n+\n+  LIR_Opr result = gen->new_register(type);\n+  value.load_item();\n+  LIR_Opr value_opr = value.result();\n+\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ membar();\n+  } else {\n+    __ membar_release();\n+  }\n+\n+  if (access.is_oop()) {\n+    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n+  }\n+\n+  assert(type == T_INT || is_reference_type(type) LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  LIR_Opr tmp_xchg = gen->new_register(T_INT);\n+  __ xchg(access.resolved_addr(), value_opr, result, tmp_xchg);\n+\n+  if (access.is_oop()) {\n+    result = load_reference_barrier_impl(access.gen(), result, LIR_OprFact::addressConst(0),\n+                                         access.decorators());\n+\n+    LIR_Opr tmp_barrier = gen->new_register(type);\n+    __ move(result, tmp_barrier);\n+    result = tmp_barrier;\n+\n+    if (ShenandoahSATBBarrier) {\n+      pre_barrier(access.gen(), access.access_emit_info(), access.decorators(), LIR_OprFact::illegalOpr, result);\n+    }\n+  }\n+\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ membar_acquire();\n+  } else {\n+    __ membar();\n+  }\n+\n+  return result;\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":162,"deletions":0,"binary":false,"changes":162,"status":"added"},{"patch":"@@ -0,0 +1,1012 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/shared\/gcArguments.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"macroAssembler_ppc.hpp\"\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahForwarding.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRuntime.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vm_version_ppc.hpp\"\n+\n+#ifdef COMPILER1\n+\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp\"\n+\n+#endif\n+\n+#define __ masm->\n+\n+void ShenandoahBarrierSetAssembler::satb_write_barrier(MacroAssembler *masm,\n+                                                       Register base, RegisterOrConstant ind_or_offs,\n+                                                       Register tmp1, Register tmp2, Register tmp3,\n+                                                       MacroAssembler::PreservationLevel preservation_level) {\n+  if (ShenandoahSATBBarrier) {\n+    __ block_comment(\"satb_write_barrier (shenandoahgc) {\");\n+    satb_write_barrier_impl(masm, 0, base, ind_or_offs, tmp1, tmp2, tmp3, preservation_level);\n+    __ block_comment(\"} satb_write_barrier (shenandoahgc)\");\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler *masm,\n+                                               Register val,\n+                                               Register tmp1, Register tmp2,\n+                                               MacroAssembler::PreservationLevel preservation_level,\n+                                               DecoratorSet decorators) {\n+  \/\/ IU barriers are also employed to avoid resurrection of weak references,\n+  \/\/ even if Shenandoah does not operate in incremental update mode.\n+  if (ShenandoahIUBarrier || ShenandoahSATBBarrier) {\n+    __ block_comment(\"iu_barrier (shenandoahgc) {\");\n+    satb_write_barrier_impl(masm, decorators, noreg, noreg, val, tmp1, tmp2, preservation_level);\n+    __ block_comment(\"} iu_barrier (shenandoahgc)\");\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler *masm, DecoratorSet decorators,\n+                                                           Register base, RegisterOrConstant ind_or_offs,\n+                                                           Register dst,\n+                                                           Register tmp1, Register tmp2,\n+                                                           MacroAssembler::PreservationLevel preservation_level) {\n+  if (ShenandoahLoadRefBarrier) {\n+    __ block_comment(\"load_reference_barrier (shenandoahgc) {\");\n+    load_reference_barrier_impl(masm, decorators, base, ind_or_offs, dst, tmp1, tmp2, preservation_level);\n+    __ block_comment(\"} load_reference_barrier (shenandoahgc)\");\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count,\n+                                                       Register preserve1, Register preserve2) {\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n+\n+  Register R11_tmp = R11_scratch1;\n+\n+  assert_different_registers(src, dst, count, R11_tmp, noreg);\n+  if (preserve1 != noreg) {\n+    \/\/ Technically not required, but likely to indicate an error.\n+    assert_different_registers(preserve1, preserve2);\n+  }\n+\n+  \/* ==== Check whether barrier is required (optimizations) ==== *\/\n+  \/\/ Fast path: Component type of array is not a reference type.\n+  if (!is_reference_type(type)) {\n+    return;\n+  }\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  \/\/ Fast path: No barrier required if for every barrier type, it is either disabled or would not store\n+  \/\/ any useful information.\n+  if ((!ShenandoahSATBBarrier || dest_uninitialized) && !ShenandoahIUBarrier && !ShenandoahLoadRefBarrier) {\n+    return;\n+  }\n+\n+  Label skip_prologue;\n+\n+  \/\/ Fast path: Array is of length zero.\n+  __ cmpdi(CCR0, count, 0);\n+  __ beq(CCR0, skip_prologue);\n+\n+  \/* ==== Check whether barrier is required (gc state) ==== *\/\n+  __ lbz(R11_tmp, in_bytes(ShenandoahThreadLocalData::gc_state_offset()),\n+         R16_thread);\n+\n+  \/\/ The set of garbage collection states requiring barriers depends on the available barrier types and the\n+  \/\/ type of the reference in question.\n+  \/\/ For instance, satb barriers may be skipped if it is certain that the overridden values are not relevant\n+  \/\/ for the garbage collector.\n+  const int required_states = ShenandoahSATBBarrier && dest_uninitialized\n+                              ? ShenandoahHeap::HAS_FORWARDED\n+                              : ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+\n+  __ andi_(R11_tmp, R11_tmp, required_states);\n+  __ beq(CCR0, skip_prologue);\n+\n+  \/* ==== Invoke runtime ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  int highest_preserve_register_index = 0;\n+  {\n+    if (preserve1 != noreg && preserve1->is_volatile()) {\n+      __ std(preserve1, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    }\n+    if (preserve2 != noreg && preserve2 != preserve1 && preserve2->is_volatile()) {\n+      __ std(preserve2, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    }\n+\n+    __ std(src, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    __ std(dst, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    __ std(count, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+\n+    __ save_LR_CR(R11_tmp);\n+    __ push_frame_reg_args(-BytesPerWord * highest_preserve_register_index,\n+                           R11_tmp);\n+  }\n+\n+  \/\/ Invoke runtime.\n+  address jrt_address = NULL;\n+  if (UseCompressedOops) {\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::arraycopy_barrier_narrow_oop_entry);\n+  } else {\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::arraycopy_barrier_oop_entry);\n+  }\n+  assert(jrt_address != nullptr, \"jrt routine cannot be found\");\n+\n+  __ call_VM_leaf(jrt_address, src, dst, count);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  {\n+    __ pop_frame();\n+    __ restore_LR_CR(R11_tmp);\n+\n+    __ ld(count, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    __ ld(dst, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    __ ld(src, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+\n+    if (preserve2 != noreg && preserve2 != preserve1 && preserve2->is_volatile()) {\n+      __ ld(preserve2, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    }\n+    if (preserve1 != noreg && preserve1->is_volatile()) {\n+      __ ld(preserve1, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    }\n+  }\n+\n+  __ bind(skip_prologue);\n+  __ block_comment(\"} arraycopy_prologue (shenandoahgc)\");\n+}\n+\n+\/\/ The to-be-enqueued value can either be determined\n+\/\/ - dynamically by passing the reference's address information (load mode) or\n+\/\/ - statically by passing a register the value is stored in (preloaded mode)\n+\/\/   - for performance optimizations in cases where the previous value is known (currently not implemented) and\n+\/\/   - for incremental-update barriers.\n+\/\/\n+\/\/ decorators:  The previous value's decorator set.\n+\/\/              In \"load mode\", the value must equal '0'.\n+\/\/ base:        Base register of the reference's address (load mode).\n+\/\/              In \"preloaded mode\", the register must equal 'noreg'.\n+\/\/ ind_or_offs: Index or offset of the reference's address (load mode).\n+\/\/              If 'base' equals 'noreg' (preloaded mode), the passed value is ignored.\n+\/\/ pre_val:     Register holding the to-be-stored value (preloaded mode).\n+\/\/              In \"load mode\", this register acts as a temporary register and must\n+\/\/              thus not be 'noreg'.  In \"preloaded mode\", its content will be sustained.\n+\/\/ tmp1\/tmp2:   Temporary registers, one of which must be non-volatile in \"preloaded mode\".\n+void ShenandoahBarrierSetAssembler::satb_write_barrier_impl(MacroAssembler *masm, DecoratorSet decorators,\n+                                                            Register base, RegisterOrConstant ind_or_offs,\n+                                                            Register pre_val,\n+                                                            Register tmp1, Register tmp2,\n+                                                            MacroAssembler::PreservationLevel preservation_level) {\n+  assert_different_registers(tmp1, tmp2, pre_val, noreg);\n+\n+  Label skip_barrier;\n+\n+  \/* ==== Determine necessary runtime invocation preservation measures ==== *\/\n+  const bool needs_frame           = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+\n+  \/\/ Check whether marking is active.\n+  __ lbz(tmp1, in_bytes(ShenandoahThreadLocalData::gc_state_offset()), R16_thread);\n+\n+  __ andi_(tmp1, tmp1, ShenandoahHeap::MARKING);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Determine the reference's previous value ==== *\/\n+  bool preloaded_mode = base == noreg;\n+  Register pre_val_save = noreg;\n+\n+  if (preloaded_mode) {\n+    \/\/ Previous value has been passed to the method, so it must not be determined manually.\n+    \/\/ In case 'pre_val' is a volatile register, it must be saved across the C-call\n+    \/\/ as callers may depend on its value.\n+    \/\/ Unless the general purposes registers are saved anyway, one of the temporary registers\n+    \/\/ (i.e., 'tmp1' and 'tmp2') is used to the preserve 'pre_val'.\n+    if (!preserve_gp_registers && pre_val->is_volatile()) {\n+      pre_val_save = !tmp1->is_volatile() ? tmp1 : tmp2;\n+      assert(!pre_val_save->is_volatile(), \"at least one of the temporary registers must be non-volatile\");\n+    }\n+\n+    if ((decorators & IS_NOT_NULL) != 0) {\n+#ifdef ASSERT\n+      __ cmpdi(CCR0, pre_val, 0);\n+      __ asm_assert_ne(\"null oop is not allowed\");\n+#endif \/\/ ASSERT\n+    } else {\n+      __ cmpdi(CCR0, pre_val, 0);\n+      __ beq(CCR0, skip_barrier);\n+    }\n+  } else {\n+    \/\/ Load from the reference address to determine the reference's current value (before the store is being performed).\n+    \/\/ Contrary to the given value in \"preloaded mode\", it is not necessary to preserve it.\n+    assert(decorators == 0, \"decorator set must be empty\");\n+    assert(base != noreg, \"base must be a register\");\n+    assert(!ind_or_offs.is_register() || ind_or_offs.as_register() != noreg, \"ind_or_offs must be a register\");\n+    if (UseCompressedOops) {\n+      __ lwz(pre_val, ind_or_offs, base);\n+    } else {\n+      __ ld(pre_val, ind_or_offs, base);\n+    }\n+\n+    __ cmpdi(CCR0, pre_val, 0);\n+    __ beq(CCR0, skip_barrier);\n+\n+    if (UseCompressedOops) {\n+      __ decode_heap_oop_not_null(pre_val);\n+    }\n+  }\n+\n+  \/* ==== Try to enqueue the to-be-stored value directly into thread's local SATB mark queue ==== *\/\n+  {\n+    Label runtime;\n+    Register Rbuffer = tmp1, Rindex = tmp2;\n+\n+    \/\/ Check whether the queue has enough capacity to store another oop.\n+    \/\/ If not, jump to the runtime to commit the buffer and to allocate a new one.\n+    \/\/ (The buffer's index corresponds to the amount of remaining free space.)\n+    __ ld(Rindex, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+    __ cmpdi(CCR0, Rindex, 0);\n+    __ beq(CCR0, runtime); \/\/ If index == 0 (buffer is full), goto runtime.\n+\n+    \/\/ Capacity suffices.  Decrement the queue's size by the size of one oop.\n+    \/\/ (The buffer is filled contrary to the heap's growing direction, i.e., it is filled downwards.)\n+    __ addi(Rindex, Rindex, -wordSize);\n+    __ std(Rindex, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+\n+    \/\/ Enqueue the previous value and skip the invocation of the runtime.\n+    __ ld(Rbuffer, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset()), R16_thread);\n+    __ stdx(pre_val, Rbuffer, Rindex);\n+    __ b(skip_barrier);\n+\n+    __ bind(runtime);\n+  }\n+\n+  \/* ==== Invoke runtime to commit SATB mark queue to gc and allocate a new buffer ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  int nbytes_save = 0;\n+\n+  if (needs_frame) {\n+    if (preserve_gp_registers) {\n+      nbytes_save = (preserve_fp_registers\n+                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n+                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n+      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+    }\n+\n+    __ save_LR_CR(tmp1);\n+    __ push_frame_reg_args(nbytes_save, tmp2);\n+  }\n+\n+  if (!preserve_gp_registers && preloaded_mode && pre_val->is_volatile()) {\n+    assert(pre_val_save != noreg, \"nv_save must not be noreg\");\n+\n+    \/\/ 'pre_val' register must be saved manually unless general-purpose are preserved in general.\n+    __ mr(pre_val_save, pre_val);\n+  }\n+\n+  \/\/ Invoke runtime.\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), pre_val, R16_thread);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  if (!preserve_gp_registers && preloaded_mode && pre_val->is_volatile()) {\n+    __ mr(pre_val, pre_val_save);\n+  }\n+\n+  if (needs_frame) {\n+    __ pop_frame();\n+    __ restore_LR_CR(tmp1);\n+\n+    if (preserve_gp_registers) {\n+      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+    }\n+  }\n+\n+  __ bind(skip_barrier);\n+}\n+\n+void ShenandoahBarrierSetAssembler::resolve_forward_pointer_not_null(MacroAssembler *masm, Register dst, Register tmp) {\n+  __ block_comment(\"resolve_forward_pointer_not_null (shenandoahgc) {\");\n+\n+  Register tmp1 = tmp,\n+           R0_tmp2 = R0;\n+  assert_different_registers(dst, tmp1, R0_tmp2, noreg);\n+\n+  \/\/ If the object has been evacuated, the mark word layout is as follows:\n+  \/\/ | forwarding pointer (62-bit) | '11' (2-bit) |\n+\n+  \/\/ The invariant that stack\/thread pointers have the lowest two bits cleared permits retrieving\n+  \/\/ the forwarding pointer solely by inversing the lowest two bits.\n+  \/\/ This invariant follows inevitably from hotspot's minimal alignment.\n+  assert(markWord::marked_value <= (unsigned long) MinObjAlignmentInBytes,\n+         \"marked value must not be higher than hotspot's minimal alignment\");\n+\n+  Label done;\n+\n+  \/\/ Load the object's mark word.\n+  __ ld(tmp1, oopDesc::mark_offset_in_bytes(), dst);\n+\n+  \/\/ Load the bit mask for the lock bits.\n+  __ li(R0_tmp2, markWord::lock_mask_in_place);\n+\n+  \/\/ Check whether all bits matching the bit mask are set.\n+  \/\/ If that is the case, the object has been evacuated and the most significant bits form the forward pointer.\n+  __ andc_(R0_tmp2, R0_tmp2, tmp1);\n+\n+  assert(markWord::lock_mask_in_place == markWord::marked_value,\n+         \"marked value must equal the value obtained when all lock bits are being set\");\n+  if (VM_Version::has_isel()) {\n+    __ xori(tmp1, tmp1, markWord::lock_mask_in_place);\n+    __ isel(dst, CCR0, Assembler::equal, false, tmp1);\n+  } else {\n+    __ bne(CCR0, done);\n+    __ xori(dst, tmp1, markWord::lock_mask_in_place);\n+  }\n+\n+  __ bind(done);\n+  __ block_comment(\"} resolve_forward_pointer_not_null (shenandoahgc)\");\n+}\n+\n+\/\/ base:        Base register of the reference's address.\n+\/\/ ind_or_offs: Index or offset of the reference's address (load mode).\n+\/\/ dst:         Reference's address.  In case the object has been evacuated, this is the to-space version\n+\/\/              of that object.\n+void ShenandoahBarrierSetAssembler::load_reference_barrier_impl(\n+    MacroAssembler *masm, DecoratorSet decorators,\n+    Register base, RegisterOrConstant ind_or_offs,\n+    Register dst,\n+    Register tmp1, Register tmp2,\n+    MacroAssembler::PreservationLevel preservation_level) {\n+  if (ind_or_offs.is_register()) {\n+    assert_different_registers(tmp1, tmp2, base, ind_or_offs.as_register(), dst, noreg);\n+  } else {\n+    assert_different_registers(tmp1, tmp2, base, dst, noreg);\n+  }\n+\n+  Label skip_barrier;\n+\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+  bool is_narrow  = UseCompressedOops && !is_native;\n+\n+  \/* ==== Check whether heap is stable ==== *\/\n+  __ lbz(tmp2, in_bytes(ShenandoahThreadLocalData::gc_state_offset()), R16_thread);\n+\n+  if (is_strong) {\n+    \/\/ For strong references, the heap is considered stable if \"has forwarded\" is not active.\n+    __ andi_(tmp1, tmp2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::EVACUATION);\n+    __ beq(CCR0, skip_barrier);\n+#ifdef ASSERT\n+    \/\/ \"evacuation\" -> (implies) \"has forwarded\".  If we reach this code, \"has forwarded\" must thus be set.\n+    __ andi_(tmp1, tmp1, ShenandoahHeap::HAS_FORWARDED);\n+    __ asm_assert_ne(\"'has forwarded' is missing\");\n+#endif \/\/ ASSERT\n+  } else {\n+    \/\/ For all non-strong references, the heap is considered stable if not any of \"has forwarded\",\n+    \/\/ \"root set processing\", and \"weak reference processing\" is active.\n+    \/\/ The additional phase conditions are in place to avoid the resurrection of weak references (see JDK-8266440).\n+    Label skip_fastpath;\n+    __ andi_(tmp1, tmp2, ShenandoahHeap::WEAK_ROOTS);\n+    __ bne(CCR0, skip_fastpath);\n+\n+    __ andi_(tmp1, tmp2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::EVACUATION);\n+    __ beq(CCR0, skip_barrier);\n+#ifdef ASSERT\n+    \/\/ \"evacuation\" -> (implies) \"has forwarded\".  If we reach this code, \"has forwarded\" must thus be set.\n+    __ andi_(tmp1, tmp1, ShenandoahHeap::HAS_FORWARDED);\n+    __ asm_assert_ne(\"'has forwarded' is missing\");\n+#endif \/\/ ASSERT\n+\n+    __ bind(skip_fastpath);\n+  }\n+\n+  \/* ==== Check whether region is in collection set ==== *\/\n+  if (is_strong) {\n+    \/\/ Shenandoah stores metadata on regions in a continuous area of memory in which a single byte corresponds to\n+    \/\/ an entire region of the shenandoah heap.  At present, only the least significant bit is of significance\n+    \/\/ and indicates whether the region is part of the collection set.\n+    \/\/\n+    \/\/ All regions are of the same size and are always aligned by a power of two.\n+    \/\/ Any address can thus be shifted by a fixed number of bits to retrieve the address prefix shared by\n+    \/\/ all objects within that region (region identification bits).\n+    \/\/\n+    \/\/  | unused bits | region identification bits | object identification bits |\n+    \/\/  (Region size depends on a couple of criteria, such as page size, user-provided arguments and the max heap size.\n+    \/\/   The number of object identification bits can thus not be determined at compile time.)\n+    \/\/\n+    \/\/ -------------------------------------------------------  <--- cs (collection set) base address\n+    \/\/ | lost space due to heap space base address                   -> 'ShenandoahHeap::in_cset_fast_test_addr()'\n+    \/\/ | (region identification bits contain heap base offset)\n+    \/\/ |------------------------------------------------------  <--- cs base address + (heap_base >> region size shift)\n+    \/\/ | collection set in the proper                                -> shift: 'region_size_bytes_shift_jint()'\n+    \/\/ |\n+    \/\/ |------------------------------------------------------  <--- cs base address + (heap_base >> region size shift)\n+    \/\/                                                                               + number of regions\n+    __ load_const_optimized(tmp2, ShenandoahHeap::in_cset_fast_test_addr(), tmp1);\n+    __ srdi(tmp1, dst, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+    __ lbzx(tmp2, tmp1, tmp2);\n+    __ andi_(tmp2, tmp2, 1);\n+    __ beq(CCR0, skip_barrier);\n+  }\n+\n+  \/* ==== Invoke runtime ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  int nbytes_save = 0;\n+\n+  const bool needs_frame           = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+\n+  if (needs_frame) {\n+    if (preserve_gp_registers) {\n+      nbytes_save = (preserve_fp_registers\n+                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n+                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n+      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+    }\n+\n+    __ save_LR_CR(tmp1);\n+    __ push_frame_reg_args(nbytes_save, tmp1);\n+  }\n+\n+  \/\/ Calculate the reference's absolute address.\n+  __ add(R4_ARG2, ind_or_offs, base);\n+\n+  \/\/ Invoke runtime.\n+  address jrt_address = nullptr;\n+\n+  if (is_strong) {\n+    if (is_narrow) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow);\n+    } else {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+    }\n+  } else if (is_weak) {\n+    if (is_narrow) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow);\n+    } else {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n+    }\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    assert(!is_narrow, \"phantom access cannot be narrow\");\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom);\n+  }\n+  assert(jrt_address != nullptr, \"jrt routine cannot be found\");\n+\n+  __ call_VM_leaf(jrt_address, dst \/* reference *\/, R4_ARG2 \/* reference address *\/);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  if (preserve_gp_registers) {\n+    __ mr(R0, R3_RET);\n+  } else {\n+    __ mr_if_needed(dst, R3_RET);\n+  }\n+\n+  if (needs_frame) {\n+    __ pop_frame();\n+    __ restore_LR_CR(tmp1);\n+\n+    if (preserve_gp_registers) {\n+      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+      __ mr(dst, R0);\n+    }\n+  }\n+\n+  __ bind(skip_barrier);\n+}\n+\n+\/\/ base:           Base register of the reference's address.\n+\/\/ ind_or_offs:    Index or offset of the reference's address.\n+\/\/ L_handle_null:  An optional label that will be jumped to if the reference is null.\n+void ShenandoahBarrierSetAssembler::load_at(\n+    MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n+    Register base, RegisterOrConstant ind_or_offs, Register dst,\n+    Register tmp1, Register tmp2,\n+    MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null) {\n+  \/\/ Register must not clash, except 'base' and 'dst'.\n+  if (ind_or_offs.is_register()) {\n+    if (base != noreg) {\n+      assert_different_registers(tmp1, tmp2, base, ind_or_offs.register_or_noreg(), R0, noreg);\n+    }\n+    assert_different_registers(tmp1, tmp2, dst, ind_or_offs.register_or_noreg(), R0, noreg);\n+  } else {\n+    if (base == noreg) {\n+      assert_different_registers(tmp1, tmp2, base, R0, noreg);\n+    }\n+    assert_different_registers(tmp1, tmp2, dst, R0, noreg);\n+  }\n+\n+  \/* ==== Apply load barrier, if required ==== *\/\n+  if (ShenandoahBarrierSet::need_load_reference_barrier(decorators, type)) {\n+    assert(is_reference_type(type), \"need_load_reference_barrier must check whether type is a reference type\");\n+\n+    \/\/ If 'dst' clashes with either 'base' or 'ind_or_offs', use an intermediate result register\n+    \/\/ to keep the values of those alive until the load reference barrier is applied.\n+    Register intermediate_dst = (dst == base || (ind_or_offs.is_register() && dst == ind_or_offs.as_register()))\n+                                ? tmp2\n+                                : dst;\n+\n+    BarrierSetAssembler::load_at(masm, decorators, type,\n+                                 base, ind_or_offs,\n+                                 intermediate_dst,\n+                                 tmp1, noreg,\n+                                 preservation_level, L_handle_null);\n+\n+    load_reference_barrier(masm, decorators,\n+                           base, ind_or_offs,\n+                           intermediate_dst,\n+                           tmp1, R0,\n+                           preservation_level);\n+\n+    __ mr_if_needed(dst, intermediate_dst);\n+  } else {\n+    BarrierSetAssembler::load_at(masm, decorators, type,\n+                                 base, ind_or_offs,\n+                                 dst,\n+                                 tmp1, tmp2,\n+                                 preservation_level, L_handle_null);\n+  }\n+\n+  \/* ==== Apply keep-alive barrier, if required (e.g., to inhibit weak reference resurrection) ==== *\/\n+  if (ShenandoahBarrierSet::need_keep_alive_barrier(decorators, type)) {\n+    iu_barrier(masm, dst, tmp1, tmp2, preservation_level);\n+  }\n+}\n+\n+\/\/ base:        Base register of the reference's address.\n+\/\/ ind_or_offs: Index or offset of the reference's address.\n+\/\/ val:         To-be-stored value\/reference's new value.\n+void ShenandoahBarrierSetAssembler::store_at(MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n+                                             Register base, RegisterOrConstant ind_or_offs, Register val,\n+                                             Register tmp1, Register tmp2, Register tmp3,\n+                                             MacroAssembler::PreservationLevel preservation_level) {\n+  if (is_reference_type(type)) {\n+    if (ShenandoahSATBBarrier) {\n+      satb_write_barrier(masm, base, ind_or_offs, tmp1, tmp2, tmp3, preservation_level);\n+    }\n+\n+    if (ShenandoahIUBarrier && val != noreg) {\n+      iu_barrier(masm, val, tmp1, tmp2, preservation_level, decorators);\n+    }\n+  }\n+\n+  BarrierSetAssembler::store_at(masm, decorators, type,\n+                                base, ind_or_offs,\n+                                val,\n+                                tmp1, tmp2, tmp3,\n+                                preservation_level);\n+}\n+\n+void ShenandoahBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler *masm,\n+                                                                  Register dst, Register jni_env, Register obj,\n+                                                                  Register tmp, Label &slowpath) {\n+  __ block_comment(\"try_resolve_jobject_in_native (shenandoahgc) {\");\n+\n+  assert_different_registers(jni_env, obj, tmp);\n+\n+  Label done;\n+\n+  \/\/ Fast path: Reference is null (JNI tags are zero for null pointers).\n+  __ cmpdi(CCR0, obj, 0);\n+  __ beq(CCR0, done);\n+\n+  \/\/ Resolve jobject using standard implementation.\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, dst, jni_env, obj, tmp, slowpath);\n+\n+  \/\/ Check whether heap is stable.\n+  __ lbz(tmp,\n+         in_bytes(ShenandoahThreadLocalData::gc_state_offset() - JavaThread::jni_environment_offset()),\n+         jni_env);\n+\n+  __ andi_(tmp, tmp, ShenandoahHeap::EVACUATION | ShenandoahHeap::HAS_FORWARDED);\n+  __ bne(CCR0, slowpath);\n+\n+  __ bind(done);\n+  __ block_comment(\"} try_resolve_jobject_in_native (shenandoahgc)\");\n+}\n+\n+\/\/ Special shenandoah CAS implementation that handles false negatives due\n+\/\/ to concurrent evacuation.  That is, the CAS operation is intended to succeed in\n+\/\/ the following scenarios (success criteria):\n+\/\/  s1) The reference pointer ('base_addr') equals the expected ('expected') pointer.\n+\/\/  s2) The reference pointer refers to the from-space version of an already-evacuated\n+\/\/      object, whereas the expected pointer refers to the to-space version of the same object.\n+\/\/ Situations in which the reference pointer refers to the to-space version of an object\n+\/\/ and the expected pointer refers to the from-space version of the same object can not occur due to\n+\/\/ shenandoah's strong to-space invariant.  This also implies that the reference stored in 'new_val'\n+\/\/ can not refer to the from-space version of an already-evacuated object.\n+\/\/\n+\/\/ To guarantee correct behavior in concurrent environments, two races must be addressed:\n+\/\/  r1) A concurrent thread may heal the reference pointer (i.e., it is no longer referring to the\n+\/\/      from-space version but to the to-space version of the object in question).\n+\/\/      In this case, the CAS operation should succeed.\n+\/\/  r2) A concurrent thread may mutate the reference (i.e., the reference pointer refers to an entirely different object).\n+\/\/      In this case, the CAS operation should fail.\n+\/\/\n+\/\/ By default, the value held in the 'result' register is zero to indicate failure of CAS,\n+\/\/ non-zero to indicate success.  If 'is_cae' is set, the result is the most recently fetched\n+\/\/ value from 'base_addr' rather than a boolean success indicator.\n+void ShenandoahBarrierSetAssembler::cmpxchg_oop(MacroAssembler *masm, Register base_addr,\n+                                                Register expected, Register new_val, Register tmp1, Register tmp2,\n+                                                bool is_cae, Register result) {\n+  __ block_comment(\"cmpxchg_oop (shenandoahgc) {\");\n+\n+  assert_different_registers(base_addr, new_val, tmp1, tmp2, result, R0);\n+  assert_different_registers(base_addr, expected, tmp1, tmp2, result, R0);\n+\n+  \/\/ Potential clash of 'success_flag' and 'tmp' is being accounted for.\n+  Register success_flag  = is_cae ? noreg  : result,\n+           current_value = is_cae ? result : tmp1,\n+           tmp           = is_cae ? tmp1   : result,\n+           initial_value = tmp2;\n+\n+  Label done, step_four;\n+\n+  __ bind(step_four);\n+\n+  \/* ==== Step 1 (\"Standard\" CAS) ==== *\/\n+  \/\/ Fast path: The values stored in 'expected' and 'base_addr' are equal.\n+  \/\/ Given that 'expected' must refer to the to-space object of an evacuated object (strong to-space invariant),\n+  \/\/ no special processing is required.\n+  if (UseCompressedOops) {\n+    __ cmpxchgw(CCR0, current_value, expected, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag, true);\n+  } else {\n+    __ cmpxchgd(CCR0, current_value, expected, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag, NULL, true);\n+  }\n+\n+  \/\/ Skip the rest of the barrier if the CAS operation succeeds immediately.\n+  \/\/ If it does not, the value stored at the address is either the from-space pointer of the\n+  \/\/ referenced object (success criteria s2)) or simply another object.\n+  __ beq(CCR0, done);\n+\n+  \/* ==== Step 2 (Null check) ==== *\/\n+  \/\/ The success criteria s2) cannot be matched with a null pointer\n+  \/\/ (null pointers cannot be subject to concurrent evacuation).  The failure of the CAS operation is thus legitimate.\n+  __ cmpdi(CCR0, current_value, 0);\n+  __ beq(CCR0, done);\n+\n+  \/* ==== Step 3 (reference pointer refers to from-space version; success criteria s2)) ==== *\/\n+  \/\/ To check whether the reference pointer refers to the from-space version, the forward\n+  \/\/ pointer of the object referred to by the reference is resolved and compared against the expected pointer.\n+  \/\/ If this check succeed, another CAS operation is issued with the from-space pointer being the expected pointer.\n+  \/\/\n+  \/\/ Save the potential from-space pointer.\n+  __ mr(initial_value, current_value);\n+\n+  \/\/ Resolve forward pointer.\n+  if (UseCompressedOops) { __ decode_heap_oop_not_null(current_value); }\n+  resolve_forward_pointer_not_null(masm, current_value, tmp);\n+  if (UseCompressedOops) { __ encode_heap_oop_not_null(current_value); }\n+\n+  if (!is_cae) {\n+    \/\/ 'success_flag' was overwritten by call to 'resovle_forward_pointer_not_null'.\n+    \/\/ Load zero into register for the potential failure case.\n+    __ li(success_flag, 0);\n+  }\n+  __ cmpd(CCR0, current_value, expected);\n+  __ bne(CCR0, done);\n+\n+  \/\/ Discard fetched value as it might be a reference to the from-space version of an object.\n+  if (UseCompressedOops) {\n+    __ cmpxchgw(CCR0, R0, initial_value, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag);\n+  } else {\n+    __ cmpxchgd(CCR0, R0, initial_value, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag);\n+  }\n+\n+  \/* ==== Step 4 (Retry CAS with to-space pointer (success criteria s2) under race r1)) ==== *\/\n+  \/\/ The reference pointer could have been healed whilst the previous CAS operation was being performed.\n+  \/\/ Another CAS operation must thus be issued with the to-space pointer being the expected pointer.\n+  \/\/ If that CAS operation fails as well, race r2) must have occurred, indicating that\n+  \/\/ the operation failure is legitimate.\n+  \/\/\n+  \/\/ To keep the code's size small and thus improving cache (icache) performance, this highly\n+  \/\/ unlikely case should be handled by the smallest possible code.  Instead of emitting a third,\n+  \/\/ explicit CAS operation, the code jumps back and reuses the first CAS operation (step 1)\n+  \/\/ (passed arguments are identical).\n+  \/\/\n+  \/\/ A failure of the CAS operation in step 1 would imply that the overall CAS operation is supposed\n+  \/\/ to fail.  Jumping back to step 1 requires, however, that step 2 and step 3 are re-executed as well.\n+  \/\/ It is thus important to ensure that a re-execution of those steps does not put program correctness\n+  \/\/ at risk:\n+  \/\/ - Step 2: Either terminates in failure (desired result) or falls through to step 3.\n+  \/\/ - Step 3: Terminates if the comparison between the forwarded, fetched pointer and the expected value\n+  \/\/           fails.  Unless the reference has been updated in the meanwhile once again, this is\n+  \/\/           guaranteed to be the case.\n+  \/\/           In case of a concurrent update, the CAS would be retried again. This is legitimate\n+  \/\/           in terms of program correctness (even though it is not desired).\n+  __ bne(CCR0, step_four);\n+\n+  __ bind(done);\n+  __ block_comment(\"} cmpxchg_oop (shenandoahgc)\");\n+}\n+\n+#undef __\n+\n+#ifdef COMPILER1\n+\n+#define __ ce->masm()->\n+\n+void ShenandoahBarrierSetAssembler::gen_pre_barrier_stub(LIR_Assembler *ce, ShenandoahPreBarrierStub *stub) {\n+  __ block_comment(\"gen_pre_barrier_stub (shenandoahgc) {\");\n+\n+  ShenandoahBarrierSetC1 *bs = (ShenandoahBarrierSetC1*) BarrierSet::barrier_set()->barrier_set_c1();\n+  __ bind(*stub->entry());\n+\n+  \/\/ GC status has already been verified by 'ShenandoahBarrierSetC1::pre_barrier'.\n+  \/\/ This stub is the slowpath of that function.\n+\n+  assert(stub->pre_val()->is_register(), \"pre_val must be a register\");\n+  Register pre_val = stub->pre_val()->as_register();\n+\n+  \/\/ If 'do_load()' returns false, the to-be-stored value is already available in 'stub->pre_val()'\n+  \/\/ (\"preloaded mode\" of the store barrier).\n+  if (stub->do_load()) {\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false, false \/*unaligned*\/);\n+  }\n+\n+  \/\/ Fast path: Reference is null.\n+  __ cmpdi(CCR0, pre_val, 0);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs1_bhintNoHint, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+\n+  \/\/ Argument passing via the stack.\n+  __ std(pre_val, -8, R1_SP);\n+\n+  __ load_const_optimized(R0, bs->pre_barrier_c1_runtime_code_blob()->code_begin());\n+  __ call_stub(R0);\n+\n+  __ b(*stub->continuation());\n+  __ block_comment(\"} gen_pre_barrier_stub (shenandoahgc)\");\n+}\n+\n+void ShenandoahBarrierSetAssembler::gen_load_reference_barrier_stub(LIR_Assembler *ce,\n+                                                                    ShenandoahLoadReferenceBarrierStub *stub) {\n+  __ block_comment(\"gen_load_reference_barrier_stub (shenandoahgc) {\");\n+\n+  ShenandoahBarrierSetC1 *bs = (ShenandoahBarrierSetC1*) BarrierSet::barrier_set()->barrier_set_c1();\n+  __ bind(*stub->entry());\n+\n+  Register obj  = stub->obj()->as_register();\n+  Register res  = stub->result()->as_register();\n+  Register addr = stub->addr()->as_pointer_register();\n+  Register tmp1 = stub->tmp1()->as_register();\n+  Register tmp2 = stub->tmp2()->as_register();\n+  assert_different_registers(addr, res, tmp1, tmp2);\n+\n+#ifdef ASSERT\n+  \/\/ Ensure that 'res' is 'R3_ARG1' and contains the same value as 'obj' to reduce the number of required\n+  \/\/ copy instructions.\n+  assert(R3_RET == res, \"res must be r3\");\n+  __ cmpd(CCR0, res, obj);\n+  __ asm_assert_eq(\"result register must contain the reference stored in obj\");\n+#endif\n+\n+  DecoratorSet decorators = stub->decorators();\n+\n+  \/* ==== Check whether region is in collection set ==== *\/\n+  \/\/ GC status (unstable) has already been verified by 'ShenandoahBarrierSetC1::load_reference_barrier_impl'.\n+  \/\/ This stub is the slowpath of that function.\n+\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+\n+  if (is_strong) {\n+    \/\/ Check whether object is in collection set.\n+    __ load_const_optimized(tmp2, ShenandoahHeap::in_cset_fast_test_addr(), tmp1);\n+    __ srdi(tmp1, obj, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+    __ lbzx(tmp2, tmp1, tmp2);\n+\n+    __ andi_(tmp2, tmp2, 1);\n+    __ bc_far_optimized(Assembler::bcondCRbiIs1_bhintNoHint, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+  }\n+\n+  address blob_addr = nullptr;\n+\n+  if (is_strong) {\n+    if (is_native) {\n+      blob_addr = bs->load_reference_barrier_strong_native_rt_code_blob()->code_begin();\n+    } else {\n+      blob_addr = bs->load_reference_barrier_strong_rt_code_blob()->code_begin();\n+    }\n+  } else if (is_weak) {\n+    blob_addr = bs->load_reference_barrier_weak_rt_code_blob()->code_begin();\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    blob_addr = bs->load_reference_barrier_phantom_rt_code_blob()->code_begin();\n+  }\n+\n+  assert(blob_addr != nullptr, \"code blob cannot be found\");\n+\n+  \/\/ Argument passing via the stack.  'obj' is passed implicitly (as asserted above).\n+  __ std(addr, -8, R1_SP);\n+\n+  __ load_const_optimized(tmp1, blob_addr, tmp2);\n+  __ call_stub(tmp1);\n+\n+  \/\/ 'res' is 'R3_RET'.  The result is thus already in the correct register.\n+\n+  __ b(*stub->continuation());\n+  __ block_comment(\"} gen_load_reference_barrier_stub (shenandoahgc)\");\n+}\n+\n+#undef __\n+\n+#define __ sasm->\n+\n+void ShenandoahBarrierSetAssembler::generate_c1_pre_barrier_runtime_stub(StubAssembler *sasm) {\n+  __ block_comment(\"generate_c1_pre_barrier_runtime_stub (shenandoahgc) {\");\n+\n+  Label runtime, skip_barrier;\n+  BarrierSet *bs = BarrierSet::barrier_set();\n+\n+  \/\/ Argument passing via the stack.\n+  const int caller_stack_slots = 3;\n+\n+  Register R0_pre_val = R0;\n+  __ ld(R0, -8, R1_SP);\n+  Register R11_tmp1 = R11_scratch1;\n+  __ std(R11_tmp1, -16, R1_SP);\n+  Register R12_tmp2 = R12_scratch2;\n+  __ std(R12_tmp2, -24, R1_SP);\n+\n+  \/* ==== Check whether marking is active ==== *\/\n+  \/\/ Even though gc status was checked in 'ShenandoahBarrierSetAssembler::gen_pre_barrier_stub',\n+  \/\/ another check is required as a safepoint might have been reached in the meantime (JDK-8140588).\n+  __ lbz(R12_tmp2, in_bytes(ShenandoahThreadLocalData::gc_state_offset()), R16_thread);\n+\n+  __ andi_(R12_tmp2, R12_tmp2, ShenandoahHeap::MARKING);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Add previous value directly to thread-local SATB mark queue ==== *\/\n+  \/\/ Check queue's capacity.  Jump to runtime if no free slot is available.\n+  __ ld(R12_tmp2, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+  __ cmpdi(CCR0, R12_tmp2, 0);\n+  __ beq(CCR0, runtime);\n+\n+  \/\/ Capacity suffices.  Decrement the queue's size by one slot (size of one oop).\n+  __ addi(R12_tmp2, R12_tmp2, -wordSize);\n+  __ std(R12_tmp2, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+\n+  \/\/ Enqueue the previous value and skip the runtime invocation.\n+  __ ld(R11_tmp1, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset()), R16_thread);\n+  __ stdx(R0_pre_val, R11_tmp1, R12_tmp2);\n+  __ b(skip_barrier);\n+\n+  __ bind(runtime);\n+\n+  \/* ==== Invoke runtime to commit SATB mark queue to gc and allocate a new buffer ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  const int nbytes_save = (MacroAssembler::num_volatile_regs + caller_stack_slots) * BytesPerWord;\n+  __ save_volatile_gprs(R1_SP, -nbytes_save);\n+  __ save_LR_CR(R11_tmp1);\n+  __ push_frame_reg_args(nbytes_save, R11_tmp1);\n+\n+  \/\/ Invoke runtime.\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), R0_pre_val, R16_thread);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  __ pop_frame();\n+  __ restore_LR_CR(R11_tmp1);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save);\n+\n+  __ bind(skip_barrier);\n+\n+  \/\/ Restore spilled registers.\n+  __ ld(R11_tmp1, -16, R1_SP);\n+  __ ld(R12_tmp2, -24, R1_SP);\n+\n+  __ blr();\n+  __ block_comment(\"} generate_c1_pre_barrier_runtime_stub (shenandoahgc)\");\n+}\n+\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler *sasm,\n+                                                                                    DecoratorSet decorators) {\n+  __ block_comment(\"generate_c1_load_reference_barrier_runtime_stub (shenandoahgc) {\");\n+\n+  \/\/ Argument passing via the stack.\n+  const int caller_stack_slots = 1;\n+\n+  \/\/ Save to-be-preserved registers.\n+  const int nbytes_save = (MacroAssembler::num_volatile_regs - 1 \/\/ 'R3_ARG1' is skipped\n+                           + caller_stack_slots) * BytesPerWord;\n+  __ save_volatile_gprs(R1_SP, -nbytes_save, true, false);\n+\n+  \/\/ Load arguments from stack.\n+  \/\/ No load required, as assured by assertions in 'ShenandoahBarrierSetAssembler::gen_load_reference_barrier_stub'.\n+  Register R3_obj = R3_ARG1;\n+  Register R4_load_addr = R4_ARG2;\n+  __ ld(R4_load_addr, -8, R1_SP);\n+\n+  Register R11_tmp = R11_scratch1;\n+\n+  \/* ==== Invoke runtime ==== *\/\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+\n+  address jrt_address = NULL;\n+\n+  if (is_strong) {\n+    if (is_native) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+    } else {\n+      if (UseCompressedOops) {\n+        jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow);\n+      } else {\n+        jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+      }\n+    }\n+  } else if (is_weak) {\n+    assert(!is_native, \"weak load reference barrier must not be called off-heap\");\n+    if (UseCompressedOops) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow);\n+    } else {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n+    }\n+  } else {\n+    assert(is_phantom, \"reference type must be phantom\");\n+    assert(is_native, \"phantom load reference barrier must be called off-heap\");\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom);\n+  }\n+  assert(jrt_address != NULL, \"load reference barrier runtime routine cannot be found\");\n+\n+  __ save_LR_CR(R11_tmp);\n+  __ push_frame_reg_args(nbytes_save, R11_tmp);\n+\n+  \/\/ Invoke runtime.  Arguments are already stored in the corresponding registers.\n+  __ call_VM_leaf(jrt_address, R3_obj, R4_load_addr);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  __ pop_frame();\n+  __ restore_LR_CR(R11_tmp);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save, true, false); \/\/ Skip 'R3_RET' register.\n+\n+  __ blr();\n+  __ block_comment(\"} generate_c1_load_reference_barrier_runtime_stub (shenandoahgc)\");\n+}\n+\n+#undef __\n+\n+#endif \/\/ COMPILER1\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":1012,"deletions":0,"binary":false,"changes":1012,"status":"added"},{"patch":"@@ -0,0 +1,118 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_PPC_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_PPC_HPP\n+#define CPU_PPC_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_PPC_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+\n+#ifdef COMPILER1\n+\n+class LIR_Assembler;\n+class ShenandoahPreBarrierStub;\n+class ShenandoahLoadReferenceBarrierStub;\n+class StubAssembler;\n+\n+#endif\n+\n+class StubCodeGenerator;\n+\n+class ShenandoahBarrierSetAssembler: public BarrierSetAssembler {\n+private:\n+\n+  \/* ==== Actual barrier implementations ==== *\/\n+  void satb_write_barrier_impl(MacroAssembler* masm, DecoratorSet decorators,\n+                               Register base, RegisterOrConstant ind_or_offs,\n+                               Register pre_val,\n+                               Register tmp1, Register tmp2,\n+                               MacroAssembler::PreservationLevel preservation_level);\n+\n+  void load_reference_barrier_impl(MacroAssembler* masm, DecoratorSet decorators,\n+                                   Register base, RegisterOrConstant ind_or_offs,\n+                                   Register dst,\n+                                   Register tmp1, Register tmp2,\n+                                   MacroAssembler::PreservationLevel preservation_level);\n+\n+  \/* ==== Helper methods for barrier implementations ==== *\/\n+  void resolve_forward_pointer_not_null(MacroAssembler* masm, Register dst, Register tmp);\n+\n+public:\n+\n+  \/* ==== C1 stubs ==== *\/\n+#ifdef COMPILER1\n+\n+  void gen_pre_barrier_stub(LIR_Assembler* ce, ShenandoahPreBarrierStub* stub);\n+\n+  void gen_load_reference_barrier_stub(LIR_Assembler* ce, ShenandoahLoadReferenceBarrierStub* stub);\n+\n+  void generate_c1_pre_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, DecoratorSet decorators);\n+\n+#endif\n+\n+  \/* ==== Available barriers (facades of the actual implementations) ==== *\/\n+  void satb_write_barrier(MacroAssembler* masm,\n+                          Register base, RegisterOrConstant ind_or_offs,\n+                          Register tmp1, Register tmp2, Register tmp3,\n+                          MacroAssembler::PreservationLevel preservation_level);\n+\n+  void iu_barrier(MacroAssembler* masm,\n+                        Register val,\n+                        Register tmp1, Register tmp2,\n+                        MacroAssembler::PreservationLevel preservation_level, DecoratorSet decorators = 0);\n+\n+  void load_reference_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                              Register base, RegisterOrConstant ind_or_offs,\n+                              Register dst,\n+                              Register tmp1, Register tmp2,\n+                              MacroAssembler::PreservationLevel preservation_level);\n+\n+  \/* ==== Helper methods used by C1 and C2 ==== *\/\n+  void cmpxchg_oop(MacroAssembler* masm, Register base_addr, Register expected, Register new_val,\n+                   Register tmp1, Register tmp2,\n+                   bool is_cae, Register result);\n+\n+  \/* ==== Access api ==== *\/\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                          Register src, Register dst, Register count, Register preserve1, Register preserve2);\n+\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Register base, RegisterOrConstant ind_or_offs, Register val,\n+                        Register tmp1, Register tmp2, Register tmp3,\n+                        MacroAssembler::PreservationLevel preservation_level);\n+\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register base, RegisterOrConstant ind_or_offs, Register dst,\n+                       Register tmp1, Register tmp2,\n+                       MacroAssembler::PreservationLevel preservation_level, Label* L_handle_null = NULL);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n+                                             Register obj, Register tmp, Label& slowpath);\n+};\n+\n+#endif \/\/ CPU_PPC_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.hpp","additions":118,"deletions":0,"binary":false,"changes":118,"status":"added"},{"patch":"@@ -0,0 +1,217 @@\n+\/\/\n+\/\/ Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+\/\/ Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+source_hpp %{\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+%}\n+\n+\/\/ Weak compareAndSwap operations are treated as strong compareAndSwap operations.\n+\/\/ This is motivated by the retry logic of ShenandoahBarrierSetAssembler::cmpxchg_oop which is hard to realise\n+\/\/ using weak CAS operations.\n+\n+instruct compareAndSwapP_shenandoah(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                    iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndSwapN_shenandoah(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                    iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndSwapP_acq_shenandoah(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                       iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndSwapN_acq_shenandoah(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                        iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangeP_shenandoah(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                        iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangeN_shenandoah(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                        iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangePAcq_shenandoah(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                           iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangeNAcq_shenandoah(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                           iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoah_ppc.ad","additions":217,"deletions":0,"binary":false,"changes":217,"status":"added"},{"patch":"@@ -38,1 +38,1 @@\n-#if !(defined AARCH64 || defined AMD64 || defined IA32)\n+#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64)\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}