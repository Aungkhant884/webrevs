{"files":[{"patch":"@@ -1367,1 +1367,6 @@\n-    copy_memory(aligned, s, d, count, rscratch1, size);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      bool add_entry = !is_oop && (!aligned || sizeof(jlong) == size);\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      copy_memory(aligned, s, d, count, rscratch1, size);\n+    }\n@@ -1433,1 +1438,6 @@\n-    copy_memory(aligned, s, d, count, rscratch1, -size);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      bool add_entry = !is_oop && (!aligned || sizeof(jlong) == size);\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      copy_memory(aligned, s, d, count, rscratch1, -size);\n+    }\n@@ -6104,0 +6114,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -6105,0 +6116,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -1145,1 +1145,1 @@\n-  int generate_forward_aligned_copy_loop(Register from, Register to, Register count, int bytes_per_count) {\n+  int generate_forward_aligned_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool unsafe_copy = false) {\n@@ -1173,2 +1173,5 @@\n-    \/\/ predecrease to exit when there is less than count_per_loop\n-    __ sub_32(count, count, count_per_loop);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      \/\/ predecrease to exit when there is less than count_per_loop\n+      __ sub_32(count, count, count_per_loop);\n@@ -1176,2 +1179,2 @@\n-    if (pld_offset != 0) {\n-      pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n+      if (pld_offset != 0) {\n+        pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n@@ -1179,1 +1182,1 @@\n-      prefetch(from, to, 0);\n+        prefetch(from, to, 0);\n@@ -1181,6 +1184,6 @@\n-      if (prefetch_before) {\n-        \/\/ If prefetch is done ahead, final PLDs that overflow the\n-        \/\/ copied area can be easily avoided. 'count' is predecreased\n-        \/\/ by the prefetch distance to optimize the inner loop and the\n-        \/\/ outer loop skips the PLD.\n-        __ subs_32(count, count, (bytes_per_loop+pld_offset)\/bytes_per_count);\n+        if (prefetch_before) {\n+          \/\/ If prefetch is done ahead, final PLDs that overflow the\n+          \/\/ copied area can be easily avoided. 'count' is predecreased\n+          \/\/ by the prefetch distance to optimize the inner loop and the\n+          \/\/ outer loop skips the PLD.\n+          __ subs_32(count, count, (bytes_per_loop+pld_offset)\/bytes_per_count);\n@@ -1188,3 +1191,3 @@\n-        \/\/ skip prefetch for small copies\n-        __ b(L_skip_pld, lt);\n-      }\n+          \/\/ skip prefetch for small copies\n+          __ b(L_skip_pld, lt);\n+        }\n@@ -1192,6 +1195,6 @@\n-      int offset = ArmCopyCacheLineSize;\n-      while (offset <= pld_offset) {\n-        prefetch(from, to, offset);\n-        offset += ArmCopyCacheLineSize;\n-      };\n-    }\n+        int offset = ArmCopyCacheLineSize;\n+        while (offset <= pld_offset) {\n+          prefetch(from, to, offset);\n+          offset += ArmCopyCacheLineSize;\n+        };\n+      }\n@@ -1200,1 +1203,1 @@\n-    const Register data_regs[8] = {R3, R4, R5, R6, R7, R8, R9, R10};\n+      const Register data_regs[8] = {R3, R4, R5, R6, R7, R8, R9, R10};\n@@ -1202,2 +1205,2 @@\n-    {\n-      \/\/ LDM (32-bit ARM) \/ LDP (AArch64) copy of 'bytes_per_loop' bytes\n+      {\n+        \/\/ LDM (32-bit ARM) \/ LDP (AArch64) copy of 'bytes_per_loop' bytes\n@@ -1205,2 +1208,2 @@\n-      \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n-      \/\/ PLD with 64 bytes cache line but the gain was not significant.\n+        \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n+        \/\/ PLD with 64 bytes cache line but the gain was not significant.\n@@ -1208,3 +1211,3 @@\n-      Label L_copy_loop;\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_copy_loop);\n+        Label L_copy_loop;\n+        __ align(OptoLoopAlignment);\n+        __ BIND(L_copy_loop);\n@@ -1212,4 +1215,4 @@\n-      if (prefetch_before) {\n-        prefetch(from, to, bytes_per_loop + pld_offset);\n-        __ BIND(L_skip_pld);\n-      }\n+        if (prefetch_before) {\n+          prefetch(from, to, bytes_per_loop + pld_offset);\n+          __ BIND(L_skip_pld);\n+        }\n@@ -1218,1 +1221,1 @@\n-      bulk_load_forward(from, data_regs, 8);\n+        bulk_load_forward(from, data_regs, 8);\n@@ -1220,10 +1223,10 @@\n-      if (split_read) {\n-        \/\/ Split the register set in two sets so that there is less\n-        \/\/ latency between LDM and STM (R3-R6 available while R7-R10\n-        \/\/ still loading) and less register locking issue when iterating\n-        \/\/ on the first LDM.\n-        __ ldmia(from, RegisterSet(R3, R6), writeback);\n-        __ ldmia(from, RegisterSet(R7, R10), writeback);\n-      } else {\n-        __ ldmia(from, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_read) {\n+          \/\/ Split the register set in two sets so that there is less\n+          \/\/ latency between LDM and STM (R3-R6 available while R7-R10\n+          \/\/ still loading) and less register locking issue when iterating\n+          \/\/ on the first LDM.\n+          __ ldmia(from, RegisterSet(R3, R6), writeback);\n+          __ ldmia(from, RegisterSet(R7, R10), writeback);\n+        } else {\n+          __ ldmia(from, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1232,1 +1235,1 @@\n-      __ subs_32(count, count, count_per_loop);\n+        __ subs_32(count, count, count_per_loop);\n@@ -1234,3 +1237,3 @@\n-      if (prefetch_after) {\n-        prefetch(from, to, pld_offset, bytes_per_loop);\n-      }\n+        if (prefetch_after) {\n+          prefetch(from, to, pld_offset, bytes_per_loop);\n+        }\n@@ -1239,1 +1242,1 @@\n-      bulk_store_forward(to, data_regs, 8);\n+        bulk_store_forward(to, data_regs, 8);\n@@ -1241,6 +1244,6 @@\n-      if (split_write) {\n-        __ stmia(to, RegisterSet(R3, R6), writeback);\n-        __ stmia(to, RegisterSet(R7, R10), writeback);\n-      } else {\n-        __ stmia(to, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_write) {\n+          __ stmia(to, RegisterSet(R3, R6), writeback);\n+          __ stmia(to, RegisterSet(R7, R10), writeback);\n+        } else {\n+          __ stmia(to, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1249,1 +1252,1 @@\n-      __ b(L_copy_loop, ge);\n+        __ b(L_copy_loop, ge);\n@@ -1251,4 +1254,5 @@\n-      if (prefetch_before) {\n-        \/\/ the inner loop may end earlier, allowing to skip PLD for the last iterations\n-        __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n-        __ b(L_skip_pld, ge);\n+        if (prefetch_before) {\n+          \/\/ the inner loop may end earlier, allowing to skip PLD for the last iterations\n+          __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n+          __ b(L_skip_pld, ge);\n+        }\n@@ -1256,3 +1260,2 @@\n-    }\n-    BLOCK_COMMENT(\"Remaining bytes:\");\n-    \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n+      BLOCK_COMMENT(\"Remaining bytes:\");\n+      \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n@@ -1260,2 +1263,2 @@\n-    \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n-    assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n+      \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n+      assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n@@ -1264,2 +1267,2 @@\n-    assert (bytes_per_loop == 64, \"adjust the code below\");\n-    assert (bytes_per_count <= 8, \"adjust the code below\");\n+      assert (bytes_per_loop == 64, \"adjust the code below\");\n+      assert (bytes_per_count <= 8, \"adjust the code below\");\n@@ -1267,3 +1270,3 @@\n-    {\n-      Label L;\n-      __ tbz(count, exact_log2(32\/bytes_per_count), L);\n+      {\n+        Label L;\n+        __ tbz(count, exact_log2(32\/bytes_per_count), L);\n@@ -1271,2 +1274,2 @@\n-      bulk_load_forward(from, data_regs, 4);\n-      bulk_store_forward(to, data_regs, 4);\n+        bulk_load_forward(from, data_regs, 4);\n+        bulk_store_forward(to, data_regs, 4);\n@@ -1274,2 +1277,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1277,3 +1280,3 @@\n-    {\n-      Label L;\n-      __ tbz(count, exact_log2(16\/bytes_per_count), L);\n+      {\n+        Label L;\n+        __ tbz(count, exact_log2(16\/bytes_per_count), L);\n@@ -1281,2 +1284,2 @@\n-      bulk_load_forward(from, data_regs, 2);\n-      bulk_store_forward(to, data_regs, 2);\n+        bulk_load_forward(from, data_regs, 2);\n+        bulk_store_forward(to, data_regs, 2);\n@@ -1284,2 +1287,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1287,3 +1290,3 @@\n-    {\n-      Label L;\n-      __ tbz(count, exact_log2(8\/bytes_per_count), L);\n+      {\n+        Label L;\n+        __ tbz(count, exact_log2(8\/bytes_per_count), L);\n@@ -1291,2 +1294,2 @@\n-      __ ldr(R3, Address(from, 8, post_indexed));\n-      __ str(R3, Address(to,   8, post_indexed));\n+        __ ldr(R3, Address(from, 8, post_indexed));\n+        __ str(R3, Address(to,   8, post_indexed));\n@@ -1294,2 +1297,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1297,3 +1300,3 @@\n-    if (bytes_per_count <= 4) {\n-      Label L;\n-      __ tbz(count, exact_log2(4\/bytes_per_count), L);\n+      if (bytes_per_count <= 4) {\n+        Label L;\n+        __ tbz(count, exact_log2(4\/bytes_per_count), L);\n@@ -1301,2 +1304,2 @@\n-      __ ldr_w(R3, Address(from, 4, post_indexed));\n-      __ str_w(R3, Address(to,   4, post_indexed));\n+        __ ldr_w(R3, Address(from, 4, post_indexed));\n+        __ str_w(R3, Address(to,   4, post_indexed));\n@@ -1304,2 +1307,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1307,3 +1310,3 @@\n-    if (bytes_per_count <= 2) {\n-      Label L;\n-      __ tbz(count, exact_log2(2\/bytes_per_count), L);\n+      if (bytes_per_count <= 2) {\n+        Label L;\n+        __ tbz(count, exact_log2(2\/bytes_per_count), L);\n@@ -1311,2 +1314,2 @@\n-      __ ldrh(R3, Address(from, 2, post_indexed));\n-      __ strh(R3, Address(to,   2, post_indexed));\n+        __ ldrh(R3, Address(from, 2, post_indexed));\n+        __ strh(R3, Address(to,   2, post_indexed));\n@@ -1314,2 +1317,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1317,3 +1320,3 @@\n-    if (bytes_per_count <= 1) {\n-      Label L;\n-      __ tbz(count, 0, L);\n+      if (bytes_per_count <= 1) {\n+        Label L;\n+        __ tbz(count, 0, L);\n@@ -1321,2 +1324,2 @@\n-      __ ldrb(R3, Address(from, 1, post_indexed));\n-      __ strb(R3, Address(to,   1, post_indexed));\n+        __ ldrb(R3, Address(from, 1, post_indexed));\n+        __ strb(R3, Address(to,   1, post_indexed));\n@@ -1324,1 +1327,2 @@\n-      __ bind(L);\n+        __ bind(L);\n+      }\n@@ -1327,3 +1331,3 @@\n-    __ tst(count, 16 \/ bytes_per_count);\n-    __ ldmia(from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n-    __ stmia(to, RegisterSet(R3, R6), writeback, ne);\n+      __ tst(count, 16 \/ bytes_per_count);\n+      __ ldmia(from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n+      __ stmia(to, RegisterSet(R3, R6), writeback, ne);\n@@ -1331,3 +1335,3 @@\n-    __ tst(count, 8 \/ bytes_per_count);\n-    __ ldmia(from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n-    __ stmia(to, RegisterSet(R3, R4), writeback, ne);\n+      __ tst(count, 8 \/ bytes_per_count);\n+      __ ldmia(from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n+      __ stmia(to, RegisterSet(R3, R4), writeback, ne);\n@@ -1335,5 +1339,5 @@\n-    if (bytes_per_count <= 4) {\n-      __ tst(count, 4 \/ bytes_per_count);\n-      __ ldr(R3, Address(from, 4, post_indexed), ne); \/\/ copy 4 bytes\n-      __ str(R3, Address(to, 4, post_indexed), ne);\n-    }\n+      if (bytes_per_count <= 4) {\n+        __ tst(count, 4 \/ bytes_per_count);\n+        __ ldr(R3, Address(from, 4, post_indexed), ne); \/\/ copy 4 bytes\n+        __ str(R3, Address(to, 4, post_indexed), ne);\n+      }\n@@ -1341,5 +1345,5 @@\n-    if (bytes_per_count <= 2) {\n-      __ tst(count, 2 \/ bytes_per_count);\n-      __ ldrh(R3, Address(from, 2, post_indexed), ne); \/\/ copy 2 bytes\n-      __ strh(R3, Address(to, 2, post_indexed), ne);\n-    }\n+      if (bytes_per_count <= 2) {\n+        __ tst(count, 2 \/ bytes_per_count);\n+        __ ldrh(R3, Address(from, 2, post_indexed), ne); \/\/ copy 2 bytes\n+        __ strh(R3, Address(to, 2, post_indexed), ne);\n+      }\n@@ -1347,4 +1351,5 @@\n-    if (bytes_per_count == 1) {\n-      __ tst(count, 1);\n-      __ ldrb(R3, Address(from, 1, post_indexed), ne);\n-      __ strb(R3, Address(to, 1, post_indexed), ne);\n+      if (bytes_per_count == 1) {\n+        __ tst(count, 1);\n+        __ ldrb(R3, Address(from, 1, post_indexed), ne);\n+        __ strb(R3, Address(to, 1, post_indexed), ne);\n+      }\n@@ -1352,1 +1357,0 @@\n-\n@@ -1380,1 +1384,1 @@\n-  int generate_backward_aligned_copy_loop(Register end_from, Register end_to, Register count, int bytes_per_count) {\n+  int generate_backward_aligned_copy_loop(Register end_from, Register end_to, Register count, int bytes_per_count, bool unsafe_copy = false) {\n@@ -1398,1 +1402,4 @@\n-    __ sub_32(count, count, count_per_loop);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      __ sub_32(count, count, count_per_loop);\n@@ -1400,2 +1407,2 @@\n-    const bool prefetch_before = pld_offset < 0;\n-    const bool prefetch_after = pld_offset > 0;\n+      const bool prefetch_before = pld_offset < 0;\n+      const bool prefetch_after = pld_offset > 0;\n@@ -1403,1 +1410,1 @@\n-    Label L_skip_pld;\n+      Label L_skip_pld;\n@@ -1405,2 +1412,2 @@\n-    if (pld_offset != 0) {\n-      pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n+      if (pld_offset != 0) {\n+        pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n@@ -1408,1 +1415,1 @@\n-      prefetch(end_from, end_to, -wordSize);\n+        prefetch(end_from, end_to, -wordSize);\n@@ -1410,4 +1417,4 @@\n-      if (prefetch_before) {\n-        __ subs_32(count, count, (bytes_per_loop + pld_offset) \/ bytes_per_count);\n-        __ b(L_skip_pld, lt);\n-      }\n+        if (prefetch_before) {\n+          __ subs_32(count, count, (bytes_per_loop + pld_offset) \/ bytes_per_count);\n+          __ b(L_skip_pld, lt);\n+        }\n@@ -1415,6 +1422,6 @@\n-      int offset = ArmCopyCacheLineSize;\n-      while (offset <= pld_offset) {\n-        prefetch(end_from, end_to, -(wordSize + offset));\n-        offset += ArmCopyCacheLineSize;\n-      };\n-    }\n+        int offset = ArmCopyCacheLineSize;\n+        while (offset <= pld_offset) {\n+          prefetch(end_from, end_to, -(wordSize + offset));\n+          offset += ArmCopyCacheLineSize;\n+        };\n+      }\n@@ -1423,1 +1430,1 @@\n-    const Register data_regs[8] = {R3, R4, R5, R6, R7, R8, R9, R10};\n+      const Register data_regs[8] = {R3, R4, R5, R6, R7, R8, R9, R10};\n@@ -1425,2 +1432,2 @@\n-    {\n-      \/\/ LDM (32-bit ARM) \/ LDP (AArch64) copy of 'bytes_per_loop' bytes\n+      {\n+        \/\/ LDM (32-bit ARM) \/ LDP (AArch64) copy of 'bytes_per_loop' bytes\n@@ -1428,2 +1435,2 @@\n-      \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n-      \/\/ PLD with 64 bytes cache line but the gain was not significant.\n+        \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n+        \/\/ PLD with 64 bytes cache line but the gain was not significant.\n@@ -1431,3 +1438,3 @@\n-      Label L_copy_loop;\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_copy_loop);\n+        Label L_copy_loop;\n+        __ align(OptoLoopAlignment);\n+        __ BIND(L_copy_loop);\n@@ -1435,4 +1442,4 @@\n-      if (prefetch_before) {\n-        prefetch(end_from, end_to, -(wordSize + bytes_per_loop + pld_offset));\n-        __ BIND(L_skip_pld);\n-      }\n+        if (prefetch_before) {\n+          prefetch(end_from, end_to, -(wordSize + bytes_per_loop + pld_offset));\n+          __ BIND(L_skip_pld);\n+        }\n@@ -1441,1 +1448,1 @@\n-      bulk_load_backward(end_from, data_regs, 8);\n+        bulk_load_backward(end_from, data_regs, 8);\n@@ -1443,6 +1450,6 @@\n-      if (split_read) {\n-        __ ldmdb(end_from, RegisterSet(R7, R10), writeback);\n-        __ ldmdb(end_from, RegisterSet(R3, R6), writeback);\n-      } else {\n-        __ ldmdb(end_from, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_read) {\n+          __ ldmdb(end_from, RegisterSet(R7, R10), writeback);\n+          __ ldmdb(end_from, RegisterSet(R3, R6), writeback);\n+        } else {\n+          __ ldmdb(end_from, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1451,1 +1458,1 @@\n-      __ subs_32(count, count, count_per_loop);\n+        __ subs_32(count, count, count_per_loop);\n@@ -1453,3 +1460,3 @@\n-      if (prefetch_after) {\n-        prefetch(end_from, end_to, -(wordSize + pld_offset), -bytes_per_loop);\n-      }\n+        if (prefetch_after) {\n+          prefetch(end_from, end_to, -(wordSize + pld_offset), -bytes_per_loop);\n+        }\n@@ -1458,1 +1465,1 @@\n-      bulk_store_backward(end_to, data_regs, 8);\n+        bulk_store_backward(end_to, data_regs, 8);\n@@ -1460,6 +1467,6 @@\n-      if (split_write) {\n-        __ stmdb(end_to, RegisterSet(R7, R10), writeback);\n-        __ stmdb(end_to, RegisterSet(R3, R6), writeback);\n-      } else {\n-        __ stmdb(end_to, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_write) {\n+          __ stmdb(end_to, RegisterSet(R7, R10), writeback);\n+          __ stmdb(end_to, RegisterSet(R3, R6), writeback);\n+        } else {\n+          __ stmdb(end_to, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1468,1 +1475,1 @@\n-      __ b(L_copy_loop, ge);\n+        __ b(L_copy_loop, ge);\n@@ -1470,3 +1477,4 @@\n-      if (prefetch_before) {\n-        __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n-        __ b(L_skip_pld, ge);\n+        if (prefetch_before) {\n+          __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n+          __ b(L_skip_pld, ge);\n+        }\n@@ -1474,3 +1482,2 @@\n-    }\n-    BLOCK_COMMENT(\"Remaining bytes:\");\n-    \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n+      BLOCK_COMMENT(\"Remaining bytes:\");\n+      \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n@@ -1478,2 +1485,2 @@\n-    \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n-    assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n+      \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n+      assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n@@ -1482,2 +1489,2 @@\n-    assert (bytes_per_loop == 64, \"adjust the code below\");\n-    assert (bytes_per_count <= 8, \"adjust the code below\");\n+      assert (bytes_per_loop == 64, \"adjust the code below\");\n+      assert (bytes_per_count <= 8, \"adjust the code below\");\n@@ -1485,3 +1492,3 @@\n-    {\n-      Label L;\n-      __ tbz(count, exact_log2(32\/bytes_per_count), L);\n+      {\n+        Label L;\n+        __ tbz(count, exact_log2(32\/bytes_per_count), L);\n@@ -1489,2 +1496,2 @@\n-      bulk_load_backward(end_from, data_regs, 4);\n-      bulk_store_backward(end_to, data_regs, 4);\n+        bulk_load_backward(end_from, data_regs, 4);\n+        bulk_store_backward(end_to, data_regs, 4);\n@@ -1492,2 +1499,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1495,3 +1502,3 @@\n-    {\n-      Label L;\n-      __ tbz(count, exact_log2(16\/bytes_per_count), L);\n+      {\n+        Label L;\n+        __ tbz(count, exact_log2(16\/bytes_per_count), L);\n@@ -1499,2 +1506,2 @@\n-      bulk_load_backward(end_from, data_regs, 2);\n-      bulk_store_backward(end_to, data_regs, 2);\n+        bulk_load_backward(end_from, data_regs, 2);\n+        bulk_store_backward(end_to, data_regs, 2);\n@@ -1502,2 +1509,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1505,3 +1512,3 @@\n-    {\n-      Label L;\n-      __ tbz(count, exact_log2(8\/bytes_per_count), L);\n+      {\n+        Label L;\n+        __ tbz(count, exact_log2(8\/bytes_per_count), L);\n@@ -1509,2 +1516,2 @@\n-      __ ldr(R3, Address(end_from, -8, pre_indexed));\n-      __ str(R3, Address(end_to,   -8, pre_indexed));\n+        __ ldr(R3, Address(end_from, -8, pre_indexed));\n+        __ str(R3, Address(end_to,   -8, pre_indexed));\n@@ -1512,2 +1519,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1515,3 +1522,3 @@\n-    if (bytes_per_count <= 4) {\n-      Label L;\n-      __ tbz(count, exact_log2(4\/bytes_per_count), L);\n+      if (bytes_per_count <= 4) {\n+        Label L;\n+        __ tbz(count, exact_log2(4\/bytes_per_count), L);\n@@ -1519,2 +1526,2 @@\n-      __ ldr_w(R3, Address(end_from, -4, pre_indexed));\n-      __ str_w(R3, Address(end_to,   -4, pre_indexed));\n+        __ ldr_w(R3, Address(end_from, -4, pre_indexed));\n+        __ str_w(R3, Address(end_to,   -4, pre_indexed));\n@@ -1522,2 +1529,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1525,3 +1532,3 @@\n-    if (bytes_per_count <= 2) {\n-      Label L;\n-      __ tbz(count, exact_log2(2\/bytes_per_count), L);\n+      if (bytes_per_count <= 2) {\n+        Label L;\n+        __ tbz(count, exact_log2(2\/bytes_per_count), L);\n@@ -1529,2 +1536,2 @@\n-      __ ldrh(R3, Address(end_from, -2, pre_indexed));\n-      __ strh(R3, Address(end_to,   -2, pre_indexed));\n+        __ ldrh(R3, Address(end_from, -2, pre_indexed));\n+        __ strh(R3, Address(end_to,   -2, pre_indexed));\n@@ -1532,2 +1539,2 @@\n-      __ bind(L);\n-    }\n+        __ bind(L);\n+      }\n@@ -1535,3 +1542,3 @@\n-    if (bytes_per_count <= 1) {\n-      Label L;\n-      __ tbz(count, 0, L);\n+      if (bytes_per_count <= 1) {\n+        Label L;\n+        __ tbz(count, 0, L);\n@@ -1539,2 +1546,2 @@\n-      __ ldrb(R3, Address(end_from, -1, pre_indexed));\n-      __ strb(R3, Address(end_to,   -1, pre_indexed));\n+        __ ldrb(R3, Address(end_from, -1, pre_indexed));\n+        __ strb(R3, Address(end_to,   -1, pre_indexed));\n@@ -1542,1 +1549,2 @@\n-      __ bind(L);\n+        __ bind(L);\n+      }\n@@ -1545,13 +1553,13 @@\n-    __ tst(count, 16 \/ bytes_per_count);\n-    __ ldmdb(end_from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n-    __ stmdb(end_to, RegisterSet(R3, R6), writeback, ne);\n-\n-    __ tst(count, 8 \/ bytes_per_count);\n-    __ ldmdb(end_from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n-    __ stmdb(end_to, RegisterSet(R3, R4), writeback, ne);\n-\n-    if (bytes_per_count <= 4) {\n-      __ tst(count, 4 \/ bytes_per_count);\n-      __ ldr(R3, Address(end_from, -4, pre_indexed), ne); \/\/ copy 4 bytes\n-      __ str(R3, Address(end_to, -4, pre_indexed), ne);\n-    }\n+      __ tst(count, 16 \/ bytes_per_count);\n+      __ ldmdb(end_from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n+      __ stmdb(end_to, RegisterSet(R3, R6), writeback, ne);\n+\n+      __ tst(count, 8 \/ bytes_per_count);\n+      __ ldmdb(end_from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n+      __ stmdb(end_to, RegisterSet(R3, R4), writeback, ne);\n+\n+      if (bytes_per_count <= 4) {\n+        __ tst(count, 4 \/ bytes_per_count);\n+        __ ldr(R3, Address(end_from, -4, pre_indexed), ne); \/\/ copy 4 bytes\n+        __ str(R3, Address(end_to, -4, pre_indexed), ne);\n+      }\n@@ -1559,5 +1567,5 @@\n-    if (bytes_per_count <= 2) {\n-      __ tst(count, 2 \/ bytes_per_count);\n-      __ ldrh(R3, Address(end_from, -2, pre_indexed), ne); \/\/ copy 2 bytes\n-      __ strh(R3, Address(end_to, -2, pre_indexed), ne);\n-    }\n+      if (bytes_per_count <= 2) {\n+        __ tst(count, 2 \/ bytes_per_count);\n+        __ ldrh(R3, Address(end_from, -2, pre_indexed), ne); \/\/ copy 2 bytes\n+        __ strh(R3, Address(end_to, -2, pre_indexed), ne);\n+      }\n@@ -1565,4 +1573,5 @@\n-    if (bytes_per_count == 1) {\n-      __ tst(count, 1);\n-      __ ldrb(R3, Address(end_from, -1, pre_indexed), ne);\n-      __ strb(R3, Address(end_to, -1, pre_indexed), ne);\n+      if (bytes_per_count == 1) {\n+        __ tst(count, 1);\n+        __ ldrb(R3, Address(end_from, -1, pre_indexed), ne);\n+        __ strb(R3, Address(end_to, -1, pre_indexed), ne);\n+      }\n@@ -1570,1 +1579,0 @@\n-\n@@ -2393,1 +2401,1 @@\n-  void copy_small_array(Register from, Register to, Register count, Register tmp, Register tmp2, int bytes_per_count, bool forward, Label & entry) {\n+  void copy_small_array(Register from, Register to, Register count, Register tmp, Register tmp2, int bytes_per_count, bool forward, Label & entry, bool unsafe_copy = false) {\n@@ -2396,1 +2404,4 @@\n-    __ align(OptoLoopAlignment);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      __ align(OptoLoopAlignment);\n@@ -2398,3 +2409,3 @@\n-    Label L_small_array_done, L_small_array_loop;\n-    __ BIND(entry);\n-    __ cbz_32(count, L_small_array_done);\n+      Label L_small_array_done, L_small_array_loop;\n+      __ BIND(entry);\n+      __ cbz_32(count, L_small_array_done);\n@@ -2402,5 +2413,5 @@\n-    __ BIND(L_small_array_loop);\n-    __ subs_32(count, count, 1);\n-    load_one(tmp, from, bytes_per_count, forward);\n-    store_one(tmp, to, bytes_per_count, forward);\n-    __ b(L_small_array_loop, gt);\n+      __ BIND(L_small_array_loop);\n+      __ subs_32(count, count, 1);\n+      load_one(tmp, from, bytes_per_count, forward);\n+      store_one(tmp, to, bytes_per_count, forward);\n+      __ b(L_small_array_loop, gt);\n@@ -2408,1 +2419,1 @@\n-    __ BIND(L_small_array_done);\n+      __ BIND(L_small_array_done);\n@@ -2410,7 +2421,7 @@\n-    Label L_small_loop;\n-    __ BIND(L_small_loop);\n-    store_one(tmp, to, bytes_per_count, forward, al, tmp2);\n-    __ BIND(entry); \/\/ entry point\n-    __ subs(count, count, 1);\n-    load_one(tmp, from, bytes_per_count, forward, ge, tmp2);\n-    __ b(L_small_loop, ge);\n+      Label L_small_loop;\n+      __ BIND(L_small_loop);\n+      store_one(tmp, to, bytes_per_count, forward, al, tmp2);\n+      __ BIND(entry); \/\/ entry point\n+      __ subs(count, count, 1);\n+      load_one(tmp, from, bytes_per_count, forward, ge, tmp2);\n+      __ b(L_small_loop, ge);\n@@ -2418,0 +2429,1 @@\n+    }\n@@ -2534,1 +2546,1 @@\n-  int align_dst_and_generate_shifted_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool forward) {\n+  int align_dst_and_generate_shifted_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool forward, bool unsafe_copy = false) {\n@@ -2638,10 +2650,0 @@\n-    load_one(Rval, from, wordSize, forward);\n-\n-    switch (bytes_per_count) {\n-      case 2:\n-        min_copy = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n-        break;\n-      case 1:\n-      {\n-        Label L1, L2, L3;\n-        int min_copy1, min_copy2, min_copy3;\n@@ -2649,5 +2651,33 @@\n-        Label L_loop_finished;\n-\n-        if (forward) {\n-            __ tbz(to, 0, L2);\n-            __ tbz(to, 1, L1);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      load_one(Rval, from, wordSize, forward);\n+\n+      switch (bytes_per_count) {\n+        case 2:\n+          min_copy = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          break;\n+        case 1:\n+        {\n+          Label L1, L2, L3;\n+          int min_copy1, min_copy2, min_copy3;\n+\n+          Label L_loop_finished;\n+\n+          if (forward) {\n+              __ tbz(to, 0, L2);\n+              __ tbz(to, 1, L1);\n+\n+              __ BIND(L3);\n+              min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n+\n+              __ BIND(L1);\n+              min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n+\n+              __ BIND(L2);\n+              min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          } else {\n+              __ tbz(to, 0, L2);\n+              __ tbnz(to, 1, L3);\n@@ -2655,3 +2685,3 @@\n-            __ BIND(L3);\n-            min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+              __ BIND(L1);\n+              min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n@@ -2659,3 +2689,3 @@\n-            __ BIND(L1);\n-            min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+               __ BIND(L3);\n+              min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n@@ -2663,5 +2693,3 @@\n-            __ BIND(L2);\n-            min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n-        } else {\n-            __ tbz(to, 0, L2);\n-            __ tbnz(to, 1, L3);\n+             __ BIND(L2);\n+              min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          }\n@@ -2669,3 +2697,1 @@\n-            __ BIND(L1);\n-            min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+          min_copy = MAX2(MAX2(min_copy1, min_copy2), min_copy3);\n@@ -2673,3 +2699,1 @@\n-             __ BIND(L3);\n-            min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+          __ BIND(L_loop_finished);\n@@ -2677,2 +2701,1 @@\n-           __ BIND(L2);\n-            min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          break;\n@@ -2680,6 +2703,3 @@\n-\n-        min_copy = MAX2(MAX2(min_copy1, min_copy2), min_copy3);\n-\n-        __ BIND(L_loop_finished);\n-\n-        break;\n+        default:\n+          ShouldNotReachHere();\n+          break;\n@@ -2687,3 +2707,0 @@\n-      default:\n-        ShouldNotReachHere();\n-        break;\n@@ -2691,1 +2708,0 @@\n-\n@@ -2716,0 +2732,7 @@\n+  address generate_unsafecopy_common_error_exit() {\n+    address start_pc = __ pc();\n+      __ mov(R0, 0);\n+      __ ret();\n+    return start_pc;\n+  }\n+\n@@ -2786,2 +2809,7 @@\n-    int count_required_to_align = from_is_aligned ? 0 : align_src(from, to, count, tmp1, bytes_per_count, forward);\n-    assert (small_copy_limit >= count_required_to_align, \"alignment could exhaust count\");\n+    int count_required_to_align = 0;\n+    {\n+      \/\/ UnsafeCopyMemoryMark page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      count_required_to_align = from_is_aligned ? 0 : align_src(from, to, count, tmp1, bytes_per_count, forward);\n+      assert (small_copy_limit >= count_required_to_align, \"alignment could exhaust count\");\n+    }\n@@ -2817,1 +2845,1 @@\n-      min_copy = generate_forward_aligned_copy_loop (from, to, count, bytes_per_count);\n+      min_copy = generate_forward_aligned_copy_loop(from, to, count, bytes_per_count, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2819,1 +2847,1 @@\n-      min_copy = generate_backward_aligned_copy_loop(from, to, count, bytes_per_count);\n+      min_copy = generate_backward_aligned_copy_loop(from, to, count, bytes_per_count, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2830,1 +2858,1 @@\n-      copy_small_array(from, to, count, tmp1, tmp2, bytes_per_count, forward, L_small_array \/* entry *\/);\n+      copy_small_array(from, to, count, tmp1, tmp2, bytes_per_count, forward, L_small_array \/* entry *\/, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2841,1 +2869,1 @@\n-      int min_copy_shifted = align_dst_and_generate_shifted_copy_loop(from, to, count, bytes_per_count, forward);\n+      int min_copy_shifted = align_dst_and_generate_shifted_copy_loop(from, to, count, bytes_per_count, forward, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -3722,0 +3750,3 @@\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -4379,0 +4410,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 32\n@@ -4380,0 +4412,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/arm\/stubGenerator_arm.cpp","additions":357,"deletions":322,"binary":false,"changes":679,"status":"modified"},{"patch":"@@ -955,0 +955,14 @@\n+  \/\/ This is common errorexit stub for UnsafeCopyMemory.\n+  address generate_unsafecopy_common_error_exit() {\n+    address start_pc = __ pc();\n+    Register tmp1 = R6_ARG4;\n+    \/\/ probably copy stub would have changed value reset it.\n+    if (VM_Version::has_mfdscr()) {\n+      __ load_const_optimized(tmp1, VM_Version::_dscr_val);\n+      __ mtdscr(tmp1);\n+    }\n+    __ li(R3_RET, 0); \/\/ return 0\n+    __ blr();\n+    return start_pc;\n+  }\n+\n@@ -992,0 +1006,3 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -993,4 +1010,4 @@\n-    \/\/ Don't try anything fancy if arrays don't have many elements.\n-    __ li(tmp3, 0);\n-    __ cmpwi(CCR0, R5_ARG3, 17);\n-    __ ble(CCR0, l_6); \/\/ copy 4 at a time\n+      \/\/ Don't try anything fancy if arrays don't have many elements.\n+      __ li(tmp3, 0);\n+      __ cmpwi(CCR0, R5_ARG3, 17);\n+      __ ble(CCR0, l_6); \/\/ copy 4 at a time\n@@ -998,18 +1015,4 @@\n-    if (!aligned) {\n-      __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-      __ andi_(tmp1, tmp1, 3);\n-      __ bne(CCR0, l_6); \/\/ If arrays don't have the same alignment mod 4, do 4 element copy.\n-\n-      \/\/ Copy elements if necessary to align to 4 bytes.\n-      __ neg(tmp1, R3_ARG1); \/\/ Compute distance to alignment boundary.\n-      __ andi_(tmp1, tmp1, 3);\n-      __ beq(CCR0, l_2);\n-\n-      __ subf(R5_ARG3, tmp1, R5_ARG3);\n-      __ bind(l_9);\n-      __ lbz(tmp2, 0, R3_ARG1);\n-      __ addic_(tmp1, tmp1, -1);\n-      __ stb(tmp2, 0, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, 1);\n-      __ addi(R4_ARG2, R4_ARG2, 1);\n-      __ bne(CCR0, l_9);\n+      if (!aligned) {\n+        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n+        __ andi_(tmp1, tmp1, 3);\n+        __ bne(CCR0, l_6); \/\/ If arrays don't have the same alignment mod 4, do 4 element copy.\n@@ -1017,2 +1020,4 @@\n-      __ bind(l_2);\n-    }\n+        \/\/ Copy elements if necessary to align to 4 bytes.\n+        __ neg(tmp1, R3_ARG1); \/\/ Compute distance to alignment boundary.\n+        __ andi_(tmp1, tmp1, 3);\n+        __ beq(CCR0, l_2);\n@@ -1020,4 +1025,11 @@\n-    \/\/ copy 8 elements at a time\n-    __ xorr(tmp2, R3_ARG1, R4_ARG2); \/\/ skip if src & dest have differing alignment mod 8\n-    __ andi_(tmp1, tmp2, 7);\n-    __ bne(CCR0, l_7); \/\/ not same alignment -> to or from is aligned -> copy 8\n+        __ subf(R5_ARG3, tmp1, R5_ARG3);\n+        __ bind(l_9);\n+        __ lbz(tmp2, 0, R3_ARG1);\n+        __ addic_(tmp1, tmp1, -1);\n+        __ stb(tmp2, 0, R4_ARG2);\n+        __ addi(R3_ARG1, R3_ARG1, 1);\n+        __ addi(R4_ARG2, R4_ARG2, 1);\n+        __ bne(CCR0, l_9);\n+\n+        __ bind(l_2);\n+      }\n@@ -1025,3 +1037,4 @@\n-    \/\/ copy a 2-element word if necessary to align to 8 bytes\n-    __ andi_(R0, R3_ARG1, 7);\n-    __ beq(CCR0, l_7);\n+      \/\/ copy 8 elements at a time\n+      __ xorr(tmp2, R3_ARG1, R4_ARG2); \/\/ skip if src & dest have differing alignment mod 8\n+      __ andi_(tmp1, tmp2, 7);\n+      __ bne(CCR0, l_7); \/\/ not same alignment -> to or from is aligned -> copy 8\n@@ -1029,8 +1042,3 @@\n-    __ lwzx(tmp2, R3_ARG1, tmp3);\n-    __ addi(R5_ARG3, R5_ARG3, -4);\n-    __ stwx(tmp2, R4_ARG2, tmp3);\n-    { \/\/ FasterArrayCopy\n-      __ addi(R3_ARG1, R3_ARG1, 4);\n-      __ addi(R4_ARG2, R4_ARG2, 4);\n-    }\n-    __ bind(l_7);\n+      \/\/ copy a 2-element word if necessary to align to 8 bytes\n+      __ andi_(R0, R3_ARG1, 7);\n+      __ beq(CCR0, l_7);\n@@ -1038,3 +1046,8 @@\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 31);\n-      __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 32 elements remain\n+      __ lwzx(tmp2, R3_ARG1, tmp3);\n+      __ addi(R5_ARG3, R5_ARG3, -4);\n+      __ stwx(tmp2, R4_ARG2, tmp3);\n+      { \/\/ FasterArrayCopy\n+        __ addi(R3_ARG1, R3_ARG1, 4);\n+        __ addi(R4_ARG2, R4_ARG2, 4);\n+      }\n+      __ bind(l_7);\n@@ -1042,3 +1055,3 @@\n-      __ srdi(tmp1, R5_ARG3, 5);\n-      __ andi_(R5_ARG3, R5_ARG3, 31);\n-      __ mtctr(tmp1);\n+      { \/\/ FasterArrayCopy\n+        __ cmpwi(CCR0, R5_ARG3, 31);\n+        __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 32 elements remain\n@@ -1046,1 +1059,3 @@\n-     if (!VM_Version::has_vsx()) {\n+        __ srdi(tmp1, R5_ARG3, 5);\n+        __ andi_(R5_ARG3, R5_ARG3, 31);\n+        __ mtctr(tmp1);\n@@ -1048,15 +1063,1 @@\n-      __ bind(l_8);\n-      \/\/ Use unrolled version for mass copying (copy 32 elements a time)\n-      \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-      \/\/ Therefore, the following sequence is made for the good of both.\n-      __ ld(tmp1, 0, R3_ARG1);\n-      __ ld(tmp2, 8, R3_ARG1);\n-      __ ld(tmp3, 16, R3_ARG1);\n-      __ ld(tmp4, 24, R3_ARG1);\n-      __ std(tmp1, 0, R4_ARG2);\n-      __ std(tmp2, 8, R4_ARG2);\n-      __ std(tmp3, 16, R4_ARG2);\n-      __ std(tmp4, 24, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, 32);\n-      __ addi(R4_ARG2, R4_ARG2, 32);\n-      __ bdnz(l_8);\n+       if (!VM_Version::has_vsx()) {\n@@ -1064,1 +1065,15 @@\n-    } else { \/\/ Processor supports VSX, so use it to mass copy.\n+        __ bind(l_8);\n+        \/\/ Use unrolled version for mass copying (copy 32 elements a time)\n+        \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n+        \/\/ Therefore, the following sequence is made for the good of both.\n+        __ ld(tmp1, 0, R3_ARG1);\n+        __ ld(tmp2, 8, R3_ARG1);\n+        __ ld(tmp3, 16, R3_ARG1);\n+        __ ld(tmp4, 24, R3_ARG1);\n+        __ std(tmp1, 0, R4_ARG2);\n+        __ std(tmp2, 8, R4_ARG2);\n+        __ std(tmp3, 16, R4_ARG2);\n+        __ std(tmp4, 24, R4_ARG2);\n+        __ addi(R3_ARG1, R3_ARG1, 32);\n+        __ addi(R4_ARG2, R4_ARG2, 32);\n+        __ bdnz(l_8);\n@@ -1066,2 +1081,1 @@\n-      \/\/ Prefetch the data into the L2 cache.\n-      __ dcbt(R3_ARG1, 0);\n+      } else { \/\/ Processor supports VSX, so use it to mass copy.\n@@ -1069,5 +1083,2 @@\n-      \/\/ If supported set DSCR pre-fetch to deepest.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-        __ mtdscr(tmp2);\n-      }\n+        \/\/ Prefetch the data into the L2 cache.\n+        __ dcbt(R3_ARG1, 0);\n@@ -1075,1 +1086,5 @@\n-      __ li(tmp1, 16);\n+        \/\/ If supported set DSCR pre-fetch to deepest.\n+        if (VM_Version::has_mfdscr()) {\n+          __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n+          __ mtdscr(tmp2);\n+        }\n@@ -1077,4 +1092,1 @@\n-      \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-      \/\/ loop contains < 8 instructions that fit inside a single\n-      \/\/ i-cache sector.\n-      __ align(32);\n+        __ li(tmp1, 16);\n@@ -1082,10 +1094,4 @@\n-      __ bind(l_10);\n-      \/\/ Use loop with VSX load\/store instructions to\n-      \/\/ copy 32 elements a time.\n-      __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-      __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-      __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n-      __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n-      __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n-      __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n-      __ bdnz(l_10);                       \/\/ Dec CTR and loop if not zero.\n+        \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n+        \/\/ loop contains < 8 instructions that fit inside a single\n+        \/\/ i-cache sector.\n+        __ align(32);\n@@ -1093,5 +1099,10 @@\n-      \/\/ Restore DSCR pre-fetch value.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-        __ mtdscr(tmp2);\n-      }\n+        __ bind(l_10);\n+        \/\/ Use loop with VSX load\/store instructions to\n+        \/\/ copy 32 elements a time.\n+        __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n+        __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n+        __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n+        __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n+        __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n+        __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n+        __ bdnz(l_10);                       \/\/ Dec CTR and loop if not zero.\n@@ -1099,2 +1110,5 @@\n-    } \/\/ VSX\n-   } \/\/ FasterArrayCopy\n+        \/\/ Restore DSCR pre-fetch value.\n+        if (VM_Version::has_mfdscr()) {\n+          __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n+          __ mtdscr(tmp2);\n+        }\n@@ -1102,1 +1116,2 @@\n-    __ bind(l_6);\n+      } \/\/ VSX\n+     } \/\/ FasterArrayCopy\n@@ -1104,6 +1119,1 @@\n-    \/\/ copy 4 elements at a time\n-    __ cmpwi(CCR0, R5_ARG3, 4);\n-    __ blt(CCR0, l_1);\n-    __ srdi(tmp1, R5_ARG3, 2);\n-    __ mtctr(tmp1); \/\/ is > 0\n-    __ andi_(R5_ARG3, R5_ARG3, 3);\n+      __ bind(l_6);\n@@ -1111,10 +1121,6 @@\n-    { \/\/ FasterArrayCopy\n-      __ addi(R3_ARG1, R3_ARG1, -4);\n-      __ addi(R4_ARG2, R4_ARG2, -4);\n-      __ bind(l_3);\n-      __ lwzu(tmp2, 4, R3_ARG1);\n-      __ stwu(tmp2, 4, R4_ARG2);\n-      __ bdnz(l_3);\n-      __ addi(R3_ARG1, R3_ARG1, 4);\n-      __ addi(R4_ARG2, R4_ARG2, 4);\n-    }\n+      \/\/ copy 4 elements at a time\n+      __ cmpwi(CCR0, R5_ARG3, 4);\n+      __ blt(CCR0, l_1);\n+      __ srdi(tmp1, R5_ARG3, 2);\n+      __ mtctr(tmp1); \/\/ is > 0\n+      __ andi_(R5_ARG3, R5_ARG3, 3);\n@@ -1122,4 +1128,10 @@\n-    \/\/ do single element copy\n-    __ bind(l_1);\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_4);\n+      { \/\/ FasterArrayCopy\n+        __ addi(R3_ARG1, R3_ARG1, -4);\n+        __ addi(R4_ARG2, R4_ARG2, -4);\n+        __ bind(l_3);\n+        __ lwzu(tmp2, 4, R3_ARG1);\n+        __ stwu(tmp2, 4, R4_ARG2);\n+        __ bdnz(l_3);\n+        __ addi(R3_ARG1, R3_ARG1, 4);\n+        __ addi(R4_ARG2, R4_ARG2, 4);\n+      }\n@@ -1127,4 +1139,4 @@\n-    { \/\/ FasterArrayCopy\n-      __ mtctr(R5_ARG3);\n-      __ addi(R3_ARG1, R3_ARG1, -1);\n-      __ addi(R4_ARG2, R4_ARG2, -1);\n+      \/\/ do single element copy\n+      __ bind(l_1);\n+      __ cmpwi(CCR0, R5_ARG3, 0);\n+      __ beq(CCR0, l_4);\n@@ -1132,4 +1144,10 @@\n-      __ bind(l_5);\n-      __ lbzu(tmp2, 1, R3_ARG1);\n-      __ stbu(tmp2, 1, R4_ARG2);\n-      __ bdnz(l_5);\n+      { \/\/ FasterArrayCopy\n+        __ mtctr(R5_ARG3);\n+        __ addi(R3_ARG1, R3_ARG1, -1);\n+        __ addi(R4_ARG2, R4_ARG2, -1);\n+\n+        __ bind(l_5);\n+        __ lbzu(tmp2, 1, R3_ARG1);\n+        __ stbu(tmp2, 1, R4_ARG2);\n+        __ bdnz(l_5);\n+      }\n@@ -1170,9 +1188,11 @@\n-\n-    __ b(l_2);\n-    __ bind(l_1);\n-    __ stbx(tmp1, R4_ARG2, R5_ARG3);\n-    __ bind(l_2);\n-    __ addic_(R5_ARG3, R5_ARG3, -1);\n-    __ lbzx(tmp1, R3_ARG1, R5_ARG3);\n-    __ bge(CCR0, l_1);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      __ b(l_2);\n+      __ bind(l_1);\n+      __ stbx(tmp1, R4_ARG2, R5_ARG3);\n+      __ bind(l_2);\n+      __ addic_(R5_ARG3, R5_ARG3, -1);\n+      __ lbzx(tmp1, R3_ARG1, R5_ARG3);\n+      __ bge(CCR0, l_1);\n+    }\n@@ -1255,0 +1275,7 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      \/\/ don't try anything fancy if arrays don't have many elements\n+      __ li(tmp3, 0);\n+      __ cmpwi(CCR0, R5_ARG3, 9);\n+      __ ble(CCR0, l_6); \/\/ copy 2 at a time\n@@ -1256,9 +1283,4 @@\n-    \/\/ don't try anything fancy if arrays don't have many elements\n-    __ li(tmp3, 0);\n-    __ cmpwi(CCR0, R5_ARG3, 9);\n-    __ ble(CCR0, l_6); \/\/ copy 2 at a time\n-\n-    if (!aligned) {\n-      __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-      __ andi_(tmp1, tmp1, 3);\n-      __ bne(CCR0, l_6); \/\/ if arrays don't have the same alignment mod 4, do 2 element copy\n+      if (!aligned) {\n+        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n+        __ andi_(tmp1, tmp1, 3);\n+        __ bne(CCR0, l_6); \/\/ if arrays don't have the same alignment mod 4, do 2 element copy\n@@ -1266,1 +1288,1 @@\n-      \/\/ At this point it is guaranteed that both, from and to have the same alignment mod 4.\n+        \/\/ At this point it is guaranteed that both, from and to have the same alignment mod 4.\n@@ -1268,3 +1290,3 @@\n-      \/\/ Copy 1 element if necessary to align to 4 bytes.\n-      __ andi_(tmp1, R3_ARG1, 3);\n-      __ beq(CCR0, l_2);\n+        \/\/ Copy 1 element if necessary to align to 4 bytes.\n+        __ andi_(tmp1, R3_ARG1, 3);\n+        __ beq(CCR0, l_2);\n@@ -1272,6 +1294,6 @@\n-      __ lhz(tmp2, 0, R3_ARG1);\n-      __ addi(R3_ARG1, R3_ARG1, 2);\n-      __ sth(tmp2, 0, R4_ARG2);\n-      __ addi(R4_ARG2, R4_ARG2, 2);\n-      __ addi(R5_ARG3, R5_ARG3, -1);\n-      __ bind(l_2);\n+        __ lhz(tmp2, 0, R3_ARG1);\n+        __ addi(R3_ARG1, R3_ARG1, 2);\n+        __ sth(tmp2, 0, R4_ARG2);\n+        __ addi(R4_ARG2, R4_ARG2, 2);\n+        __ addi(R5_ARG3, R5_ARG3, -1);\n+        __ bind(l_2);\n@@ -1279,1 +1301,1 @@\n-      \/\/ At this point the positions of both, from and to, are at least 4 byte aligned.\n+        \/\/ At this point the positions of both, from and to, are at least 4 byte aligned.\n@@ -1281,5 +1303,5 @@\n-      \/\/ Copy 4 elements at a time.\n-      \/\/ Align to 8 bytes, but only if both, from and to, have same alignment mod 8.\n-      __ xorr(tmp2, R3_ARG1, R4_ARG2);\n-      __ andi_(tmp1, tmp2, 7);\n-      __ bne(CCR0, l_7); \/\/ not same alignment mod 8 -> copy 4, either from or to will be unaligned\n+        \/\/ Copy 4 elements at a time.\n+        \/\/ Align to 8 bytes, but only if both, from and to, have same alignment mod 8.\n+        __ xorr(tmp2, R3_ARG1, R4_ARG2);\n+        __ andi_(tmp1, tmp2, 7);\n+        __ bne(CCR0, l_7); \/\/ not same alignment mod 8 -> copy 4, either from or to will be unaligned\n@@ -1287,3 +1309,3 @@\n-      \/\/ Copy a 2-element word if necessary to align to 8 bytes.\n-      __ andi_(R0, R3_ARG1, 7);\n-      __ beq(CCR0, l_7);\n+        \/\/ Copy a 2-element word if necessary to align to 8 bytes.\n+        __ andi_(R0, R3_ARG1, 7);\n+        __ beq(CCR0, l_7);\n@@ -1291,6 +1313,7 @@\n-      __ lwzx(tmp2, R3_ARG1, tmp3);\n-      __ addi(R5_ARG3, R5_ARG3, -2);\n-      __ stwx(tmp2, R4_ARG2, tmp3);\n-      { \/\/ FasterArrayCopy\n-        __ addi(R3_ARG1, R3_ARG1, 4);\n-        __ addi(R4_ARG2, R4_ARG2, 4);\n+        __ lwzx(tmp2, R3_ARG1, tmp3);\n+        __ addi(R5_ARG3, R5_ARG3, -2);\n+        __ stwx(tmp2, R4_ARG2, tmp3);\n+        { \/\/ FasterArrayCopy\n+          __ addi(R3_ARG1, R3_ARG1, 4);\n+          __ addi(R4_ARG2, R4_ARG2, 4);\n+        }\n@@ -1298,16 +1321,0 @@\n-    }\n-\n-    __ bind(l_7);\n-\n-    \/\/ Copy 4 elements at a time; either the loads or the stores can\n-    \/\/ be unaligned if aligned == false.\n-\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 15);\n-      __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 16 elements remain\n-\n-      __ srdi(tmp1, R5_ARG3, 4);\n-      __ andi_(R5_ARG3, R5_ARG3, 15);\n-      __ mtctr(tmp1);\n-\n-      if (!VM_Version::has_vsx()) {\n@@ -1315,15 +1322,1 @@\n-        __ bind(l_8);\n-        \/\/ Use unrolled version for mass copying (copy 16 elements a time).\n-        \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-        \/\/ Therefore, the following sequence is made for the good of both.\n-        __ ld(tmp1, 0, R3_ARG1);\n-        __ ld(tmp2, 8, R3_ARG1);\n-        __ ld(tmp3, 16, R3_ARG1);\n-        __ ld(tmp4, 24, R3_ARG1);\n-        __ std(tmp1, 0, R4_ARG2);\n-        __ std(tmp2, 8, R4_ARG2);\n-        __ std(tmp3, 16, R4_ARG2);\n-        __ std(tmp4, 24, R4_ARG2);\n-        __ addi(R3_ARG1, R3_ARG1, 32);\n-        __ addi(R4_ARG2, R4_ARG2, 32);\n-        __ bdnz(l_8);\n+      __ bind(l_7);\n@@ -1331,1 +1324,2 @@\n-      } else { \/\/ Processor supports VSX, so use it to mass copy.\n+      \/\/ Copy 4 elements at a time; either the loads or the stores can\n+      \/\/ be unaligned if aligned == false.\n@@ -1333,2 +1327,59 @@\n-        \/\/ Prefetch src data into L2 cache.\n-        __ dcbt(R3_ARG1, 0);\n+      { \/\/ FasterArrayCopy\n+        __ cmpwi(CCR0, R5_ARG3, 15);\n+        __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 16 elements remain\n+\n+        __ srdi(tmp1, R5_ARG3, 4);\n+        __ andi_(R5_ARG3, R5_ARG3, 15);\n+        __ mtctr(tmp1);\n+\n+        if (!VM_Version::has_vsx()) {\n+\n+          __ bind(l_8);\n+          \/\/ Use unrolled version for mass copying (copy 16 elements a time).\n+          \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n+          \/\/ Therefore, the following sequence is made for the good of both.\n+          __ ld(tmp1, 0, R3_ARG1);\n+          __ ld(tmp2, 8, R3_ARG1);\n+          __ ld(tmp3, 16, R3_ARG1);\n+          __ ld(tmp4, 24, R3_ARG1);\n+          __ std(tmp1, 0, R4_ARG2);\n+          __ std(tmp2, 8, R4_ARG2);\n+          __ std(tmp3, 16, R4_ARG2);\n+          __ std(tmp4, 24, R4_ARG2);\n+          __ addi(R3_ARG1, R3_ARG1, 32);\n+          __ addi(R4_ARG2, R4_ARG2, 32);\n+          __ bdnz(l_8);\n+\n+        } else { \/\/ Processor supports VSX, so use it to mass copy.\n+\n+          \/\/ Prefetch src data into L2 cache.\n+          __ dcbt(R3_ARG1, 0);\n+\n+          \/\/ If supported set DSCR pre-fetch to deepest.\n+          if (VM_Version::has_mfdscr()) {\n+            __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n+            __ mtdscr(tmp2);\n+          }\n+          __ li(tmp1, 16);\n+\n+          \/\/ Backbranch target aligned to 32-byte. It's not aligned 16-byte\n+          \/\/ as loop contains < 8 instructions that fit inside a single\n+          \/\/ i-cache sector.\n+          __ align(32);\n+\n+          __ bind(l_9);\n+          \/\/ Use loop with VSX load\/store instructions to\n+          \/\/ copy 16 elements a time.\n+          __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load from src.\n+          __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst.\n+          __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  \/\/ Load from src + 16.\n+          __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); \/\/ Store to dst + 16.\n+          __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32.\n+          __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32.\n+          __ bdnz(l_9);                        \/\/ Dec CTR and loop if not zero.\n+\n+          \/\/ Restore DSCR pre-fetch value.\n+          if (VM_Version::has_mfdscr()) {\n+            __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n+            __ mtdscr(tmp2);\n+          }\n@@ -1336,4 +1387,0 @@\n-        \/\/ If supported set DSCR pre-fetch to deepest.\n-        if (VM_Version::has_mfdscr()) {\n-          __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-          __ mtdscr(tmp2);\n@@ -1341,1 +1388,2 @@\n-        __ li(tmp1, 16);\n+      } \/\/ FasterArrayCopy\n+      __ bind(l_6);\n@@ -1343,4 +1391,6 @@\n-        \/\/ Backbranch target aligned to 32-byte. It's not aligned 16-byte\n-        \/\/ as loop contains < 8 instructions that fit inside a single\n-        \/\/ i-cache sector.\n-        __ align(32);\n+      \/\/ copy 2 elements at a time\n+      { \/\/ FasterArrayCopy\n+        __ cmpwi(CCR0, R5_ARG3, 2);\n+        __ blt(CCR0, l_1);\n+        __ srdi(tmp1, R5_ARG3, 1);\n+        __ andi_(R5_ARG3, R5_ARG3, 1);\n@@ -1348,10 +1398,3 @@\n-        __ bind(l_9);\n-        \/\/ Use loop with VSX load\/store instructions to\n-        \/\/ copy 16 elements a time.\n-        __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load from src.\n-        __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst.\n-        __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  \/\/ Load from src + 16.\n-        __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); \/\/ Store to dst + 16.\n-        __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32.\n-        __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32.\n-        __ bdnz(l_9);                        \/\/ Dec CTR and loop if not zero.\n+        __ addi(R3_ARG1, R3_ARG1, -4);\n+        __ addi(R4_ARG2, R4_ARG2, -4);\n+        __ mtctr(tmp1);\n@@ -1359,5 +1402,4 @@\n-        \/\/ Restore DSCR pre-fetch value.\n-        if (VM_Version::has_mfdscr()) {\n-          __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-          __ mtdscr(tmp2);\n-        }\n+        __ bind(l_3);\n+        __ lwzu(tmp2, 4, R3_ARG1);\n+        __ stwu(tmp2, 4, R4_ARG2);\n+        __ bdnz(l_3);\n@@ -1365,0 +1407,2 @@\n+        __ addi(R3_ARG1, R3_ARG1, 4);\n+        __ addi(R4_ARG2, R4_ARG2, 4);\n@@ -1366,13 +1410,0 @@\n-    } \/\/ FasterArrayCopy\n-    __ bind(l_6);\n-\n-    \/\/ copy 2 elements at a time\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 2);\n-      __ blt(CCR0, l_1);\n-      __ srdi(tmp1, R5_ARG3, 1);\n-      __ andi_(R5_ARG3, R5_ARG3, 1);\n-\n-      __ addi(R3_ARG1, R3_ARG1, -4);\n-      __ addi(R4_ARG2, R4_ARG2, -4);\n-      __ mtctr(tmp1);\n@@ -1380,4 +1411,4 @@\n-      __ bind(l_3);\n-      __ lwzu(tmp2, 4, R3_ARG1);\n-      __ stwu(tmp2, 4, R4_ARG2);\n-      __ bdnz(l_3);\n+      \/\/ do single element copy\n+      __ bind(l_1);\n+      __ cmpwi(CCR0, R5_ARG3, 0);\n+      __ beq(CCR0, l_4);\n@@ -1385,2 +1416,10 @@\n-      __ addi(R3_ARG1, R3_ARG1, 4);\n-      __ addi(R4_ARG2, R4_ARG2, 4);\n+      { \/\/ FasterArrayCopy\n+        __ mtctr(R5_ARG3);\n+        __ addi(R3_ARG1, R3_ARG1, -2);\n+        __ addi(R4_ARG2, R4_ARG2, -2);\n+\n+        __ bind(l_5);\n+        __ lhzu(tmp2, 2, R3_ARG1);\n+        __ sthu(tmp2, 2, R4_ARG2);\n+        __ bdnz(l_5);\n+      }\n@@ -1389,15 +1428,0 @@\n-    \/\/ do single element copy\n-    __ bind(l_1);\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_4);\n-\n-    { \/\/ FasterArrayCopy\n-      __ mtctr(R5_ARG3);\n-      __ addi(R3_ARG1, R3_ARG1, -2);\n-      __ addi(R4_ARG2, R4_ARG2, -2);\n-\n-      __ bind(l_5);\n-      __ lhzu(tmp2, 2, R3_ARG1);\n-      __ sthu(tmp2, 2, R4_ARG2);\n-      __ bdnz(l_5);\n-    }\n@@ -1435,9 +1459,12 @@\n-    __ sldi(tmp1, R5_ARG3, 1);\n-    __ b(l_2);\n-    __ bind(l_1);\n-    __ sthx(tmp2, R4_ARG2, tmp1);\n-    __ bind(l_2);\n-    __ addic_(tmp1, tmp1, -2);\n-    __ lhzx(tmp2, R3_ARG1, tmp1);\n-    __ bge(CCR0, l_1);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      __ sldi(tmp1, R5_ARG3, 1);\n+      __ b(l_2);\n+      __ bind(l_1);\n+      __ sthx(tmp2, R4_ARG2, tmp1);\n+      __ bind(l_2);\n+      __ addic_(tmp1, tmp1, -2);\n+      __ lhzx(tmp2, R3_ARG1, tmp1);\n+      __ bge(CCR0, l_1);\n+    }\n@@ -1591,1 +1618,5 @@\n-    generate_disjoint_int_copy_core(aligned);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_disjoint_int_copy_core(aligned);\n+    }\n@@ -1739,2 +1770,5 @@\n-\n-    generate_conjoint_int_copy_core(aligned);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_conjoint_int_copy_core(aligned);\n+    }\n@@ -1862,1 +1896,5 @@\n-    generate_disjoint_long_copy_core(aligned);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_disjoint_long_copy_core(aligned);\n+    }\n@@ -1866,1 +1904,1 @@\n-    return start;\n+  return start;\n@@ -1989,2 +2027,5 @@\n-    generate_conjoint_long_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_conjoint_long_copy_core(aligned);\n+    }\n@@ -3023,0 +3064,3 @@\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -3595,0 +3639,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -3596,0 +3641,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":319,"deletions":271,"binary":false,"changes":590,"status":"modified"},{"patch":"@@ -1079,0 +1079,11 @@\n+  address generate_unsafecopy_common_error_exit() {\n+    address start_pc = __ pc();\n+    if (UseBlockCopy) {\n+      __ wrasi(G0, Assembler::ASI_PRIMARY_NOFAULT);\n+      __ membar(Assembler::StoreLoad);\n+    }\n+    __ retl();\n+    __ delayed()->mov(G0, O0); \/\/ return 0\n+    return start_pc;\n+  }\n+\n@@ -1110,4 +1121,3 @@\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 23); \/\/ 16 + 7\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n-    __ delayed()->mov(G0, offset);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -1115,30 +1125,4 @@\n-    if (aligned) {\n-      \/\/ 'aligned' == true when it is known statically during compilation\n-      \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n-      \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n-      \/\/\n-      \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n-      \/\/ and 8 bytes - in 64-bits VM. So we do it only for 32-bits VM\n-      \/\/\n-    } else {\n-      \/\/ copy bytes to align 'to' on 8 byte boundary\n-      __ andcc(to, 7, G1); \/\/ misaligned bytes\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->neg(G1);\n-      __ inc(G1, 8);       \/\/ bytes need to copy to next 8-bytes alignment\n-      __ sub(count, G1, count);\n-    __ BIND(L_align);\n-      __ ldub(from, 0, O3);\n-      __ deccc(G1);\n-      __ inc(from);\n-      __ stb(O3, to, 0);\n-      __ br(Assembler::notZero, false, Assembler::pt, L_align);\n-      __ delayed()->inc(to);\n-    __ BIND(L_skip_alignment);\n-    }\n-    if (!aligned) {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise fall through to the next\n-      \/\/ code for aligned copy.\n-      \/\/ The compare above (count >= 23) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 23); \/\/ 16 + 7\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n+      __ delayed()->mov(G0, offset);\n@@ -1146,2 +1130,33 @@\n-      copy_16_bytes_forward_with_shift(from, to, count, 0, L_copy_byte);\n-    }\n+      if (aligned) {\n+        \/\/ 'aligned' == true when it is known statically during compilation\n+        \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n+        \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n+        \/\/\n+        \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n+        \/\/ and 8 bytes - in 64-bits VM. So we do it only for 32-bits VM\n+        \/\/\n+      } else {\n+        \/\/ copy bytes to align 'to' on 8 byte boundary\n+        __ andcc(to, 7, G1); \/\/ misaligned bytes\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->neg(G1);\n+        __ inc(G1, 8);       \/\/ bytes need to copy to next 8-bytes alignment\n+        __ sub(count, G1, count);\n+      __ BIND(L_align);\n+        __ ldub(from, 0, O3);\n+        __ deccc(G1);\n+        __ inc(from);\n+        __ stb(O3, to, 0);\n+        __ br(Assembler::notZero, false, Assembler::pt, L_align);\n+        __ delayed()->inc(to);\n+      __ BIND(L_skip_alignment);\n+      }\n+      if (!aligned) {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise fall through to the next\n+        \/\/ code for aligned copy.\n+        \/\/ The compare above (count >= 23) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n+\n+        copy_16_bytes_forward_with_shift(from, to, count, 0, L_copy_byte);\n+      }\n@@ -1149,1 +1164,1 @@\n-    \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n+      \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n@@ -1152,1 +1167,1 @@\n-     generate_disjoint_long_copy_core(aligned);\n+      generate_disjoint_long_copy_core(aligned);\n@@ -1155,10 +1170,11 @@\n-    \/\/ copy tailing bytes\n-    __ BIND(L_copy_byte);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_byte_loop);\n-      __ ldub(from, offset, O3);\n-      __ deccc(count);\n-      __ stb(O3, to, offset);\n-      __ brx(Assembler::notZero, false, Assembler::pt, L_copy_byte_loop);\n-      __ delayed()->inc(offset);\n+      \/\/ copy tailing bytes\n+      __ BIND(L_copy_byte);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_byte_loop);\n+        __ ldub(from, offset, O3);\n+        __ deccc(count);\n+        __ stb(O3, to, offset);\n+        __ brx(Assembler::notZero, false, Assembler::pt, L_copy_byte_loop);\n+        __ delayed()->inc(offset);\n+    }\n@@ -1210,1 +1226,3 @@\n-    __ add(to, count, end_to);       \/\/ offset after last copied element\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -1212,4 +1230,1 @@\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 23); \/\/ 16 + 7\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n-    __ delayed()->add(from, count, end_from);\n+      __ add(to, count, end_to);       \/\/ offset after last copied element\n@@ -1217,3 +1232,4 @@\n-    {\n-      \/\/ Align end of arrays since they could be not aligned even\n-      \/\/ when arrays itself are aligned.\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 23); \/\/ 16 + 7\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n+      __ delayed()->add(from, count, end_from);\n@@ -1221,25 +1237,3 @@\n-      \/\/ copy bytes to align 'end_to' on 8 byte boundary\n-      __ andcc(end_to, 7, G1); \/\/ misaligned bytes\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->nop();\n-      __ sub(count, G1, count);\n-    __ BIND(L_align);\n-      __ dec(end_from);\n-      __ dec(end_to);\n-      __ ldub(end_from, 0, O3);\n-      __ deccc(G1);\n-      __ brx(Assembler::notZero, false, Assembler::pt, L_align);\n-      __ delayed()->stb(O3, end_to, 0);\n-    __ BIND(L_skip_alignment);\n-    }\n-    if (aligned) {\n-      \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n-      \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n-      \/\/ in unaligned case.\n-      __ dec(count, 16);\n-    } else {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise jump to the next\n-      \/\/ code for aligned copy (and substracting 16 from 'count' before jump).\n-      \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      {\n+        \/\/ Align end of arrays since they could be not aligned even\n+        \/\/ when arrays itself are aligned.\n@@ -1247,2 +1241,53 @@\n-      copy_16_bytes_backward_with_shift(end_from, end_to, count, 16,\n-                                        L_aligned_copy, L_copy_byte);\n+        \/\/ copy bytes to align 'end_to' on 8 byte boundary\n+        __ andcc(end_to, 7, G1); \/\/ misaligned bytes\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->nop();\n+        __ sub(count, G1, count);\n+      __ BIND(L_align);\n+        __ dec(end_from);\n+        __ dec(end_to);\n+        __ ldub(end_from, 0, O3);\n+        __ deccc(G1);\n+        __ brx(Assembler::notZero, false, Assembler::pt, L_align);\n+        __ delayed()->stb(O3, end_to, 0);\n+      __ BIND(L_skip_alignment);\n+      }\n+      if (aligned) {\n+        \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n+        \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n+        \/\/ in unaligned case.\n+        __ dec(count, 16);\n+      } else {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise jump to the next\n+        \/\/ code for aligned copy (and substracting 16 from 'count' before jump).\n+        \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n+\n+       copy_16_bytes_backward_with_shift(end_from, end_to, count, 16,\n+                                          L_aligned_copy, L_copy_byte);\n+      }\n+      \/\/ copy 4 elements (16 bytes) at a time\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_aligned_copy);\n+        __ dec(end_from, 16);\n+        __ ldx(end_from, 8, O3);\n+        __ ldx(end_from, 0, O4);\n+        __ dec(end_to, 16);\n+        __ deccc(count, 16);\n+        __ stx(O3, end_to, 8);\n+        __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n+        __ delayed()->stx(O4, end_to, 0);\n+        __ inc(count, 16);\n+\n+      \/\/ copy 1 element (2 bytes) at a time\n+      __ BIND(L_copy_byte);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_byte_loop);\n+        __ dec(end_from);\n+        __ dec(end_to);\n+        __ ldub(end_from, 0, O4);\n+        __ deccc(count);\n+        __ brx(Assembler::greater, false, Assembler::pt, L_copy_byte_loop);\n+        __ delayed()->stb(O4, end_to, 0);\n@@ -1250,24 +1295,0 @@\n-    \/\/ copy 4 elements (16 bytes) at a time\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_aligned_copy);\n-      __ dec(end_from, 16);\n-      __ ldx(end_from, 8, O3);\n-      __ ldx(end_from, 0, O4);\n-      __ dec(end_to, 16);\n-      __ deccc(count, 16);\n-      __ stx(O3, end_to, 8);\n-      __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n-      __ delayed()->stx(O4, end_to, 0);\n-      __ inc(count, 16);\n-\n-    \/\/ copy 1 element (2 bytes) at a time\n-    __ BIND(L_copy_byte);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_byte_loop);\n-      __ dec(end_from);\n-      __ dec(end_to);\n-      __ ldub(end_from, 0, O4);\n-      __ deccc(count);\n-      __ brx(Assembler::greater, false, Assembler::pt, L_copy_byte_loop);\n-      __ delayed()->stb(O4, end_to, 0);\n@@ -1314,4 +1335,7 @@\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n-    __ delayed()->mov(G0, offset);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n+      __ delayed()->mov(G0, offset);\n@@ -1319,18 +1343,40 @@\n-    if (aligned) {\n-      \/\/ 'aligned' == true when it is known statically during compilation\n-      \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n-      \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n-      \/\/\n-      \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n-      \/\/ and 8 bytes - in 64-bits VM.\n-      \/\/\n-    } else {\n-      \/\/ copy 1 element if necessary to align 'to' on an 4 bytes\n-      __ andcc(to, 3, G0);\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->lduh(from, 0, O3);\n-      __ inc(from, 2);\n-      __ inc(to, 2);\n-      __ dec(count);\n-      __ sth(O3, to, -2);\n-    __ BIND(L_skip_alignment);\n+      if (aligned) {\n+        \/\/ 'aligned' == true when it is known statically during compilation\n+        \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n+        \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n+        \/\/\n+        \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n+        \/\/ and 8 bytes - in 64-bits VM.\n+        \/\/\n+      } else {\n+        \/\/ copy 1 element if necessary to align 'to' on an 4 bytes\n+        __ andcc(to, 3, G0);\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->lduh(from, 0, O3);\n+        __ inc(from, 2);\n+        __ inc(to, 2);\n+        __ dec(count);\n+        __ sth(O3, to, -2);\n+      __ BIND(L_skip_alignment);\n+\n+        \/\/ copy 2 elements to align 'to' on an 8 byte boundary\n+        __ andcc(to, 7, G0);\n+        __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n+        __ delayed()->lduh(from, 0, O3);\n+        __ dec(count, 2);\n+        __ lduh(from, 2, O4);\n+        __ inc(from, 4);\n+        __ inc(to, 4);\n+        __ sth(O3, to, -4);\n+        __ sth(O4, to, -2);\n+      __ BIND(L_skip_alignment2);\n+      }\n+      if (!aligned) {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise fall through to the next\n+        \/\/ code for aligned copy.\n+        \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n+\n+        copy_16_bytes_forward_with_shift(from, to, count, 1, L_copy_2_bytes);\n+      }\n@@ -1338,18 +1384,5 @@\n-      \/\/ copy 2 elements to align 'to' on an 8 byte boundary\n-      __ andcc(to, 7, G0);\n-      __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n-      __ delayed()->lduh(from, 0, O3);\n-      __ dec(count, 2);\n-      __ lduh(from, 2, O4);\n-      __ inc(from, 4);\n-      __ inc(to, 4);\n-      __ sth(O3, to, -4);\n-      __ sth(O4, to, -2);\n-    __ BIND(L_skip_alignment2);\n-    }\n-    if (!aligned) {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise fall through to the next\n-      \/\/ code for aligned copy.\n-      \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n+        __ and3(count, 3, G4); \/\/ Save\n+        __ srl(count, 2, count);\n+       generate_disjoint_long_copy_core(aligned);\n+        __ mov(G4, count); \/\/ restore\n@@ -1357,1 +1390,10 @@\n-      copy_16_bytes_forward_with_shift(from, to, count, 1, L_copy_2_bytes);\n+      \/\/ copy 1 element at a time\n+      __ BIND(L_copy_2_bytes);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_2_bytes_loop);\n+        __ lduh(from, offset, O3);\n+        __ deccc(count);\n+        __ sth(O3, to, offset);\n+        __ brx(Assembler::notZero, false, Assembler::pt, L_copy_2_bytes_loop);\n+        __ delayed()->inc(offset, 2);\n@@ -1360,17 +1402,0 @@\n-    \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n-      __ and3(count, 3, G4); \/\/ Save\n-      __ srl(count, 2, count);\n-     generate_disjoint_long_copy_core(aligned);\n-      __ mov(G4, count); \/\/ restore\n-\n-    \/\/ copy 1 element at a time\n-    __ BIND(L_copy_2_bytes);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_2_bytes_loop);\n-      __ lduh(from, offset, O3);\n-      __ deccc(count);\n-      __ sth(O3, to, offset);\n-      __ brx(Assembler::notZero, false, Assembler::pt, L_copy_2_bytes_loop);\n-      __ delayed()->inc(offset, 2);\n-\n@@ -1642,8 +1667,0 @@\n-    __ sllx(count, LogBytesPerShort, byte_count);\n-    __ add(to, byte_count, end_to);  \/\/ offset after last copied element\n-\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n-    __ delayed()->add(from, byte_count, end_from);\n-\n@@ -1651,2 +1668,2 @@\n-      \/\/ Align end of arrays since they could be not aligned even\n-      \/\/ when arrays itself are aligned.\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -1654,9 +1671,2 @@\n-      \/\/ copy 1 element if necessary to align 'end_to' on an 4 bytes\n-      __ andcc(end_to, 3, G0);\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->lduh(end_from, -2, O3);\n-      __ dec(end_from, 2);\n-      __ dec(end_to, 2);\n-      __ dec(count);\n-      __ sth(O3, end_to, 0);\n-    __ BIND(L_skip_alignment);\n+      __ sllx(count, LogBytesPerShort, byte_count);\n+      __ add(to, byte_count, end_to);  \/\/ offset after last copied element\n@@ -1664,23 +1674,42 @@\n-      \/\/ copy 2 elements to align 'end_to' on an 8 byte boundary\n-      __ andcc(end_to, 7, G0);\n-      __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n-      __ delayed()->lduh(end_from, -2, O3);\n-      __ dec(count, 2);\n-      __ lduh(end_from, -4, O4);\n-      __ dec(end_from, 4);\n-      __ dec(end_to, 4);\n-      __ sth(O3, end_to, 2);\n-      __ sth(O4, end_to, 0);\n-    __ BIND(L_skip_alignment2);\n-    }\n-    if (aligned) {\n-      \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n-      \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n-      \/\/ in unaligned case.\n-      __ dec(count, 8);\n-    } else {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise jump to the next\n-      \/\/ code for aligned copy (and substracting 8 from 'count' before jump).\n-      \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n+      __ delayed()->add(from, byte_count, end_from);\n+\n+      {\n+        \/\/ Align end of arrays since they could be not aligned even\n+        \/\/ when arrays itself are aligned.\n+\n+        \/\/ copy 1 element if necessary to align 'end_to' on an 4 bytes\n+        __ andcc(end_to, 3, G0);\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->lduh(end_from, -2, O3);\n+        __ dec(end_from, 2);\n+        __ dec(end_to, 2);\n+        __ dec(count);\n+        __ sth(O3, end_to, 0);\n+      __ BIND(L_skip_alignment);\n+\n+        \/\/ copy 2 elements to align 'end_to' on an 8 byte boundary\n+        __ andcc(end_to, 7, G0);\n+        __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n+        __ delayed()->lduh(end_from, -2, O3);\n+        __ dec(count, 2);\n+        __ lduh(end_from, -4, O4);\n+        __ dec(end_from, 4);\n+        __ dec(end_to, 4);\n+        __ sth(O3, end_to, 2);\n+        __ sth(O4, end_to, 0);\n+      __ BIND(L_skip_alignment2);\n+      }\n+      if (aligned) {\n+        \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n+        \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n+        \/\/ in unaligned case.\n+        __ dec(count, 8);\n+      } else {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise jump to the next\n+        \/\/ code for aligned copy (and substracting 8 from 'count' before jump).\n+        \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n@@ -1688,1 +1717,1 @@\n-      copy_16_bytes_backward_with_shift(end_from, end_to, count, 8,\n+        copy_16_bytes_backward_with_shift(end_from, end_to, count, 8,\n@@ -1690,0 +1719,24 @@\n+      }\n+      \/\/ copy 4 elements (16 bytes) at a time\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_aligned_copy);\n+        __ dec(end_from, 16);\n+        __ ldx(end_from, 8, O3);\n+        __ ldx(end_from, 0, O4);\n+        __ dec(end_to, 16);\n+        __ deccc(count, 8);\n+        __ stx(O3, end_to, 8);\n+        __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n+        __ delayed()->stx(O4, end_to, 0);\n+        __ inc(count, 8);\n+\n+      \/\/ copy 1 element (2 bytes) at a time\n+      __ BIND(L_copy_2_bytes);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+      __ BIND(L_copy_2_bytes_loop);\n+        __ dec(end_from, 2);\n+        __ dec(end_to, 2);\n+        __ lduh(end_from, 0, O4);\n+        __ deccc(count);\n+        __ brx(Assembler::greater, false, Assembler::pt, L_copy_2_bytes_loop);\n+        __ delayed()->sth(O4, end_to, 0);\n@@ -1691,24 +1744,0 @@\n-    \/\/ copy 4 elements (16 bytes) at a time\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_aligned_copy);\n-      __ dec(end_from, 16);\n-      __ ldx(end_from, 8, O3);\n-      __ ldx(end_from, 0, O4);\n-      __ dec(end_to, 16);\n-      __ deccc(count, 8);\n-      __ stx(O3, end_to, 8);\n-      __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n-      __ delayed()->stx(O4, end_to, 0);\n-      __ inc(count, 8);\n-\n-    \/\/ copy 1 element (2 bytes) at a time\n-    __ BIND(L_copy_2_bytes);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-    __ BIND(L_copy_2_bytes_loop);\n-      __ dec(end_from, 2);\n-      __ dec(end_to, 2);\n-      __ lduh(end_from, 0, O4);\n-      __ deccc(count);\n-      __ brx(Assembler::greater, false, Assembler::pt, L_copy_2_bytes_loop);\n-      __ delayed()->sth(O4, end_to, 0);\n-\n@@ -1873,3 +1902,5 @@\n-\n-    generate_disjoint_int_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_disjoint_int_copy_core(aligned);\n+    }\n@@ -2008,3 +2039,5 @@\n-\n-    generate_conjoint_int_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_conjoint_int_copy_core(aligned);\n+    }\n@@ -2159,2 +2192,5 @@\n-    generate_disjoint_long_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, true, false);\n+      generate_disjoint_long_copy_core(aligned);\n+    }\n@@ -2235,3 +2271,5 @@\n-\n-    generate_conjoint_long_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, true, false);\n+      generate_conjoint_long_copy_core(aligned);\n+    }\n@@ -2932,0 +2970,3 @@\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -5824,0 +5865,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -5825,0 +5867,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/sparc\/stubGenerator_sparc.cpp","additions":288,"deletions":243,"binary":false,"changes":531,"status":"modified"},{"patch":"@@ -792,0 +792,2 @@\n+    case 0x6F: \/\/ movdq\n+    case 0x7F: \/\/ movdq\n@@ -4268,0 +4270,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -891,42 +891,27 @@\n-\n-    __ subptr(to, from); \/\/ to --> to_from\n-    __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-    __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n-    if (!UseUnalignedLoadStores && !aligned && (t == T_BYTE || t == T_SHORT)) {\n-      \/\/ align source address at 4 bytes address boundary\n-      if (t == T_BYTE) {\n-        \/\/ One byte misalignment happens only for byte arrays\n-        __ testl(from, 1);\n-        __ jccb(Assembler::zero, L_skip_align1);\n-        __ movb(rax, Address(from, 0));\n-        __ movb(Address(from, to_from, Address::times_1, 0), rax);\n-        __ increment(from);\n-        __ decrement(count);\n-      __ BIND(L_skip_align1);\n-      }\n-      \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n-      __ testl(from, 2);\n-      __ jccb(Assembler::zero, L_skip_align2);\n-      __ movw(rax, Address(from, 0));\n-      __ movw(Address(from, to_from, Address::times_1, 0), rax);\n-      __ addptr(from, 2);\n-      __ subl(count, 1<<(shift-1));\n-    __ BIND(L_skip_align2);\n-    }\n-    if (!VM_Version::supports_mmx()) {\n-      __ mov(rax, count);      \/\/ save 'count'\n-      __ shrl(count, shift); \/\/ bytes count\n-      __ addptr(to_from, from);\/\/ restore 'to'\n-      __ rep_mov();\n-      __ subptr(to_from, from);\/\/ restore 'to_from'\n-      __ mov(count, rax);      \/\/ restore 'count'\n-      __ jmpb(L_copy_2_bytes); \/\/ all dwords were copied\n-    } else {\n-      if (!UseUnalignedLoadStores) {\n-        \/\/ align to 8 bytes, we know we are 4 byte aligned to start\n-        __ testptr(from, 4);\n-        __ jccb(Assembler::zero, L_copy_64_bytes);\n-        __ movl(rax, Address(from, 0));\n-        __ movl(Address(from, to_from, Address::times_1, 0), rax);\n-        __ addptr(from, 4);\n-        __ subl(count, 1<<shift);\n+    {\n+      bool add_entry = (t != T_OBJECT && (!aligned || t == T_INT));\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      __ subptr(to, from); \/\/ to --> to_from\n+      __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n+      __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n+      if (!UseUnalignedLoadStores && !aligned && (t == T_BYTE || t == T_SHORT)) {\n+        \/\/ align source address at 4 bytes address boundary\n+        if (t == T_BYTE) {\n+          \/\/ One byte misalignment happens only for byte arrays\n+          __ testl(from, 1);\n+          __ jccb(Assembler::zero, L_skip_align1);\n+          __ movb(rax, Address(from, 0));\n+          __ movb(Address(from, to_from, Address::times_1, 0), rax);\n+          __ increment(from);\n+          __ decrement(count);\n+        __ BIND(L_skip_align1);\n+        }\n+        \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n+        __ testl(from, 2);\n+        __ jccb(Assembler::zero, L_skip_align2);\n+        __ movw(rax, Address(from, 0));\n+        __ movw(Address(from, to_from, Address::times_1, 0), rax);\n+        __ addptr(from, 2);\n+        __ subl(count, 1<<(shift-1));\n+      __ BIND(L_skip_align2);\n@@ -934,8 +919,8 @@\n-    __ BIND(L_copy_64_bytes);\n-      __ mov(rax, count);\n-      __ shrl(rax, shift+1);  \/\/ 8 bytes chunk count\n-      \/\/\n-      \/\/ Copy 8-byte chunks through MMX registers, 8 per iteration of the loop\n-      \/\/\n-      if (UseXMMForArrayCopy) {\n-        xmm_copy_forward(from, to_from, rax);\n+      if (!VM_Version::supports_mmx()) {\n+        __ mov(rax, count);      \/\/ save 'count'\n+        __ shrl(count, shift); \/\/ bytes count\n+        __ addptr(to_from, from);\/\/ restore 'to'\n+        __ rep_mov();\n+        __ subptr(to_from, from);\/\/ restore 'to_from'\n+        __ mov(count, rax);      \/\/ restore 'count'\n+        __ jmpb(L_copy_2_bytes); \/\/ all dwords were copied\n@@ -943,1 +928,20 @@\n-        mmx_copy_forward(from, to_from, rax);\n+        if (!UseUnalignedLoadStores) {\n+          \/\/ align to 8 bytes, we know we are 4 byte aligned to start\n+          __ testptr(from, 4);\n+          __ jccb(Assembler::zero, L_copy_64_bytes);\n+          __ movl(rax, Address(from, 0));\n+          __ movl(Address(from, to_from, Address::times_1, 0), rax);\n+          __ addptr(from, 4);\n+          __ subl(count, 1<<shift);\n+         }\n+      __ BIND(L_copy_64_bytes);\n+        __ mov(rax, count);\n+        __ shrl(rax, shift+1);  \/\/ 8 bytes chunk count\n+        \/\/\n+        \/\/ Copy 8-byte chunks through MMX registers, 8 per iteration of the loop\n+        \/\/\n+        if (UseXMMForArrayCopy) {\n+          xmm_copy_forward(from, to_from, rax);\n+        } else {\n+          mmx_copy_forward(from, to_from, rax);\n+        }\n@@ -945,24 +949,26 @@\n-    }\n-    \/\/ copy tailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(count, 1<<shift);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(from, 0));\n-    __ movl(Address(from, to_from, Address::times_1, 0), rax);\n-    if (t == T_BYTE || t == T_SHORT) {\n-      __ addptr(from, 4);\n-    __ BIND(L_copy_2_bytes);\n-      \/\/ copy tailing word\n-      __ testl(count, 1<<(shift-1));\n-      __ jccb(Assembler::zero, L_copy_byte);\n-      __ movw(rax, Address(from, 0));\n-      __ movw(Address(from, to_from, Address::times_1, 0), rax);\n-      if (t == T_BYTE) {\n-        __ addptr(from, 2);\n-      __ BIND(L_copy_byte);\n-        \/\/ copy tailing byte\n-        __ testl(count, 1);\n-        __ jccb(Assembler::zero, L_exit);\n-        __ movb(rax, Address(from, 0));\n-        __ movb(Address(from, to_from, Address::times_1, 0), rax);\n-      __ BIND(L_exit);\n+      \/\/ copy tailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(count, 1<<shift);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rax, Address(from, 0));\n+      __ movl(Address(from, to_from, Address::times_1, 0), rax);\n+      if (t == T_BYTE || t == T_SHORT) {\n+        __ addptr(from, 4);\n+      __ BIND(L_copy_2_bytes);\n+        \/\/ copy tailing word\n+        __ testl(count, 1<<(shift-1));\n+        __ jccb(Assembler::zero, L_copy_byte);\n+        __ movw(rax, Address(from, 0));\n+        __ movw(Address(from, to_from, Address::times_1, 0), rax);\n+        if (t == T_BYTE) {\n+          __ addptr(from, 2);\n+        __ BIND(L_copy_byte);\n+          \/\/ copy tailing byte\n+          __ testl(count, 1);\n+          __ jccb(Assembler::zero, L_exit);\n+          __ movb(rax, Address(from, 0));\n+          __ movb(Address(from, to_from, Address::times_1, 0), rax);\n+        __ BIND(L_exit);\n+        } else {\n+        __ BIND(L_copy_byte);\n+        }\n@@ -970,1 +976,1 @@\n-      __ BIND(L_copy_byte);\n+      __ BIND(L_copy_2_bytes);\n@@ -972,2 +978,0 @@\n-    } else {\n-    __ BIND(L_copy_2_bytes);\n@@ -976,0 +980,3 @@\n+    if (VM_Version::supports_mmx() && !UseXMMForArrayCopy) {\n+      __ emms();\n+    }\n@@ -1081,22 +1088,5 @@\n-    \/\/ copy from high to low\n-    __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-    __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n-    if (t == T_BYTE || t == T_SHORT) {\n-      \/\/ Align the end of destination array at 4 bytes address boundary\n-      __ lea(end, Address(dst, count, sf, 0));\n-      if (t == T_BYTE) {\n-        \/\/ One byte misalignment happens only for byte arrays\n-        __ testl(end, 1);\n-        __ jccb(Assembler::zero, L_skip_align1);\n-        __ decrement(count);\n-        __ movb(rdx, Address(from, count, sf, 0));\n-        __ movb(Address(to, count, sf, 0), rdx);\n-      __ BIND(L_skip_align1);\n-      }\n-      \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n-      __ testl(end, 2);\n-      __ jccb(Assembler::zero, L_skip_align2);\n-      __ subptr(count, 1<<(shift-1));\n-      __ movw(rdx, Address(from, count, sf, 0));\n-      __ movw(Address(to, count, sf, 0), rdx);\n-    __ BIND(L_skip_align2);\n+    {\n+      bool add_entry = (t != T_OBJECT && (!aligned || t == T_INT));\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      \/\/ copy from high to low\n@@ -1104,25 +1094,23 @@\n-      __ jcc(Assembler::below, L_copy_4_bytes);\n-    }\n-\n-    if (!VM_Version::supports_mmx()) {\n-      __ std();\n-      __ mov(rax, count); \/\/ Save 'count'\n-      __ mov(rdx, to);    \/\/ Save 'to'\n-      __ lea(rsi, Address(from, count, sf, -4));\n-      __ lea(rdi, Address(to  , count, sf, -4));\n-      __ shrptr(count, shift); \/\/ bytes count\n-      __ rep_mov();\n-      __ cld();\n-      __ mov(count, rax); \/\/ restore 'count'\n-      __ andl(count, (1<<shift)-1);      \/\/ mask the number of rest elements\n-      __ movptr(from, Address(rsp, 12+4)); \/\/ reread 'from'\n-      __ mov(to, rdx);   \/\/ restore 'to'\n-      __ jmpb(L_copy_2_bytes); \/\/ all dword were copied\n-   } else {\n-      \/\/ Align to 8 bytes the end of array. It is aligned to 4 bytes already.\n-      __ testptr(end, 4);\n-      __ jccb(Assembler::zero, L_copy_8_bytes);\n-      __ subl(count, 1<<shift);\n-      __ movl(rdx, Address(from, count, sf, 0));\n-      __ movl(Address(to, count, sf, 0), rdx);\n-      __ jmpb(L_copy_8_bytes);\n+      __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n+      if (t == T_BYTE || t == T_SHORT) {\n+        \/\/ Align the end of destination array at 4 bytes address boundary\n+        __ lea(end, Address(dst, count, sf, 0));\n+        if (t == T_BYTE) {\n+          \/\/ One byte misalignment happens only for byte arrays\n+          __ testl(end, 1);\n+          __ jccb(Assembler::zero, L_skip_align1);\n+          __ decrement(count);\n+          __ movb(rdx, Address(from, count, sf, 0));\n+          __ movb(Address(to, count, sf, 0), rdx);\n+        __ BIND(L_skip_align1);\n+        }\n+        \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n+        __ testl(end, 2);\n+        __ jccb(Assembler::zero, L_skip_align2);\n+        __ subptr(count, 1<<(shift-1));\n+        __ movw(rdx, Address(from, count, sf, 0));\n+        __ movw(Address(to, count, sf, 0), rdx);\n+      __ BIND(L_skip_align2);\n+        __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n+        __ jcc(Assembler::below, L_copy_4_bytes);\n+      }\n@@ -1130,6 +1118,14 @@\n-      __ align(OptoLoopAlignment);\n-      \/\/ Move 8 bytes\n-    __ BIND(L_copy_8_bytes_loop);\n-      if (UseXMMForArrayCopy) {\n-        __ movq(xmm0, Address(from, count, sf, 0));\n-        __ movq(Address(to, count, sf, 0), xmm0);\n+      if (!VM_Version::supports_mmx()) {\n+        __ std();\n+        __ mov(rax, count); \/\/ Save 'count'\n+        __ mov(rdx, to);    \/\/ Save 'to'\n+        __ lea(rsi, Address(from, count, sf, -4));\n+        __ lea(rdi, Address(to  , count, sf, -4));\n+        __ shrptr(count, shift); \/\/ bytes count\n+        __ rep_mov();\n+        __ cld();\n+        __ mov(count, rax); \/\/ restore 'count'\n+        __ andl(count, (1<<shift)-1);      \/\/ mask the number of rest elements\n+        __ movptr(from, Address(rsp, 12+4)); \/\/ reread 'from'\n+        __ mov(to, rdx);   \/\/ restore 'to'\n+        __ jmpb(L_copy_2_bytes); \/\/ all dword were copied\n@@ -1137,2 +1133,25 @@\n-        __ movq(mmx0, Address(from, count, sf, 0));\n-        __ movq(Address(to, count, sf, 0), mmx0);\n+        \/\/ Align to 8 bytes the end of array. It is aligned to 4 bytes already.\n+        __ testptr(end, 4);\n+        __ jccb(Assembler::zero, L_copy_8_bytes);\n+        __ subl(count, 1<<shift);\n+        __ movl(rdx, Address(from, count, sf, 0));\n+        __ movl(Address(to, count, sf, 0), rdx);\n+        __ jmpb(L_copy_8_bytes);\n+\n+        __ align(OptoLoopAlignment);\n+        \/\/ Move 8 bytes\n+      __ BIND(L_copy_8_bytes_loop);\n+        if (UseXMMForArrayCopy) {\n+          __ movq(xmm0, Address(from, count, sf, 0));\n+          __ movq(Address(to, count, sf, 0), xmm0);\n+        } else {\n+          __ movq(mmx0, Address(from, count, sf, 0));\n+          __ movq(Address(to, count, sf, 0), mmx0);\n+        }\n+      __ BIND(L_copy_8_bytes);\n+        __ subl(count, 2<<shift);\n+        __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n+        __ addl(count, 2<<shift);\n+        if (!UseXMMForArrayCopy) {\n+          __ emms();\n+        }\n@@ -1140,6 +1159,29 @@\n-    __ BIND(L_copy_8_bytes);\n-      __ subl(count, 2<<shift);\n-      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n-      __ addl(count, 2<<shift);\n-      if (!UseXMMForArrayCopy) {\n-        __ emms();\n+    __ BIND(L_copy_4_bytes);\n+      \/\/ copy prefix qword\n+      __ testl(count, 1<<shift);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rdx, Address(from, count, sf, -4));\n+      __ movl(Address(to, count, sf, -4), rdx);\n+\n+      if (t == T_BYTE || t == T_SHORT) {\n+          __ subl(count, (1<<shift));\n+        __ BIND(L_copy_2_bytes);\n+          \/\/ copy prefix dword\n+          __ testl(count, 1<<(shift-1));\n+          __ jccb(Assembler::zero, L_copy_byte);\n+          __ movw(rdx, Address(from, count, sf, -2));\n+          __ movw(Address(to, count, sf, -2), rdx);\n+          if (t == T_BYTE) {\n+            __ subl(count, 1<<(shift-1));\n+          __ BIND(L_copy_byte);\n+            \/\/ copy prefix byte\n+            __ testl(count, 1);\n+            __ jccb(Assembler::zero, L_exit);\n+            __ movb(rdx, Address(from, 0));\n+            __ movb(Address(to, 0), rdx);\n+          __ BIND(L_exit);\n+          } else {\n+          __ BIND(L_copy_byte);\n+          }\n+      } else {\n+      __ BIND(L_copy_2_bytes);\n@@ -1148,6 +1190,0 @@\n-  __ BIND(L_copy_4_bytes);\n-    \/\/ copy prefix qword\n-    __ testl(count, 1<<shift);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rdx, Address(from, count, sf, -4));\n-    __ movl(Address(to, count, sf, -4), rdx);\n@@ -1155,22 +1191,2 @@\n-    if (t == T_BYTE || t == T_SHORT) {\n-        __ subl(count, (1<<shift));\n-      __ BIND(L_copy_2_bytes);\n-        \/\/ copy prefix dword\n-        __ testl(count, 1<<(shift-1));\n-        __ jccb(Assembler::zero, L_copy_byte);\n-        __ movw(rdx, Address(from, count, sf, -2));\n-        __ movw(Address(to, count, sf, -2), rdx);\n-        if (t == T_BYTE) {\n-          __ subl(count, 1<<(shift-1));\n-        __ BIND(L_copy_byte);\n-          \/\/ copy prefix byte\n-          __ testl(count, 1);\n-          __ jccb(Assembler::zero, L_exit);\n-          __ movb(rdx, Address(from, 0));\n-          __ movb(Address(to, 0), rdx);\n-        __ BIND(L_exit);\n-        } else {\n-        __ BIND(L_copy_byte);\n-        }\n-    } else {\n-    __ BIND(L_copy_2_bytes);\n+    if (VM_Version::supports_mmx() && !UseXMMForArrayCopy) {\n+      __ emms();\n@@ -1178,1 +1194,0 @@\n-\n@@ -1214,4 +1229,10 @@\n-    __ subptr(to, from); \/\/ to --> to_from\n-    if (VM_Version::supports_mmx()) {\n-      if (UseXMMForArrayCopy) {\n-        xmm_copy_forward(from, to_from, count);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, true, true);\n+      __ subptr(to, from); \/\/ to --> to_from\n+      if (VM_Version::supports_mmx()) {\n+        if (UseXMMForArrayCopy) {\n+          xmm_copy_forward(from, to_from, count);\n+        } else {\n+          mmx_copy_forward(from, to_from, count);\n+        }\n@@ -1219,1 +1240,9 @@\n-        mmx_copy_forward(from, to_from, count);\n+        __ jmpb(L_copy_8_bytes);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_8_bytes_loop);\n+        __ fild_d(Address(from, 0));\n+        __ fistp_d(Address(from, to_from, Address::times_1));\n+        __ addptr(from, 8);\n+      __ BIND(L_copy_8_bytes);\n+        __ decrement(count);\n+        __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n@@ -1221,10 +1250,3 @@\n-    } else {\n-      __ jmpb(L_copy_8_bytes);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_8_bytes_loop);\n-      __ fild_d(Address(from, 0));\n-      __ fistp_d(Address(from, to_from, Address::times_1));\n-      __ addptr(from, 8);\n-    __ BIND(L_copy_8_bytes);\n-      __ decrement(count);\n-      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n+    }\n+    if (VM_Version::supports_mmx() && !UseXMMForArrayCopy) {\n+      __ emms();\n@@ -1269,1 +1291,3 @@\n-    __ jmpb(L_copy_8_bytes);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, true, true);\n@@ -1271,6 +1295,12 @@\n-    __ align(OptoLoopAlignment);\n-  __ BIND(L_copy_8_bytes_loop);\n-    if (VM_Version::supports_mmx()) {\n-      if (UseXMMForArrayCopy) {\n-        __ movq(xmm0, Address(from, count, Address::times_8));\n-        __ movq(Address(to, count, Address::times_8), xmm0);\n+      __ jmpb(L_copy_8_bytes);\n+\n+      __ align(OptoLoopAlignment);\n+    __ BIND(L_copy_8_bytes_loop);\n+      if (VM_Version::supports_mmx()) {\n+        if (UseXMMForArrayCopy) {\n+          __ movq(xmm0, Address(from, count, Address::times_8));\n+          __ movq(Address(to, count, Address::times_8), xmm0);\n+        } else {\n+          __ movq(mmx0, Address(from, count, Address::times_8));\n+          __ movq(Address(to, count, Address::times_8), mmx0);\n+        }\n@@ -1278,2 +1308,2 @@\n-        __ movq(mmx0, Address(from, count, Address::times_8));\n-        __ movq(Address(to, count, Address::times_8), mmx0);\n+        __ fild_d(Address(from, count, Address::times_8));\n+        __ fistp_d(Address(to, count, Address::times_8));\n@@ -1281,7 +1311,3 @@\n-    } else {\n-      __ fild_d(Address(from, count, Address::times_8));\n-      __ fistp_d(Address(to, count, Address::times_8));\n-    }\n-  __ BIND(L_copy_8_bytes);\n-    __ decrement(count);\n-    __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n+    __ BIND(L_copy_8_bytes);\n+      __ decrement(count);\n+      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n@@ -1289,0 +1315,1 @@\n+    }\n@@ -3949,1 +3976,1 @@\n-\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -3951,0 +3978,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":230,"deletions":200,"binary":false,"changes":430,"status":"modified"},{"patch":"@@ -1454,1 +1454,0 @@\n-\n@@ -1503,44 +1502,47 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(byte_count, count);\n-    __ shrptr(count, 3); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count); \/\/ make the count negative\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(byte_count, 4);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 4);\n-    __ addptr(end_to, 4);\n-\n-    \/\/ Check for and copy trailing word\n-  __ BIND(L_copy_2_bytes);\n-    __ testl(byte_count, 2);\n-    __ jccb(Assembler::zero, L_copy_byte);\n-    __ movw(rax, Address(end_from, 8));\n-    __ movw(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 2);\n-    __ addptr(end_to, 2);\n-\n-    \/\/ Check for and copy trailing byte\n-  __ BIND(L_copy_byte);\n-    __ testl(byte_count, 1);\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movb(rax, Address(end_from, 8));\n-    __ movb(Address(end_to, 8), rax);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(byte_count, count);\n+      __ shrptr(count, 3); \/\/ count => qword_count\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count); \/\/ make the count negative\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(byte_count, 4);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rax, Address(end_from, 8));\n+      __ movl(Address(end_to, 8), rax);\n+\n+      __ addptr(end_from, 4);\n+      __ addptr(end_to, 4);\n+\n+      \/\/ Check for and copy trailing word\n+    __ BIND(L_copy_2_bytes);\n+      __ testl(byte_count, 2);\n+      __ jccb(Assembler::zero, L_copy_byte);\n+      __ movw(rax, Address(end_from, 8));\n+      __ movw(Address(end_to, 8), rax);\n+\n+      __ addptr(end_from, 2);\n+      __ addptr(end_to, 2);\n+\n+      \/\/ Check for and copy trailing byte\n+    __ BIND(L_copy_byte);\n+      __ testl(byte_count, 1);\n+      __ jccb(Assembler::zero, L_exit);\n+      __ movb(rax, Address(end_from, 8));\n+      __ movb(Address(end_to, 8), rax);\n+    }\n@@ -1548,0 +1550,1 @@\n+    address ucme_exit_pc = __ pc();\n@@ -1555,4 +1558,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n-\n+    {\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+      __ jmp(L_copy_4_bytes);\n+    }\n@@ -1603,35 +1608,38 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(byte_count, count);\n-    __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.\n-\n-    \/\/ Check for and copy trailing byte\n-    __ testl(byte_count, 1);\n-    __ jcc(Assembler::zero, L_copy_2_bytes);\n-    __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-    __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-    __ decrement(byte_count); \/\/ Adjust for possible trailing word\n-\n-    \/\/ Check for and copy trailing word\n-  __ BIND(L_copy_2_bytes);\n-    __ testl(byte_count, 2);\n-    __ jcc(Assembler::zero, L_copy_4_bytes);\n-    __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-    __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(byte_count, 4);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, qword_count, Address::times_8));\n-    __ movl(Address(to, qword_count, Address::times_8), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(byte_count, count);\n+      __ shrptr(count, 3);   \/\/ count => qword_count\n+\n+      \/\/ Copy from high to low addresses.\n+\n+      \/\/ Check for and copy trailing byte\n+      __ testl(byte_count, 1);\n+      __ jcc(Assembler::zero, L_copy_2_bytes);\n+      __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n+      __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n+      __ decrement(byte_count); \/\/ Adjust for possible trailing word\n+\n+      \/\/ Check for and copy trailing word\n+    __ BIND(L_copy_2_bytes);\n+      __ testl(byte_count, 2);\n+      __ jcc(Assembler::zero, L_copy_4_bytes);\n+      __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n+      __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(byte_count, 4);\n+      __ jcc(Assembler::zero, L_copy_bytes);\n+      __ movl(rax, Address(from, qword_count, Address::times_8));\n+      __ movl(Address(to, qword_count, Address::times_8), rax);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -1645,3 +1653,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -1705,37 +1716,40 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(word_count, count);\n-    __ shrptr(count, 2); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Original 'dest' is trashed, so we can't use it as a\n-    \/\/ base register for a possible trailing word copy\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(word_count, 2);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 4);\n-    __ addptr(end_to, 4);\n-\n-    \/\/ Check for and copy trailing word\n-  __ BIND(L_copy_2_bytes);\n-    __ testl(word_count, 1);\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movw(rax, Address(end_from, 8));\n-    __ movw(Address(end_to, 8), rax);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(word_count, count);\n+      __ shrptr(count, 2); \/\/ count => qword_count\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+      \/\/ Original 'dest' is trashed, so we can't use it as a\n+      \/\/ base register for a possible trailing word copy\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(word_count, 2);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rax, Address(end_from, 8));\n+      __ movl(Address(end_to, 8), rax);\n+\n+      __ addptr(end_from, 4);\n+      __ addptr(end_to, 4);\n+\n+      \/\/ Check for and copy trailing word\n+    __ BIND(L_copy_2_bytes);\n+      __ testl(word_count, 1);\n+      __ jccb(Assembler::zero, L_exit);\n+      __ movw(rax, Address(end_from, 8));\n+      __ movw(Address(end_to, 8), rax);\n+    }\n@@ -1743,0 +1757,1 @@\n+    address ucme_exit_pc = __ pc();\n@@ -1750,3 +1765,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n+    {\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+      __ jmp(L_copy_4_bytes);\n+    }\n@@ -1819,27 +1837,30 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(word_count, count);\n-    __ shrptr(count, 2); \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-    \/\/ Check for and copy trailing word\n-    __ testl(word_count, 1);\n-    __ jccb(Assembler::zero, L_copy_4_bytes);\n-    __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-    __ movw(Address(to, word_count, Address::times_2, -2), rax);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(word_count, 2);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, qword_count, Address::times_8));\n-    __ movl(Address(to, qword_count, Address::times_8), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(word_count, count);\n+      __ shrptr(count, 2); \/\/ count => qword_count\n+\n+      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+      \/\/ Check for and copy trailing word\n+      __ testl(word_count, 1);\n+      __ jccb(Assembler::zero, L_copy_4_bytes);\n+      __ movw(rax, Address(from, word_count, Address::times_2, -2));\n+      __ movw(Address(to, word_count, Address::times_2, -2), rax);\n+\n+     \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(word_count, 2);\n+      __ jcc(Assembler::zero, L_copy_bytes);\n+      __ movl(rax, Address(from, qword_count, Address::times_8));\n+      __ movl(Address(to, qword_count, Address::times_8), rax);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -1853,3 +1874,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -1926,24 +1950,27 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(dword_count, count);\n-    __ shrptr(count, 1); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(dword_count, count);\n+      __ shrptr(count, 1); \/\/ count => qword_count\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n+      __ jccb(Assembler::zero, L_exit);\n+      __ movl(rax, Address(end_from, 8));\n+      __ movl(Address(end_to, 8), rax);\n+    }\n@@ -1951,0 +1978,1 @@\n+    address ucme_exit_pc = __ pc();\n@@ -1959,3 +1987,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n+    {\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+      __ jmp(L_copy_4_bytes);\n+    }\n@@ -2022,20 +2053,23 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(dword_count, count);\n-    __ shrptr(count, 1); \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-    \/\/ Check for and copy trailing dword\n-    __ testl(dword_count, 1);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-    __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(dword_count, count);\n+      __ shrptr(count, 1); \/\/ count => qword_count\n+\n+      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+      \/\/ Check for and copy trailing dword\n+      __ testl(dword_count, 1);\n+      __ jcc(Assembler::zero, L_copy_bytes);\n+      __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n+      __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -2052,2 +2086,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -2123,14 +2161,17 @@\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -2148,2 +2189,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -2216,0 +2261,3 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -2217,8 +2265,1 @@\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+      __ jmp(L_copy_bytes);\n@@ -2226,0 +2267,7 @@\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -2236,0 +2284,3 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -2237,3 +2288,3 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -6124,0 +6175,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 16\n@@ -6125,0 +6177,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":288,"deletions":233,"binary":false,"changes":521,"status":"modified"},{"patch":"@@ -2586,1 +2586,5 @@\n-      if ((thread->thread_state() == _thread_in_vm &&\n+\n+      bool is_unsafe_arraycopy = (thread->thread_state() == _thread_in_native || in_java) && UnsafeCopyMemory::contains_pc(pc);\n+      if (((thread->thread_state() == _thread_in_vm ||\n+           thread->thread_state() == _thread_in_native ||\n+           is_unsafe_arraycopy) &&\n@@ -2589,1 +2593,5 @@\n-        return Handle_Exception(exceptionInfo, SharedRuntime::handle_unsafe_access(thread, (address)Assembler::locate_next_instruction(pc)));\n+        address next_pc =  Assembler::locate_next_instruction(pc);\n+        if (is_unsafe_arraycopy) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n+        return Handle_Exception(exceptionInfo, SharedRuntime::handle_unsafe_access(thread, next_pc));\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -444,1 +444,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -446,0 +447,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -464,1 +468,2 @@\n-      else if (thread->thread_state() == _thread_in_vm &&\n+      else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -467,0 +472,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/os_aix_ppc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -586,1 +586,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -588,0 +589,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -662,1 +666,2 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -666,0 +671,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -196,1 +196,2 @@\n-    else*\/ if (thread->thread_state() == _thread_in_vm &&\n+    else*\/ if ((thread->thread_state() == _thread_in_vm ||\n+               thread->thread_state() == _thread_in_native) &&\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -376,1 +376,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -378,0 +379,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -396,1 +400,2 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                 thread->thread_state() == _thread_in_native) &&\n@@ -400,0 +405,3 @@\n+      if (UnsafeCopyMemory::contains_pc(pc)) {\n+        next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+      }\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -402,1 +402,1 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        if ((nm != NULL && nm->has_unsafe_access()) || (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc))) {\n@@ -415,1 +415,2 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -445,0 +446,3 @@\n+    if (UnsafeCopyMemory::contains_pc(pc)) {\n+      next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+    }\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -473,1 +473,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -475,0 +476,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -489,1 +493,2 @@\n-      else if (thread->thread_state() == _thread_in_vm &&\n+      else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -492,0 +497,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n@@ -493,1 +501,1 @@\n-        os::Linux::ucontext_set_pc(uc, pc + 4);\n+        os::Linux::ucontext_set_pc(uc, next_pc);\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/os_linux_ppc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -473,1 +473,2 @@\n-      } else if (thread->thread_state() == _thread_in_vm &&\n+      } else if ((thread->thread_state() == _thread_in_vm ||\n+                  thread->thread_state() == _thread_in_native) &&\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/os_linux_s390.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -388,1 +388,5 @@\n-  if (nm != NULL && nm->has_unsafe_access()) {\n+  bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+  if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n+    if (is_unsafe_arraycopy) {\n+      npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+    }\n@@ -568,1 +572,2 @@\n-        thread->thread_state() == _thread_in_vm &&\n+        (thread->thread_state() == _thread_in_vm ||\n+         thread->thread_state() == _thread_in_native) &&\n@@ -570,0 +575,3 @@\n+      if (UnsafeCopyMemory::contains_pc(pc)) {\n+        npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+      }\n","filename":"src\/hotspot\/os_cpu\/linux_sparc\/os_linux_sparc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -444,1 +444,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -446,0 +447,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -492,3 +496,4 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n-               sig == SIGBUS && \/* info->si_code == BUS_OBJERR && *\/\n-               thread->doing_unsafe_access()) {\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n+               (sig == SIGBUS && \/* info->si_code == BUS_OBJERR && *\/\n+               thread->doing_unsafe_access())) {\n@@ -496,0 +501,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -210,1 +210,2 @@\n-    else*\/ if (thread->thread_state() == _thread_in_vm &&\n+    else*\/ if ((thread->thread_state() == _thread_in_vm ||\n+               thread->thread_state() == _thread_in_native) &&\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -442,1 +442,2 @@\n-    if (thread->thread_state() == _thread_in_vm) {\n+    if (thread->thread_state() == _thread_in_vm ||\n+        thread->thread_state() == _thread_in_native) {\n@@ -444,0 +445,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n@@ -482,1 +486,5 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n+          if (is_unsafe_arraycopy) {\n+            npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n","filename":"src\/hotspot\/os_cpu\/solaris_sparc\/os_solaris_sparc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -529,1 +529,2 @@\n-    if (thread->thread_state() == _thread_in_vm) {\n+    if (thread->thread_state() == _thread_in_vm ||\n+         thread->thread_state() == _thread_in_native) {\n@@ -532,0 +533,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n@@ -548,1 +552,2 @@\n-          if (nm != NULL && nm->has_unsafe_access()) {\n+          bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n+          if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy)) {\n@@ -550,0 +555,3 @@\n+            if (is_unsafe_arraycopy) {\n+              next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+            }\n","filename":"src\/hotspot\/os_cpu\/solaris_x86\/os_solaris_x86.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -4247,0 +4247,8 @@\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* doing_unsafe_access_addr = basic_plus_adr(top(), thread, in_bytes(JavaThread::doing_unsafe_access_offset()));\n+  BasicType doing_unsafe_access_bt = T_BYTE;\n+  assert((sizeof(bool) * CHAR_BIT) == 8, \"not implemented\");\n+\n+  \/\/ update volatile field\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(1), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n@@ -4255,0 +4263,2 @@\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(0), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -149,0 +149,19 @@\n+\/**\n+ * Helper class to wrap memory accesses in JavaThread::doing_unsafe_access()\n+ *\/\n+class GuardUnsafeAccess {\n+  JavaThread* _thread;\n+\n+public:\n+  GuardUnsafeAccess(JavaThread* thread) : _thread(thread) {\n+    \/\/ native\/off-heap access which may raise SIGBUS if accessing\n+    \/\/ memory mapped file data in a region of the file which has\n+    \/\/ been truncated and is now invalid.\n+    _thread->set_doing_unsafe_access(true);\n+  }\n+\n+  ~GuardUnsafeAccess() {\n+    _thread->set_doing_unsafe_access(false);\n+  }\n+};\n+\n@@ -190,19 +209,0 @@\n-  \/**\n-   * Helper class to wrap memory accesses in JavaThread::doing_unsafe_access()\n-   *\/\n-  class GuardUnsafeAccess {\n-    JavaThread* _thread;\n-\n-  public:\n-    GuardUnsafeAccess(JavaThread* thread) : _thread(thread) {\n-      \/\/ native\/off-heap access which may raise SIGBUS if accessing\n-      \/\/ memory mapped file data in a region of the file which has\n-      \/\/ been truncated and is now invalid\n-      _thread->set_doing_unsafe_access(true);\n-    }\n-\n-    ~GuardUnsafeAccess() {\n-      _thread->set_doing_unsafe_access(false);\n-    }\n-  };\n-\n@@ -412,2 +412,8 @@\n-\n-  Copy::conjoint_memory_atomic(src, dst, sz);\n+  {\n+    GuardUnsafeAccess guard(thread);\n+    if (StubRoutines::unsafe_arraycopy() != NULL) {\n+      StubRoutines::UnsafeArrayCopy_stub()(src, dst, sz);\n+    } else {\n+      Copy::conjoint_memory_atomic(src, dst, sz);\n+    }\n+  }\n@@ -429,1 +435,5 @@\n-    Copy::conjoint_swap(src, dst, sz, esz);\n+    {\n+      JavaThread* thread = JavaThread::thread_from_jni_environment(env);\n+      GuardUnsafeAccess guard(thread);\n+      Copy::conjoint_swap(src, dst, sz, esz);\n+    }\n@@ -440,1 +450,4 @@\n-      Copy::conjoint_swap(src, dst, sz, esz);\n+      {\n+        GuardUnsafeAccess guard(thread);\n+        Copy::conjoint_swap(src, dst, sz, esz);\n+      }\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":36,"deletions":23,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -41,0 +41,4 @@\n+UnsafeCopyMemory* UnsafeCopyMemory::_table                      = NULL;\n+int UnsafeCopyMemory::_table_length                             = 0;\n+int UnsafeCopyMemory::_table_max_length                         = 0;\n+address UnsafeCopyMemory::_common_exit_stub_pc                  = NULL;\n@@ -117,1 +121,0 @@\n-\n@@ -188,0 +191,25 @@\n+void UnsafeCopyMemory::create_table(int max_size) {\n+  UnsafeCopyMemory::_table = new UnsafeCopyMemory[max_size];\n+  UnsafeCopyMemory::_table_max_length = max_size;\n+}\n+\n+bool UnsafeCopyMemory::contains_pc(address pc) {\n+  for (int i = 0; i < UnsafeCopyMemory::_table_length; i++) {\n+    UnsafeCopyMemory* entry = &UnsafeCopyMemory::_table[i];\n+    if (pc >= entry->start_pc() && pc < entry->end_pc()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+address UnsafeCopyMemory::page_error_continue_pc(address pc) {\n+  for (int i = 0; i < UnsafeCopyMemory::_table_length; i++) {\n+    UnsafeCopyMemory* entry = &UnsafeCopyMemory::_table[i];\n+    if (pc >= entry->start_pc() && pc < entry->end_pc()) {\n+      return entry->error_exit_pc();\n+    }\n+  }\n+  return NULL;\n+}\n+\n@@ -580,0 +608,22 @@\n+\n+UnsafeCopyMemoryMark::UnsafeCopyMemoryMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc) {\n+  _cgen = cgen;\n+  _ucm_entry = NULL;\n+  if (add_entry) {\n+    address err_exit_pc = NULL;\n+    if (!continue_at_scope_end) {\n+      err_exit_pc = error_exit_pc != NULL ? error_exit_pc : UnsafeCopyMemory::common_exit_stub_pc();\n+    }\n+    assert(err_exit_pc != NULL || continue_at_scope_end, \"error exit not set\");\n+    _ucm_entry = UnsafeCopyMemory::add_to_table(_cgen->assembler()->pc(), NULL, err_exit_pc);\n+  }\n+}\n+\n+UnsafeCopyMemoryMark::~UnsafeCopyMemoryMark() {\n+  if (_ucm_entry != NULL) {\n+    _ucm_entry->set_end_pc(_cgen->assembler()->pc());\n+    if (_ucm_entry->error_exit_pc() == NULL) {\n+      _ucm_entry->set_error_exit_pc(_cgen->assembler()->pc());\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -77,0 +77,45 @@\n+class UnsafeCopyMemory : public CHeapObj<mtCode> {\n+ private:\n+  address _start_pc;\n+  address _end_pc;\n+  address _error_exit_pc;\n+ public:\n+  static address           _common_exit_stub_pc;\n+  static UnsafeCopyMemory* _table;\n+  static int               _table_length;\n+  static int               _table_max_length;\n+  UnsafeCopyMemory() : _start_pc(NULL), _end_pc(NULL), _error_exit_pc(NULL) {}\n+  void    set_start_pc(address pc)      { _start_pc = pc; }\n+  void    set_end_pc(address pc)        { _end_pc = pc; }\n+  void    set_error_exit_pc(address pc) { _error_exit_pc = pc; }\n+  address start_pc()      const { return _start_pc; }\n+  address end_pc()        const { return _end_pc; }\n+  address error_exit_pc() const { return _error_exit_pc; }\n+\n+  static void    set_common_exit_stub_pc(address pc) { _common_exit_stub_pc = pc; }\n+  static address common_exit_stub_pc()               { return _common_exit_stub_pc; }\n+\n+  static UnsafeCopyMemory* add_to_table(address start_pc, address end_pc, address error_exit_pc) {\n+    guarantee(_table_length < _table_max_length, \"Incorrect UnsafeCopyMemory::_table_max_length\");\n+    UnsafeCopyMemory* entry = &_table[_table_length];\n+    entry->set_start_pc(start_pc);\n+    entry->set_end_pc(end_pc);\n+    entry->set_error_exit_pc(error_exit_pc);\n+\n+    _table_length++;\n+    return entry;\n+  }\n+\n+  static bool    contains_pc(address pc);\n+  static address page_error_continue_pc(address pc);\n+  static void    create_table(int max_size);\n+};\n+\n+class UnsafeCopyMemoryMark : public StackObj {\n+ private:\n+  UnsafeCopyMemory*  _ucm_entry;\n+  StubCodeGenerator* _cgen;\n+ public:\n+  UnsafeCopyMemoryMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc = NULL);\n+  ~UnsafeCopyMemoryMark();\n+};\n@@ -333,1 +378,0 @@\n-\n@@ -337,1 +381,5 @@\n-  static address unsafe_arraycopy()    { return _unsafe_arraycopy; }\n+  static address unsafe_arraycopy()     { return _unsafe_arraycopy; }\n+\n+  typedef void (*UnsafeArrayCopyStub)(const void* src, void* dst, size_t count);\n+  static UnsafeArrayCopyStub UnsafeArrayCopy_stub()         { return CAST_TO_FN_PTR(UnsafeArrayCopyStub,  _unsafe_arraycopy); }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":50,"deletions":2,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -1737,0 +1737,1 @@\n+  static ByteSize doing_unsafe_access_offset() { return byte_offset_of(JavaThread, _doing_unsafe_access); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,155 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8191278\n+ * @requires os.family != \"windows\"\n+ * @summary Check that SIGBUS errors caused by memory accesses in Unsafe_CopyMemory()\n+ * and UnsafeCopySwapMemory() get converted to java.lang.InternalError exceptions.\n+ * @modules java.base\/jdk.internal.misc\n+ *\n+ * @library \/test\/lib\n+ * @build sun.hotspot.WhiteBox\n+ * @run main ClassFileInstaller sun.hotspot.WhiteBox\n+ *      sun.hotspot.WhiteBox$WhiteBoxPermission\n+ *\n+ * @run main\/othervm -XX:CompileCommand=exclude,*InternalErrorTest.main -XX:CompileCommand=inline,*.get -XX:CompileCommand=inline,*Unsafe.* -Xbootclasspath\/a:.  -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI InternalErrorTest\n+ *\/\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.lang.reflect.Field;\n+import java.lang.reflect.Method;\n+import java.nio.MappedByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import jdk.internal.misc.Unsafe;\n+import sun.hotspot.WhiteBox;\n+\n+\/\/ Test that illegal memory access errors in Unsafe_CopyMemory0() and\n+\/\/ UnsafeCopySwapMemory() that cause SIGBUS errors result in\n+\/\/ java.lang.InternalError exceptions, not JVM crashes.\n+public class InternalErrorTest {\n+\n+    private static final Unsafe unsafe = Unsafe.getUnsafe();\n+    private static final int pageSize = WhiteBox.getWhiteBox().getVMPageSize();\n+    private static final String expectedErrorMsg = \"fault occurred in a recent unsafe memory access\";\n+    private static final String failureMsg1 = \"InternalError not thrown\";\n+    private static final String failureMsg2 = \"Wrong InternalError: \";\n+\n+    public static void main(String[] args) throws Throwable {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+\n+        String currentDir = System.getProperty(\"test.classes\");\n+        File file = new File(currentDir, \"tmpFile.txt\");\n+\n+        StringBuilder s = new StringBuilder();\n+        for (int i = 1; i < pageSize + 1000; i++) {\n+            s.append(\"1\");\n+        }\n+        Files.write(file.toPath(), s.toString().getBytes());\n+        FileChannel fileChannel = new RandomAccessFile(file, \"r\").getChannel();\n+        MappedByteBuffer buffer =\n+            fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size());\n+\n+        \/\/ Get address of mapped memory.\n+        long mapAddr = 0;\n+        try {\n+            Field af = java.nio.Buffer.class.getDeclaredField(\"address\");\n+            af.setAccessible(true);\n+            mapAddr = af.getLong(buffer);\n+        } catch (Exception f) {\n+            throw f;\n+        }\n+        long allocMem = unsafe.allocateMemory(4000);\n+\n+        for (int i = 0; i < 3; i++) {\n+            test(buffer, unsafe, mapAddr, allocMem, i);\n+        }\n+\n+        Files.write(file.toPath(), \"2\".getBytes());\n+        buffer.position(buffer.position() + pageSize);\n+        for (int i = 0; i < 3; i++) {\n+            try {\n+                test(buffer, unsafe, mapAddr, allocMem, i);\n+                WhiteBox.getWhiteBox().forceSafepoint();\n+                throw new RuntimeException(failureMsg1);\n+            } catch (InternalError e) {\n+                if (!e.getMessage().contains(expectedErrorMsg)) {\n+                    throw new RuntimeException(failureMsg2 + e.getMessage());\n+                }\n+            }\n+        }\n+\n+        Method m = InternalErrorTest.class.getMethod(\"test\", MappedByteBuffer.class, Unsafe.class, long.class, long.class, int.class);\n+        WhiteBox.getWhiteBox().enqueueMethodForCompilation(m, 3);\n+\n+        for (int i = 0; i < 3; i++) {\n+            try {\n+                test(buffer, unsafe, mapAddr, allocMem, i);\n+                WhiteBox.getWhiteBox().forceSafepoint();\n+                throw new RuntimeException(failureMsg1);\n+            } catch (InternalError e) {\n+                if (!e.getMessage().contains(expectedErrorMsg)) {\n+                    throw new RuntimeException(failureMsg2 + e.getMessage());\n+                }\n+            }\n+        }\n+\n+        WhiteBox.getWhiteBox().enqueueMethodForCompilation(m, 4);\n+\n+        for (int i = 0; i < 3; i++) {\n+            try {\n+                test(buffer, unsafe, mapAddr, allocMem, i);\n+                WhiteBox.getWhiteBox().forceSafepoint();\n+                throw new RuntimeException(failureMsg1);\n+            } catch (InternalError e) {\n+                if (!e.getMessage().contains(expectedErrorMsg)) {\n+                    throw new RuntimeException(failureMsg2 + e.getMessage());\n+                }\n+            }\n+        }\n+\n+        System.out.println(\"Success\");\n+    }\n+\n+    public static void test(MappedByteBuffer buffer, Unsafe unsafe, long mapAddr, long allocMem, int type) {\n+        switch (type) {\n+            case 0:\n+                \/\/ testing Unsafe.copyMemory, trying to access a word from next page after truncation.\n+                buffer.get(new byte[8]);\n+                break;\n+            case 1:\n+                \/\/ testing Unsafe.copySwapMemory, trying to access next  page after truncation.\n+                unsafe.copySwapMemory(null, mapAddr + pageSize, new byte[4000], 16, 2000, 2);\n+                break;\n+            case 2:\n+                \/\/ testing Unsafe.copySwapMemory, trying to access next  page after truncation.\n+                unsafe.copySwapMemory(null, mapAddr + pageSize, null, allocMem, 2000, 2);\n+                break;\n+        }\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/Unsafe\/InternalErrorTest.java","additions":155,"deletions":0,"binary":false,"changes":155,"status":"added"}]}