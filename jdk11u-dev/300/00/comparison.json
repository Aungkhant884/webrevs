{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -430,31 +430,0 @@\n-\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Support for int32_t Atomic::xchg(int32_t exchange_value, volatile int32_t* dest)\n-  \/\/\n-  \/\/ xchg exists as far back as 8086, lock needed for MP only\n-  \/\/ Stack layout immediately after call:\n-  \/\/\n-  \/\/ 0 [ret addr ] <--- rsp\n-  \/\/ 1 [  ex     ]\n-  \/\/ 2 [  dest   ]\n-  \/\/\n-  \/\/ Result:   *dest <- ex, return (old *dest)\n-  \/\/\n-  \/\/ Note: win32 does not currently use this code\n-\n-  address generate_atomic_xchg() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_xchg\");\n-    address start = __ pc();\n-\n-    __ push(rdx);\n-    Address exchange(rsp, 2 * wordSize);\n-    Address dest_addr(rsp, 3 * wordSize);\n-    __ movl(rax, exchange);\n-    __ movptr(rdx, dest_addr);\n-    __ xchgl(rax, Address(rdx, 0));\n-    __ pop(rdx);\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n@@ -3793,3 +3762,0 @@\n-    \/\/ These are currently used by Solaris\/Intel\n-    StubRoutines::_atomic_xchg_entry            = generate_atomic_xchg();\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":1,"deletions":35,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -553,160 +553,0 @@\n-  \/\/ Support for jint atomic::xchg(jint exchange_value, volatile jint* dest)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: exchange_value\n-  \/\/    c_rarg0: dest\n-  \/\/\n-  \/\/ Result:\n-  \/\/    *dest <- ex, return (orig *dest)\n-  address generate_atomic_xchg() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_xchg\");\n-    address start = __ pc();\n-\n-    __ movl(rax, c_rarg0); \/\/ Copy to eax we need a return value anyhow\n-    __ xchgl(rax, Address(c_rarg1, 0)); \/\/ automatic LOCK\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Support for intptr_t atomic::xchg_long(jlong exchange_value, volatile jlong* dest)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: exchange_value\n-  \/\/    c_rarg1: dest\n-  \/\/\n-  \/\/ Result:\n-  \/\/    *dest <- ex, return (orig *dest)\n-  address generate_atomic_xchg_long() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_xchg_long\");\n-    address start = __ pc();\n-\n-    __ movptr(rax, c_rarg0); \/\/ Copy to eax we need a return value anyhow\n-    __ xchgptr(rax, Address(c_rarg1, 0)); \/\/ automatic LOCK\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Support for jint atomic::atomic_cmpxchg(jint exchange_value, volatile jint* dest,\n-  \/\/                                         jint compare_value)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: exchange_value\n-  \/\/    c_rarg1: dest\n-  \/\/    c_rarg2: compare_value\n-  \/\/\n-  \/\/ Result:\n-  \/\/    if ( compare_value == *dest ) {\n-  \/\/       *dest = exchange_value\n-  \/\/       return compare_value;\n-  \/\/    else\n-  \/\/       return *dest;\n-  address generate_atomic_cmpxchg() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_cmpxchg\");\n-    address start = __ pc();\n-\n-    __ movl(rax, c_rarg2);\n-   if ( os::is_MP() ) __ lock();\n-    __ cmpxchgl(c_rarg0, Address(c_rarg1, 0));\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Support for int8_t atomic::atomic_cmpxchg(int8_t exchange_value, volatile int8_t* dest,\n-  \/\/                                           int8_t compare_value)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: exchange_value\n-  \/\/    c_rarg1: dest\n-  \/\/    c_rarg2: compare_value\n-  \/\/\n-  \/\/ Result:\n-  \/\/    if ( compare_value == *dest ) {\n-  \/\/       *dest = exchange_value\n-  \/\/       return compare_value;\n-  \/\/    else\n-  \/\/       return *dest;\n-  address generate_atomic_cmpxchg_byte() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_cmpxchg_byte\");\n-    address start = __ pc();\n-\n-    __ movsbq(rax, c_rarg2);\n-   if ( os::is_MP() ) __ lock();\n-    __ cmpxchgb(c_rarg0, Address(c_rarg1, 0));\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Support for int64_t atomic::atomic_cmpxchg(int64_t exchange_value,\n-  \/\/                                            volatile int64_t* dest,\n-  \/\/                                            int64_t compare_value)\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: exchange_value\n-  \/\/    c_rarg1: dest\n-  \/\/    c_rarg2: compare_value\n-  \/\/\n-  \/\/ Result:\n-  \/\/    if ( compare_value == *dest ) {\n-  \/\/       *dest = exchange_value\n-  \/\/       return compare_value;\n-  \/\/    else\n-  \/\/       return *dest;\n-  address generate_atomic_cmpxchg_long() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_cmpxchg_long\");\n-    address start = __ pc();\n-\n-    __ movq(rax, c_rarg2);\n-   if ( os::is_MP() ) __ lock();\n-    __ cmpxchgq(c_rarg0, Address(c_rarg1, 0));\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Support for jint atomic::add(jint add_value, volatile jint* dest)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: add_value\n-  \/\/    c_rarg1: dest\n-  \/\/\n-  \/\/ Result:\n-  \/\/    *dest += add_value\n-  \/\/    return *dest;\n-  address generate_atomic_add() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_add\");\n-    address start = __ pc();\n-\n-    __ movl(rax, c_rarg0);\n-   if ( os::is_MP() ) __ lock();\n-    __ xaddl(Address(c_rarg1, 0), c_rarg0);\n-    __ addl(rax, c_rarg0);\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Support for intptr_t atomic::add_ptr(intptr_t add_value, volatile intptr_t* dest)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/    c_rarg0: add_value\n-  \/\/    c_rarg1: dest\n-  \/\/\n-  \/\/ Result:\n-  \/\/    *dest += add_value\n-  \/\/    return *dest;\n-  address generate_atomic_add_long() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_add_long\");\n-    address start = __ pc();\n-\n-    __ movptr(rax, c_rarg0); \/\/ Copy to eax we need a return value anyhow\n-   if ( os::is_MP() ) __ lock();\n-    __ xaddptr(Address(c_rarg1, 0), c_rarg0);\n-    __ addptr(rax, c_rarg0);\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n@@ -5879,7 +5719,0 @@\n-    StubRoutines::_atomic_xchg_entry          = generate_atomic_xchg();\n-    StubRoutines::_atomic_xchg_long_entry     = generate_atomic_xchg_long();\n-    StubRoutines::_atomic_cmpxchg_entry       = generate_atomic_cmpxchg();\n-    StubRoutines::_atomic_cmpxchg_byte_entry  = generate_atomic_cmpxchg_byte();\n-    StubRoutines::_atomic_cmpxchg_long_entry  = generate_atomic_cmpxchg_long();\n-    StubRoutines::_atomic_add_entry           = generate_atomic_add();\n-    StubRoutines::_atomic_add_long_entry      = generate_atomic_add_long();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":168,"binary":false,"changes":169,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include <intrin.h>\n@@ -30,15 +31,0 @@\n-\/\/ The following alternative implementations are needed because\n-\/\/ Windows 95 doesn't support (some of) the corresponding Windows NT\n-\/\/ calls. Furthermore, these versions allow inlining in the caller.\n-\/\/ (More precisely: The documentation for InterlockedExchange says\n-\/\/ it is supported for Windows 95. However, when single-stepping\n-\/\/ through the assembly code we cannot step into the routine and\n-\/\/ when looking at the routine address we see only garbage code.\n-\/\/ Better safe then sorry!). Was bug 7\/31\/98 (gri).\n-\/\/\n-\/\/ Performance note: On uniprocessors, the 'lock' prefixes are not\n-\/\/ necessary (and expensive). We should generate separate cases if\n-\/\/ this becomes a performance problem.\n-\n-#pragma warning(disable: 4035) \/\/ Disables warnings reporting missing return statement\n-\n@@ -53,23 +39,13 @@\n-#ifdef AMD64\n-template<>\n-template<typename I, typename D>\n-inline D Atomic::PlatformAdd<4>::add_and_fetch(I add_value, D volatile* dest,\n-                                               atomic_memory_order order) const {\n-  return add_using_helper<int32_t>(os::atomic_add_func, add_value, dest);\n-}\n-\n-template<>\n-template<typename I, typename D>\n-inline D Atomic::PlatformAdd<8>::add_and_fetch(I add_value, D volatile* dest,\n-                                               atomic_memory_order order) const {\n-  return add_using_helper<int64_t>(os::atomic_add_long_func, add_value, dest);\n-}\n-\n-#define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \\\n-  template<>                                                            \\\n-  template<typename T>                                                  \\\n-  inline T Atomic::PlatformXchg<ByteSize>::operator()(T exchange_value, \\\n-                                                      T volatile* dest, \\\n-                                                      atomic_memory_order order) const { \\\n-    STATIC_ASSERT(ByteSize == sizeof(T));                               \\\n-    return xchg_using_helper<StubType>(StubName, exchange_value, dest); \\\n+\/\/ The Interlocked* APIs only take long and will not accept __int32. That is\n+\/\/ acceptable on Windows, since long is a 32-bits integer type.\n+\n+#define DEFINE_INTRINSIC_ADD(IntrinsicName, IntrinsicType)                \\\n+  template<>                                                              \\\n+  template<typename I, typename D>                                        \\\n+  inline D Atomic::PlatformAdd<sizeof(IntrinsicType)>::add_and_fetch(I add_value, \\\n+                                                                     D volatile* dest, \\\n+                                                                     atomic_memory_order order) const { \\\n+    STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(D));                    \\\n+    return PrimitiveConversions::cast<D>(                                 \\\n+      IntrinsicName(reinterpret_cast<IntrinsicType volatile *>(dest),     \\\n+                    PrimitiveConversions::cast<IntrinsicType>(add_value))); \\\n@@ -78,14 +54,15 @@\n-DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)\n-DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)\n-\n-#undef DEFINE_STUB_XCHG\n-\n-#define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)               \\\n-  template<>                                                            \\\n-  template<typename T>                                                  \\\n-  inline T Atomic::PlatformCmpxchg<ByteSize>::operator()(T exchange_value, \\\n-                                                         T volatile* dest, \\\n-                                                         T compare_value, \\\n-                                                         atomic_memory_order order) const { \\\n-    STATIC_ASSERT(ByteSize == sizeof(T));                               \\\n-    return cmpxchg_using_helper<StubType>(StubName, exchange_value, dest, compare_value); \\\n+DEFINE_INTRINSIC_ADD(InterlockedAdd, long)\n+DEFINE_INTRINSIC_ADD(InterlockedAdd64, __int64)\n+\n+#undef DEFINE_INTRINSIC_ADD\n+\n+#define DEFINE_INTRINSIC_XCHG(IntrinsicName, IntrinsicType)               \\\n+  template<>                                                              \\\n+  template<typename T>                                                    \\\n+  inline T Atomic::PlatformXchg<sizeof(IntrinsicType)>::operator()(T exchange_value, \\\n+                                                                   T volatile* dest, \\\n+                                                                   atomic_memory_order order) const { \\\n+    STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \\\n+    return PrimitiveConversions::cast<T>(                                 \\\n+      IntrinsicName(reinterpret_cast<IntrinsicType volatile *>(dest),     \\\n+                    PrimitiveConversions::cast<IntrinsicType>(exchange_value))); \\\n@@ -94,20 +71,21 @@\n-DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)\n-DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)\n-DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)\n-\n-#undef DEFINE_STUB_CMPXCHG\n-\n-#else \/\/ !AMD64\n-\n-template<>\n-template<typename I, typename D>\n-inline D Atomic::PlatformAdd<4>::add_and_fetch(I add_value, D volatile* dest,\n-                                               atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(I));\n-  STATIC_ASSERT(4 == sizeof(D));\n-  __asm {\n-    mov edx, dest;\n-    mov eax, add_value;\n-    mov ecx, eax;\n-    lock xadd dword ptr [edx], eax;\n-    add eax, ecx;\n+DEFINE_INTRINSIC_XCHG(InterlockedExchange, long)\n+DEFINE_INTRINSIC_XCHG(InterlockedExchange64, __int64)\n+\n+#undef DEFINE_INTRINSIC_XCHG\n+\n+\/\/ Note: the order of the parameters is different between\n+\/\/ Atomic::PlatformCmpxchg<*>::operator() and the\n+\/\/ InterlockedCompareExchange* API.\n+\n+#define DEFINE_INTRINSIC_CMPXCHG(IntrinsicName, IntrinsicType)            \\\n+  template<>                                                              \\\n+  template<typename T>                                                    \\\n+  inline T Atomic::PlatformCmpxchg<sizeof(IntrinsicType)>::operator()(T exchange_value, \\\n+                                                                      T volatile* dest, \\\n+                                                                      T compare_value, \\\n+                                                                      atomic_memory_order order) const { \\\n+    STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \\\n+    return PrimitiveConversions::cast<T>(                                 \\\n+      IntrinsicName(reinterpret_cast<IntrinsicType volatile *>(dest),     \\\n+                    PrimitiveConversions::cast<IntrinsicType>(exchange_value), \\\n+                    PrimitiveConversions::cast<IntrinsicType>(compare_value))); \\\n@@ -115,1 +93,0 @@\n-}\n@@ -117,13 +94,3 @@\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformXchg<4>::operator()(T exchange_value,\n-                                             T volatile* dest,\n-                                             atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(T));\n-  \/\/ alternative for InterlockedExchange\n-  __asm {\n-    mov eax, exchange_value;\n-    mov ecx, dest;\n-    xchg eax, dword ptr [ecx];\n-  }\n-}\n+DEFINE_INTRINSIC_CMPXCHG(_InterlockedCompareExchange8, char) \/\/ Use the intrinsic as InterlockedCompareExchange8 does not exist\n+DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange, long)\n+DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange64, __int64)\n@@ -131,15 +98,1 @@\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T exchange_value,\n-                                                T volatile* dest,\n-                                                T compare_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(1 == sizeof(T));\n-  \/\/ alternative for InterlockedCompareExchange\n-  __asm {\n-    mov edx, dest\n-    mov cl, exchange_value\n-    mov al, compare_value\n-    lock cmpxchg byte ptr [edx], cl\n-  }\n-}\n+#undef DEFINE_INTRINSIC_CMPXCHG\n@@ -147,15 +100,1 @@\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T exchange_value,\n-                                                T volatile* dest,\n-                                                T compare_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(T));\n-  \/\/ alternative for InterlockedCompareExchange\n-  __asm {\n-    mov edx, dest\n-    mov ecx, exchange_value\n-    mov eax, compare_value\n-    lock cmpxchg dword ptr [edx], ecx\n-  }\n-}\n+#ifndef AMD64\n@@ -163,24 +102,1 @@\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T exchange_value,\n-                                                T volatile* dest,\n-                                                T compare_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  int32_t ex_lo  = (int32_t)exchange_value;\n-  int32_t ex_hi  = *( ((int32_t*)&exchange_value) + 1 );\n-  int32_t cmp_lo = (int32_t)compare_value;\n-  int32_t cmp_hi = *( ((int32_t*)&compare_value) + 1 );\n-  __asm {\n-    push ebx\n-    push edi\n-    mov eax, cmp_lo\n-    mov edx, cmp_hi\n-    mov edi, dest\n-    mov ebx, ex_lo\n-    mov ecx, ex_hi\n-    lock cmpxchg8b qword ptr [edi]\n-    pop edi\n-    pop ebx\n-  }\n-}\n+#pragma warning(disable: 4035) \/\/ Disables warnings reporting missing return statement\n@@ -217,2 +133,0 @@\n-#endif \/\/ AMD64\n-\n@@ -221,0 +135,2 @@\n+#endif \/\/ AMD64\n+\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/atomic_windows_x86.hpp","additions":59,"deletions":143,"binary":false,"changes":202,"status":"modified"},{"patch":"@@ -216,132 +216,0 @@\n-\/\/ Atomics and Stub Functions\n-\n-typedef int32_t   xchg_func_t            (int32_t,  volatile int32_t*);\n-typedef int64_t   xchg_long_func_t       (int64_t,  volatile int64_t*);\n-typedef int32_t   cmpxchg_func_t         (int32_t,  volatile int32_t*, int32_t);\n-typedef int8_t    cmpxchg_byte_func_t    (int8_t,   volatile int8_t*,  int8_t);\n-typedef int64_t   cmpxchg_long_func_t    (int64_t,  volatile int64_t*, int64_t);\n-typedef int32_t   add_func_t             (int32_t,  volatile int32_t*);\n-typedef int64_t   add_long_func_t        (int64_t,  volatile int64_t*);\n-\n-#ifdef AMD64\n-\n-int32_t os::atomic_xchg_bootstrap(int32_t exchange_value, volatile int32_t* dest) {\n-  \/\/ try to use the stub:\n-  xchg_func_t* func = CAST_TO_FN_PTR(xchg_func_t*, StubRoutines::atomic_xchg_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_xchg_func = func;\n-    return (*func)(exchange_value, dest);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  int32_t old_value = *dest;\n-  *dest = exchange_value;\n-  return old_value;\n-}\n-\n-int64_t os::atomic_xchg_long_bootstrap(int64_t exchange_value, volatile int64_t* dest) {\n-  \/\/ try to use the stub:\n-  xchg_long_func_t* func = CAST_TO_FN_PTR(xchg_long_func_t*, StubRoutines::atomic_xchg_long_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_xchg_long_func = func;\n-    return (*func)(exchange_value, dest);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  int64_t old_value = *dest;\n-  *dest = exchange_value;\n-  return old_value;\n-}\n-\n-\n-int32_t os::atomic_cmpxchg_bootstrap(int32_t exchange_value, volatile int32_t* dest, int32_t compare_value) {\n-  \/\/ try to use the stub:\n-  cmpxchg_func_t* func = CAST_TO_FN_PTR(cmpxchg_func_t*, StubRoutines::atomic_cmpxchg_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_cmpxchg_func = func;\n-    return (*func)(exchange_value, dest, compare_value);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  int32_t old_value = *dest;\n-  if (old_value == compare_value)\n-    *dest = exchange_value;\n-  return old_value;\n-}\n-\n-int8_t os::atomic_cmpxchg_byte_bootstrap(int8_t exchange_value, volatile int8_t* dest, int8_t compare_value) {\n-  \/\/ try to use the stub:\n-  cmpxchg_byte_func_t* func = CAST_TO_FN_PTR(cmpxchg_byte_func_t*, StubRoutines::atomic_cmpxchg_byte_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_cmpxchg_byte_func = func;\n-    return (*func)(exchange_value, dest, compare_value);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  int8_t old_value = *dest;\n-  if (old_value == compare_value)\n-    *dest = exchange_value;\n-  return old_value;\n-}\n-\n-#endif \/\/ AMD64\n-\n-int64_t os::atomic_cmpxchg_long_bootstrap(int64_t exchange_value, volatile int64_t* dest, int64_t compare_value) {\n-  \/\/ try to use the stub:\n-  cmpxchg_long_func_t* func = CAST_TO_FN_PTR(cmpxchg_long_func_t*, StubRoutines::atomic_cmpxchg_long_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_cmpxchg_long_func = func;\n-    return (*func)(exchange_value, dest, compare_value);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  int64_t old_value = *dest;\n-  if (old_value == compare_value)\n-    *dest = exchange_value;\n-  return old_value;\n-}\n-\n-#ifdef AMD64\n-\n-int32_t os::atomic_add_bootstrap(int32_t add_value, volatile int32_t* dest) {\n-  \/\/ try to use the stub:\n-  add_func_t* func = CAST_TO_FN_PTR(add_func_t*, StubRoutines::atomic_add_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_add_func = func;\n-    return (*func)(add_value, dest);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  return (*dest) += add_value;\n-}\n-\n-int64_t os::atomic_add_long_bootstrap(int64_t add_value, volatile int64_t* dest) {\n-  \/\/ try to use the stub:\n-  add_long_func_t* func = CAST_TO_FN_PTR(add_long_func_t*, StubRoutines::atomic_add_long_entry());\n-\n-  if (func != NULL) {\n-    os::atomic_add_long_func = func;\n-    return (*func)(add_value, dest);\n-  }\n-  assert(Threads::number_of_threads() == 0, \"for bootstrap only\");\n-\n-  return (*dest) += add_value;\n-}\n-\n-xchg_func_t*         os::atomic_xchg_func         = os::atomic_xchg_bootstrap;\n-xchg_long_func_t*    os::atomic_xchg_long_func    = os::atomic_xchg_long_bootstrap;\n-cmpxchg_func_t*      os::atomic_cmpxchg_func      = os::atomic_cmpxchg_bootstrap;\n-cmpxchg_byte_func_t* os::atomic_cmpxchg_byte_func = os::atomic_cmpxchg_byte_bootstrap;\n-add_func_t*          os::atomic_add_func          = os::atomic_add_bootstrap;\n-add_long_func_t*     os::atomic_add_long_func     = os::atomic_add_long_bootstrap;\n-\n-#endif \/\/ AMD64\n-\n-cmpxchg_long_func_t* os::atomic_cmpxchg_long_func = os::atomic_cmpxchg_long_bootstrap;\n-\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/os_windows_x86.cpp","additions":0,"deletions":132,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,28 +31,0 @@\n-#ifdef AMD64\n-  static int32_t   (*atomic_xchg_func)          (int32_t, volatile int32_t*);\n-  static int64_t   (*atomic_xchg_long_func)     (int64_t, volatile int64_t*);\n-\n-  static int32_t   (*atomic_cmpxchg_func)       (int32_t,  volatile int32_t*, int32_t);\n-  static int8_t    (*atomic_cmpxchg_byte_func)  (int8_t,   volatile int8_t*,  int8_t);\n-  static int64_t   (*atomic_cmpxchg_long_func)  (int64_t,  volatile int64_t*, int64_t);\n-\n-  static int32_t   (*atomic_add_func)           (int32_t,  volatile int32_t*);\n-  static int64_t   (*atomic_add_long_func)      (int64_t,  volatile int64_t*);\n-\n-  static int32_t   atomic_xchg_bootstrap        (int32_t,  volatile int32_t*);\n-  static int64_t   atomic_xchg_long_bootstrap   (int64_t,  volatile int64_t*);\n-\n-  static int32_t   atomic_cmpxchg_bootstrap     (int32_t,  volatile int32_t*, int32_t);\n-  static int8_t    atomic_cmpxchg_byte_bootstrap(int8_t,   volatile int8_t*,  int8_t);\n-#else\n-\n-  static int64_t (*atomic_cmpxchg_long_func)  (int64_t, volatile int64_t*, int64_t);\n-\n-#endif \/\/ AMD64\n-\n-  static int64_t atomic_cmpxchg_long_bootstrap(int64_t, volatile int64_t*, int64_t);\n-\n-#ifdef AMD64\n-  static int32_t  atomic_add_bootstrap         (int32_t,  volatile int32_t*);\n-  static int64_t  atomic_add_long_bootstrap    (int64_t,  volatile int64_t*);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/os_windows_x86.hpp","additions":1,"deletions":29,"binary":false,"changes":30,"status":"modified"}]}