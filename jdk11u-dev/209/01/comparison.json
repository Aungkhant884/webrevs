{"files":[{"patch":"@@ -109,0 +109,11 @@\n+static void run_foreground_task_if_needed(AbstractGangTask* task, uint num_workers,\n+                                          bool add_foreground_work) {\n+  if (add_foreground_work) {\n+    log_develop_trace(gc, workgang)(\"Running work gang: %s task: %s worker: foreground\",\n+      Thread::current()->name(), task->name());\n+    task->work(num_workers);\n+    log_develop_trace(gc, workgang)(\"Finished work gang: %s task: %s worker: foreground \"\n+      \"thread: \" PTR_FORMAT, Thread::current()->name(), task->name(), p2i(Thread::current()));\n+  }\n+}\n+\n@@ -139,1 +150,1 @@\n-  void coordinator_execute_on_workers(AbstractGangTask* task, uint num_workers) {\n+  void coordinator_execute_on_workers(AbstractGangTask* task, uint num_workers, bool add_foreground_work) {\n@@ -147,0 +158,2 @@\n+    run_foreground_task_if_needed(task, num_workers, add_foreground_work);\n+\n@@ -202,1 +215,1 @@\n-  void coordinator_execute_on_workers(AbstractGangTask* task, uint num_workers) {\n+  void coordinator_execute_on_workers(AbstractGangTask* task, uint num_workers, bool add_foreground_work) {\n@@ -211,0 +224,2 @@\n+    run_foreground_task_if_needed(task, num_workers, add_foreground_work);\n+\n@@ -277,1 +292,1 @@\n-void WorkGang::run_task(AbstractGangTask* task, uint num_workers) {\n+void WorkGang::run_task(AbstractGangTask* task, uint num_workers, bool add_foreground_work) {\n@@ -284,1 +299,1 @@\n-  _dispatcher->coordinator_execute_on_workers(task, num_workers);\n+  _dispatcher->coordinator_execute_on_workers(task, num_workers, add_foreground_work);\n","filename":"src\/hotspot\/share\/gc\/shared\/workgroup.cpp","additions":19,"deletions":4,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -93,1 +93,2 @@\n-  virtual void coordinator_execute_on_workers(AbstractGangTask* task, uint num_workers) = 0;\n+  virtual void coordinator_execute_on_workers(AbstractGangTask* task, uint num_workers,\n+                                              bool add_foreground_work) = 0;\n@@ -225,2 +226,3 @@\n-  \/\/ number currently exists.\n-  void run_task(AbstractGangTask* task, uint num_workers);\n+  \/\/ number currently exists. If the add_foreground_work flag is true, the current thread\n+  \/\/ is used to run the task too.\n+  void run_task(AbstractGangTask* task, uint num_workers, bool add_foreground_work = false);\n","filename":"src\/hotspot\/share\/gc\/shared\/workgroup.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -510,0 +510,3 @@\n+  _gzip(\"-gz\", \"If specified, the heap dump is written in gzipped format \"\n+               \"using the given compression level. 1 (recommended) is the fastest, \"\n+               \"9 the strongest compression.\", \"INT\", false, \"1\"),\n@@ -514,0 +517,1 @@\n+  _dcmdparser.add_dcmd_option(&_gzip);\n@@ -518,0 +522,11 @@\n+  jlong level = -1; \/\/ -1 means no compression.\n+\n+  if (_gzip.is_set()) {\n+    level = _gzip.value();\n+\n+    if (level < 1 || level > 9) {\n+      output()->print_cr(\"Compression level out of range (1-9): \" JLONG_FORMAT, level);\n+      return;\n+    }\n+  }\n+\n@@ -522,1 +537,1 @@\n-  int res = dumper.dump(_filename.value(), _overwrite.value());\n+  int res = dumper.dump(_filename.value(), output(), (int) level, _overwrite.value());\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -334,0 +334,1 @@\n+  DCmdArgument<jlong> _gzip;\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/workgroup.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"services\/heapDumperCompression.hpp\"\n@@ -376,1 +378,1 @@\n-\/\/ Supports I\/O operations on a dump file\n+\/\/ Supports I\/O operations for a dump\n@@ -381,2 +383,2 @@\n-    io_buffer_max_size = 8*M,\n-    io_buffer_min_size = 64*K,\n+    io_buffer_max_size = 1*M,\n+    io_buffer_max_waste = 10*K,\n@@ -386,3 +388,0 @@\n-  int _fd;              \/\/ file descriptor (-1 if dump file not open)\n-  julong _bytes_written; \/\/ number of byte written to dump file\n-\n@@ -398,4 +397,1 @@\n-  char* _error;   \/\/ error message when I\/O fails\n-\n-  void set_file_descriptor(int fd)              { _fd = fd; }\n-  int file_descriptor() const                   { return _fd; }\n+  CompressionBackend _backend; \/\/ Does the actual writing.\n@@ -403,1 +399,0 @@\n-  bool is_open() const                          { return file_descriptor() >= 0; }\n@@ -411,1 +406,2 @@\n-  void set_error(const char* error)             { _error = (char*)os::strdup(error); }\n+  \/\/ Can be called if we have enough room in the buffer.\n+  void write_fast(void* s, size_t len);\n@@ -413,2 +409,2 @@\n-  \/\/ all I\/O go through this function\n-  void write_internal(void* s, size_t len);\n+  \/\/ Returns true if we have enough room in the buffer for 'len' bytes.\n+  bool can_write_fast(size_t len);\n@@ -417,2 +413,2 @@\n-  DumpWriter(const char* path, bool overwrite);\n-  ~DumpWriter();\n+  \/\/ Takes ownership of the writer and compressor.\n+  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);\n@@ -420,1 +416,1 @@\n-  void close();\n+  ~DumpWriter();\n@@ -423,1 +419,1 @@\n-  julong bytes_written() const          { return _bytes_written; }\n+  julong bytes_written() const          { return (julong) _backend.get_written(); }\n@@ -425,1 +421,1 @@\n-  char* error() const                   { return _error; }\n+  char const* error() const             { return _backend.error(); }\n@@ -429,1 +425,1 @@\n-  void write_u1(u1 x)                   { write_raw((void*)&x, 1); }\n+  void write_u1(u1 x);\n@@ -444,1 +440,0 @@\n-};\n@@ -446,16 +441,5 @@\n-DumpWriter::DumpWriter(const char* path, bool overwrite) : _fd(-1), _bytes_written(0), _pos(0),\n-                                                           _in_dump_segment(false), _error(NULL) {\n-  \/\/ try to allocate an I\/O buffer of io_buffer_size. If there isn't\n-  \/\/ sufficient memory then reduce size until we can allocate something.\n-  _size = io_buffer_max_size;\n-  do {\n-    _buffer = (char*)os::malloc(_size, mtInternal);\n-    if (_buffer == NULL) {\n-      _size = _size >> 1;\n-    }\n-  } while (_buffer == NULL && _size >= io_buffer_min_size);\n-\n-  if (_buffer == NULL) {\n-    set_error(\"Could not allocate buffer memory for heap dump\");\n-  } else {\n-    _fd = os::create_binary_file(path, overwrite);    \/\/ don't replace existing file\n+  \/\/ Called by threads used for parallel writing.\n+  void writer_loop()                    { _backend.thread_loop(false); }\n+  \/\/ Called when finished to release the threads.\n+  void deactivate()                     { flush(); _backend.deactivate(); }\n+};\n@@ -463,5 +447,8 @@\n-    \/\/ if the open failed we record the error\n-    if (_fd < 0) {\n-      set_error(os::strerror(errno));\n-    }\n-  }\n+\/\/ Check for error after constructing the object and destroy it in case of an error.\n+DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :\n+  _buffer(NULL),\n+  _size(0),\n+  _pos(0),\n+  _in_dump_segment(false),\n+  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {\n+  flush();\n@@ -471,3 +458,1 @@\n-  close();\n-  os::free(_buffer);\n-  os::free(_error);\n+  flush();\n@@ -476,8 +461,6 @@\n-\/\/ closes dump file (if open)\n-void DumpWriter::close() {\n-  \/\/ flush and close dump file\n-  if (is_open()) {\n-    flush();\n-    os::close(file_descriptor());\n-    set_file_descriptor(-1);\n-  }\n+void DumpWriter::write_fast(void* s, size_t len) {\n+  assert(!_in_dump_segment || (_sub_record_left >= len), \"sub-record too large\");\n+  assert(buffer_size() - position() >= len, \"Must fit\");\n+  debug_only(_sub_record_left -= len);\n+  memcpy(buffer() + position(), s, len);\n+  set_position(position() + len);\n@@ -486,22 +469,2 @@\n-\/\/ write directly to the file\n-void DumpWriter::write_internal(void* s, size_t len) {\n-  if (is_open()) {\n-    const char* pos = (char*)s;\n-    ssize_t n = 0;\n-    while (len > 0) {\n-      uint tmp = (uint)MIN2(len, (size_t)INT_MAX);\n-      n = os::write(file_descriptor(), pos, tmp);\n-\n-      if (n < 0) {\n-        \/\/ EINTR cannot happen here, os::write will take care of that\n-        set_error(os::strerror(errno));\n-        os::close(file_descriptor());\n-        set_file_descriptor(-1);\n-        return;\n-      }\n-\n-      _bytes_written += n;\n-      pos += n;\n-      len -= n;\n-    }\n-  }\n+bool DumpWriter::can_write_fast(size_t len) {\n+  return buffer_size() - position() >= len;\n@@ -515,8 +478,4 @@\n-  \/\/ flush buffer to make room\n-  if (len > buffer_size() - position()) {\n-    assert(!_in_dump_segment || _is_huge_sub_record, \"Cannot overflow in non-huge sub-record.\");\n-    flush();\n-\n-    \/\/ If larger than the buffer, just write it directly.\n-    if (len > buffer_size()) {\n-      write_internal(s, len);\n+  \/\/ flush buffer to make room.\n+  while (len > buffer_size() - position()) {\n+    assert(!_in_dump_segment || _is_huge_sub_record,\n+           \"Cannot overflow in non-huge sub-record.\");\n@@ -524,2 +483,6 @@\n-      return;\n-    }\n+    size_t to_write = buffer_size() - position();\n+    memcpy(buffer() + position(), s, to_write);\n+    s = (void*) ((char*) s + to_write);\n+    len -= to_write;\n+    set_position(position() + to_write);\n+    flush();\n@@ -534,2 +497,9 @@\n-  write_internal(buffer(), position());\n-  set_position(0);\n+  _backend.get_new_buffer(&_buffer, &_pos, &_size);\n+}\n+\n+\/\/ Makes sure we inline the fast write into the write_u* functions. This is a big speedup.\n+#define WRITE_KNOWN_TYPE(p, len) do { if (can_write_fast((len))) write_fast((p), (len)); \\\n+                                      else write_raw((p), (len)); } while (0)\n+\n+void DumpWriter::write_u1(u1 x) {\n+  WRITE_KNOWN_TYPE((void*) &x, 1);\n@@ -541,1 +511,1 @@\n-  write_raw((void*)&v, 2);\n+  WRITE_KNOWN_TYPE((void*)&v, 2);\n@@ -547,1 +517,1 @@\n-  write_raw((void*)&v, 4);\n+  WRITE_KNOWN_TYPE((void*)&v, 4);\n@@ -553,1 +523,1 @@\n-  write_raw((void*)&v, 8);\n+  WRITE_KNOWN_TYPE((void*)&v, 8);\n@@ -596,1 +566,2 @@\n-      Bytes::put_Java_u4((address) (buffer() + 5), (u4) (position() - dump_segment_header_size));\n+      Bytes::put_Java_u4((address) (buffer() + 5),\n+                         (u4) (position() - dump_segment_header_size));\n@@ -608,1 +579,0 @@\n-      assert(position() == 0, \"Must be at the start\");\n@@ -611,0 +581,2 @@\n+    assert(position() == 0, \"Must be at the start\");\n+\n@@ -1505,1 +1477,1 @@\n-class VM_HeapDumper : public VM_GC_Operation {\n+class VM_HeapDumper : public VM_GC_Operation, public AbstractGangTask {\n@@ -1560,1 +1532,2 @@\n-                    gc_before_heap_dump) {\n+                    gc_before_heap_dump),\n+    AbstractGangTask(\"dump heap\") {\n@@ -1591,0 +1564,1 @@\n+  void work(uint worker_id);\n@@ -1593,0 +1567,1 @@\n+\n@@ -1821,0 +1796,19 @@\n+  WorkGang* gang = ch->get_safepoint_workers();\n+\n+  if (gang == NULL) {\n+    work(0);\n+  } else {\n+    gang->run_task(this, gang->active_workers(), true);\n+  }\n+\n+  \/\/ Now we clear the global variables, so that a future dumper can run.\n+  clear_global_dumper();\n+  clear_global_writer();\n+}\n+\n+void VM_HeapDumper::work(uint worker_id) {\n+  if (!Thread::current()->is_VM_thread()) {\n+    writer()->writer_loop();\n+    return;\n+  }\n+\n@@ -1822,1 +1816,0 @@\n-  size_t used = ch->used();\n@@ -1878,3 +1871,2 @@\n-  \/\/ Now we clear the global variables, so that a future dumper might run.\n-  clear_global_dumper();\n-  clear_global_writer();\n+  \/\/ We are done with writing. Release the worker threads.\n+  writer()->deactivate();\n@@ -1896,0 +1888,1 @@\n+      ResourceMark rm;\n@@ -1938,1 +1931,1 @@\n-int HeapDumper::dump(const char* path, bool overwrite) {\n+int HeapDumper::dump(const char* path, outputStream* out, int compression, bool overwrite) {\n@@ -1947,2 +1940,13 @@\n-  \/\/ create the dump writer. If the file can be opened then bail\n-  DumpWriter writer(path, overwrite);\n+  AbstractCompressor* compressor = NULL;\n+\n+  if (compression > 0) {\n+    compressor = new (std::nothrow) GZipCompressor(compression);\n+\n+    if (compressor == NULL) {\n+      set_error(\"Could not allocate gzip compressor\");\n+      return -1;\n+    }\n+  }\n+\n+  DumpWriter writer(new (std::nothrow) FileWriter(path, overwrite), compressor);\n+\n@@ -1967,2 +1971,1 @@\n-  \/\/ close dump file and record any error that the writer may have encountered\n-  writer.close();\n+  \/\/ record any error that the writer may have encountered\n@@ -2006,1 +2009,1 @@\n-void HeapDumper::set_error(char* error) {\n+void HeapDumper::set_error(char const* error) {\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":107,"deletions":104,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,1 +57,1 @@\n-  void set_error(char* error);\n+  void set_error(char const* error);\n@@ -74,1 +74,2 @@\n-  int dump(const char* path, bool overwrite = false);\n+  \/\/ compression >= 0 creates a gzipped file with the given compression level.\n+  int dump(const char* path, outputStream* out = NULL, int compression = -1, bool overwrite = false);\n","filename":"src\/hotspot\/share\/services\/heapDumper.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,473 @@\n+\/*\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"jvm.h\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"services\/heapDumperCompression.hpp\"\n+\n+\n+char const* FileWriter::open_writer() {\n+  assert(_fd < 0, \"Must not already be open\");\n+\n+  _fd = os::create_binary_file(_path, _overwrite);    \/\/ don't replace existing file\n+\n+  if (_fd < 0) {\n+    return os::strerror(errno);\n+  }\n+\n+  return NULL;\n+}\n+\n+FileWriter::~FileWriter() {\n+  if (_fd >= 0) {\n+    os::close(_fd);\n+    _fd = -1;\n+  }\n+}\n+\n+char const* FileWriter::write_buf(char* buf, ssize_t size) {\n+  assert(_fd >= 0, \"Must be open\");\n+  assert(size > 0, \"Must write at least one byte\");\n+\n+  ssize_t n = (ssize_t) os::write(_fd, buf, (uint) size);\n+\n+  if (n <= 0) {\n+    return os::strerror(errno);\n+  }\n+\n+  return NULL;\n+}\n+\n+\n+typedef char const* (*GzipInitFunc)(size_t, size_t*, size_t*, int);\n+typedef size_t(*GzipCompressFunc)(char*, size_t, char*, size_t, char*, size_t,\n+                                  int, char*, char const**);\n+\n+static GzipInitFunc gzip_init_func;\n+static GzipCompressFunc gzip_compress_func;\n+\n+void* GZipCompressor::load_gzip_func(char const* name) {\n+  char path[JVM_MAXPATHLEN];\n+  char ebuf[1024];\n+  void* handle;\n+\n+  if (os::dll_locate_lib(path, sizeof(path), Arguments::get_dll_dir(), \"zip\")) {\n+    handle = os::dll_load(path, ebuf, sizeof ebuf);\n+\n+    if (handle != NULL) {\n+      return os::dll_lookup(handle, name);\n+    }\n+  }\n+\n+  return NULL;\n+}\n+\n+char const* GZipCompressor::init(size_t block_size, size_t* needed_out_size,\n+                                 size_t* needed_tmp_size) {\n+  _block_size = block_size;\n+  _is_first = true;\n+\n+  if (gzip_compress_func == NULL) {\n+    gzip_compress_func = (GzipCompressFunc) load_gzip_func(\"ZIP_GZip_Fully\");\n+\n+    if (gzip_compress_func == NULL) {\n+      return \"Cannot get ZIP_GZip_Fully function\";\n+    }\n+  }\n+\n+  if (gzip_init_func == NULL) {\n+    gzip_init_func = (GzipInitFunc) load_gzip_func(\"ZIP_GZip_InitParams\");\n+\n+    if (gzip_init_func == NULL) {\n+      return \"Cannot get ZIP_GZip_InitParams function\";\n+    }\n+  }\n+\n+  char const* result = gzip_init_func(block_size, needed_out_size,\n+                                      needed_tmp_size, _level);\n+  *needed_out_size += 1024; \/\/ Add extra space for the comment in the first chunk.\n+\n+  return result;\n+}\n+\n+char const* GZipCompressor::compress(char* in, size_t in_size, char* out, size_t out_size,\n+                                     char* tmp, size_t tmp_size, size_t* compressed_size) {\n+  char const* msg = NULL;\n+\n+  if (_is_first) {\n+    char buf[128];\n+    \/\/ Write the block size used as a comment in the first gzip chunk, so the\n+    \/\/ code used to read it later can make a good choice of the buffer sizes it uses.\n+    jio_snprintf(buf, sizeof(buf), \"HPROF BLOCKSIZE=\" SIZE_FORMAT, _block_size);\n+    *compressed_size = gzip_compress_func(in, in_size, out, out_size, tmp, tmp_size, _level,\n+                                          buf, &msg);\n+    _is_first = false;\n+  } else {\n+    *compressed_size = gzip_compress_func(in, in_size, out, out_size, tmp, tmp_size, _level,\n+                                          NULL, &msg);\n+  }\n+\n+  return msg;\n+}\n+\n+WorkList::WorkList() {\n+  _head._next = &_head;\n+  _head._prev = &_head;\n+}\n+\n+void WorkList::insert(WriteWork* before, WriteWork* work) {\n+  work->_prev = before;\n+  work->_next = before->_next;\n+  before->_next = work;\n+  work->_next->_prev = work;\n+}\n+\n+WriteWork* WorkList::remove(WriteWork* work) {\n+  if (work != NULL) {\n+    assert(work->_next != work, \"Invalid next\");\n+    assert(work->_prev != work, \"Invalid prev\");\n+    work->_prev->_next = work->_next;;\n+    work->_next->_prev = work->_prev;\n+    work->_next = NULL;\n+    work->_prev = NULL;\n+  }\n+\n+  return work;\n+}\n+\n+void WorkList::add_by_id(WriteWork* work) {\n+  if (is_empty()) {\n+    add_first(work);\n+  } else {\n+    WriteWork* last_curr = &_head;\n+    WriteWork* curr = _head._next;\n+\n+    while (curr->_id < work->_id) {\n+      last_curr = curr;\n+      curr = curr->_next;\n+\n+      if (curr == &_head) {\n+        add_last(work);\n+        return;\n+      }\n+    }\n+\n+    insert(last_curr, work);\n+  }\n+}\n+\n+\n+\n+CompressionBackend::CompressionBackend(AbstractWriter* writer,\n+     AbstractCompressor* compressor, size_t block_size, size_t max_waste) :\n+  _active(false),\n+  _err(NULL),\n+  _nr_of_threads(0),\n+  _works_created(0),\n+  _work_creation_failed(false),\n+  _id_to_write(0),\n+  _next_id(0),\n+  _in_size(block_size),\n+  _max_waste(max_waste),\n+  _out_size(0),\n+  _tmp_size(0),\n+  _written(0),\n+  _writer(writer),\n+  _compressor(compressor),\n+  _lock(new (std::nothrow) PaddedMonitor(Mutex::leaf, \"HProf Compression Backend\",\n+    true, Mutex::_safepoint_check_never)) {\n+  if (_writer == NULL) {\n+    set_error(\"Could not allocate writer\");\n+  } else if (_lock == NULL) {\n+    set_error(\"Could not allocate lock\");\n+  } else {\n+    set_error(_writer->open_writer());\n+  }\n+\n+  if (_compressor != NULL) {\n+    set_error(_compressor->init(_in_size, &_out_size, &_tmp_size));\n+  }\n+\n+  _current = allocate_work(_in_size, _out_size, _tmp_size);\n+\n+  if (_current == NULL) {\n+    set_error(\"Could not allocate memory for buffer\");\n+  }\n+\n+  _active = (_err == NULL);\n+}\n+\n+CompressionBackend::~CompressionBackend() {\n+  assert(!_active, \"Must not be active by now\");\n+  assert(_nr_of_threads == 0, \"Must have no active threads\");\n+  assert(_to_compress.is_empty() && _finished.is_empty(), \"Still work to do\");\n+\n+  free_work_list(&_unused);\n+  free_work(_current);\n+  assert(_works_created == 0, \"All work must have been freed\");\n+\n+  delete _compressor;\n+  delete _writer;\n+  delete _lock;\n+}\n+\n+void CompressionBackend::deactivate() {\n+  assert(_active, \"Must be active\");\n+\n+  MonitorLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+\n+  \/\/ Make sure we write the last partially filled buffer.\n+  if ((_current != NULL) && (_current->_in_used > 0)) {\n+    _current->_id = _next_id++;\n+    _to_compress.add_last(_current);\n+    _current = NULL;\n+    ml.notify_all();\n+  }\n+\n+  \/\/ Wait for the threads to drain the compression work list.\n+  while (!_to_compress.is_empty()) {\n+    \/\/ If we have no threads, compress the current one itself.\n+    if (_nr_of_threads == 0) {\n+      MutexUnlockerEx mu(_lock, Mutex::_no_safepoint_check_flag);\n+      thread_loop(true);\n+    } else {\n+      ml.wait(Mutex::_no_safepoint_check_flag);\n+    }\n+  }\n+\n+  _active = false;\n+  ml.notify_all();\n+}\n+\n+void CompressionBackend::thread_loop(bool single_run) {\n+  \/\/ Register if this is a worker thread.\n+  if (!single_run) {\n+    MonitorLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+    _nr_of_threads++;\n+  }\n+\n+  while (true) {\n+    WriteWork* work = get_work();\n+\n+    if (work == NULL) {\n+      assert(!single_run, \"Should never happen for single thread\");\n+      MonitorLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+      _nr_of_threads--;\n+      assert(_nr_of_threads >= 0, \"Too many threads finished\");\n+      ml.notify_all();\n+\n+      return;\n+    } else {\n+      do_compress(work);\n+      finish_work(work);\n+    }\n+\n+    if (single_run) {\n+      return;\n+    }\n+  }\n+}\n+\n+void CompressionBackend::set_error(char const* new_error) {\n+  if ((new_error != NULL) && (_err == NULL)) {\n+    _err = new_error;\n+  }\n+}\n+\n+WriteWork* CompressionBackend::allocate_work(size_t in_size, size_t out_size,\n+                                             size_t tmp_size) {\n+  WriteWork* result = (WriteWork*) os::malloc(sizeof(WriteWork), mtInternal);\n+\n+  if (result == NULL) {\n+    _work_creation_failed = true;\n+    return NULL;\n+  }\n+\n+  _works_created++;\n+  result->_in = (char*) os::malloc(in_size, mtInternal);\n+  result->_in_max = in_size;\n+  result->_in_used = 0;\n+  result->_out = NULL;\n+  result->_tmp = NULL;\n+\n+  if (result->_in == NULL) {\n+    goto fail;\n+  }\n+\n+  if (out_size > 0) {\n+    result->_out = (char*) os::malloc(out_size, mtInternal);\n+    result->_out_used = 0;\n+    result->_out_max = out_size;\n+\n+    if (result->_out == NULL) {\n+      goto fail;\n+    }\n+  }\n+\n+  if (tmp_size > 0) {\n+    result->_tmp = (char*) os::malloc(tmp_size, mtInternal);\n+    result->_tmp_max = tmp_size;\n+\n+    if (result->_tmp == NULL) {\n+      goto fail;\n+    }\n+  }\n+\n+  return result;\n+\n+fail:\n+  free_work(result);\n+  _work_creation_failed = true;\n+  return NULL;\n+}\n+\n+void CompressionBackend::free_work(WriteWork* work) {\n+  if (work != NULL) {\n+    os::free(work->_in);\n+    os::free(work->_out);\n+    os::free(work->_tmp);\n+    os::free(work);\n+    --_works_created;\n+  }\n+}\n+\n+void CompressionBackend::free_work_list(WorkList* list) {\n+  while (!list->is_empty()) {\n+    free_work(list->remove_first());\n+  }\n+}\n+\n+WriteWork* CompressionBackend::get_work() {\n+  MonitorLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+\n+  while (_active && _to_compress.is_empty()) {\n+    ml.wait(Mutex::_no_safepoint_check_flag);\n+  }\n+\n+  return _to_compress.remove_first();\n+}\n+\n+void CompressionBackend::get_new_buffer(char** buffer, size_t* used, size_t* max) {\n+  if (_active) {\n+    MonitorLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+\n+    if (*used > 0) {\n+      _current->_in_used += *used;\n+\n+      \/\/ Check if we do not waste more than _max_waste. If yes, write the buffer.\n+      \/\/ Otherwise return the rest of the buffer as the new buffer.\n+      if (_current->_in_max - _current->_in_used <= _max_waste) {\n+        _current->_id = _next_id++;\n+        _to_compress.add_last(_current);\n+        _current = NULL;\n+        ml.notify_all();\n+      } else {\n+        *buffer = _current->_in + _current->_in_used;\n+        *used = 0;\n+        *max = _current->_in_max - _current->_in_used;\n+\n+        return;\n+      }\n+    }\n+\n+    while ((_current == NULL) && _unused.is_empty() && _active) {\n+      \/\/ Add more work objects if needed.\n+      if (!_work_creation_failed && (_works_created <= _nr_of_threads)) {\n+        WriteWork* work = allocate_work(_in_size, _out_size, _tmp_size);\n+\n+        if (work != NULL) {\n+          _unused.add_first(work);\n+        }\n+      } else if (!_to_compress.is_empty() && (_nr_of_threads == 0)) {\n+        \/\/ If we have no threads, compress the current one itself.\n+        MutexUnlockerEx mu(_lock, Mutex::_no_safepoint_check_flag);\n+        thread_loop(true);\n+      } else {\n+        ml.wait(Mutex::_no_safepoint_check_flag);\n+      }\n+    }\n+\n+    if (_current == NULL) {\n+      _current = _unused.remove_first();\n+    }\n+\n+    if (_current != NULL) {\n+      _current->_in_used = 0;\n+      _current->_out_used = 0;\n+      *buffer = _current->_in;\n+      *used = 0;\n+      *max = _current->_in_max;\n+\n+      return;\n+    }\n+  }\n+\n+  *buffer = NULL;\n+  *used = 0;\n+  *max = 0;\n+\n+  return;\n+}\n+\n+void CompressionBackend::do_compress(WriteWork* work) {\n+  if (_compressor != NULL) {\n+    char const* msg = _compressor->compress(work->_in, work->_in_used, work->_out,\n+                                            work->_out_max,\n+    work->_tmp, _tmp_size, &work->_out_used);\n+\n+    if (msg != NULL) {\n+      MutexLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+      set_error(msg);\n+    }\n+  }\n+}\n+\n+void CompressionBackend::finish_work(WriteWork* work) {\n+  MonitorLockerEx ml(_lock, Mutex::_no_safepoint_check_flag);\n+\n+  _finished.add_by_id(work);\n+\n+  \/\/ Write all finished works as far as we can.\n+  while (!_finished.is_empty() && (_finished.first()->_id == _id_to_write)) {\n+    WriteWork* to_write = _finished.remove_first();\n+    size_t size = _compressor == NULL ? to_write->_in_used : to_write->_out_used;\n+    char* p = _compressor == NULL ? to_write->_in : to_write->_out;\n+    char const* msg = NULL;\n+\n+    if (_err == NULL) {\n+      _written += size;\n+      MutexUnlockerEx mu(_lock, Mutex::_no_safepoint_check_flag);\n+      msg = _writer->write_buf(p, (ssize_t) size);\n+    }\n+\n+    set_error(msg);\n+    _unused.add_first(to_write);\n+    _id_to_write++;\n+  }\n+\n+  ml.notify_all();\n+}\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.cpp","additions":473,"deletions":0,"binary":false,"changes":473,"status":"added"},{"patch":"@@ -0,0 +1,233 @@\n+\/*\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_SERVICES_HEAPDUMPERCOMPRESSION_HPP\n+#define SHARE_SERVICES_HEAPDUMPERCOMPRESSION_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+\n+\n+\/\/ Interface for a compression  implementation.\n+class AbstractCompressor : public CHeapObj<mtInternal> {\n+public:\n+  virtual ~AbstractCompressor() { }\n+\n+  \/\/ Initializes the compressor. Returns a static error message in case of an error.\n+  \/\/ Otherwise initializes the needed out and tmp size for the given block size.\n+  virtual char const* init(size_t block_size, size_t* needed_out_size,\n+                           size_t* needed_tmp_size) = 0;\n+\n+  \/\/ Does the actual compression. Returns NULL on success and a static error\n+  \/\/ message otherwise. Sets the 'compressed_size'.\n+  virtual char const* compress(char* in, size_t in_size, char* out, size_t out_size,\n+                               char* tmp, size_t tmp_size, size_t* compressed_size) = 0;\n+};\n+\n+\/\/ Interface for a writer implementation.\n+class AbstractWriter : public CHeapObj<mtInternal> {\n+public:\n+  virtual ~AbstractWriter() { }\n+\n+  \/\/ Opens the writer. Returns NULL on success and a static error message otherwise.\n+  virtual char const* open_writer() = 0;\n+\n+  \/\/ Does the write. Returns NULL on success and a static error message otherwise.\n+  virtual char const* write_buf(char* buf, ssize_t size) = 0;\n+};\n+\n+\n+\/\/ A writer for a file.\n+class FileWriter : public AbstractWriter {\n+private:\n+  char const* _path;\n+  bool _overwrite;\n+  int _fd;\n+\n+public:\n+  FileWriter(char const* path, bool overwrite) : _path(path), _overwrite(overwrite), _fd(-1) { }\n+\n+  ~FileWriter();\n+\n+  \/\/ Opens the writer. Returns NULL on success and a static error message otherwise.\n+  virtual char const* open_writer();\n+\n+  \/\/ Does the write. Returns NULL on success and a static error message otherwise.\n+  virtual char const* write_buf(char* buf, ssize_t size);\n+};\n+\n+\n+\/\/ A compressor using the gzip format.\n+class GZipCompressor : public AbstractCompressor {\n+private:\n+  int _level;\n+  size_t _block_size;\n+  bool _is_first;\n+\n+  void* load_gzip_func(char const* name);\n+\n+public:\n+  GZipCompressor(int level) : _level(level), _block_size(0), _is_first(false) {\n+  }\n+\n+  virtual char const* init(size_t block_size, size_t* needed_out_size,\n+                           size_t* needed_tmp_size);\n+\n+  virtual char const* compress(char* in, size_t in_size, char* out, size_t out_size,\n+                               char* tmp, size_t tmp_size, size_t* compressed_size);\n+};\n+\n+\n+\/\/ The data needed to write a single buffer (and compress it optionally).\n+struct WriteWork {\n+  \/\/ The id of the work.\n+  int64_t _id;\n+\n+  \/\/ The input buffer where the raw data is\n+  char* _in;\n+  size_t _in_used;\n+  size_t _in_max;\n+\n+  \/\/ The output buffer where the compressed data is. Is NULL when compression is disabled.\n+  char* _out;\n+  size_t _out_used;\n+  size_t _out_max;\n+\n+  \/\/ The temporary space needed for compression. Is NULL when compression is disabled.\n+  char* _tmp;\n+  size_t _tmp_max;\n+\n+  \/\/ Used to link WriteWorks into lists.\n+  WriteWork* _next;\n+  WriteWork* _prev;\n+};\n+\n+\/\/ A list for works.\n+class WorkList {\n+private:\n+  WriteWork _head;\n+\n+  void insert(WriteWork* before, WriteWork* work);\n+  WriteWork* remove(WriteWork* work);\n+\n+public:\n+  WorkList();\n+\n+  \/\/ Return true if the list is empty.\n+  bool is_empty() { return _head._next == &_head; }\n+\n+  \/\/ Adds to the beginning of the list.\n+  void add_first(WriteWork* work) { insert(&_head, work); }\n+\n+  \/\/ Adds to the end of the list.\n+  void add_last(WriteWork* work) { insert(_head._prev, work); }\n+\n+  \/\/ Adds so the ids are ordered.\n+  void add_by_id(WriteWork* work);\n+\n+  \/\/ Returns the first element.\n+  WriteWork* first() { return is_empty() ? NULL : _head._next; }\n+\n+  \/\/ Returns the last element.\n+  WriteWork* last() { return is_empty() ? NULL : _head._prev; }\n+\n+  \/\/ Removes the first element. Returns NULL if empty.\n+  WriteWork* remove_first() { return remove(first()); }\n+\n+  \/\/ Removes the last element. Returns NULL if empty.\n+  WriteWork* remove_last() { return remove(first()); }\n+};\n+\n+\n+class Monitor;\n+\n+\/\/ This class is used by the DumpWriter class. It supplies the DumpWriter with\n+\/\/ chunks of memory to write the heap dump data into. When the DumpWriter needs a\n+\/\/ new memory chunk, it calls get_new_buffer(), which commits the old chunk used\n+\/\/ and returns a new chunk. The old chunk is then added to a queue to be compressed\n+\/\/ and then written in the background.\n+class CompressionBackend : StackObj {\n+  bool _active;\n+  char const * _err;\n+\n+  int _nr_of_threads;\n+  int _works_created;\n+  bool _work_creation_failed;\n+\n+  int64_t _id_to_write;\n+  int64_t _next_id;\n+\n+  size_t _in_size;\n+  size_t _max_waste;\n+  size_t _out_size;\n+  size_t _tmp_size;\n+\n+  size_t _written;\n+\n+  AbstractWriter* const _writer;\n+  AbstractCompressor* const _compressor;\n+\n+  Monitor* const _lock;\n+\n+  WriteWork* _current;\n+  WorkList _to_compress;\n+  WorkList _unused;\n+  WorkList _finished;\n+\n+  void set_error(char const* new_error);\n+\n+  WriteWork* allocate_work(size_t in_size, size_t out_size, size_t tmp_size);\n+  void free_work(WriteWork* work);\n+  void free_work_list(WorkList* list);\n+\n+  WriteWork* get_work();\n+  void do_compress(WriteWork* work);\n+  void finish_work(WriteWork* work);\n+\n+public:\n+  \/\/ compressor can be NULL if no compression is used.\n+  \/\/ Takes ownership of the writer and compressor.\n+  \/\/ block_size is the buffer size of a WriteWork.\n+  \/\/ max_waste is the maximum number of bytes to leave\n+  \/\/ empty in the buffer when it is written.\n+  CompressionBackend(AbstractWriter* writer, AbstractCompressor* compressor,\n+    size_t block_size, size_t max_waste);\n+\n+  ~CompressionBackend();\n+\n+  size_t get_written() const { return _written; }\n+\n+  char const* error() const { return _err; }\n+\n+  \/\/ Commits the old buffer (using the value in *used) and sets up a new one.\n+  void get_new_buffer(char** buffer, size_t* used, size_t* max);\n+\n+  \/\/ The entry point for a worker thread. If single_run is true, we only handle one entry.\n+  void thread_loop(bool single_run);\n+\n+  \/\/ Shuts down the backend, releasing all threads.\n+  void deactivate();\n+};\n+\n+\n+#endif \/\/ SHARE_SERVICES_HEAPDUMPERCOMPRESSION_HPP\n","filename":"src\/hotspot\/share\/services\/heapDumperCompression.hpp","additions":233,"deletions":0,"binary":false,"changes":233,"status":"added"},{"patch":"@@ -1592,0 +1592,106 @@\n+\n+static voidpf tracking_zlib_alloc(voidpf opaque, uInt items, uInt size) {\n+  size_t* needed = (size_t*) opaque;\n+  *needed += (size_t) items * (size_t) size;\n+  return (voidpf) calloc((size_t) items, (size_t) size);\n+}\n+\n+static void tracking_zlib_free(voidpf opaque, voidpf address) {\n+  free((void*) address);\n+}\n+\n+static voidpf zlib_block_alloc(voidpf opaque, uInt items, uInt size) {\n+  char** range = (char**) opaque;\n+  voidpf result = NULL;\n+  size_t needed = (size_t) items * (size_t) size;\n+\n+  if (range[1] - range[0] >= (ptrdiff_t) needed) {\n+    result = (voidpf) range[0];\n+    range[0] += needed;\n+  }\n+\n+  return result;\n+}\n+\n+static void zlib_block_free(voidpf opaque, voidpf address) {\n+  \/* Nothing to do. *\/\n+}\n+\n+static char const* deflateInit2Wrapper(z_stream* strm, int level) {\n+  int err = deflateInit2(strm, level >= 0 && level <= 9 ? level : Z_DEFAULT_COMPRESSION,\n+                         Z_DEFLATED, 31, 8, Z_DEFAULT_STRATEGY);\n+  if (err == Z_MEM_ERROR) {\n+    return \"Out of memory in deflateInit2\";\n+  }\n+\n+  if (err != Z_OK) {\n+    return \"Internal error in deflateInit2\";\n+  }\n+\n+  return NULL;\n+}\n+\n+JNIEXPORT char const*\n+ZIP_GZip_InitParams(size_t inLen, size_t* outLen, size_t* tmpLen, int level) {\n+  z_stream strm;\n+  *tmpLen = 0;\n+  char const* errorMsg;\n+\n+  memset(&strm, 0, sizeof(z_stream));\n+  strm.zalloc = tracking_zlib_alloc;\n+  strm.zfree = tracking_zlib_free;\n+  strm.opaque = (voidpf) tmpLen;\n+\n+  errorMsg = deflateInit2Wrapper(&strm, level);\n+\n+  if (errorMsg == NULL) {\n+    *outLen = (size_t) deflateBound(&strm, (uLong) inLen);\n+    deflateEnd(&strm);\n+  }\n+\n+  return errorMsg;\n+}\n+\n+JNIEXPORT size_t\n+ZIP_GZip_Fully(char* inBuf, size_t inLen, char* outBuf, size_t outLen, char* tmp, size_t tmpLen,\n+               int level, char* comment, char const** pmsg) {\n+  z_stream strm;\n+  gz_header hdr;\n+  int err;\n+  char* block[] = {tmp, tmpLen + tmp};\n+  size_t result = 0;\n+\n+  memset(&strm, 0, sizeof(z_stream));\n+  strm.zalloc = zlib_block_alloc;\n+  strm.zfree = zlib_block_free;\n+  strm.opaque = (voidpf) block;\n+\n+  *pmsg = deflateInit2Wrapper(&strm, level);\n+\n+  if (*pmsg == NULL) {\n+    strm.next_out = (Bytef *) outBuf;\n+    strm.avail_out = (uInt) outLen;\n+    strm.next_in = (Bytef *) inBuf;\n+    strm.avail_in = (uInt) inLen;\n+\n+    if (comment != NULL) {\n+      memset(&hdr, 0, sizeof(hdr));\n+      hdr.comment = (Bytef*) comment;\n+      deflateSetHeader(&strm, &hdr);\n+    }\n+\n+    err = deflate(&strm, Z_FINISH);\n+\n+    if (err == Z_OK || err == Z_BUF_ERROR) {\n+      *pmsg = \"Buffer too small\";\n+    } else if (err != Z_STREAM_END) {\n+      *pmsg = \"Intern deflate error\";\n+    } else {\n+      result = (size_t) strm.total_out;\n+    }\n+\n+    deflateEnd(&strm);\n+  }\n+\n+  return result;\n+}\n","filename":"src\/java.base\/share\/native\/libzip\/zip_util.c","additions":106,"deletions":0,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -0,0 +1,139 @@\n+\/*\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.io.IOException;\n+import java.util.List;\n+\n+import jdk.test.lib.hprof.HprofParser;\n+import jdk.test.lib.hprof.parser.Reader;\n+import jdk.test.lib.hprof.model.Snapshot;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.dcmd.PidJcmdExecutor;\n+import jdk.test.lib.process.OutputAnalyzer;\n+\n+\/*\n+ * @test\n+ * @summary Test of diagnostic command GC.heap_dump with gzipped output (Serial, Parallel and G1)\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UseSerialGC HeapDumpCompressedTest\n+ * @run main\/othervm -XX:+UseParallelGC HeapDumpCompressedTest\n+ * @run main\/othervm -XX:+UseG1GC HeapDumpCompressedTest\n+ *\/\n+\n+\/*\n+ * @test\n+ * @requires vm.gc.Z\n+ * @summary Test of diagnostic command GC.heap_dump with gzipped output (Z GC)\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseZGC HeapDumpCompressedTest\n+ *\/\n+\n+\/*\n+ * @test\n+ * @requires vm.gc.Shenandoah\n+ * @summary Test of diagnostic command GC.heap_dump with gzipped output (Shenandoah GC)\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC HeapDumpCompressedTest\n+ *\/\n+\n+\/*\n+ * @test\n+ * @requires vm.gc.Epsilon\n+ * @summary Test of diagnostic command GC.heap_dump with gzipped output (Epsilon GC)\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC HeapDumpCompressedTest\n+ *\/\n+\n+public class HeapDumpCompressedTest {\n+    public static HeapDumpCompressedTest ref;\n+\n+    public static void main(String[] args) throws Exception {\n+        PidJcmdExecutor executor = new PidJcmdExecutor();\n+        ref = new HeapDumpCompressedTest();\n+        File dump = new File(\"jcmd.gc.heap_dump.\" + System.currentTimeMillis() + \".hprof.gz\");\n+\n+        if (dump.exists()) {\n+            dump.delete();\n+        }\n+\n+        \/\/ Check we detect an invalid compression level.\n+        OutputAnalyzer output = executor.execute(\"GC.heap_dump -gz=0 \" +\n+                                                  dump.getAbsolutePath());\n+        output.shouldContain(\"Compression level out of range\");\n+\n+        \/\/ Check we can create a gzipped dump.\n+        output = executor.execute(\"GC.heap_dump -gz=1 \" + dump.getAbsolutePath());\n+        output.shouldContain(\"Heap dump file created\");\n+\n+        \/\/ Check we detect an already present heap dump.\n+        output = executor.execute(\"GC.heap_dump -gz=1 \" + dump.getAbsolutePath());\n+        output.shouldContain(\"Unable to create \");\n+\n+        verifyHeapDump(dump);\n+        dump.delete();\n+    }\n+\n+    private static void verifyHeapDump(File dump) throws Exception {\n+\n+        Asserts.assertTrue(dump.exists() && dump.isFile(),\n+                           \"Could not create dump file \" + dump.getAbsolutePath());\n+\n+        try {\n+            File out = HprofParser.parse(dump);\n+\n+            Asserts.assertTrue(out != null && out.exists() && out.isFile(),\n+                               \"Could not find hprof parser output file\");\n+            List<String> lines = Files.readAllLines(out.toPath());\n+            Asserts.assertTrue(lines.size() > 0, \"hprof parser output file is empty\");\n+            for (String line : lines) {\n+                Asserts.assertFalse(line.matches(\".*WARNING(?!.*Failed to resolve \" +\n+                                                 \"object.*constantPoolOop.*).*\"));\n+            }\n+\n+            out.delete();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+            Asserts.fail(\"Could not parse dump file \" + dump.getAbsolutePath());\n+        }\n+    }\n+}\n+\n","filename":"test\/hotspot\/jtreg\/serviceability\/dcmd\/gc\/HeapDumpCompressedTest.java","additions":139,"deletions":0,"binary":false,"changes":139,"status":"added"},{"patch":"@@ -0,0 +1,577 @@\n+\/*\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.test.lib.hprof.parser;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.Closeable;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.zip.DataFormatException;\n+import java.util.zip.Inflater;\n+\n+public class GzipRandomAccess implements AutoCloseable, Closeable {\n+    \/\/ A comparator which compares chunks by their file offset.\n+    private static FileOffsetComparator fileOffsetComp = new FileOffsetComparator();\n+\n+    \/\/ A comparator which compares chunks by their offset.\n+    private static OffsetComparator offsetComp = new OffsetComparator();\n+\n+    \/\/ The size to use when reading from the random access file.\n+    private static final int READ_SIZE = 65536;\n+\n+    \/\/ The last used buffer.\n+    private Buffer last;\n+\n+    \/\/ The underlying random access file to use.\n+    private final RandomAccessFile raf;\n+\n+    \/\/ The length of the random access file.\n+    private final long fileSize;\n+\n+    \/\/ The maximum size of a buffer cache.\n+    private final int cacheSize;\n+\n+    \/\/ The maximum numbers of buffers to cache.\n+    private final int maxCachedBuffers;\n+\n+    \/\/ A buffer used to read from the file.\n+    private final byte[] in;\n+\n+    \/\/ A sorted list of the buffers we know so far.\n+    private final ArrayList<Buffer> buffers;\n+\n+    \/\/ The inflater to use.\n+    private final Inflater inf;\n+\n+    \/\/ The head of the list which contains the buffers with cached data.\n+    private final Buffer cacheHead;\n+\n+    \/\/ The number of cached buffers in the list.\n+    private int cachedBuffers;\n+\n+    \/\/ This is private to ensure we only handle the specific hprof gzip files\n+    \/\/ written by the VM.\n+    private GzipRandomAccess(String file, int bufferSize, int maxCachedBuffers)\n+            throws IOException {\n+        last = null;\n+        raf = new RandomAccessFile(file, \"r\");\n+        fileSize = raf.length();\n+        this.cacheSize = bufferSize;\n+        this.maxCachedBuffers = maxCachedBuffers;\n+        cachedBuffers = 0;\n+        in = new byte[READ_SIZE];\n+        buffers = new ArrayList<>();\n+        inf = new Inflater(true);\n+        cacheHead = new Buffer(-1, -1);\n+        cacheHead.next = cacheHead;\n+        cacheHead.prev = cacheHead;\n+        buffers.add(new Buffer(0, 0));\n+    }\n+\n+    \/**\n+     * Clears the cache.\n+     *\/\n+    public synchronized void clearCache() {\n+        while (cacheHead.next != cacheHead) {\n+            assert cacheHead.next.cache != null;\n+            Buffer buf = cacheHead.next;\n+            remove(buf);\n+            buf.cache = null;\n+        }\n+\n+        last = null;\n+        cachedBuffers = 0;\n+    }\n+\n+    \/**\n+     * Returns an approximate file offset for the given offset. The return value should\n+     * only be used for progress indication and the like. Note that you should only query\n+     * offsets you've already read.\n+     *\n+     * @param offset The offset.\n+     * @return The approximate file offset.\n+     *\/\n+    public synchronized long getFileOffset(long offset) {\n+        int pos = Collections.binarySearch(buffers, new Buffer(0, offset), offsetComp);\n+        int realPos = pos >= 0 ? pos : -pos - 2;\n+\n+        if (realPos >= buffers.size() - 1) {\n+            return buffers.get(buffers.size() - 1).fileOffset;\n+        }\n+\n+        \/\/ Assume uniform compression.\n+        Buffer buf = buffers.get(realPos);\n+        long diff = offset - buf.offset;\n+        long bufFileEnd = buffers.get(realPos + 1).fileOffset;\n+        long fileDiff = bufFileEnd - buf.fileOffset;\n+        double filePerDiff = (double) Math.max(1, fileDiff) \/ Math.max(1, buf.cacheLen);\n+\n+        return buf.fileOffset + (long) (filePerDiff * diff);\n+    }\n+\n+    \/**\n+     * @return Returns the size of the underlying file.\n+     *\/\n+    public long getFileSize() {\n+        return fileSize;\n+    }\n+\n+    \/**\n+     * Returns an @link {@link InputStream} to read from the given offset. Note\n+     * that closing the input stream does not close the underlying @link\n+     * {@link GzipRandomAccess} object.\n+     *\n+     * @param offset The offset.\n+     * @return The input stream.\n+     *\/\n+    public InputStream asStream(long offset) {\n+        return new InputStreamImpl(offset, this);\n+    }\n+\n+    \/**\n+     * Returns a @link ReadBuffer which uses this object to do the actual\n+     * operation. Note that closing the returned object closes the\n+     * underlying @link {@link GzipRandomAccess} object.\n+     *\n+     * @return The @link ReadBuffer.\n+     *\/\n+    public ReadBuffer asFileBuffer() {\n+        return new ReadBufferImpl(this);\n+    }\n+\n+    \/**\n+     * Closes the object and clears the cache. The object is unusable after this\n+     * call.\n+     *\/\n+    @Override\n+    public synchronized void close() throws IOException {\n+        clearCache();\n+        buffers.clear();\n+        raf.close();\n+        inf.end();\n+    }\n+\n+    \/**\n+     * Reads bytes from the gzip file.\n+     *\n+     * @param offset The offset from which to start the read.\n+     * @param b The buffer to read into.\n+     * @param off The offset in the buffer to use.\n+     * @param len The number of bytes to read at most.\n+     * @return The number of bytes read or -1 if we are at the end of the file.\n+     * @throws IOException On error.\n+     *\/\n+    public synchronized int read(long offset, byte[] b, int off, int len) throws IOException {\n+        Buffer buf = last;\n+\n+        while (buf == null || (buf.offset > offset) || (buf.offset + buf.cacheLen <= offset)) {\n+            int pos = Collections.binarySearch(buffers, new Buffer(0, offset), offsetComp);\n+            buf = buffers.get(pos >= 0 ? pos : -pos - 2);\n+\n+            if (buf.fileOffset >= fileSize) {\n+                return -1;\n+            }\n+\n+            if (buf.cache != null) {\n+                \/\/ If already loaded, move to front of the cache list.\n+                last = buf;\n+\n+                if (cacheHead.next != buf) {\n+                    remove(buf);\n+                    addFirst(buf);\n+                }\n+            } else {\n+                try {\n+                    \/\/ Note that the load will also add the following buffer to the list,\n+                    \/\/ so the while loop will eventually terminate.\n+                    loadBuffer(buf);\n+                } catch (DataFormatException e) {\n+                    throw new IOException(e);\n+                }\n+            }\n+        }\n+\n+        int copyOffset = (int) (offset - buf.offset);\n+        int toCopyMax = buf.cacheLen - copyOffset;\n+        int toCopy = Math.min(toCopyMax, len);\n+\n+        if (toCopy <= 0) {\n+            return -1;\n+        }\n+\n+        System.arraycopy(buf.cache, copyOffset, b, off, toCopy);\n+\n+        return toCopy;\n+    }\n+\n+    \/**\n+     * Returns the access object for the given file or <code>null<\/code> if not\n+     * supported for the file.\n+     *\n+     * @param file The file name.\n+     * @param cacheSizeInMB The size of the cache to use in megabytes.\n+     * @return The access object or <code>null<\/code>.\n+     *\/\n+    public static GzipRandomAccess getAccess(String file, int cacheSizeInMB)\n+            throws IOException {\n+        try  (RandomAccessFile raf = new RandomAccessFile(file, \"r\")) {\n+            int header = raf.readInt();\n+\n+            if ((header >>> 8) != 0x1f8b08) {\n+                \/\/ No gzip with deflate.\n+                return null;\n+            }\n+\n+            if ((header & 16) == 0) {\n+                \/\/ No comment\n+                return null;\n+            }\n+\n+            raf.readInt(); \/\/ timestamp\n+            raf.readChar(); \/\/ Extra flags and os.\n+\n+            if ((header & 4) != 0) {\n+                \/\/ Skip extra flags.\n+                raf.skipBytes(raf.read() + (raf.read() << 256));\n+            }\n+\n+            \/\/ Skip name\n+            if ((header & 8) != 0) {\n+                while (raf.read() != 0) {\n+                    \/\/ Wait for the last 0.\n+                }\n+            }\n+\n+            \/\/ Read the comment.\n+            ByteArrayOutputStream bos = new ByteArrayOutputStream();\n+            int b;\n+\n+            while ((b = raf.read()) > 0) {\n+                bos.write(b);\n+            }\n+\n+            \/\/ Check if the block size is included in the comment.\n+            String comment = bos.toString(\"UTF-8\");\n+            String expectedPrefix = \"HPROF BLOCKSIZE=\";\n+\n+            if (comment.startsWith(expectedPrefix)) {\n+                String chunkSizeStr = comment.substring(expectedPrefix.length()).split(\" \")[0];\n+\n+                try {\n+                    int chunkSize = Integer.parseInt(chunkSizeStr);\n+\n+                    if (chunkSize > 0) {\n+                        long nrOfChunks = Math.max(1000, cacheSizeInMB * 1024L * 1024L \/\n+                                                         chunkSize);\n+\n+                        return new GzipRandomAccess(file, chunkSize, (int) nrOfChunks);\n+                    }\n+                } catch (NumberFormatException e) {\n+                    \/\/ Could not parse.\n+                }\n+            }\n+        }\n+\n+        return null;\n+    }\n+\n+    \/\/ Loads the content of a buffer. If this is the first time the buffer is\n+    \/\/ loaded, the next buffer is added too (but not loaded).\n+    private void loadBuffer(Buffer buf) throws IOException, DataFormatException {\n+        \/\/ If we have used all caches, take a cache from the least recently used cached buffer.\n+        if (cachedBuffers >= maxCachedBuffers) {\n+            Buffer toRemove = cacheHead.prev;\n+            remove(toRemove);\n+            buf.cache = toRemove.cache;\n+            toRemove.cache = null;\n+        } else {\n+            \/\/ Otherwise allocate a new cache.\n+            buf.cache = new byte[cacheSize];\n+            cachedBuffers += 1;\n+        }\n+\n+        \/\/ Move to front of LRU list.\n+        last = buf;\n+        addFirst(buf);\n+\n+        \/\/ Fill in the cache\n+        inf.reset();\n+        raf.seek(buf.fileOffset);\n+\n+        int read = raf.read(in, 0, READ_SIZE);\n+        int inCount = read;\n+        int outCount = 0;\n+\n+        \/\/ Skip header, but check at least a little\n+        if (read < 4) {\n+            throw new IOException(\"Missing data\");\n+        }\n+\n+        if ((in[0] != 0x1f) || ((in[1] & 0xff) != 0x8b)) {\n+            throw new IOException(\"Missing gzip id\");\n+        }\n+\n+        if (in[2] != 8) {\n+            throw new IOException(\"Only supports deflate\");\n+        }\n+\n+        int off = 10;\n+\n+        \/\/ Extras\n+        if ((in[3] & 4) != 0) {\n+            int len = (in[off + 1] & 0xff) * 256 + (in[off] & 0xff);\n+            off += 2 + len;\n+        }\n+\n+        \/\/ Name\n+        if ((in[3] & 8) != 0) {\n+            int len = 0;\n+\n+            while (in[off + len] != 0) {\n+                ++len;\n+            }\n+\n+            off += len + 1;\n+        }\n+\n+        \/\/ Comment\n+        if ((in[3] & 16) != 0) {\n+            int len = 0;\n+\n+            while (in[off + len] != 0) {\n+                ++len;\n+            }\n+\n+            off += len + 1;\n+        }\n+\n+        \/\/ Header CRC\n+        if ((in[3] & 2) != 0) {\n+            off += 2;\n+        }\n+\n+        inf.setInput(in, off, read - off);\n+        outCount = inf.inflate(buf.cache, 0, buf.cache.length);\n+\n+        while (!inf.finished()) {\n+            if (inf.needsInput()) {\n+                read = raf.read(in, 0, READ_SIZE);\n+                inf.setInput(in, 0, read);\n+                inCount += read;\n+            }\n+\n+            outCount += inf.inflate(buf.cache, outCount, buf.cache.length - outCount);\n+        }\n+\n+        \/\/ Add the following buffer too if needed.\n+        if ((inf.getRemaining() != 0) || (inCount + buf.fileOffset + 8 != fileSize)) {\n+            long nextFileOffset = inCount - inf.getRemaining() + buf.fileOffset + 8 \/* CRC *\/;\n+            long nextOffset = outCount + buf.offset;\n+\n+            Buffer nextChunk = new Buffer(nextFileOffset, nextOffset);\n+            int pos = Collections.binarySearch(buffers, nextChunk, fileOffsetComp);\n+\n+            if (pos < 0) {\n+                buffers.add(-pos - 1, nextChunk);\n+            }\n+        }\n+\n+        buf.cacheLen = outCount;\n+    }\n+\n+    \/\/ Adds the buffer to the front of the LRU list.\n+    private void addFirst(Buffer buf) {\n+        assert buf.next == null;\n+        assert buf.prev == null;\n+        assert buf.cache != null;\n+\n+        if (cacheHead.prev == cacheHead) {\n+            cacheHead.prev = buf;\n+        }\n+\n+        cacheHead.next.prev = buf;\n+        buf.next = cacheHead.next;\n+        buf.prev = cacheHead;\n+        cacheHead.next = buf;\n+    }\n+\n+    \/\/ Removes the buffer from the LRU list.\n+    private void remove(Buffer buf) {\n+        assert buf.prev != null;\n+        assert buf.next != null;\n+        assert buf.cache != null;\n+        assert cacheHead.prev != cacheHead;\n+\n+        buf.prev.next = buf.next;\n+        buf.next.prev = buf.prev;\n+        buf.next = null;\n+        buf.prev = null;\n+    }\n+\n+    \/\/ Represents a gzipped buffer. The gzipped hprof file consists of a list of these buffers.\n+    private static class Buffer {\n+        public byte[] cache;\n+        public int cacheLen;\n+        public final long fileOffset;\n+        public final long offset;\n+        public Buffer next;\n+        public Buffer prev;\n+\n+        public Buffer(long fileOffset, long offset) {\n+            this.cache = null;\n+            this.cacheLen = 0;\n+            this.fileOffset = fileOffset;\n+            this.offset = offset;\n+            this.next = null;\n+            this.prev = null;\n+        }\n+    }\n+\n+    \/\/ Compares chunks by file offset.\n+    private static class FileOffsetComparator implements Comparator<Buffer> {\n+\n+        @Override\n+        public int compare(Buffer x, Buffer y) {\n+            return Long.compare(x.fileOffset, y.fileOffset);\n+        }\n+    }\n+\n+    \/\/ Compares chunks by offset.\n+    private static class OffsetComparator implements Comparator<Buffer> {\n+\n+        @Override\n+        public int compare(Buffer x, Buffer y) {\n+            return Long.compare(x.offset, y.offset);\n+        }\n+    }\n+\n+    \/\/ Implements an InputStream for this object.\n+    private static class InputStreamImpl extends InputStream {\n+\n+        private long offset;\n+        private final GzipRandomAccess access;\n+\n+        public InputStreamImpl(long offset, GzipRandomAccess access) {\n+            this.offset = offset;\n+            this.access = access;\n+        }\n+\n+        @Override\n+        public synchronized int read(byte[] b, int off, int len) throws IOException {\n+            int read = access.read(offset, b, off, len);\n+\n+            if (read > 0) {\n+                this.offset += read;\n+            }\n+\n+            return read;\n+        }\n+\n+        @Override\n+        public int read() throws IOException {\n+            byte[] b = new byte[1];\n+            int read = read(b, 0, 1);\n+\n+            if (read != 1) {\n+                return -1;\n+            }\n+\n+            return b[0] & 0xff;\n+        }\n+    }\n+\n+    \/\/ Implements a ReadBuffer for this object.\n+    public static class ReadBufferImpl implements ReadBuffer {\n+\n+        private final GzipRandomAccess access;\n+        private final byte[] tmp = new byte[8];\n+\n+        public ReadBufferImpl(GzipRandomAccess access) {\n+            this.access = access;\n+        }\n+\n+        private void readFully(long pos, int len) throws IOException {\n+            int left = len;\n+            int off = 0;\n+\n+            while (left > 0) {\n+                int read = access.read(pos, tmp, off, left);\n+\n+                if (read <= 0) {\n+                    throw new EOFException(\"Could not read at \" + pos);\n+                }\n+\n+                left -= read;\n+                off += read;\n+                pos += read;\n+            }\n+        }\n+\n+        private int readInt(int offset) {\n+            return (((tmp[offset + 0] & 0xff) << 24) | ((tmp[offset + 1] & 0xff) << 16) |\n+                     ((tmp[offset + 2] & 0xff) << 8) | (tmp[offset + 3] & 0xff));\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            access.close();\n+        }\n+\n+        @Override\n+        public char getChar(long pos) throws IOException {\n+            readFully(pos, 2);\n+            return (char) (((tmp[0] & 0xff) << 8) | (tmp[1] & 0xff));\n+        }\n+\n+        @Override\n+        public byte getByte(long pos) throws IOException {\n+            readFully(pos, 1);\n+            return tmp[0];\n+        }\n+\n+        @Override\n+        public short getShort(long pos) throws IOException {\n+            return (short) getChar(pos);\n+        }\n+\n+        @Override\n+        public int getInt(long pos) throws IOException {\n+            readFully(pos, 4);\n+            return readInt(0);\n+        }\n+\n+        @Override\n+        public long getLong(long pos) throws IOException {\n+            readFully(pos, 8);\n+            long i1 = readInt(0);\n+            int i2 = readInt(4);\n+\n+            return (i1 << 32) | (i2 & 0xffffffffl);\n+        }\n+    }\n+}\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/parser\/GzipRandomAccess.java","additions":577,"deletions":0,"binary":false,"changes":577,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -159,1 +159,1 @@\n-    public HprofReader(String fileName, PositionDataInputStream in,\n+    public HprofReader(ReadBuffer readBuffer, PositionDataInputStream in,\n@@ -163,2 +163,1 @@\n-        RandomAccessFile file = new RandomAccessFile(fileName, \"r\");\n-        this.snapshot = new Snapshot(MappedReadBuffer.create(file));\n+        this.snapshot = new Snapshot(readBuffer);\n@@ -178,0 +177,7 @@\n+    public HprofReader(String fileName, PositionDataInputStream in,\n+                       int dumpNumber, boolean callStack, int debugLevel)\n+                       throws IOException {\n+        this(MappedReadBuffer.create(new RandomAccessFile(fileName, \"r\")),\n+            in, dumpNumber, callStack, debugLevel);\n+    }\n+\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/parser\/HprofReader.java","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -84,0 +84,1 @@\n+        GzipRandomAccess access = null;\n@@ -92,0 +93,14 @@\n+            } else if ((access = GzipRandomAccess.getAccess(heapFile, 16)) != null) {\n+                in.close();\n+                try (PositionDataInputStream in2 = new PositionDataInputStream(\n+                        new BufferedInputStream(access.asStream(0)))) {\n+                    i = in2.readInt();\n+                    if (i == HprofReader.MAGIC_NUMBER) {\n+                        Reader r\n+                            = new HprofReader(access.asFileBuffer(), in2, dumpNumber,\n+                                              callStack, debugLevel);\n+                        return r.read();\n+                    } else {\n+                        throw new IOException(\"Wrong magic number in gzipped file: \" + i);\n+                    }\n+                }\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/parser\/Reader.java","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"}]}