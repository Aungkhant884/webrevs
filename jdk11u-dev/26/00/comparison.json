{"files":[{"patch":"@@ -142,1 +142,102 @@\n-void G1ParScanThreadState::trim_queue() {\n+template <class T> void G1ParScanThreadState::do_oop_evac(T* p) {\n+  \/\/ Reference should not be NULL here as such are never pushed to the task queue.\n+  oop obj = RawAccess<IS_NOT_NULL>::oop_load(p);\n+\n+  \/\/ Although we never intentionally push references outside of the collection\n+  \/\/ set, due to (benign) races in the claim mechanism during RSet scanning more\n+  \/\/ than one thread might claim the same card. So the same card may be\n+  \/\/ processed multiple times, and so we might get references into old gen here.\n+  \/\/ So we need to redo this check.\n+  const InCSetState in_cset_state = _g1h->in_cset_state(obj);\n+  if (in_cset_state.is_in_cset()) {\n+    markOop m = obj->mark_raw();\n+    if (m->is_marked()) {\n+      obj = (oop) m->decode_pointer();\n+    } else {\n+      obj = do_copy_to_survivor_space(in_cset_state, obj, m);\n+    }\n+    RawAccess<IS_NOT_NULL>::oop_store(p, obj);\n+  } else if (in_cset_state.is_humongous()) {\n+    _g1h->set_humongous_is_live(obj);\n+  } else {\n+    assert(in_cset_state.is_default(),\n+           \"In_cset_state must be NotInCSet here, but is \" CSETSTATE_FORMAT, in_cset_state.value());\n+  }\n+\n+  assert(obj != NULL, \"Must be\");\n+  if (!HeapRegion::is_in_same_region(p, obj)) {\n+    HeapRegion* from = _g1h->heap_region_containing(p);\n+    update_rs(from, p, obj);\n+  }\n+}\n+\n+void G1ParScanThreadState::do_oop_partial_array(oop* p) {\n+  assert(has_partial_array_mask(p), \"invariant\");\n+  oop from_obj = clear_partial_array_mask(p);\n+\n+  assert(_g1h->is_in_reserved(from_obj), \"must be in heap.\");\n+  assert(from_obj->is_objArray(), \"must be obj array\");\n+  objArrayOop from_obj_array = objArrayOop(from_obj);\n+  \/\/ The from-space object contains the real length.\n+  int length                 = from_obj_array->length();\n+\n+  assert(from_obj->is_forwarded(), \"must be forwarded\");\n+  oop to_obj                 = from_obj->forwardee();\n+  assert(from_obj != to_obj, \"should not be chunking self-forwarded objects\");\n+  objArrayOop to_obj_array   = objArrayOop(to_obj);\n+  \/\/ We keep track of the next start index in the length field of the\n+  \/\/ to-space object.\n+  int next_index             = to_obj_array->length();\n+  assert(0 <= next_index && next_index < length,\n+         \"invariant, next index: %d, length: %d\", next_index, length);\n+\n+  int start                  = next_index;\n+  int end                    = length;\n+  int remainder              = end - start;\n+  \/\/ We'll try not to push a range that's smaller than ParGCArrayScanChunk.\n+  if (remainder > 2 * ParGCArrayScanChunk) {\n+    end = start + ParGCArrayScanChunk;\n+    to_obj_array->set_length(end);\n+    \/\/ Push the remainder before we process the range in case another\n+    \/\/ worker has run out of things to do and can steal it.\n+    oop* from_obj_p = set_partial_array_mask(from_obj);\n+    push_on_queue(from_obj_p);\n+  } else {\n+    assert(length == end, \"sanity\");\n+    \/\/ We'll process the final range for this object. Restore the length\n+    \/\/ so that the heap remains parsable in case of evacuation failure.\n+    to_obj_array->set_length(end);\n+  }\n+  _scanner.set_region(_g1h->heap_region_containing(to_obj));\n+  \/\/ Process indexes [start,end). It will also process the header\n+  \/\/ along with the first chunk (i.e., the chunk with start == 0).\n+  \/\/ Note that at this point the length field of to_obj_array is not\n+  \/\/ correct given that we are using it to keep track of the next\n+  \/\/ start index. oop_iterate_range() (thankfully!) ignores the length\n+  \/\/ field and only relies on the start \/ end parameters.  It does\n+  \/\/ however return the size of the object which will be incorrect. So\n+  \/\/ we have to ignore it even if we wanted to use it.\n+  to_obj_array->oop_iterate_range(&_scanner, start, end);\n+}\n+\n+void G1ParScanThreadState::dispatch_reference(StarTask ref) {\n+  assert(verify_task(ref), \"sanity\");\n+  if (ref.is_narrow()) {\n+    narrowOop* ref_to_scan = (narrowOop*)ref;\n+    assert(!has_partial_array_mask(ref_to_scan), \"NarrowOop* elements should never be partial arrays.\");\n+    do_oop_evac(ref_to_scan);\n+  } else {\n+    oop* ref_to_scan = (oop*)ref;\n+    if (!has_partial_array_mask(ref_to_scan)) {\n+      do_oop_evac(ref_to_scan);\n+    } else {\n+      do_oop_partial_array(ref_to_scan);\n+    }\n+  }\n+}\n+\n+\/\/ Process tasks until overflow queue is empty and local queue\n+\/\/ contains no more than threshold entries.  NOINLINE to prevent\n+\/\/ inlining into steal_and_trim_queue.\n+ATTRIBUTE_FLATTEN NOINLINE\n+void G1ParScanThreadState::trim_queue_to_threshold(uint threshold) {\n@@ -145,3 +246,52 @@\n-    \/\/ Fully drain the queue.\n-    trim_queue_to_threshold(0);\n-  } while (!_refs->is_empty());\n+    while (_refs->pop_overflow(ref)) {\n+      if (!_refs->try_push_to_taskqueue(ref)) {\n+        dispatch_reference(ref);\n+      }\n+    }\n+\n+    while (_refs->pop_local(ref, threshold)) {\n+      dispatch_reference(ref);\n+    }\n+  } while (!_refs->overflow_empty());\n+}\n+\n+ATTRIBUTE_FLATTEN\n+void G1ParScanThreadState::steal_and_trim_queue(RefToScanQueueSet *task_queues) {\n+  StarTask stolen_task;\n+  while (task_queues->steal(_worker_id, &_hash_seed, stolen_task)) {\n+    assert(verify_task(stolen_task), \"sanity\");\n+    dispatch_reference(stolen_task);\n+    \/\/ Processing stolen task may have added tasks to our queue.\n+    trim_queue();\n+  }\n+}\n+\n+NOINLINE\n+HeapWord* G1ParScanThreadState::allocate_copy_slow(InCSetState const state,\n+                                                   InCSetState* dest_state,\n+                                                   oop old,\n+                                                   size_t word_sz,\n+                                                   uint age) {\n+  HeapWord* obj_ptr = NULL;\n+  \/\/ Try slow-path allocation unless we're allocating old and old is already full.\n+  if (!(dest_state->is_old() && _old_gen_is_full)) {\n+    bool plab_refill_failed = false;\n+    obj_ptr = _plab_allocator->allocate_direct_or_new_plab(*dest_state, word_sz, &plab_refill_failed);\n+    if (obj_ptr == NULL) {\n+      obj_ptr = allocate_in_next_plab(state, dest_state, word_sz, plab_refill_failed);\n+    }\n+  }\n+  if (obj_ptr != NULL) {\n+    if (_g1h->_gc_tracer_stw->should_report_promotion_events()) {\n+      \/\/ The events are checked individually as part of the actual commit\n+      report_promotion_event(*dest_state, old, word_sz, age, obj_ptr);\n+    }\n+  }\n+  return obj_ptr;\n+}\n+\n+NOINLINE\n+void G1ParScanThreadState::undo_allocation(InCSetState dest_state,\n+                                           HeapWord* obj_ptr,\n+                                           size_t word_sz) {\n+  _plab_allocator->undo_allocation(dest_state, obj_ptr, word_sz);\n@@ -214,3 +364,8 @@\n-oop G1ParScanThreadState::copy_to_survivor_space(InCSetState const state,\n-                                                 oop const old,\n-                                                 markOop const old_mark) {\n+\/\/ Private inline function, for direct internal use and providing the\n+\/\/ implementation of the public not-inline function.\n+oop G1ParScanThreadState::do_copy_to_survivor_space(InCSetState const state,\n+                                                    oop const old,\n+                                                    markOop const old_mark) {\n+  assert(state.is_in_cset(),\n+         \"Unexpected region attr type: \" CSETSTATE_FORMAT, state.value());\n+\n@@ -219,4 +374,7 @@\n-  \/\/ +1 to make the -1 indexes valid...\n-  const int young_index = from_region->young_index_in_cset()+1;\n-  assert( (from_region->is_young() && young_index >  0) ||\n-         (!from_region->is_young() && young_index == 0), \"invariant\" );\n+  {\n+    \/\/ +1 to make the -1 indexes valid...\n+    const int young_index = from_region->young_index_in_cset()+1;\n+    assert( (from_region->is_young() && young_index >  0) ||\n+           (!from_region->is_young() && young_index == 0), \"invariant\" );\n+    _surviving_young_words[young_index] += word_sz;\n+  }\n@@ -226,5 +384,0 @@\n-  \/\/ The second clause is to prevent premature evacuation failure in case there\n-  \/\/ is still space in survivor, but old gen is full.\n-  if (_old_gen_is_full && dest_state.is_old()) {\n-    return handle_evacuation_failure_par(old, old_mark);\n-  }\n@@ -236,2 +389,1 @@\n-    bool plab_refill_failed = false;\n-    obj_ptr = _plab_allocator->allocate_direct_or_new_plab(dest_state, word_sz, &plab_refill_failed);\n+    obj_ptr = allocate_copy_slow(state, &dest_state, old, word_sz, age);\n@@ -239,10 +391,3 @@\n-      obj_ptr = allocate_in_next_plab(state, &dest_state, word_sz, plab_refill_failed);\n-      if (obj_ptr == NULL) {\n-        \/\/ This will either forward-to-self, or detect that someone else has\n-        \/\/ installed a forwarding pointer.\n-        return handle_evacuation_failure_par(old, old_mark);\n-      }\n-    }\n-    if (_g1h->_gc_tracer_stw->should_report_promotion_events()) {\n-      \/\/ The events are checked individually as part of the actual commit\n-      report_promotion_event(dest_state, old, word_sz, age, obj_ptr);\n+      \/\/ This will either forward-to-self, or detect that someone else has\n+      \/\/ installed a forwarding pointer.\n+      return handle_evacuation_failure_par(old, old_mark);\n@@ -305,2 +450,0 @@\n-    _surviving_young_words[young_index] += word_sz;\n-\n@@ -321,1 +464,1 @@\n-    _plab_allocator->undo_allocation(dest_state, obj_ptr, word_sz);\n+    undo_allocation(dest_state, obj_ptr, word_sz);\n@@ -326,0 +469,8 @@\n+\/\/ Public not-inline entry point.\n+ATTRIBUTE_FLATTEN\n+oop G1ParScanThreadState::copy_to_survivor_space(InCSetState state,\n+                                                 oop old,\n+                                                 markOop old_mark) {\n+  return do_copy_to_survivor_space(state, old, old_mark);\n+}\n+\n@@ -356,0 +507,1 @@\n+NOINLINE\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":183,"deletions":31,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -168,0 +168,14 @@\n+  HeapWord* allocate_copy_slow(InCSetState const state,\n+                               InCSetState* dest_state,\n+                               oop old,\n+                               size_t word_sz,\n+                               uint age);\n+\n+  void undo_allocation(InCSetState dest_state,\n+                       HeapWord* obj_ptr,\n+                       size_t word_sz);\n+\n+  inline oop do_copy_to_survivor_space(InCSetState state,\n+                                       oop old,\n+                                       markOop old_mark);\n+\n@@ -194,0 +208,2 @@\n+  void trim_queue_to_threshold(uint threshold);\n+\n@@ -195,1 +211,0 @@\n-  inline bool is_partially_trimmed() const;\n@@ -197,1 +212,0 @@\n-  inline void trim_queue_to_threshold(uint threshold);\n@@ -199,1 +213,1 @@\n-  oop copy_to_survivor_space(InCSetState const state, oop const obj, markOop const old_mark);\n+  oop copy_to_survivor_space(InCSetState state, oop obj, markOop old_mark);\n@@ -201,2 +215,3 @@\n-  void trim_queue();\n-  void trim_queue_partially();\n+  inline void trim_queue();\n+  inline void trim_queue_partially();\n+  void steal_and_trim_queue(RefToScanQueueSet *task_queues);\n@@ -207,2 +222,0 @@\n-  inline void steal_and_trim_queue(RefToScanQueueSet *task_queues);\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":20,"deletions":7,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -33,32 +33,0 @@\n-template <class T> void G1ParScanThreadState::do_oop_evac(T* p) {\n-  \/\/ Reference should not be NULL here as such are never pushed to the task queue.\n-  oop obj = RawAccess<IS_NOT_NULL>::oop_load(p);\n-\n-  \/\/ Although we never intentionally push references outside of the collection\n-  \/\/ set, due to (benign) races in the claim mechanism during RSet scanning more\n-  \/\/ than one thread might claim the same card. So the same card may be\n-  \/\/ processed multiple times, and so we might get references into old gen here.\n-  \/\/ So we need to redo this check.\n-  const InCSetState in_cset_state = _g1h->in_cset_state(obj);\n-  if (in_cset_state.is_in_cset()) {\n-    markOop m = obj->mark_raw();\n-    if (m->is_marked()) {\n-      obj = (oop) m->decode_pointer();\n-    } else {\n-      obj = copy_to_survivor_space(in_cset_state, obj, m);\n-    }\n-    RawAccess<IS_NOT_NULL>::oop_store(p, obj);\n-  } else if (in_cset_state.is_humongous()) {\n-    _g1h->set_humongous_is_live(obj);\n-  } else {\n-    assert(in_cset_state.is_default(),\n-           \"In_cset_state must be NotInCSet here, but is \" CSETSTATE_FORMAT, in_cset_state.value());\n-  }\n-\n-  assert(obj != NULL, \"Must be\");\n-  if (!HeapRegion::is_in_same_region(p, obj)) {\n-    HeapRegion* from = _g1h->heap_region_containing(p);\n-    update_rs(from, p, obj);\n-  }\n-}\n-\n@@ -70,85 +38,1 @@\n-inline void G1ParScanThreadState::do_oop_partial_array(oop* p) {\n-  assert(has_partial_array_mask(p), \"invariant\");\n-  oop from_obj = clear_partial_array_mask(p);\n-\n-  assert(_g1h->is_in_reserved(from_obj), \"must be in heap.\");\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n-  objArrayOop from_obj_array = objArrayOop(from_obj);\n-  \/\/ The from-space object contains the real length.\n-  int length                 = from_obj_array->length();\n-\n-  assert(from_obj->is_forwarded(), \"must be forwarded\");\n-  oop to_obj                 = from_obj->forwardee();\n-  assert(from_obj != to_obj, \"should not be chunking self-forwarded objects\");\n-  objArrayOop to_obj_array   = objArrayOop(to_obj);\n-  \/\/ We keep track of the next start index in the length field of the\n-  \/\/ to-space object.\n-  int next_index             = to_obj_array->length();\n-  assert(0 <= next_index && next_index < length,\n-         \"invariant, next index: %d, length: %d\", next_index, length);\n-\n-  int start                  = next_index;\n-  int end                    = length;\n-  int remainder              = end - start;\n-  \/\/ We'll try not to push a range that's smaller than ParGCArrayScanChunk.\n-  if (remainder > 2 * ParGCArrayScanChunk) {\n-    end = start + ParGCArrayScanChunk;\n-    to_obj_array->set_length(end);\n-    \/\/ Push the remainder before we process the range in case another\n-    \/\/ worker has run out of things to do and can steal it.\n-    oop* from_obj_p = set_partial_array_mask(from_obj);\n-    push_on_queue(from_obj_p);\n-  } else {\n-    assert(length == end, \"sanity\");\n-    \/\/ We'll process the final range for this object. Restore the length\n-    \/\/ so that the heap remains parsable in case of evacuation failure.\n-    to_obj_array->set_length(end);\n-  }\n-  _scanner.set_region(_g1h->heap_region_containing(to_obj));\n-  \/\/ Process indexes [start,end). It will also process the header\n-  \/\/ along with the first chunk (i.e., the chunk with start == 0).\n-  \/\/ Note that at this point the length field of to_obj_array is not\n-  \/\/ correct given that we are using it to keep track of the next\n-  \/\/ start index. oop_iterate_range() (thankfully!) ignores the length\n-  \/\/ field and only relies on the start \/ end parameters.  It does\n-  \/\/ however return the size of the object which will be incorrect. So\n-  \/\/ we have to ignore it even if we wanted to use it.\n-  to_obj_array->oop_iterate_range(&_scanner, start, end);\n-}\n-\n-inline void G1ParScanThreadState::deal_with_reference(oop* ref_to_scan) {\n-  if (!has_partial_array_mask(ref_to_scan)) {\n-    do_oop_evac(ref_to_scan);\n-  } else {\n-    do_oop_partial_array(ref_to_scan);\n-  }\n-}\n-\n-inline void G1ParScanThreadState::deal_with_reference(narrowOop* ref_to_scan) {\n-  assert(!has_partial_array_mask(ref_to_scan), \"NarrowOop* elements should never be partial arrays.\");\n-  do_oop_evac(ref_to_scan);\n-}\n-\n-inline void G1ParScanThreadState::dispatch_reference(StarTask ref) {\n-  assert(verify_task(ref), \"sanity\");\n-  if (ref.is_narrow()) {\n-    deal_with_reference((narrowOop*)ref);\n-  } else {\n-    deal_with_reference((oop*)ref);\n-  }\n-}\n-\n-void G1ParScanThreadState::steal_and_trim_queue(RefToScanQueueSet *task_queues) {\n-  StarTask stolen_task;\n-  while (task_queues->steal(_worker_id, &_hash_seed, stolen_task)) {\n-    assert(verify_task(stolen_task), \"sanity\");\n-    dispatch_reference(stolen_task);\n-\n-    \/\/ We've just processed a reference and we might have made\n-    \/\/ available new entries on the queues. So we have to make sure\n-    \/\/ we drain the queues as necessary.\n-    trim_queue();\n-  }\n-}\n-\n-inline bool G1ParScanThreadState::needs_partial_trimming() const {\n+bool G1ParScanThreadState::needs_partial_trimming() const {\n@@ -158,19 +42,1 @@\n-inline bool G1ParScanThreadState::is_partially_trimmed() const {\n-  return _refs->overflow_empty() && _refs->size() <= _stack_trim_lower_threshold;\n-}\n-\n-inline void G1ParScanThreadState::trim_queue_to_threshold(uint threshold) {\n-  StarTask ref;\n-  \/\/ Drain the overflow stack first, so other threads can potentially steal.\n-  while (_refs->pop_overflow(ref)) {\n-    if (!_refs->try_push_to_taskqueue(ref)) {\n-      dispatch_reference(ref);\n-    }\n-  }\n-\n-  while (_refs->pop_local(ref, threshold)) {\n-    dispatch_reference(ref);\n-  }\n-}\n-\n-inline void G1ParScanThreadState::trim_queue_partially() {\n+void G1ParScanThreadState::trim_queue_partially() {\n@@ -182,3 +48,3 @@\n-  do {\n-    trim_queue_to_threshold(_stack_trim_lower_threshold);\n-  } while (!is_partially_trimmed());\n+  trim_queue_to_threshold(_stack_trim_lower_threshold);\n+  assert(_refs->overflow_empty(), \"invariant\");\n+  assert(_refs->size() <= _stack_trim_lower_threshold, \"invariant\");\n@@ -188,0 +54,7 @@\n+void G1ParScanThreadState::trim_queue() {\n+  StarTask ref;\n+  trim_queue_to_threshold(0);\n+  assert(_refs->overflow_empty(), \"invariant\");\n+  assert(_refs->taskqueue_empty(), \"invariant\");\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.inline.hpp","additions":12,"deletions":139,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -46,0 +46,4 @@\n+#ifndef ATTRIBUTE_FLATTEN\n+#define ATTRIBUTE_FLATTEN\n+#endif\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -274,0 +274,1 @@\n+#define ATTRIBUTE_FLATTEN __attribute__ ((flatten))\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions_gcc.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}