{"files":[{"patch":"@@ -377,1 +377,1 @@\n-extern int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,\n+extern int vec_spill_helper(CodeBuffer *cbuf, bool is_load,\n@@ -438,1 +438,1 @@\n-    vec_spill_helper(__ code(), false \/* do_size *\/, false \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n+    vec_spill_helper(__ code(), false \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n@@ -444,1 +444,1 @@\n-    vec_spill_helper(__ code(), false \/* do_size *\/, true \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n+    vec_spill_helper(__ code(), true \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2132,1 +2132,1 @@\n-static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,\n+static void vec_mov_helper(CodeBuffer *cbuf, int src_lo, int dst_lo,\n@@ -2134,3 +2134,0 @@\n-  \/\/ In 64-bit VM size calculation is very complex. Emitting instructions\n-  \/\/ into scratch buffer is used to get size in 64-bit VM.\n-  LP64_ONLY( assert(!do_size, \"this method calculates size only for 32-bit VM\"); )\n@@ -2143,1 +2140,0 @@\n-    int offset = __ offset();\n@@ -2175,6 +2171,0 @@\n-    int size = __ offset() - offset;\n-#ifdef ASSERT\n-    \/\/ VEX_2bytes prefix is used if UseAVX > 0, so it takes the same 2 bytes as SIMD prefix.\n-    assert(!do_size || size == 4, \"incorrect size calculattion\");\n-#endif\n-    return size;\n@@ -2182,1 +2172,1 @@\n-  } else if (!do_size) {\n+  } else {\n@@ -2198,2 +2188,0 @@\n-  \/\/ VEX_2bytes prefix is used if UseAVX > 0, and it takes the same 2 bytes as SIMD prefix.\n-  return (UseAVX > 2) ? 6 : 4;\n@@ -2202,1 +2190,1 @@\n-int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,\n+void vec_spill_helper(CodeBuffer *cbuf, bool is_load,\n@@ -2204,3 +2192,0 @@\n-  \/\/ In 64-bit VM size calculation is very complex. Emitting instructions\n-  \/\/ into scratch buffer is used to get size in 64-bit VM.\n-  LP64_ONLY( assert(!do_size, \"this method calculates size only for 32-bit VM\"); )\n@@ -2209,1 +2194,0 @@\n-    int offset = __ offset();\n@@ -2287,7 +2271,0 @@\n-    int size = __ offset() - offset;\n-#ifdef ASSERT\n-    int offset_size = (stack_offset == 0) ? 0 : ((stack_offset < 0x80) ? 1 : (UseAVX > 2) ? 6 : 4);\n-    \/\/ VEX_2bytes prefix is used if UseAVX > 0, so it takes the same 2 bytes as SIMD prefix.\n-    assert(!do_size || size == (5+offset_size), \"incorrect size calculattion\");\n-#endif\n-    return size;\n@@ -2295,1 +2272,1 @@\n-  } else if (!do_size) {\n+  } else {\n@@ -2335,41 +2312,0 @@\n-  bool is_single_byte = false;\n-  int vec_len = 0;\n-  if ((UseAVX > 2) && (stack_offset != 0)) {\n-    int tuple_type = Assembler::EVEX_FVM;\n-    int input_size = Assembler::EVEX_32bit;\n-    switch (ireg) {\n-    case Op_VecS:\n-      tuple_type = Assembler::EVEX_T1S;\n-      break;\n-    case Op_VecD:\n-      tuple_type = Assembler::EVEX_T1S;\n-      input_size = Assembler::EVEX_64bit;\n-      break;\n-    case Op_VecX:\n-      break;\n-    case Op_VecY:\n-      vec_len = 1;\n-      break;\n-    case Op_VecZ:\n-      vec_len = 2;\n-      break;\n-    }\n-    is_single_byte = Assembler::query_compressed_disp_byte(stack_offset, true, vec_len, tuple_type, input_size, 0);\n-  }\n-  int offset_size = 0;\n-  int size = 5;\n-  if (UseAVX > 2 ) {\n-    if (VM_Version::supports_avx512novl() && (vec_len == 2)) {\n-      offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);\n-      size += 2; \/\/ Need an additional two bytes for EVEX encoding\n-    } else if (VM_Version::supports_avx512novl() && (vec_len < 2)) {\n-      offset_size = (stack_offset == 0) ? 0 : ((stack_offset <= 127) ? 1 : 4);\n-    } else {\n-      offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);\n-      size += 2; \/\/ Need an additional two bytes for EVEX encodding\n-    }\n-  } else {\n-    offset_size = (stack_offset == 0) ? 0 : ((stack_offset <= 127) ? 1 : 4);\n-  }\n-  \/\/ VEX_2bytes prefix is used if UseAVX > 0, so it takes the same 2 bytes as SIMD prefix.\n-  return size+offset_size;\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":68,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -948,1 +948,1 @@\n-static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,\n+static void vec_mov_helper(CodeBuffer *cbuf, int src_lo, int dst_lo,\n@@ -951,1 +951,1 @@\n-static int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,\n+void vec_spill_helper(CodeBuffer *cbuf, bool is_load,\n@@ -954,1 +954,1 @@\n-static int vec_stack_to_stack_helper(CodeBuffer *cbuf, bool do_size, int src_offset,\n+static void vec_stack_to_stack_helper(CodeBuffer *cbuf, int src_offset,\n@@ -956,24 +956,0 @@\n-  int calc_size = 0;\n-  int src_offset_size = (src_offset == 0) ? 0 : ((src_offset < 0x80) ? 1 : 4);\n-  int dst_offset_size = (dst_offset == 0) ? 0 : ((dst_offset < 0x80) ? 1 : 4);\n-  switch (ireg) {\n-  case Op_VecS:\n-    calc_size = 3+src_offset_size + 3+dst_offset_size;\n-    break;\n-  case Op_VecD: {\n-    calc_size = 3+src_offset_size + 3+dst_offset_size;\n-    int tmp_src_offset = src_offset + 4;\n-    int tmp_dst_offset = dst_offset + 4;\n-    src_offset_size = (tmp_src_offset == 0) ? 0 : ((tmp_src_offset < 0x80) ? 1 : 4);\n-    dst_offset_size = (tmp_dst_offset == 0) ? 0 : ((tmp_dst_offset < 0x80) ? 1 : 4);\n-    calc_size += 3+src_offset_size + 3+dst_offset_size;\n-    break;\n-  }\n-  case Op_VecX:\n-  case Op_VecY:\n-  case Op_VecZ:\n-    calc_size = 6 + 6 + 5+src_offset_size + 5+dst_offset_size;\n-    break;\n-  default:\n-    ShouldNotReachHere();\n-  }\n@@ -982,1 +958,0 @@\n-    int offset = __ offset();\n@@ -1015,3 +990,0 @@\n-    int size = __ offset() - offset;\n-    assert(size == calc_size, \"incorrect size calculation\");\n-    return size;\n@@ -1019,1 +991,1 @@\n-  } else if (!do_size) {\n+  } else {\n@@ -1059,1 +1031,0 @@\n-  return calc_size;\n@@ -1091,1 +1062,1 @@\n-      return vec_stack_to_stack_helper(cbuf, do_size, src_offset, dst_offset, ireg, st);\n+      vec_stack_to_stack_helper(cbuf, src_offset, dst_offset, ireg, st);\n@@ -1093,1 +1064,1 @@\n-      return vec_mov_helper(cbuf, do_size, src_first, dst_first, src_second, dst_second, ireg, st);\n+      vec_mov_helper(cbuf, src_first, dst_first, src_second, dst_second, ireg, st);\n@@ -1096,1 +1067,1 @@\n-      return vec_spill_helper(cbuf, do_size, false, stack_offset, src_first, ireg, st);\n+      vec_spill_helper(cbuf, false, stack_offset, src_first, ireg, st);\n@@ -1099,1 +1070,1 @@\n-      return vec_spill_helper(cbuf, do_size, true,  stack_offset, dst_first, ireg, st);\n+      vec_spill_helper(cbuf, true,  stack_offset, dst_first, ireg, st);\n@@ -1103,0 +1074,1 @@\n+    return 0;\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":9,"deletions":37,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -1036,1 +1036,1 @@\n-static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,\n+static void vec_mov_helper(CodeBuffer *cbuf, int src_lo, int dst_lo,\n@@ -1039,1 +1039,1 @@\n-int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,\n+void vec_spill_helper(CodeBuffer *cbuf, bool is_load,\n@@ -1154,1 +1154,1 @@\n-      vec_mov_helper(cbuf, false, src_first, dst_first, src_second, dst_second, ireg, st);\n+      vec_mov_helper(cbuf, src_first, dst_first, src_second, dst_second, ireg, st);\n@@ -1157,1 +1157,1 @@\n-      vec_spill_helper(cbuf, false, false, stack_offset, src_first, ireg, st);\n+      vec_spill_helper(cbuf, false, stack_offset, src_first, ireg, st);\n@@ -1160,1 +1160,1 @@\n-      vec_spill_helper(cbuf, false, true,  stack_offset, dst_first, ireg, st);\n+      vec_spill_helper(cbuf, true,  stack_offset, dst_first, ireg, st);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"}]}