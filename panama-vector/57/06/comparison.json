{"files":[{"patch":"@@ -2438,0 +2438,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -992,0 +992,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2155,0 +2155,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1545,0 +1545,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1819,0 +1819,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -821,0 +821,5 @@\n+  do_intrinsic(_VectorBinaryMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_binary_masked_op_name, vector_binary_masked_op_sig, F_S)    \\\n+   do_signature(vector_binary_masked_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"      \\\n+                                            \"Ljava\/lang\/Object;Ljdk\/internal\/vm\/vector\/VectorSupport$BinaryMaskedOperation;)Ljava\/lang\/Object;\") \\\n+   do_name(vector_binary_masked_op_name,     \"binaryMaskedOp\")                                                                                 \\\n+                                                                                                                                               \\\n@@ -850,0 +855,6 @@\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorStoreMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_store_masked_op_name, vector_store_masked_op_sig, F_S)             \\\n+   do_signature(vector_store_masked_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$Vector;\"    \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;I\"                            \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorMaskedOperation;)V\")                            \\\n+   do_name(vector_store_masked_op_name,     \"storeMasked\")                                                                                     \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -663,0 +663,1 @@\n+  case vmIntrinsics::_VectorBinaryMaskedOp:\n@@ -668,0 +669,1 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -636,0 +636,2 @@\n+  case vmIntrinsics::_VectorBinaryMaskedOp:\n+    return inline_vector_nary_masked_operation(2);\n@@ -646,0 +648,2 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store=*\/true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -313,0 +313,1 @@\n+  bool inline_vector_nary_masked_operation(int n);\n@@ -317,0 +318,1 @@\n+  bool inline_vector_mem_masked_operation(bool is_store);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -320,0 +320,2 @@\n+  static const bool match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -379,0 +379,220 @@\n+\/\/ public static\n+\/\/ <V, M>\n+\/\/ V binaryMaskedOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/                  int length, V v1, V v2, M m,\n+\/\/                  BinaryMaskedOperation<V, M> defaultImpl) {\n+\/\/\n+\/\/ TODO: add the mask support for unary\/ternay mask op. After then, the original intrinsics and above method\n+\/\/ \"LibraryCallKit::inline_vector_nary_operation\" could be removed.\n+\/\/\n+\/\/ The prototype intrinsics might be:\n+\/\/\n+\/\/ public static\n+\/\/ <V, M>\n+\/\/ V unaryMaskedOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/                 int length, V v, M m,\n+\/\/                 UnaryMaskedOperation<V, M> defaultImpl) {\n+\/\/\n+\/\/ public static\n+\/\/ <V, M>\n+\/\/ V ternaryMaskedOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/                   int length, V v1, V v2, V v3, M m,\n+\/\/                   TernaryMaskedOperation<V, M> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_nary_masked_operation(int n) {\n+  const TypeInt*     opr          = gvn().type(argument(0))->isa_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n+\n+  if (opr == NULL || vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      !opr->is_con() || vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass=%s maskclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int opc = VectorSupport::vop2ideal(opr->get_con(), elem_bt);\n+  int sopc = VectorNode::opcode(opc, elem_bt);\n+  if ((opc != Op_CallLeafVector) && (sopc == 0)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** operation not supported: opc=%s bt=%s\", NodeClassNames[opc], type2name(elem_bt));\n+    }\n+    return false; \/\/ operation not supported\n+  }\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  \/\/ \"argument(n + 5)\" should be the mask object. We assume it is \"null\" when no mask\n+  \/\/ is used to control this operation.\n+  bool is_masked_op = gvn().type(argument(n + 5)) != TypePtr::NULL_PTR;\n+  if (is_vector_mask(vbox_klass)) {\n+    assert(!is_masked_op, \"mask operations do not need mask to control\");\n+  }\n+\n+  if (is_masked_op && !is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (opc == Op_CallLeafVector) {\n+    if (!UseVectorStubs) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** vector stubs support is disabled\");\n+      }\n+      return false;\n+    }\n+    if (!Matcher::supports_vector_calling_convention()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** no vector calling conventions supported\");\n+      }\n+      return false;\n+    }\n+    if (!Matcher::vector_size_supported(elem_bt, num_elem)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** vector size (vlen=%d, etype=%s) is not supported\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false;\n+    }\n+  }\n+\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  VectorMaskUseType mask_use_type = is_vector_mask(vbox_klass) ? VecMaskUseAll\n+                                      : is_masked_op ? VecMaskUseLoad : VecMaskNotUsed;\n+  if ((sopc != 0) && !arch_supports_vector(sopc, num_elem, elem_bt, mask_use_type)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+                    n, sopc, num_elem, type2name(elem_bt),\n+                    is_vector_mask(vbox_klass) ? 1 : 0);\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* opd1 = NULL; Node* opd2 = NULL; Node* opd3 = NULL;\n+  switch (n) {\n+    case 3: {\n+      opd3 = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+      if (opd3 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v3=%s\",\n+                        NodeClassNames[argument(7)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      \/\/ fall-through\n+    }\n+    case 2: {\n+      opd2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      if (opd2 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v2=%s\",\n+                        NodeClassNames[argument(6)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      \/\/ fall-through\n+    }\n+    case 1: {\n+      opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      if (opd1 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v1=%s\",\n+                        NodeClassNames[argument(5)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      break;\n+    }\n+    default: fatal(\"unsupported arity: %d\", n);\n+  }\n+\n+  Node* mask = NULL;\n+  bool use_predicate = false;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(n + 5), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(n + 5)->Opcode()]);\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+    use_predicate = sopc != 0 && Matcher::match_rule_supported_vector_masked(sopc, num_elem, elem_bt);\n+    if (!use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+      return false;\n+    }\n+  }\n+\n+  Node* operation = NULL;\n+  if (opc == Op_CallLeafVector) {\n+    assert(UseVectorStubs, \"sanity\");\n+    operation = gen_call_to_svml(opr->get_con(), elem_bt, num_elem, opd1, opd2);\n+    if (operation == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** svml call failed\");\n+      }\n+      return false;\n+     }\n+  } else {\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    switch (n) {\n+      case 1:\n+      case 2: {\n+        operation = VectorNode::make(sopc, opd1, opd2, vt);\n+        break;\n+      }\n+      case 3: {\n+        operation = VectorNode::make(sopc, opd1, opd2, opd3, vt);\n+        break;\n+      }\n+      default: fatal(\"unsupported arity: %d\", n);\n+    }\n+  }\n+\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      \/\/ TODO: add predicate implementation for masked operation.\n+    } else {\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n+\n+  \/\/ Wrap it up in VectorBox to keep object type information.\n+  Node* vbox = box_vector(operation, vbox_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n@@ -802,0 +1022,128 @@\n+\/\/    <C, V extends Vector<?>,\n+\/\/     M extends VectorMask<?>>\n+\/\/    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                     int length, Object base, long offset,   \/\/ Unsafe addressing\n+\/\/                     V v, M m,\n+\/\/                     C container, int index,      \/\/ Arguments for default implementation\n+\/\/                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+\/\/\n+\/\/    TODO: Handle special case where load\/store happens from\/to byte array but element type\n+\/\/    is not byte. And also add the mask support for vector load. The intrinsic looks like:\n+\/\/\n+\/\/    <C, V extends Vector<?>,\n+\/\/     M extends VectorMask<?>>\n+\/\/    V loadMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                 int vlen, Object base, long offset,\n+\/\/                 M m,   \/\/ mask arguemnt\n+\/\/                 Object container, int index,\n+\/\/                 LoadMaskedOperation<C, V, M> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_mem_masked_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+\n+  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int sopc = is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked;\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUseLoad) ||\n+      !Matcher::match_rule_supported_vector_masked(sopc, num_elem, elem_bt)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=yes\",\n+                    is_store, is_store ? \"storeMask\" : \"loadMask\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS;\n+  Node* addr = make_unsafe_address(base, offset, decorators, elem_bt, true);\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+  if (arr_type != NULL && elem_bt != arr_type->elem()->array_element_basic_type()) {\n+    return false;\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(!is_vector_mask(vbox_klass) && is_vector_mask(mbox_klass), \"Invalid class type\");\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      return false; \/\/ operand unboxing failed\n+    }\n+    Node* mask = unbox_vector(argument(8), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(8)->Opcode()]);\n+      }\n+      return false;\n+    }\n+\n+    set_all_memory(reset_memory());\n+    Node* vstore = gvn().transform(new StoreVectorMaskedNode(control(), memory(addr), addr, val, addr_type, mask));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    \/\/ TODO: mask support for load op.\n+    assert(false, \"unimplemented masked memory operation\");\n+  }\n+\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":348,"deletions":0,"binary":false,"changes":348,"status":"modified"},{"patch":"@@ -799,2 +799,2 @@\n-                                                     idx == MemNode::ValueIn ||\n-                                                     idx == MemNode::ValueIn + 1; }\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1; }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -269,0 +269,14 @@\n+    @IntrinsicCandidate\n+    public static\n+    <V, M>\n+    V binaryMaskedOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+                     Class<?> elementType, int length, V v1, V v2, M m,\n+                     BinaryMaskedOperation<V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v1, v2, m);\n+    }\n+\n+    public interface BinaryMaskedOperation<V, M> {\n+        V apply(V v1, V v2, M mask);\n+    }\n+\n@@ -340,0 +354,17 @@\n+    public interface StoreVectorMaskedOperation<C, V extends Vector<?>, M extends VectorMask<?>> {\n+        void store(C container, int index, V v, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>,\n+     M extends VectorMask<?>>\n+    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+                     int length, Object base, long offset,   \/\/ Unsafe addressing\n+                     V v, M m,\n+                     C container, int index,      \/\/ Arguments for default implementation\n+                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v, m);\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -130,0 +130,11 @@\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    \/*package-private*\/ final\n+    <F> AbstractVector<F> check(VectorMask<F> mask) {\n+        VectorSpecies<F> maskSpecies = mask.vectorSpecies();\n+        if (maskSpecies != vspecies()) {\n+            throw AbstractSpecies.checkFailed(mask, this);\n+        }\n+        return (AbstractVector<F>) this;\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractVector.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte128Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Byte128Vector lanewise0(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewise0Template(op, Byte128Mask.class, v, (Byte128Mask) m);  \/\/ specialize\n@@ -830,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte256Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Byte256Vector lanewise0(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewise0Template(op, Byte256Mask.class, v, (Byte256Mask) m);  \/\/ specialize\n@@ -862,0 +862,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte512Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Byte512Vector lanewise0(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewise0Template(op, Byte512Mask.class, v, (Byte512Mask) m);  \/\/ specialize\n@@ -926,0 +926,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte64Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Byte64Vector lanewise0(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewise0Template(op, Byte64Mask.class, v, (Byte64Mask) m);  \/\/ specialize\n@@ -814,0 +814,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public ByteMaxVector lanewise(Binary op, Vector<Byte> v) {\n-        return (ByteMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    ByteMaxVector lanewise0(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewise0Template(op, ByteMaxMask.class, v, (ByteMaxMask) m);  \/\/ specialize\n@@ -800,0 +800,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -553,1 +555,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -556,1 +558,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -594,3 +596,0 @@\n-    public abstract\n-    ByteVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Byte> v);\n@@ -598,3 +597,3 @@\n-    final\n-    ByteVector lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<Byte> v) {\n+    public final\n+    ByteVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Byte> v) {\n@@ -626,1 +625,1 @@\n-                VectorMask<Byte> eqz = that.eq((byte)0);\n+                VectorMask<Byte> eqz = that.eq((byte) 0);\n@@ -632,32 +631,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), byte.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -665,3 +633,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ByteVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ByteVector.class);\n@@ -673,0 +638,1 @@\n+    @Override\n@@ -679,4 +645,29 @@\n-        if (op == DIV) {\n-            VectorMask<Byte> eqz = that.eq((byte)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Byte> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (byte) 0);\n+                that = that.blend((byte) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == ROR || op == ROL) {\n+                return blend(lanewise(op, v), m);\n+            } else if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Byte> eqz = that.eq((byte)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -684,3 +675,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -688,1 +676,1 @@\n-        return blend(lanewise(op, v), m);\n+        return lanewise0(op, that, m);\n@@ -690,0 +678,48 @@\n+\n+    abstract\n+    ByteVector lanewise0(VectorOperators.Binary op,\n+                                   Vector<Byte> v,\n+                                   VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<Byte>> maskClass,\n+                                           Vector<Byte> v, VectorMask<Byte> m) {\n+        ByteVector that = (ByteVector) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (byte)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (byte)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+                default: return null;\n+                }}));\n+    }\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<ByteVector, VectorMask<Byte>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, ByteVector.class);\n+\n@@ -752,1 +788,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n+            \/\/ TODO: calls masked lanewiseShift() once it is supported\n+            return blend(lanewise(op, e), m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (byte) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -793,1 +836,8 @@\n-        return blend(lanewise(op, e), m);\n+        byte e1 = (byte) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -2983,1 +3033,0 @@\n-            \/\/ FIXME: optimize\n@@ -2986,1 +3035,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3231,0 +3280,16 @@\n+    abstract\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":122,"deletions":57,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double128Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Double128Vector lanewise0(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewise0Template(op, Double128Mask.class, v, (Double128Mask) m);  \/\/ specialize\n@@ -798,0 +798,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double256Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Double256Vector lanewise0(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewise0Template(op, Double256Mask.class, v, (Double256Mask) m);  \/\/ specialize\n@@ -802,0 +802,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double512Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Double512Vector lanewise0(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewise0Template(op, Double512Mask.class, v, (Double512Mask) m);  \/\/ specialize\n@@ -810,0 +810,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double64Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Double64Vector lanewise0(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewise0Template(op, Double64Mask.class, v, (Double64Mask) m);  \/\/ specialize\n@@ -796,0 +796,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public DoubleMaxVector lanewise(Binary op, Vector<Double> v) {\n-        return (DoubleMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    DoubleMaxVector lanewise0(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewise0Template(op, DoubleMaxMask.class, v, (DoubleMaxMask) m);  \/\/ specialize\n@@ -795,0 +795,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -620,3 +622,0 @@\n-    public abstract\n-    DoubleVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Double> v);\n@@ -624,3 +623,3 @@\n-    final\n-    DoubleVector lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<Double> v) {\n+    public final\n+    DoubleVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Double> v) {\n@@ -642,28 +641,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), double.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.min(a, b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -671,3 +643,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<DoubleVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, DoubleVector.class);\n@@ -679,0 +648,1 @@\n+    @Override\n@@ -684,1 +654,10 @@\n-        return blend(lanewise(op, v), m);\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+        return lanewise0(op, that, m);\n@@ -686,0 +665,44 @@\n+\n+    abstract\n+    DoubleVector lanewise0(VectorOperators.Binary op,\n+                                   Vector<Double> v,\n+                                   VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<Double>> maskClass,\n+                                           Vector<Double> v, VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)Math.min(a, b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+                case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double) Math.hypot(a, b));\n+                default: return null;\n+                }}));\n+    }\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<DoubleVector, VectorMask<Double>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, DoubleVector.class);\n+\n@@ -742,1 +765,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -781,1 +804,6 @@\n-        return blend(lanewise(op, e), m);\n+        double e1 = (double) e;\n+        if ((long)e1 != e\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -2896,1 +2924,0 @@\n-            \/\/ FIXME: optimize\n@@ -2899,1 +2926,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3185,0 +3212,16 @@\n+    abstract\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":86,"deletions":43,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float128Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Float128Vector lanewise0(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewise0Template(op, Float128Mask.class, v, (Float128Mask) m);  \/\/ specialize\n@@ -802,0 +802,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float256Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Float256Vector lanewise0(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewise0Template(op, Float256Mask.class, v, (Float256Mask) m);  \/\/ specialize\n@@ -810,0 +810,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float512Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Float512Vector lanewise0(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewise0Template(op, Float512Mask.class, v, (Float512Mask) m);  \/\/ specialize\n@@ -826,0 +826,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float64Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Float64Vector lanewise0(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewise0Template(op, Float64Mask.class, v, (Float64Mask) m);  \/\/ specialize\n@@ -798,0 +798,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public FloatMaxVector lanewise(Binary op, Vector<Float> v) {\n-        return (FloatMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    FloatMaxVector lanewise0(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewise0Template(op, FloatMaxMask.class, v, (FloatMaxMask) m);  \/\/ specialize\n@@ -795,0 +795,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -620,3 +622,0 @@\n-    public abstract\n-    FloatVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Float> v);\n@@ -624,3 +623,3 @@\n-    final\n-    FloatVector lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<Float> v) {\n+    public final\n+    FloatVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Float> v) {\n@@ -642,28 +641,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), float.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.min(a, b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -671,3 +643,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<FloatVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, FloatVector.class);\n@@ -679,0 +648,1 @@\n+    @Override\n@@ -684,1 +654,10 @@\n-        return blend(lanewise(op, v), m);\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+        return lanewise0(op, that, m);\n@@ -686,0 +665,44 @@\n+\n+    abstract\n+    FloatVector lanewise0(VectorOperators.Binary op,\n+                                   Vector<Float> v,\n+                                   VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<Float>> maskClass,\n+                                           Vector<Float> v, VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)Math.min(a, b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+                case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float) Math.hypot(a, b));\n+                default: return null;\n+                }}));\n+    }\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<FloatVector, VectorMask<Float>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, FloatVector.class);\n+\n@@ -742,1 +765,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -781,1 +804,6 @@\n-        return blend(lanewise(op, e), m);\n+        float e1 = (float) e;\n+        if ((long)e1 != e\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -2902,1 +2930,0 @@\n-            \/\/ FIXME: optimize\n@@ -2905,1 +2932,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3172,0 +3199,16 @@\n+    abstract\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":86,"deletions":43,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int128Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Int128Vector lanewise0(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewise0Template(op, Int128Mask.class, v, (Int128Mask) m);  \/\/ specialize\n@@ -806,0 +806,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int256Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Int256Vector lanewise0(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewise0Template(op, Int256Mask.class, v, (Int256Mask) m);  \/\/ specialize\n@@ -814,0 +814,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int512Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Int512Vector lanewise0(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewise0Template(op, Int512Mask.class, v, (Int512Mask) m);  \/\/ specialize\n@@ -830,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int64Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Int64Vector lanewise0(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewise0Template(op, Int64Mask.class, v, (Int64Mask) m);  \/\/ specialize\n@@ -802,0 +802,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public IntMaxVector lanewise(Binary op, Vector<Integer> v) {\n-        return (IntMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    IntMaxVector lanewise0(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewise0Template(op, IntMaxMask.class, v, (IntMaxMask) m);  \/\/ specialize\n@@ -811,0 +811,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -553,1 +555,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -556,1 +558,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -594,3 +596,0 @@\n-    public abstract\n-    IntVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Integer> v);\n@@ -598,3 +597,3 @@\n-    final\n-    IntVector lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<Integer> v) {\n+    public final\n+    IntVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Integer> v) {\n@@ -626,1 +625,1 @@\n-                VectorMask<Integer> eqz = that.eq((int)0);\n+                VectorMask<Integer> eqz = that.eq((int) 0);\n@@ -632,32 +631,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), int.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -665,3 +633,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<IntVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, IntVector.class);\n@@ -673,0 +638,1 @@\n+    @Override\n@@ -679,4 +645,29 @@\n-        if (op == DIV) {\n-            VectorMask<Integer> eqz = that.eq((int)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Integer> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (int) 0);\n+                that = that.blend((int) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == ROR || op == ROL) {\n+                return blend(lanewise(op, v), m);\n+            } else if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Integer> eqz = that.eq((int)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -684,3 +675,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -688,1 +676,1 @@\n-        return blend(lanewise(op, v), m);\n+        return lanewise0(op, that, m);\n@@ -690,0 +678,48 @@\n+\n+    abstract\n+    IntVector lanewise0(VectorOperators.Binary op,\n+                                   Vector<Integer> v,\n+                                   VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<Integer>> maskClass,\n+                                           Vector<Integer> v, VectorMask<Integer> m) {\n+        IntVector that = (IntVector) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (int)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (int)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+                default: return null;\n+                }}));\n+    }\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<IntVector, VectorMask<Integer>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, IntVector.class);\n+\n@@ -752,1 +788,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n+            \/\/ TODO: calls masked lanewiseShift() once it is supported\n+            return blend(lanewise(op, e), m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (int) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -793,1 +836,8 @@\n-        return blend(lanewise(op, e), m);\n+        int e1 = (int) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -3005,1 +3055,0 @@\n-            \/\/ FIXME: optimize\n@@ -3008,1 +3057,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3275,0 +3324,16 @@\n+    abstract\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":122,"deletions":57,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long128Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Long128Vector lanewise0(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewise0Template(op, Long128Mask.class, v, (Long128Mask) m);  \/\/ specialize\n@@ -792,0 +792,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long256Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Long256Vector lanewise0(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewise0Template(op, Long256Mask.class, v, (Long256Mask) m);  \/\/ specialize\n@@ -796,0 +796,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long512Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Long512Vector lanewise0(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewise0Template(op, Long512Mask.class, v, (Long512Mask) m);  \/\/ specialize\n@@ -804,0 +804,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long64Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Long64Vector lanewise0(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewise0Template(op, Long64Mask.class, v, (Long64Mask) m);  \/\/ specialize\n@@ -790,0 +790,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public LongMaxVector lanewise(Binary op, Vector<Long> v) {\n-        return (LongMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    LongMaxVector lanewise0(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewise0Template(op, LongMaxMask.class, v, (LongMaxMask) m);  \/\/ specialize\n@@ -790,0 +790,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -511,1 +513,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -514,1 +516,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -552,3 +554,0 @@\n-    public abstract\n-    LongVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Long> v);\n@@ -556,3 +555,3 @@\n-    final\n-    LongVector lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<Long> v) {\n+    public final\n+    LongVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Long> v) {\n@@ -584,1 +583,1 @@\n-                VectorMask<Long> eqz = that.eq((long)0);\n+                VectorMask<Long> eqz = that.eq((long) 0);\n@@ -590,32 +589,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), long.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -623,3 +591,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<LongVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, LongVector.class);\n@@ -631,0 +596,1 @@\n+    @Override\n@@ -637,4 +603,29 @@\n-        if (op == DIV) {\n-            VectorMask<Long> eqz = that.eq((long)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Long> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (long) 0);\n+                that = that.blend((long) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == ROR || op == ROL) {\n+                return blend(lanewise(op, v), m);\n+            } else if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Long> eqz = that.eq((long)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -642,3 +633,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -646,1 +634,1 @@\n-        return blend(lanewise(op, v), m);\n+        return lanewise0(op, that, m);\n@@ -648,0 +636,48 @@\n+\n+    abstract\n+    LongVector lanewise0(VectorOperators.Binary op,\n+                                   Vector<Long> v,\n+                                   VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<Long>> maskClass,\n+                                           Vector<Long> v, VectorMask<Long> m) {\n+        LongVector that = (LongVector) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (long)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (long)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+                default: return null;\n+                }}));\n+    }\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<LongVector, VectorMask<Long>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, LongVector.class);\n+\n@@ -710,1 +746,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n+            \/\/ TODO: calls masked lanewiseShift() once it is supported\n+            return blend(lanewise(op, e), m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (long) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -2889,1 +2932,0 @@\n-            \/\/ FIXME: optimize\n@@ -2892,1 +2934,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3178,0 +3220,16 @@\n+    abstract\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":114,"deletions":56,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short128Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Short128Vector lanewise0(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewise0Template(op, Short128Mask.class, v, (Short128Mask) m);  \/\/ specialize\n@@ -820,0 +820,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short256Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Short256Vector lanewise0(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewise0Template(op, Short256Mask.class, v, (Short256Mask) m);  \/\/ specialize\n@@ -836,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short512Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Short512Vector lanewise0(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewise0Template(op, Short512Mask.class, v, (Short512Mask) m);  \/\/ specialize\n@@ -868,0 +868,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short64Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    Short64Vector lanewise0(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewise0Template(op, Short64Mask.class, v, (Short64Mask) m);  \/\/ specialize\n@@ -812,0 +812,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public ShortMaxVector lanewise(Binary op, Vector<Short> v) {\n-        return (ShortMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    ShortMaxVector lanewise0(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewise0Template(op, ShortMaxMask.class, v, (ShortMaxMask) m);  \/\/ specialize\n@@ -806,0 +806,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -553,1 +555,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -556,1 +558,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -594,3 +596,0 @@\n-    public abstract\n-    ShortVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Short> v);\n@@ -598,3 +597,3 @@\n-    final\n-    ShortVector lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<Short> v) {\n+    public final\n+    ShortVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Short> v) {\n@@ -626,1 +625,1 @@\n-                VectorMask<Short> eqz = that.eq((short)0);\n+                VectorMask<Short> eqz = that.eq((short) 0);\n@@ -632,32 +631,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), short.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -665,3 +633,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ShortVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ShortVector.class);\n@@ -673,0 +638,1 @@\n+    @Override\n@@ -679,4 +645,29 @@\n-        if (op == DIV) {\n-            VectorMask<Short> eqz = that.eq((short)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Short> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n+                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == ROR || op == ROL) {\n+                return blend(lanewise(op, v), m);\n+            } else if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Short> eqz = that.eq((short)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -684,3 +675,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -688,1 +676,1 @@\n-        return blend(lanewise(op, v), m);\n+        return lanewise0(op, that, m);\n@@ -690,0 +678,48 @@\n+\n+    abstract\n+    ShortVector lanewise0(VectorOperators.Binary op,\n+                                   Vector<Short> v,\n+                                   VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<Short>> maskClass,\n+                                           Vector<Short> v, VectorMask<Short> m) {\n+        ShortVector that = (ShortVector) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (short)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (short)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+                default: return null;\n+                }}));\n+    }\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<ShortVector, VectorMask<Short>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, ShortVector.class);\n+\n@@ -752,1 +788,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n+            \/\/ TODO: calls masked lanewiseShift() once it is supported\n+            return blend(lanewise(op, e), m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (short) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -793,1 +836,8 @@\n-        return blend(lanewise(op, e), m);\n+        short e1 = (short) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -3138,1 +3188,0 @@\n-            \/\/ FIXME: optimize\n@@ -3141,1 +3190,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3551,0 +3600,16 @@\n+    abstract\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":122,"deletions":57,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -222,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -566,1 +568,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -569,1 +571,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -642,3 +644,0 @@\n-    public abstract\n-    $abstractvectortype$ lanewise(VectorOperators.Binary op,\n-                                  Vector<$Boxtype$> v);\n@@ -646,3 +645,3 @@\n-    final\n-    $abstractvectortype$ lanewiseTemplate(VectorOperators.Binary op,\n-                                          Vector<$Boxtype$> v) {\n+    public final\n+    $abstractvectortype$ lanewise(VectorOperators.Binary op,\n+                                  Vector<$Boxtype$> v) {\n@@ -683,1 +682,1 @@\n-                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$) 0);\n@@ -690,44 +689,1 @@\n-        int opc = opCode(op);\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), $type$.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.min(a, b));\n-#if[BITWISE]\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-#end[BITWISE]\n-#if[FP]\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n-#end[FP]\n-                default: return null;\n-                }}));\n+        return lanewise0(op, that, null);\n@@ -735,3 +691,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<$abstractvectortype$>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, $Type$Vector.class);\n@@ -743,0 +696,1 @@\n+    @Override\n@@ -748,1 +702,0 @@\n-#if[BITWISE]\n@@ -750,4 +703,14 @@\n-        if (op == DIV) {\n-            VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        check(m);\n+\n+        if (opKind(op, VO_SPECIAL {#if[!FP]? | VO_SHIFT})) {\n+            if (op == FIRST_NONZERO) {\n+#if[FP]\n+                return blend(lanewise(op, v), m);\n+#else[FP]\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<$Boxbitstype$> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n+                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+#end[FP]\n@@ -755,3 +718,23 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+#if[BITWISE]\n+#if[!FP]\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+#end[!FP]\n+            if (op == ROR || op == ROL) {\n+                return blend(lanewise(op, v), m);\n+            } else if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n+            }\n+#end[BITWISE]\n@@ -759,0 +742,44 @@\n+        return lanewise0(op, that, m);\n+    }\n+\n+    abstract\n+    $abstractvectortype$ lanewise0(VectorOperators.Binary op,\n+                                   Vector<$Boxtype$> v,\n+                                   VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewise0Template(VectorOperators.Binary op,\n+                                           Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                           Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v;\n+        int opc = opCode(op);\n+        return VectorSupport.binaryMaskedOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, m,\n+            BIN_MASKED_IMPL.find(op, opc, (opc_) -> {\n+              switch (opc_) {\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.min(a, b));\n+#if[BITWISE]\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> ($type$)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> ($type$)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n@@ -760,1 +787,12 @@\n-        return blend(lanewise(op, v), m);\n+#if[FP]\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+                case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.hypot(a, b));\n+#end[FP]\n+                default: return null;\n+                }}));\n@@ -762,0 +800,4 @@\n+    private static final\n+    ImplCache<Binary, BinaryMaskedOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        BIN_MASKED_IMPL = new ImplCache<>(Binary.class, $Type$Vector.class);\n+\n@@ -826,1 +868,10 @@\n-        return blend(lanewise(op, e), m);\n+#if[BITWISE]\n+        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n+            \/\/ TODO: calls masked lanewiseShift() once it is supported\n+            return blend(lanewise(op, e), m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = ($type$) ~e;\n+        }\n+#end[BITWISE]\n+        return lanewise(op, broadcast(e), m);\n@@ -870,1 +921,10 @@\n-        return blend(lanewise(op, e), m);\n+        $type$ e1 = ($type$) e;\n+        if ((long)e1 != e\n+#if[BITWISE]\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+#end[BITWISE]\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -3684,1 +3744,0 @@\n-            \/\/ FIXME: optimize\n@@ -3687,1 +3746,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -4176,0 +4235,16 @@\n+    abstract\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":145,"deletions":70,"binary":false,"changes":215,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -280,2 +280,2 @@\n-    public $vectortype$ lanewise(Binary op, Vector<$Boxtype$> v) {\n-        return ($vectortype$) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    $vectortype$ lanewise0(Binary op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewise0Template(op, $masktype$.class, v, ($masktype$) m);  \/\/ specialize\n@@ -1096,0 +1096,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"}]}