{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,1 +52,1 @@\n-unsigned char tuple_table[Assembler::EVEX_ETUP + 1][Assembler::AVX_512bit + 1] = {\n+static const unsigned char tuple_table[Assembler::EVEX_ETUP + 1][Assembler::AVX_512bit + 1] = {\n@@ -217,1 +217,1 @@\n-  _attributes = NULL;\n+  _attributes = nullptr;\n@@ -267,1 +267,1 @@\n-  assert(inst_mark() != NULL, \"must be inside InstructionMark\");\n+  assert(inst_mark() != nullptr, \"must be inside InstructionMark\");\n@@ -351,0 +351,6 @@\n+void Assembler::emit_arith_operand_imm32(int op1, Register rm, Address adr, int32_t imm32) {\n+  assert(op1 == 0x81, \"unexpected opcode\");\n+  emit_int8(op1);\n+  emit_operand(rm, adr, 4);\n+  emit_int32(imm32);\n+}\n@@ -537,1 +543,1 @@\n-  int reg_enc = (intptr_t)reg;\n+  int reg_enc = reg->raw_encoding();\n@@ -544,1 +550,1 @@\n-  int xmmreg_enc = (intptr_t)xmmreg;\n+  int xmmreg_enc = xmmreg->raw_encoding();\n@@ -549,0 +555,7 @@\n+static int raw_encode(KRegister kreg) {\n+  assert(kreg == knoreg || kreg->is_valid(), \"sanity\");\n+  int kreg_enc = kreg->raw_encoding();\n+  assert(kreg_enc == -1 || is_valid_encoding(kreg_enc), \"sanity\");\n+  return kreg_enc;\n+}\n+\n@@ -587,1 +600,1 @@\n-                                    int rip_relative_correction) {\n+                                    int post_addr_length) {\n@@ -674,2 +687,2 @@\n-      assert(inst_mark() != NULL, \"must be inside InstructionMark\");\n-      address next_ip = pc() + sizeof(int32_t) + rip_relative_correction;\n+      assert(inst_mark() != nullptr, \"must be inside InstructionMark\");\n+      address next_ip = pc() + sizeof(int32_t) + post_addr_length;\n@@ -698,1 +711,1 @@\n-                             int rip_relative_correction) {\n+                             int post_addr_length) {\n@@ -701,1 +714,1 @@\n-                      scale, disp, rspec, rip_relative_correction);\n+                      scale, disp, rspec, post_addr_length);\n@@ -706,1 +719,2 @@\n-                             RelocationHolder const& rspec) {\n+                             RelocationHolder const& rspec,\n+                             int post_addr_length) {\n@@ -710,1 +724,1 @@\n-                      scale, disp, rspec);\n+                      scale, disp, rspec, post_addr_length);\n@@ -715,1 +729,2 @@\n-                             RelocationHolder const& rspec) {\n+                             RelocationHolder const& rspec,\n+                             int post_addr_length) {\n@@ -719,1 +734,17 @@\n-                      scale, disp, rspec, \/* rip_relative_correction *\/ 0);\n+                      scale, disp, rspec, post_addr_length);\n+}\n+\n+void Assembler::emit_operand(KRegister kreg, Address adr,\n+                             int post_addr_length) {\n+  emit_operand(kreg, adr._base, adr._index, adr._scale, adr._disp,\n+               adr._rspec,\n+               post_addr_length);\n+}\n+\n+void Assembler::emit_operand(KRegister kreg, Register base, Register index,\n+                             Address::ScaleFactor scale, int disp,\n+                             RelocationHolder const& rspec,\n+                             int post_addr_length) {\n+  assert(!index->is_valid() || index != rsp, \"illegal addressing mode\");\n+  emit_operand_helper(raw_encode(kreg), raw_encode(base), raw_encode(index),\n+                      scale, disp, rspec, post_addr_length);\n@@ -876,0 +907,2 @@\n+    case 0x10: \/\/ movups\n+    case 0x11: \/\/ movups\n@@ -1204,1 +1237,1 @@\n-  assert(inst != NULL && inst < pc(), \"must point to beginning of instruction\");\n+  assert(inst != nullptr && inst < pc(), \"must point to beginning of instruction\");\n@@ -1225,5 +1258,2 @@\n-void Assembler::emit_operand(Register reg, Address adr,\n-                             int rip_relative_correction) {\n-  emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp,\n-               adr._rspec,\n-               rip_relative_correction);\n+void Assembler::emit_operand(Register reg, Address adr, int post_addr_length) {\n+  emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp, adr._rspec, post_addr_length);\n@@ -1232,7 +1262,6 @@\n-void Assembler::emit_operand(XMMRegister reg, Address adr) {\n-    if (adr.isxmmindex()) {\n-       emit_operand(reg, adr._base, adr._xmmindex, adr._scale, adr._disp, adr._rspec);\n-    } else {\n-       emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp,\n-       adr._rspec);\n-    }\n+void Assembler::emit_operand(XMMRegister reg, Address adr, int post_addr_length) {\n+  if (adr.isxmmindex()) {\n+     emit_operand(reg, adr._base, adr._xmmindex, adr._scale, adr._disp, adr._rspec, post_addr_length);\n+  } else {\n+     emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp, adr._rspec, post_addr_length);\n+  }\n@@ -1253,1 +1282,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -1265,1 +1294,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1288,0 +1317,1 @@\n+  emit_int8(0x66);\n@@ -1305,1 +1335,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -1317,1 +1347,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1381,1 +1411,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1398,1 +1428,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1407,1 +1437,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1432,1 +1462,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1456,1 +1486,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1480,1 +1510,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1502,1 +1532,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -1525,1 +1555,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -1532,1 +1562,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1553,1 +1583,1 @@\n-  emit_operand(dst, src2);\n+  emit_operand(dst, src2, 0);\n@@ -1588,1 +1618,1 @@\n-  emit_operand(rbx, src);\n+  emit_operand(rbx, src, 0);\n@@ -1605,1 +1635,1 @@\n-  emit_operand(rdx, src);\n+  emit_operand(rdx, src, 0);\n@@ -1621,1 +1651,1 @@\n-  emit_operand(rcx, src);\n+  emit_operand(rcx, src, 0);\n@@ -1656,1 +1686,1 @@\n-  emit_operand(rdx, adr);\n+  emit_operand(rdx, adr, 0);\n@@ -1663,2 +1693,2 @@\n-  \/\/ Entry is NULL in case of a scratch emit.\n-  assert(entry == NULL || is_simm32(disp), \"disp=\" INTPTR_FORMAT \" must be 32bit offset (call2)\", disp);\n+  \/\/ Entry is null in case of a scratch emit.\n+  assert(entry == nullptr || is_simm32(disp), \"disp=\" INTPTR_FORMAT \" must be 32bit offset (call2)\", disp);\n@@ -1694,1 +1724,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1708,3 +1738,1 @@\n-  emit_int8((unsigned char)0x81);\n-  emit_operand(rdi, dst, 4);\n-  emit_int32(imm32);\n+  emit_arith_operand(0x81, as_Register(7), dst, imm32);\n@@ -1723,1 +1751,1 @@\n-void Assembler::cmpl(Register dst, Address  src) {\n+void Assembler::cmpl(Register dst, Address src) {\n@@ -1727,1 +1755,7 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::cmpl_imm32(Address dst, int32_t imm32) {\n+  InstructionMark im(this);\n+  prefix(dst);\n+  emit_arith_operand_imm32(0x81, as_Register(7), dst, imm32);\n@@ -1732,2 +1766,3 @@\n-  assert(!dst.base_needs_rex() && !dst.index_needs_rex(), \"no extended registers\");\n-  emit_int16(0x66, (unsigned char)0x81);\n+  emit_int8(0x66);\n+  prefix(dst);\n+  emit_int8((unsigned char)0x81);\n@@ -1745,1 +1780,1 @@\n-  emit_operand(reg, adr);\n+  emit_operand(reg, adr, 0);\n@@ -1753,1 +1788,1 @@\n-  emit_operand(reg, adr);\n+  emit_operand(reg, adr, 0);\n@@ -1763,1 +1798,1 @@\n-  emit_operand(reg, adr);\n+  emit_operand(reg, adr, 0);\n@@ -1776,1 +1811,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1794,1 +1829,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1882,1 +1917,1 @@\n-  emit_operand(crc, adr);\n+  emit_operand(crc, adr, 0);\n@@ -1899,0 +1934,49 @@\n+void Assembler::vcvtps2ph(XMMRegister dst, XMMRegister src, int imm8, int vector_len) {\n+  assert(VM_Version::supports_evex() || VM_Version::supports_f16c(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/*uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x1D, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::evcvtps2ph(Address dst, KRegister mask, XMMRegister src, int imm8, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/*uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_HVM, \/* input_size_in_bits *\/ EVEX_64bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8(0x1D);\n+  emit_operand(src, dst, 1);\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::vcvtps2ph(Address dst, XMMRegister src, int imm8, int vector_len) {\n+  assert(VM_Version::supports_evex() || VM_Version::supports_f16c(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/*uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_HVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8(0x1D);\n+  emit_operand(src, dst, 1);\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::vcvtph2ps(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_evex() || VM_Version::supports_f16c(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x13, (0xC0 | encode));\n+}\n+\n+void Assembler::vcvtph2ps(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_evex() || VM_Version::supports_f16c(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/*uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_HVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x13);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -1929,1 +2013,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1946,1 +2030,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1963,1 +2047,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -1987,1 +2071,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2098,0 +2182,7 @@\n+void Assembler::vcvttpd2dq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE6, (0xC0 | encode));\n+}\n+\n@@ -2105,0 +2196,8 @@\n+void Assembler::evcvttps2qq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x7A, (0xC0 | encode));\n+}\n+\n@@ -2106,1 +2205,1 @@\n-  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  assert(VM_Version::supports_avx512dq(), \"\");\n@@ -2114,1 +2213,1 @@\n-  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  assert(VM_Version::supports_avx512dq(), \"\");\n@@ -2122,1 +2221,1 @@\n-  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  assert(VM_Version::supports_avx512dq(), \"\");\n@@ -2130,1 +2229,1 @@\n-  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  assert(VM_Version::supports_avx512dq(), \"\");\n@@ -2193,1 +2292,1 @@\n-  assert(UseAVX > 2  && VM_Version::supports_avx512bw(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n@@ -2240,0 +2339,8 @@\n+void Assembler::evpmovsqd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x25, (0xC0 | encode));\n+}\n+\n@@ -2245,1 +2352,1 @@\n-  emit_operand(rcx, dst);\n+  emit_operand(rcx, dst, 0);\n@@ -2256,1 +2363,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2274,1 +2381,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2293,7 +2400,0 @@\n-void Assembler::idivl(Address src) {\n-  InstructionMark im(this);\n-  prefix(src);\n-  emit_int8((unsigned char)0xF7);\n-  emit_operand(as_Register(7), src);\n-}\n-\n@@ -2322,1 +2422,1 @@\n-    emit_operand(dst, src);\n+    emit_operand(dst, src, 1);\n@@ -2326,1 +2426,1 @@\n-    emit_operand(dst, src);\n+    emit_operand(dst, src, 4);\n@@ -2345,1 +2445,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2354,1 +2454,1 @@\n-  emit_operand(rax, dst);\n+  emit_operand(rax, dst, 0);\n@@ -2362,1 +2462,1 @@\n-    assert(dst != NULL, \"jcc most probably wrong\");\n+    assert(dst != nullptr, \"jcc most probably wrong\");\n@@ -2414,1 +2514,1 @@\n-  emit_operand(rsp, adr);\n+  emit_operand(rsp, adr, 0);\n@@ -2420,1 +2520,1 @@\n-    assert(entry != NULL, \"jmp most probably wrong\");\n+    assert(entry != nullptr, \"jmp most probably wrong\");\n@@ -2451,1 +2551,1 @@\n-  assert(dest != NULL, \"must have a target\");\n+  assert(dest != nullptr, \"must have a target\");\n@@ -2454,1 +2554,1 @@\n-  emit_data(disp, rspec.reloc(), call32_operand);\n+  emit_data(disp, rspec, call32_operand);\n@@ -2461,1 +2561,1 @@\n-    assert(entry != NULL, \"jmp most probably wrong\");\n+    assert(entry != nullptr, \"jmp most probably wrong\");\n@@ -2485,1 +2585,1 @@\n-    emit_operand(as_Register(2), src);\n+    emit_operand(as_Register(2), src, 0);\n@@ -2491,1 +2591,1 @@\n-    emit_operand(as_Register(2), src);\n+    emit_operand(as_Register(2), src, 0);\n@@ -2499,1 +2599,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2521,0 +2621,9 @@\n+void Assembler::lzcntl(Register dst, Address src) {\n+  assert(VM_Version::supports_lzcnt(), \"encoding is treated as BSR\");\n+  InstructionMark im(this);\n+  emit_int8((unsigned char)0xF3);\n+  prefix(src, dst);\n+  emit_int16(0x0F, (unsigned char)0xBD);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -2566,1 +2675,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2578,0 +2687,11 @@\n+void Assembler::movddup(XMMRegister dst, Address src) {\n+  NOT_LP64(assert(VM_Version::supports_sse3(), \"\"));\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_DUP, \/* input_size_in_bits *\/ EVEX_64bit);\n+  attributes.set_rex_vex_w_reverted();\n+  simd_prefix(dst, xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x12);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -2582,0 +2702,1 @@\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_DUP, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -2585,1 +2706,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2629,1 +2750,1 @@\n-  emit_operand((Register)dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2638,1 +2759,1 @@\n-  emit_operand((Register)src, dst);\n+  emit_operand(src, dst, 0);\n@@ -2675,1 +2796,1 @@\n-  emit_operand((Register)dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2684,1 +2805,1 @@\n-  emit_operand((Register)src, dst);\n+  emit_operand(src, dst, 0);\n@@ -2964,1 +3085,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -2989,1 +3110,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -2999,1 +3120,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3016,1 +3137,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3026,1 +3147,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3044,1 +3165,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3062,1 +3183,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3075,1 +3196,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3078,11 +3199,7 @@\n-\/\/ Move Unaligned EVEX enabled Vector (programmable : 8,16,32,64)\n-void Assembler::evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  if (merge) {\n-    attributes.reset_is_clear_context();\n-  }\n-  int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n-  emit_int16(0x6F, (0xC0 | encode));\n+void Assembler::vpmaskmovd(XMMRegister dst, XMMRegister mask, Address src, int vector_len) {\n+  assert((VM_Version::supports_avx2() && vector_len == AVX_256bit), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ false, \/* uses_vl *\/ false);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8C);\n+  emit_operand(dst, src, 0);\n@@ -3091,2 +3208,2 @@\n-void Assembler::evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n+void Assembler::vpmaskmovq(XMMRegister dst, XMMRegister mask, Address src, int vector_len) {\n+  assert((VM_Version::supports_avx2() && vector_len == AVX_256bit), \"\");\n@@ -3094,10 +3211,4 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n-  attributes.set_is_evex_instruction();\n-  if (merge) {\n-    attributes.reset_is_clear_context();\n-  }\n-  vex_prefix(src, 0, dst->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n-  emit_int8(0x6F);\n-  emit_operand(dst, src);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ false, \/* uses_vl *\/ false);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8C);\n+  emit_operand(dst, src, 0);\n@@ -3106,3 +3217,2 @@\n-void Assembler::evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n-  assert(src != xnoreg, \"sanity\");\n+void Assembler::vmaskmovps(XMMRegister dst, Address src, XMMRegister mask, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n@@ -3110,3 +3220,38 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x2C);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::vmaskmovpd(XMMRegister dst, Address src, XMMRegister mask, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x2D);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::vmaskmovps(Address dst, XMMRegister src, XMMRegister mask, int vector_len) {\n+  assert(UseAVX > 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(dst, mask->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x2E);\n+  emit_operand(src, dst, 0);\n+}\n+\n+void Assembler::vmaskmovpd(Address dst, XMMRegister src, XMMRegister mask, int vector_len) {\n+  assert(UseAVX > 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(dst, mask->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x2F);\n+  emit_operand(src, dst, 0);\n+}\n+\n+\/\/ Move Unaligned EVEX enabled Vector (programmable : 8,16,32,64)\n+void Assembler::evmovdqub(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -3117,3 +3262,7 @@\n-  vex_prefix(dst, 0, src->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n-  emit_int8(0x7F);\n-  emit_operand(src, dst);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6F, (0xC0 | encode));\n+}\n+\n+void Assembler::evmovdqub(XMMRegister dst, XMMRegister src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdqub(dst, k0, src, \/*merge*\/ false, vector_len);\n@@ -3125,1 +3274,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3134,1 +3283,6 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::evmovdqub(XMMRegister dst, Address src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdqub(dst, k0, src, \/*merge*\/ false, vector_len);\n@@ -3141,1 +3295,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3150,1 +3304,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3153,13 +3307,3 @@\n-void Assembler::evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n-  InstructionMark im(this);\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n-  attributes.set_is_evex_instruction();\n-  if (merge) {\n-    attributes.reset_is_clear_context();\n-  }\n-  int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n-  vex_prefix(src, 0, dst->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n-  emit_int8(0x6F);\n-  emit_operand(dst, src);\n+void Assembler::evmovdquw(XMMRegister dst, Address src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdquw(dst, k0, src, \/*merge*\/ false, vector_len);\n@@ -3171,1 +3315,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3180,1 +3324,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3183,14 +3327,3 @@\n-void Assembler::evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n-  assert(src != xnoreg, \"sanity\");\n-  InstructionMark im(this);\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n-  attributes.set_is_evex_instruction();\n-  if (merge) {\n-    attributes.reset_is_clear_context();\n-  }\n-  int prefix = (_legacy_mode_bw) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n-  vex_prefix(dst, 0, src->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n-  emit_int8(0x7F);\n-  emit_operand(src, dst);\n+void Assembler::evmovdquw(Address dst, XMMRegister src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdquw(dst, k0, src, \/*merge*\/ false, vector_len);\n@@ -3203,1 +3336,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3212,1 +3345,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3249,1 +3382,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3270,1 +3403,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3275,1 +3408,0 @@\n-  if (dst->encoding() == src->encoding()) return;\n@@ -3308,1 +3440,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3329,1 +3461,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3349,1 +3481,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3364,1 +3496,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3378,1 +3510,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3389,1 +3521,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3400,1 +3532,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3430,1 +3562,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3455,1 +3587,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3467,1 +3599,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3484,1 +3616,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3495,1 +3627,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3502,1 +3634,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3510,0 +3642,40 @@\n+void Assembler::movups(XMMRegister dst, Address src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_32bit);\n+  simd_prefix(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x10);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::vmovups(XMMRegister dst, Address src, int vector_len) {\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_32bit);\n+  simd_prefix(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x10);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::movups(Address dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_32bit);\n+  simd_prefix(src, xnoreg, dst, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x11);\n+  emit_operand(src, dst, 0);\n+}\n+\n+void Assembler::vmovups(Address dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_32bit);\n+  simd_prefix(src, xnoreg, dst, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x11);\n+  emit_operand(src, dst, 0);\n+}\n+\n@@ -3525,1 +3697,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3533,1 +3705,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3540,1 +3712,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3553,1 +3725,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3565,1 +3737,1 @@\n-  emit_operand(rsp, src);\n+  emit_operand(rsp, src, 0);\n@@ -3581,1 +3753,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3599,1 +3771,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3618,1 +3790,1 @@\n-  emit_operand(as_Register(3), dst);\n+  emit_operand(as_Register(3), dst, 0);\n@@ -3952,1 +4124,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -3964,1 +4136,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -3979,1 +4151,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -4018,1 +4190,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -4081,1 +4253,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -4095,1 +4267,2 @@\n-  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex(), \"\");\n+  assert((vector_len == AVX_256bit && VM_Version::supports_avx2()) ||\n+         (vector_len == AVX_512bit && VM_Version::supports_evex()), \"\");\n@@ -4103,1 +4276,2 @@\n-  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex(), \"\");\n+  assert((vector_len == AVX_256bit && VM_Version::supports_avx2()) ||\n+         (vector_len == AVX_512bit && VM_Version::supports_evex()), \"\");\n@@ -4109,1 +4283,10 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::vpermps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert((vector_len == AVX_256bit && VM_Version::supports_avx2()) ||\n+         (vector_len == AVX_512bit && VM_Version::supports_evex()), \"\");\n+  \/\/ VEX.NDS.XXX.66.0F38.W0 16 \/r\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x16, (0xC0 | encode));\n@@ -4133,0 +4316,7 @@\n+void Assembler::vpermilps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x0C, (0xC0 | encode));\n+}\n+\n@@ -4186,1 +4376,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -4240,1 +4430,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 0);\n@@ -4254,1 +4444,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 0);\n@@ -4274,1 +4464,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 1);\n@@ -4287,1 +4477,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 0);\n@@ -4300,1 +4490,1 @@\n-  emit_operand(as_Register(kdst->encoding()), src);\n+  emit_operand(as_Register(kdst->encoding()), src, 0);\n@@ -4338,1 +4528,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 0);\n@@ -4380,1 +4570,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 0);\n@@ -4391,0 +4581,10 @@\n+void Assembler::evpcmpeqq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x29, (0xC0 | encode));\n+}\n+\n@@ -4427,1 +4627,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 0);\n@@ -4466,8 +4666,0 @@\n-void Assembler::vpmaskmovd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n-  assert((VM_Version::supports_avx2() && vector_len == AVX_256bit), \"\");\n-  InstructionMark im(this);\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n-  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int8((unsigned char)0x8C);\n-  emit_operand(dst, src);\n-}\n@@ -4488,1 +4680,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -4505,1 +4697,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -4522,1 +4714,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -4539,1 +4731,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -4556,1 +4748,1 @@\n-  emit_operand(dst,src);\n+  emit_operand(dst, src, 1);\n@@ -4580,1 +4772,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -4604,1 +4796,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -4621,1 +4813,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -4660,1 +4852,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -4720,1 +4912,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -4751,1 +4943,18 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::evpmovzxbd(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx512vl(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_HVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x31);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::evpmovzxbd(XMMRegister dst, Address src, int vector_len) {\n+  evpmovzxbd(dst, k0, src, vector_len);\n@@ -4842,1 +5051,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -4856,1 +5065,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -4868,1 +5077,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -4914,0 +5123,34 @@\n+void Assembler::evpmadd52luq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  evpmadd52luq(dst, k0, src1, src2, false, vector_len);\n+}\n+\n+void Assembler::evpmadd52luq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512ifma(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xB4, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmadd52huq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  evpmadd52huq(dst, k0, src1, src2, false, vector_len);\n+}\n+\n+void Assembler::evpmadd52huq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512ifma(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xB5, (0xC0 | encode));\n+}\n+\n@@ -4935,1 +5178,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5007,1 +5250,1 @@\n-  emit_operand(rax, dst);\n+  emit_operand(rax, dst, 0);\n@@ -5016,1 +5259,1 @@\n-  emit_operand(rax, src); \/\/ 0, src\n+  emit_operand(rax, src, 0); \/\/ 0, src\n@@ -5024,1 +5267,1 @@\n-  emit_operand(rax, src); \/\/ 0, src\n+  emit_operand(rax, src, 0); \/\/ 0, src\n@@ -5032,1 +5275,1 @@\n-  emit_operand(rcx, src); \/\/ 1, src\n+  emit_operand(rcx, src, 0); \/\/ 1, src\n@@ -5040,1 +5283,1 @@\n-  emit_operand(rdx, src); \/\/ 2, src\n+  emit_operand(rdx, src, 0); \/\/ 2, src\n@@ -5048,1 +5291,1 @@\n-  emit_operand(rbx, src); \/\/ 3, src\n+  emit_operand(rbx, src, 0); \/\/ 3, src\n@@ -5056,1 +5299,1 @@\n-  emit_operand(rcx, src); \/\/ 1, src\n+  emit_operand(rcx, src, 0); \/\/ 1, src\n@@ -5070,0 +5313,12 @@\n+void Assembler::evpshufb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x00, (0xC0 | encode));\n+}\n+\n@@ -5086,1 +5341,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5117,1 +5372,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -5129,0 +5384,10 @@\n+void Assembler::vpshufhw(XMMRegister dst, XMMRegister src, int mode, int vector_len) {\n+    assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+            (vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+            (vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : false)), \"\");\n+    NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+    InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+    int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+    emit_int24(0x70, (0xC0 | encode), mode & 0xFF);\n+}\n+\n@@ -5146,1 +5411,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -5150,0 +5415,10 @@\n+void Assembler::vpshuflw(XMMRegister dst, XMMRegister src, int mode, int vector_len) {\n+    assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+            (vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+            (vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : false)), \"\");\n+    NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+    InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+    int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+    emit_int24(0x70, (0xC0 | encode), mode & 0xFF);\n+}\n+\n@@ -5159,1 +5434,1 @@\n-void Assembler::pshufpd(XMMRegister dst, XMMRegister src, int imm8) {\n+void Assembler::shufpd(XMMRegister dst, XMMRegister src, int imm8) {\n@@ -5167,1 +5442,1 @@\n-void Assembler::vpshufpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n+void Assembler::vshufpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n@@ -5174,1 +5449,1 @@\n-void Assembler::pshufps(XMMRegister dst, XMMRegister src, int imm8) {\n+void Assembler::shufps(XMMRegister dst, XMMRegister src, int imm8) {\n@@ -5182,1 +5457,1 @@\n-void Assembler::vpshufps(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n+void Assembler::vshufps(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n@@ -5230,1 +5505,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5249,1 +5524,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5266,0 +5541,7 @@\n+void Assembler::vtestps(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x0E, (0xC0 | encode));\n+}\n+\n@@ -5267,2 +5549,2 @@\n-  assert(VM_Version::supports_avx512vlbw(), \"\");\n-  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W0 DB \/r\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : VM_Version::supports_avx512vlbw(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F38.W0 DB \/r\n@@ -5272,1 +5554,19 @@\n-  emit_int16((unsigned char)0x26, (0xC0 | encode));\n+  emit_int16(0x26, (0xC0 | encode));\n+}\n+\n+void Assembler::evptestmd(KRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx512vl(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F38.W0 DB \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x27, (0xC0 | encode));\n+}\n+\n+void Assembler::evptestnmd(KRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx512vl(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.F3.0F38.W0 DB \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x27, (0xC0 | encode));\n@@ -5283,1 +5583,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5301,1 +5601,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5311,6 +5611,42 @@\n-void Assembler::punpcklqdq(XMMRegister dst, XMMRegister src) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_rex_vex_w_reverted();\n-  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16(0x6C, (0xC0 | encode));\n+void Assembler::punpcklqdq(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6C, (0xC0 | encode));\n+}\n+\n+void Assembler::evpunpcklqdq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  evpunpcklqdq(dst, k0, src1, src2, false, vector_len);\n+}\n+\n+void Assembler::evpunpcklqdq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6C, (0xC0 | encode));\n+}\n+\n+void Assembler::evpunpckhqdq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  evpunpckhqdq(dst, k0, src1, src2, false, vector_len);\n+}\n+\n+void Assembler::evpunpckhqdq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6D, (0xC0 | encode));\n@@ -5341,1 +5677,1 @@\n-  emit_operand(rsi, src);\n+  emit_operand(rsi, src, 0);\n@@ -5501,1 +5837,1 @@\n-    emit_operand(as_Register(4), dst);\n+    emit_operand(as_Register(4), dst, 0);\n@@ -5505,1 +5841,1 @@\n-    emit_operand(as_Register(4), dst);\n+    emit_operand(as_Register(4), dst, 1);\n@@ -5514,1 +5850,1 @@\n-  emit_operand(as_Register(4), dst);\n+  emit_operand(as_Register(4), dst, 0);\n@@ -5538,1 +5874,1 @@\n-    emit_operand(as_Register(7), dst);\n+    emit_operand(as_Register(7), dst, 0);\n@@ -5542,1 +5878,1 @@\n-    emit_operand(as_Register(7), dst);\n+    emit_operand(as_Register(7), dst, 1);\n@@ -5551,1 +5887,1 @@\n-  emit_operand(as_Register(7), dst);\n+  emit_operand(as_Register(7), dst, 0);\n@@ -5585,1 +5921,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5724,1 +6060,1 @@\n-  emit_operand(as_Register(5), dst);\n+  emit_operand(as_Register(5), dst, 0);\n@@ -5733,1 +6069,1 @@\n-    emit_operand(as_Register(5), dst);\n+    emit_operand(as_Register(5), dst, 0);\n@@ -5737,1 +6073,1 @@\n-    emit_operand(as_Register(5), dst);\n+    emit_operand(as_Register(5), dst, 1);\n@@ -5763,0 +6099,12 @@\n+#ifdef _LP64\n+void Assembler::shldq(Register dst, Register src, int8_t imm8) {\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n+  emit_int32(0x0F, (unsigned char)0xA4, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::shrdq(Register dst, Register src, int8_t imm8) {\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n+  emit_int32(0x0F, (unsigned char)0xAC, (0xC0 | encode), imm8);\n+}\n+#endif\n+\n@@ -5781,1 +6129,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -5801,1 +6149,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5822,1 +6170,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5832,1 +6180,1 @@\n-    emit_operand(as_Register(3), dst);\n+    emit_operand(as_Register(3), dst, 0);\n@@ -5838,1 +6186,1 @@\n-    emit_operand(as_Register(3), dst);\n+    emit_operand(as_Register(3), dst, 0);\n@@ -5852,1 +6200,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -5870,1 +6218,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5894,1 +6242,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5911,1 +6259,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5934,4 +6282,1 @@\n-  if (imm32 >= 0 && is8bit(imm32)) {\n-    testb(dst, imm32);\n-    return;\n-  }\n+  prefix(dst);\n@@ -5940,1 +6285,1 @@\n-  emit_operand(as_Register(0), dst);\n+  emit_operand(as_Register(0), dst, 4);\n@@ -5945,4 +6290,0 @@\n-  if (imm32 >= 0 && is8bit(imm32) && dst->has_byte_register()) {\n-    testb(dst, imm32);\n-    return;\n-  }\n@@ -5972,1 +6313,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -5984,0 +6325,9 @@\n+void Assembler::tzcntl(Register dst, Address src) {\n+  assert(VM_Version::supports_bmi1(), \"tzcnt instruction not supported\");\n+  InstructionMark im(this);\n+  emit_int8((unsigned char)0xF3);\n+  prefix(src, dst);\n+  emit_int16(0x0F, (unsigned char)0xBC);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -5991,0 +6341,9 @@\n+void Assembler::tzcntq(Register dst, Address src) {\n+  assert(VM_Version::supports_bmi1(), \"tzcnt instruction not supported\");\n+  InstructionMark im(this);\n+  emit_int8((unsigned char)0xF3);\n+  prefixq(src, dst);\n+  emit_int16(0x0F, (unsigned char)0xBC);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -5999,1 +6358,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6017,1 +6376,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6035,1 +6394,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -6043,1 +6402,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -6050,1 +6409,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -6058,1 +6417,1 @@\n-    assert(entry != NULL, \"abort entry NULL\");\n+    assert(entry != nullptr, \"abort entry null\");\n@@ -6073,1 +6432,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6081,1 +6440,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6088,1 +6447,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6119,1 +6478,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6131,1 +6490,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -6138,1 +6497,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6145,1 +6504,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -6163,1 +6522,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6181,1 +6540,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6208,1 +6567,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6226,1 +6585,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6258,1 +6617,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6276,1 +6635,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6294,1 +6653,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6312,1 +6671,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6342,1 +6701,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6376,1 +6735,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6386,1 +6745,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6441,1 +6800,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6451,1 +6810,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6484,1 +6843,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6517,1 +6876,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6527,1 +6886,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6570,1 +6929,1 @@\n-  emit_operand(dst, src2);\n+  emit_operand(dst, src2, 0);\n@@ -6598,1 +6957,1 @@\n-  emit_operand(dst, src2);\n+  emit_operand(dst, src2, 0);\n@@ -6608,1 +6967,1 @@\n-  emit_operand(dst, src2);\n+  emit_operand(dst, src2, 0);\n@@ -6649,1 +7008,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6659,1 +7018,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6689,1 +7048,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -6710,1 +7069,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -6730,1 +7089,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6747,1 +7106,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6772,1 +7131,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6783,1 +7142,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6809,1 +7168,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6819,1 +7178,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6862,1 +7221,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6872,1 +7231,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6898,1 +7257,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6908,1 +7267,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -6955,1 +7314,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7016,1 +7375,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7026,1 +7385,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7036,1 +7395,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7047,1 +7406,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7122,1 +7481,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7132,1 +7491,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7142,1 +7501,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7153,1 +7512,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7200,1 +7559,1 @@\n-void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+void Assembler::evpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n@@ -7222,1 +7581,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7232,1 +7591,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7235,1 +7594,1 @@\n-void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+void Assembler::evpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n@@ -7243,1 +7602,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7718,1 +8077,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7721,5 +8080,6 @@\n-void Assembler::vpandq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xDB, (0xC0 | encode));\n+void Assembler::evpandq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  evpandq(dst, k0, nds, src, false, vector_len);\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  evpandq(dst, k0, nds, src, false, vector_len);\n@@ -7835,1 +8195,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7838,5 +8198,2 @@\n-void Assembler::vporq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-  assert(VM_Version::supports_evex(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xEB, (0xC0 | encode));\n+void Assembler::evporq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  evporq(dst, k0, nds, src, false, vector_len);\n@@ -7845,0 +8202,3 @@\n+void Assembler::evporq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  evporq(dst, k0, nds, src, false, vector_len);\n+}\n@@ -7872,1 +8232,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7902,1 +8262,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7938,1 +8298,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7966,1 +8326,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7981,1 +8341,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -7985,1 +8345,2 @@\n-  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n@@ -7997,1 +8358,2 @@\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n@@ -8008,1 +8370,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8012,1 +8374,2 @@\n-  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n@@ -8024,1 +8387,2 @@\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n@@ -8035,1 +8399,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8055,1 +8419,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8076,4 +8440,0 @@\n-\/\/ Register is a class, but it would be assigned numerical value.\n-\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n-PRAGMA_DIAG_PUSH\n-PRAGMA_NONNULL_IGNORED\n@@ -8097,1 +8457,0 @@\n-PRAGMA_DIAG_POP\n@@ -8136,1 +8495,2 @@\n-  assert(VM_Version::supports_avx512cd() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_avx512cd(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -8148,1 +8508,2 @@\n-  assert(VM_Version::supports_avx512cd() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_avx512cd(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -8180,1 +8541,1 @@\n-  emit_operand(dst, src3);\n+  emit_operand(dst, src3, 1);\n@@ -8185,2 +8546,2 @@\n-  assert(VM_Version::supports_evex(), \"requires EVEX support\");\n-  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires VL support\");\n+  assert(VM_Version::supports_evex(), \"requires AVX512F\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n@@ -8195,0 +8556,14 @@\n+void Assembler::vpternlogq(XMMRegister dst, int imm8, XMMRegister src2, Address src3, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"requires EVEX support\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires VL support\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV, \/* input_size_in_bits *\/ EVEX_64bit);\n+  vex_prefix(src3, src2->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8(0x25);\n+  emit_operand(dst, src3, 1);\n+  emit_int8(imm8);\n+}\n+\n@@ -8295,1 +8670,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -8325,1 +8700,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -8368,1 +8743,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -8396,1 +8771,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -8426,1 +8801,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 1);\n@@ -8456,1 +8831,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -8487,1 +8862,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -8532,1 +8907,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -8560,1 +8935,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -8591,1 +8966,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -8636,1 +9011,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 1);\n@@ -8659,1 +9034,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8679,1 +9054,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8742,1 +9117,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8769,1 +9144,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8798,1 +9173,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8827,1 +9202,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8856,1 +9231,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8885,1 +9260,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8912,1 +9287,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8939,1 +9314,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8968,1 +9343,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -8997,1 +9372,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9026,1 +9401,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9055,1 +9430,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9082,1 +9457,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9111,1 +9486,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9138,1 +9513,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9167,1 +9542,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9196,1 +9571,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9225,1 +9600,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9254,1 +9629,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9284,1 +9659,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9313,1 +9688,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9342,1 +9717,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9371,1 +9746,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9402,1 +9777,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9433,1 +9808,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9462,1 +9837,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9491,1 +9866,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9517,1 +9892,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9543,1 +9918,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9569,1 +9944,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9595,1 +9970,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9963,1 +10338,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -9989,1 +10364,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10017,1 +10392,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10045,1 +10420,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10072,1 +10447,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10098,1 +10473,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10126,1 +10501,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10154,1 +10529,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10184,1 +10559,1 @@\n-  emit_operand(dst, src3);\n+  emit_operand(dst, src3, 1);\n@@ -10215,1 +10590,1 @@\n-  emit_operand(dst, src3);\n+  emit_operand(dst, src3, 1);\n@@ -10219,0 +10594,8 @@\n+void Assembler::gf2p8affineqb(XMMRegister dst, XMMRegister src, int imm8) {\n+  assert(VM_Version::supports_gfni(), \"\");\n+  assert(VM_Version::supports_sse(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24((unsigned char)0xCE, (unsigned char)(0xC0 | encode), imm8);\n+}\n+\n@@ -10244,1 +10627,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10266,1 +10649,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10280,1 +10663,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10303,1 +10686,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10325,1 +10708,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10349,1 +10732,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10362,1 +10745,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10413,1 +10796,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10426,1 +10809,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10439,1 +10822,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10452,1 +10835,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10469,1 +10852,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10487,1 +10870,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10505,1 +10888,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10523,1 +10906,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -10537,1 +10920,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -10551,1 +10934,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -10565,1 +10948,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -10579,1 +10962,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -10613,0 +10996,20 @@\n+void Assembler::vfpclassss(KRegister kdst, XMMRegister src, uint8_t imm8) {\n+  \/\/ Encoding: EVEX.LIG.66.0F3A.W0 67 \/r ib\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ false);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24((unsigned char)0x67, (unsigned char)(0xC0 | encode), imm8);\n+}\n+\n+void Assembler::vfpclasssd(KRegister kdst, XMMRegister src, uint8_t imm8) {\n+  \/\/ Encoding: EVEX.LIG.66.0F3A.W1 67 \/r ib\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ false);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24((unsigned char)0x67, (unsigned char)(0xC0 | encode), imm8);\n+}\n+\n@@ -10616,1 +11019,1 @@\n-  emit_operand32(rbp, adr);\n+  emit_operand32(rbp, adr, 0);\n@@ -10622,1 +11025,1 @@\n-  emit_operand32(rdi, adr);\n+  emit_operand32(rdi, adr, 0);\n@@ -10625,1 +11028,1 @@\n-void Assembler::emit_operand32(Register reg, Address adr) {\n+void Assembler::emit_operand32(Register reg, Address adr, int post_addr_length) {\n@@ -10628,2 +11031,1 @@\n-  emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp,\n-               adr._rspec);\n+  emit_operand(reg, adr._base, adr._index, adr._scale, adr._disp, adr._rspec, post_addr_length);\n@@ -10655,1 +11057,1 @@\n-  emit_operand(rdi, src1);\n+  emit_operand(rdi, src1, 4);\n@@ -10665,1 +11067,1 @@\n-  emit_operand(rcx, adr);\n+  emit_operand(rcx, adr, 0);\n@@ -10692,1 +11094,1 @@\n-  emit_operand32(rax, src);\n+  emit_operand32(rax, src, 0);\n@@ -10698,1 +11100,1 @@\n-  emit_operand32(rax, src);\n+  emit_operand32(rax, src, 0);\n@@ -10724,1 +11126,1 @@\n-  emit_operand32(rbx, src);\n+  emit_operand32(rbx, src, 0);\n@@ -10730,1 +11132,1 @@\n-  emit_operand32(rbx, src);\n+  emit_operand32(rbx, src, 0);\n@@ -10752,1 +11154,1 @@\n-  emit_operand32(rsi, src);\n+  emit_operand32(rsi, src, 0);\n@@ -10758,1 +11160,1 @@\n-  emit_operand32(rsi, src);\n+  emit_operand32(rsi, src, 0);\n@@ -10779,1 +11181,1 @@\n-  emit_operand32(rdi, src);\n+  emit_operand32(rdi, src, 0);\n@@ -10785,1 +11187,1 @@\n-  emit_operand32(rdi, src);\n+  emit_operand32(rdi, src, 0);\n@@ -10803,1 +11205,1 @@\n-  emit_operand32(rbp, adr);\n+  emit_operand32(rbp, adr, 0);\n@@ -10809,1 +11211,1 @@\n-  emit_operand32(rax, adr);\n+  emit_operand32(rax, adr, 0);\n@@ -10823,1 +11225,1 @@\n-  emit_operand32(rdx, adr);\n+  emit_operand32(rdx, adr, 0);\n@@ -10829,1 +11231,1 @@\n-  emit_operand32(rdi, adr);\n+  emit_operand32(rdi, adr, 0);\n@@ -10835,1 +11237,1 @@\n-  emit_operand32(rbx, adr);\n+  emit_operand32(rbx, adr, 0);\n@@ -10845,1 +11247,1 @@\n-  emit_operand32(rax, adr);\n+  emit_operand32(rax, adr, 0);\n@@ -10851,1 +11253,1 @@\n-  emit_operand32(rax, adr);\n+  emit_operand32(rax, adr, 0);\n@@ -10862,1 +11264,1 @@\n-  emit_operand32(rbp, src);\n+  emit_operand32(rbp, src, 0);\n@@ -10868,1 +11270,1 @@\n-  emit_operand32(rsp, src);\n+  emit_operand32(rsp, src, 0);\n@@ -10902,1 +11304,1 @@\n-  emit_operand32(rcx, src);\n+  emit_operand32(rcx, src, 0);\n@@ -10908,1 +11310,1 @@\n-  emit_operand32(rcx, src);\n+  emit_operand32(rcx, src, 0);\n@@ -10922,1 +11324,1 @@\n-  emit_operand32(rsi, dst);\n+  emit_operand32(rsi, dst, 0);\n@@ -10928,1 +11330,1 @@\n-  emit_operand32(rdi, src);\n+  emit_operand32(rdi, src, 0);\n@@ -10946,1 +11348,1 @@\n-  emit_operand32(rsp, src);\n+  emit_operand32(rsp, src, 0);\n@@ -10960,1 +11362,1 @@\n-  emit_operand32(rdx, adr);\n+  emit_operand32(rdx, adr, 0);\n@@ -10966,1 +11368,1 @@\n-  emit_operand32(rdx, adr);\n+  emit_operand32(rdx, adr, 0);\n@@ -10972,1 +11374,1 @@\n-  emit_operand32(rbx, adr);\n+  emit_operand32(rbx, adr, 0);\n@@ -10982,1 +11384,1 @@\n-  emit_operand32(rbx, adr);\n+  emit_operand32(rbx, adr, 0);\n@@ -10992,1 +11394,1 @@\n-  emit_operand32(rsp, src);\n+  emit_operand32(rsp, src, 0);\n@@ -10998,1 +11400,1 @@\n-  emit_operand32(rsp, src);\n+  emit_operand32(rsp, src, 0);\n@@ -11016,1 +11418,1 @@\n-  emit_operand32(rbp, src);\n+  emit_operand32(rbp, src, 0);\n@@ -11022,1 +11424,1 @@\n-  emit_operand32(rbp, src);\n+  emit_operand32(rbp, src, 0);\n@@ -11512,1 +11914,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 1);\n@@ -11545,1 +11947,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 1);\n@@ -11580,1 +11982,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 1);\n@@ -11615,1 +12017,1 @@\n-  emit_operand(as_Register(dst_enc), src);\n+  emit_operand(as_Register(dst_enc), src, 1);\n@@ -11619,4 +12021,0 @@\n-\/\/ Register is a class, but it would be assigned numerical value.\n-\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n-PRAGMA_DIAG_PUSH\n-PRAGMA_NONNULL_IGNORED\n@@ -11646,1 +12044,0 @@\n-PRAGMA_DIAG_POP\n@@ -11815,1 +12212,15 @@\n-void Assembler::pext(Register dst, Register src1, Register src2) {\n+void Assembler::pextl(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF5, (0xC0 | encode));\n+}\n+\n+void Assembler::pdepl(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF5, (0xC0 | encode));\n+}\n+\n+void Assembler::pextq(Register dst, Register src1, Register src2) {\n@@ -11822,1 +12233,1 @@\n-void Assembler::pdep(Register dst, Register src1, Register src2) {\n+void Assembler::pdepq(Register dst, Register src1, Register src2) {\n@@ -11829,0 +12240,68 @@\n+void Assembler::pextl(Register dst, Register src1, Address src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF5);\n+  emit_operand(dst, src2, 0);\n+}\n+\n+void Assembler::pdepl(Register dst, Register src1, Address src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF5);\n+  emit_operand(dst, src2, 0);\n+}\n+\n+void Assembler::pextq(Register dst, Register src1, Address src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF5);\n+  emit_operand(dst, src2, 0);\n+}\n+\n+void Assembler::pdepq(Register dst, Register src1, Address src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF5);\n+  emit_operand(dst, src2, 0);\n+}\n+\n+void Assembler::sarxl(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF7, (0xC0 | encode));\n+}\n+\n+void Assembler::sarxl(Register dst, Address src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src1, src2->encoding(), dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(dst, src1, 0);\n+}\n+\n+void Assembler::sarxq(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF7, (0xC0 | encode));\n+}\n+\n+void Assembler::sarxq(Register dst, Address src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src1, src2->encoding(), dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(dst, src1, 0);\n+}\n+\n@@ -11836,0 +12315,9 @@\n+void Assembler::shlxl(Register dst, Address src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src1, src2->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(dst, src1, 0);\n+}\n+\n@@ -11843,0 +12331,9 @@\n+void Assembler::shlxq(Register dst, Address src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src1, src2->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(dst, src1, 0);\n+}\n+\n@@ -11850,0 +12347,9 @@\n+void Assembler::shrxl(Register dst, Address src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src1, src2->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(dst, src1, 0);\n+}\n+\n@@ -11857,0 +12363,9 @@\n+void Assembler::shrxq(Register dst, Address src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src1, src2->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(dst, src1, 0);\n+}\n+\n@@ -11922,1 +12437,2 @@\n-  assert(VM_Version::supports_avx512_vbmi2() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_avx512_vbmi2(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -11934,1 +12450,2 @@\n-  assert(VM_Version::supports_avx512_vbmi2() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_avx512_vbmi2(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -11946,1 +12463,2 @@\n-  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -11958,1 +12476,2 @@\n-  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -11970,1 +12489,2 @@\n-  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -11982,1 +12502,2 @@\n-  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -12007,1 +12528,1 @@\n-  emit_operand(rax, dst);\n+  emit_operand(rax, dst, 4);\n@@ -12038,0 +12559,2 @@\n+\/\/ 64bit only pieces of the assembler\n+\n@@ -12043,1 +12566,0 @@\n-\/\/ 64bit only pieces of the assembler\n@@ -12047,9 +12569,26 @@\n-bool Assembler::reachable(AddressLiteral adr) {\n-  int64_t disp;\n-  relocInfo::relocType relocType = adr.reloc();\n-\n-  \/\/ None will force a 64bit literal to the code stream. Likely a placeholder\n-  \/\/ for something that will be patched later and we need to certain it will\n-  \/\/ always be reachable.\n-  if (relocType == relocInfo::none) {\n-    return false;\n+\/\/ Determine whether an address is always reachable in rip-relative addressing mode\n+\/\/ when accessed from the code cache.\n+static bool is_always_reachable(address target, relocInfo::relocType reloc_type) {\n+  switch (reloc_type) {\n+    \/\/ This should be rip-relative and easily reachable.\n+    case relocInfo::internal_word_type: {\n+      return true;\n+    }\n+    \/\/ This should be rip-relative within the code cache and easily\n+    \/\/ reachable until we get huge code caches. (At which point\n+    \/\/ IC code is going to have issues).\n+    case relocInfo::virtual_call_type:\n+    case relocInfo::opt_virtual_call_type:\n+    case relocInfo::static_call_type:\n+    case relocInfo::static_stub_type: {\n+      return true;\n+    }\n+    case relocInfo::runtime_call_type:\n+    case relocInfo::external_word_type:\n+    case relocInfo::poll_return_type: \/\/ these are really external_word but need special\n+    case relocInfo::poll_type: {      \/\/ relocs to identify them\n+      return CodeCache::contains(target);\n+    }\n+    default: {\n+      return false;\n+    }\n@@ -12057,2 +12596,5 @@\n-  if (relocType == relocInfo::internal_word_type) {\n-    \/\/ This should be rip relative and easily reachable.\n+}\n+\n+\/\/ Determine whether an address is reachable in rip-relative addressing mode from the code cache.\n+static bool is_reachable(address target, relocInfo::relocType reloc_type) {\n+  if (is_always_reachable(target, reloc_type)) {\n@@ -12061,8 +12603,32 @@\n-  if (relocType == relocInfo::virtual_call_type ||\n-      relocType == relocInfo::opt_virtual_call_type ||\n-      relocType == relocInfo::static_call_type ||\n-      relocType == relocInfo::static_stub_type ) {\n-    \/\/ This should be rip relative within the code cache and easily\n-    \/\/ reachable until we get huge code caches. (At which point\n-    \/\/ ic code is going to have issues).\n-    return true;\n+  switch (reloc_type) {\n+    \/\/ None will force a 64bit literal to the code stream. Likely a placeholder\n+    \/\/ for something that will be patched later and we need to certain it will\n+    \/\/ always be reachable.\n+    case relocInfo::none: {\n+      return false;\n+    }\n+    case relocInfo::runtime_call_type:\n+    case relocInfo::external_word_type:\n+    case relocInfo::poll_return_type: \/\/ these are really external_word but need special\n+    case relocInfo::poll_type: {      \/\/ relocs to identify them\n+      assert(!CodeCache::contains(target), \"always reachable\");\n+      if (ForceUnreachable) {\n+        return false; \/\/ stress the correction code\n+      }\n+      \/\/ For external_word_type\/runtime_call_type if it is reachable from where we\n+      \/\/ are now (possibly a temp buffer) and where we might end up\n+      \/\/ anywhere in the code cache then we are always reachable.\n+      \/\/ This would have to change if we ever save\/restore shared code to be more pessimistic.\n+      \/\/ Code buffer has to be allocated in the code cache, so check against\n+      \/\/ code cache boundaries cover that case.\n+      \/\/\n+      \/\/ In rip-relative addressing mode, an effective address is formed by adding displacement\n+      \/\/ to the 64-bit RIP of the next instruction which is not known yet. Considering target address\n+      \/\/ is guaranteed to be outside of the code cache, checking against code cache boundaries is enough\n+      \/\/ to account for that.\n+      return Assembler::is_simm32(target - CodeCache::low_bound()) &&\n+             Assembler::is_simm32(target - CodeCache::high_bound());\n+    }\n+    default: {\n+      return false;\n+    }\n@@ -12070,4 +12636,5 @@\n-  if (relocType != relocInfo::external_word_type &&\n-      relocType != relocInfo::poll_return_type &&  \/\/ these are really external_word but need special\n-      relocType != relocInfo::poll_type &&         \/\/ relocs to identify them\n-      relocType != relocInfo::runtime_call_type ) {\n+}\n+\n+bool Assembler::reachable(AddressLiteral adr) {\n+  assert(CodeCache::contains(pc()), \"required\");\n+  if (adr.is_lval()) {\n@@ -12076,0 +12643,2 @@\n+  return is_reachable(adr.target(), adr.reloc());\n+}\n@@ -12077,34 +12646,4 @@\n-  \/\/ Stress the correction code\n-  if (ForceUnreachable) {\n-    \/\/ Must be runtimecall reloc, see if it is in the codecache\n-    \/\/ Flipping stuff in the codecache to be unreachable causes issues\n-    \/\/ with things like inline caches where the additional instructions\n-    \/\/ are not handled.\n-    if (CodeCache::find_blob(adr._target) == NULL) {\n-      return false;\n-    }\n-  }\n-  \/\/ For external_word_type\/runtime_call_type if it is reachable from where we\n-  \/\/ are now (possibly a temp buffer) and where we might end up\n-  \/\/ anywhere in the codeCache then we are always reachable.\n-  \/\/ This would have to change if we ever save\/restore shared code\n-  \/\/ to be more pessimistic.\n-  disp = (int64_t)adr._target - ((int64_t)CodeCache::low_bound() + sizeof(int));\n-  if (!is_simm32(disp)) return false;\n-  disp = (int64_t)adr._target - ((int64_t)CodeCache::high_bound() + sizeof(int));\n-  if (!is_simm32(disp)) return false;\n-\n-  disp = (int64_t)adr._target - ((int64_t)pc() + sizeof(int));\n-\n-  \/\/ Because rip relative is a disp + address_of_next_instruction and we\n-  \/\/ don't know the value of address_of_next_instruction we apply a fudge factor\n-  \/\/ to make sure we will be ok no matter the size of the instruction we get placed into.\n-  \/\/ We don't have to fudge the checks above here because they are already worst case.\n-\n-  \/\/ 12 == override\/rex byte, opcode byte, rm byte, sib byte, a 4-byte disp , 4-byte literal\n-  \/\/ + 4 because better safe than sorry.\n-  const int fudge = 12 + 4;\n-  if (disp < 0) {\n-    disp -= fudge;\n-  } else {\n-    disp += fudge;\n+bool Assembler::always_reachable(AddressLiteral adr) {\n+  assert(CodeCache::contains(pc()), \"required\");\n+  if (adr.is_lval()) {\n+    return false;\n@@ -12112,1 +12651,1 @@\n-  return is_simm32(disp);\n+  return is_always_reachable(adr.target(), adr.reloc());\n@@ -12130,1 +12669,1 @@\n-  assert(inst_mark() != NULL, \"must be inside InstructionMark\");\n+  assert(inst_mark() != nullptr, \"must be inside InstructionMark\");\n@@ -12164,1 +12703,1 @@\n-      prefix(REX_B);\n+      p = (Prefix)(p | REX_B);\n@@ -12409,1 +12948,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12426,1 +12965,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -12437,1 +12976,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12479,1 +13018,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12490,1 +13029,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -12506,1 +13045,1 @@\n-  emit_operand(dst, src2);\n+  emit_operand(dst, src2, 0);\n@@ -12537,1 +13076,1 @@\n-  emit_operand(rbx, src);\n+  emit_operand(rbx, src, 0);\n@@ -12553,1 +13092,1 @@\n-  emit_operand(rdx, src);\n+  emit_operand(rdx, src, 0);\n@@ -12569,1 +13108,1 @@\n-  emit_operand(rcx, src);\n+  emit_operand(rcx, src, 0);\n@@ -12580,1 +13119,1 @@\n-  emit_operand(rdi, adr);\n+  emit_operand(rdi, adr, 0);\n@@ -12595,1 +13134,1 @@\n-  emit_operand(rdi, adr);\n+  emit_operand(rdi, adr, 0);\n@@ -12610,1 +13149,1 @@\n-  emit_operand(rsi, adr);\n+  emit_operand(rsi, adr, 0);\n@@ -12621,1 +13160,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12626,3 +13165,2 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0x81);\n-  emit_operand(rdi, dst, 4);\n-  emit_int32(imm32);\n+  prefixq(dst);\n+  emit_arith_operand(0x81, as_Register(7), dst, imm32);\n@@ -12639,1 +13177,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -12650,1 +13188,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12656,1 +13194,1 @@\n-  emit_operand(reg, adr);\n+  emit_operand(reg, adr, 0);\n@@ -12673,1 +13211,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12683,1 +13221,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12692,1 +13230,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12734,1 +13272,1 @@\n-  emit_operand(rcx, dst);\n+  emit_operand(rcx, dst, 0);\n@@ -12739,1 +13277,1 @@\n-  emit_operand(as_Register(1), src);\n+  emit_operand(as_Register(1), src, 0);\n@@ -12744,1 +13282,1 @@\n-  emit_operand(as_Register(5), src);\n+  emit_operand(as_Register(5), src, 0);\n@@ -12749,1 +13287,1 @@\n-  emit_operand(as_Register(0), dst);\n+  emit_operand(as_Register(0), dst, 0);\n@@ -12754,1 +13292,1 @@\n-  emit_operand(as_Register(4), dst);\n+  emit_operand(as_Register(4), dst, 0);\n@@ -12762,7 +13300,0 @@\n-void Assembler::idivq(Address src) {\n-  InstructionMark im(this);\n-  prefixq(src);\n-  emit_int8((unsigned char)0xF7);\n-  emit_operand(as_Register(7), src);\n-}\n-\n@@ -12789,1 +13320,1 @@\n-    emit_operand(dst, src);\n+    emit_operand(dst, src, 1);\n@@ -12793,1 +13324,1 @@\n-    emit_operand(dst, src);\n+    emit_operand(dst, src, 4);\n@@ -12811,1 +13342,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12832,1 +13363,1 @@\n-  emit_operand(rax, dst);\n+  emit_operand(rax, dst, 0);\n@@ -12842,1 +13373,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12903,0 +13434,9 @@\n+void Assembler::lzcntq(Register dst, Address src) {\n+  assert(VM_Version::supports_lzcnt(), \"encoding is treated as BSR\");\n+  InstructionMark im(this);\n+  emit_int8((unsigned char)0xF3);\n+  prefixq(src, dst);\n+  emit_int16(0x0F, (unsigned char)0xBD);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -12930,1 +13470,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12936,1 +13476,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -12942,1 +13482,1 @@\n-  emit_operand(as_Register(0), dst);\n+  emit_operand(as_Register(0), dst, 4);\n@@ -12957,1 +13497,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -12987,1 +13527,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13000,1 +13540,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13013,1 +13553,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13026,1 +13566,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13037,1 +13577,1 @@\n-  emit_operand(rsp, src);\n+  emit_operand(rsp, src, 0);\n@@ -13060,1 +13600,1 @@\n-  emit_operand(as_Register(3), dst);\n+  emit_operand(as_Register(3), dst, 0);\n@@ -13097,1 +13637,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -13105,0 +13645,5 @@\n+void Assembler::orq_imm32(Register dst, int32_t imm32) {\n+  (void) prefixq_and_encode(dst->encoding());\n+  emit_arith_imm32(0x81, 0xC8, dst, imm32);\n+}\n+\n@@ -13108,1 +13653,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13123,1 +13668,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13136,1 +13681,1 @@\n-  emit_operand(rax, dst);\n+  emit_operand(rax, dst, 0);\n@@ -13150,1 +13695,1 @@\n-static u_char* popa_code  = NULL;\n+static u_char* popa_code  = nullptr;\n@@ -13153,1 +13698,1 @@\n-static u_char* pusha_code = NULL;\n+static u_char* pusha_code = nullptr;\n@@ -13156,1 +13701,1 @@\n-static u_char* vzup_code  = NULL;\n+static u_char* vzup_code  = nullptr;\n@@ -13203,1 +13748,1 @@\n-  assert(src != NULL, \"code to copy must have been pre-computed\");\n+  assert(src != nullptr, \"code to copy must have been pre-computed\");\n@@ -13271,0 +13816,7 @@\n+void Assembler::vzeroall() {\n+  assert(VM_Version::supports_avx(), \"requires AVX\");\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  (void)vex_prefix_and_encode(0, 0, 0, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x77);\n+}\n+\n@@ -13274,1 +13826,1 @@\n-  emit_operand(rsi, src);\n+  emit_operand(rsi, src, 0);\n@@ -13297,2 +13849,1 @@\n-\n-void Assembler::rorxq(Register dst, Register src, int imm8) {\n+void Assembler::rorxl(Register dst, Register src, int imm8) {\n@@ -13300,1 +13851,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -13305,1 +13856,1 @@\n-void Assembler::rorxd(Register dst, Register src, int imm8) {\n+void Assembler::rorxl(Register dst, Address src, int imm8) {\n@@ -13307,0 +13858,1 @@\n+  InstructionMark im(this);\n@@ -13308,0 +13860,9 @@\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8((unsigned char)0xF0);\n+  emit_operand(dst, src, 1);\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::rorxq(Register dst, Register src, int imm8) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -13312,0 +13873,10 @@\n+void Assembler::rorxq(Register dst, Address src, int imm8) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8((unsigned char)0xF0);\n+  emit_operand(dst, src, 1);\n+  emit_int8(imm8);\n+}\n+\n@@ -13318,1 +13889,1 @@\n-    emit_operand(as_Register(4), dst);\n+    emit_operand(as_Register(4), dst, 0);\n@@ -13322,1 +13893,1 @@\n-    emit_operand(as_Register(4), dst);\n+    emit_operand(as_Register(4), dst, 1);\n@@ -13330,1 +13901,1 @@\n-  emit_operand(as_Register(4), dst);\n+  emit_operand(as_Register(4), dst, 0);\n@@ -13353,1 +13924,1 @@\n-    emit_operand(as_Register(7), dst);\n+    emit_operand(as_Register(7), dst, 0);\n@@ -13357,1 +13928,1 @@\n-    emit_operand(as_Register(7), dst);\n+    emit_operand(as_Register(7), dst, 1);\n@@ -13365,1 +13936,1 @@\n-  emit_operand(as_Register(7), dst);\n+  emit_operand(as_Register(7), dst, 0);\n@@ -13398,1 +13969,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13440,1 +14011,1 @@\n-  emit_operand(as_Register(5), dst);\n+  emit_operand(as_Register(5), dst, 0);\n@@ -13448,1 +14019,1 @@\n-    emit_operand(as_Register(5), dst);\n+    emit_operand(as_Register(5), dst, 0);\n@@ -13452,1 +14023,1 @@\n-    emit_operand(as_Register(5), dst);\n+    emit_operand(as_Register(5), dst, 1);\n@@ -13466,1 +14037,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -13483,1 +14054,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13492,4 +14063,0 @@\n-  if (imm32 >= 0) {\n-    testl(dst, imm32);\n-    return;\n-  }\n@@ -13498,1 +14065,1 @@\n-  emit_operand(as_Register(0), dst);\n+  emit_operand(as_Register(0), dst, 4);\n@@ -13503,4 +14070,0 @@\n-  if (imm32 >= 0) {\n-    testl(dst, imm32);\n-    return;\n-  }\n@@ -13530,1 +14093,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13536,1 +14099,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n@@ -13542,1 +14105,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13558,1 +14121,1 @@\n-  emit_operand(dst, src);\n+  emit_operand(dst, src, 0);\n@@ -13575,1 +14138,1 @@\n-  emit_operand(src, dst);\n+  emit_operand(src, dst, 0);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":1230,"deletions":667,"binary":false,"changes":1897,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,1 +52,3 @@\n-    n_register_parameters = 0   \/\/ 0 registers used to pass arguments\n+    n_register_parameters = 0,   \/\/ 0 registers used to pass arguments\n+    n_int_register_parameters_j   = 0,\n+    n_float_register_parameters_j = 0\n@@ -64,4 +66,4 @@\n-REGISTER_DECLARATION(Register, c_rarg0, rcx);\n-REGISTER_DECLARATION(Register, c_rarg1, rdx);\n-REGISTER_DECLARATION(Register, c_rarg2, r8);\n-REGISTER_DECLARATION(Register, c_rarg3, r9);\n+constexpr Register c_rarg0 = rcx;\n+constexpr Register c_rarg1 = rdx;\n+constexpr Register c_rarg2 =  r8;\n+constexpr Register c_rarg3 =  r9;\n@@ -69,4 +71,4 @@\n-REGISTER_DECLARATION(XMMRegister, c_farg0, xmm0);\n-REGISTER_DECLARATION(XMMRegister, c_farg1, xmm1);\n-REGISTER_DECLARATION(XMMRegister, c_farg2, xmm2);\n-REGISTER_DECLARATION(XMMRegister, c_farg3, xmm3);\n+constexpr XMMRegister c_farg0 = xmm0;\n+constexpr XMMRegister c_farg1 = xmm1;\n+constexpr XMMRegister c_farg2 = xmm2;\n+constexpr XMMRegister c_farg3 = xmm3;\n@@ -76,15 +78,15 @@\n-REGISTER_DECLARATION(Register, c_rarg0, rdi);\n-REGISTER_DECLARATION(Register, c_rarg1, rsi);\n-REGISTER_DECLARATION(Register, c_rarg2, rdx);\n-REGISTER_DECLARATION(Register, c_rarg3, rcx);\n-REGISTER_DECLARATION(Register, c_rarg4, r8);\n-REGISTER_DECLARATION(Register, c_rarg5, r9);\n-\n-REGISTER_DECLARATION(XMMRegister, c_farg0, xmm0);\n-REGISTER_DECLARATION(XMMRegister, c_farg1, xmm1);\n-REGISTER_DECLARATION(XMMRegister, c_farg2, xmm2);\n-REGISTER_DECLARATION(XMMRegister, c_farg3, xmm3);\n-REGISTER_DECLARATION(XMMRegister, c_farg4, xmm4);\n-REGISTER_DECLARATION(XMMRegister, c_farg5, xmm5);\n-REGISTER_DECLARATION(XMMRegister, c_farg6, xmm6);\n-REGISTER_DECLARATION(XMMRegister, c_farg7, xmm7);\n+constexpr Register c_rarg0 = rdi;\n+constexpr Register c_rarg1 = rsi;\n+constexpr Register c_rarg2 = rdx;\n+constexpr Register c_rarg3 = rcx;\n+constexpr Register c_rarg4 =  r8;\n+constexpr Register c_rarg5 =  r9;\n+\n+constexpr XMMRegister c_farg0 = xmm0;\n+constexpr XMMRegister c_farg1 = xmm1;\n+constexpr XMMRegister c_farg2 = xmm2;\n+constexpr XMMRegister c_farg3 = xmm3;\n+constexpr XMMRegister c_farg4 = xmm4;\n+constexpr XMMRegister c_farg5 = xmm5;\n+constexpr XMMRegister c_farg6 = xmm6;\n+constexpr XMMRegister c_farg7 = xmm7;\n@@ -110,3 +112,3 @@\n-REGISTER_DECLARATION(Register, j_rarg0, c_rarg1);\n-REGISTER_DECLARATION(Register, j_rarg1, c_rarg2);\n-REGISTER_DECLARATION(Register, j_rarg2, c_rarg3);\n+constexpr Register j_rarg0 = c_rarg1;\n+constexpr Register j_rarg1 = c_rarg2;\n+constexpr Register j_rarg2 = c_rarg3;\n@@ -115,2 +117,2 @@\n-REGISTER_DECLARATION(Register, j_rarg3, rdi);\n-REGISTER_DECLARATION(Register, j_rarg4, rsi);\n+constexpr Register j_rarg3 = rdi;\n+constexpr Register j_rarg4 = rsi;\n@@ -118,2 +120,2 @@\n-REGISTER_DECLARATION(Register, j_rarg3, c_rarg4);\n-REGISTER_DECLARATION(Register, j_rarg4, c_rarg5);\n+constexpr Register j_rarg3 = c_rarg4;\n+constexpr Register j_rarg4 = c_rarg5;\n@@ -121,1 +123,1 @@\n-REGISTER_DECLARATION(Register, j_rarg5, c_rarg0);\n+constexpr Register j_rarg5 = c_rarg0;\n@@ -123,8 +125,8 @@\n-REGISTER_DECLARATION(XMMRegister, j_farg0, xmm0);\n-REGISTER_DECLARATION(XMMRegister, j_farg1, xmm1);\n-REGISTER_DECLARATION(XMMRegister, j_farg2, xmm2);\n-REGISTER_DECLARATION(XMMRegister, j_farg3, xmm3);\n-REGISTER_DECLARATION(XMMRegister, j_farg4, xmm4);\n-REGISTER_DECLARATION(XMMRegister, j_farg5, xmm5);\n-REGISTER_DECLARATION(XMMRegister, j_farg6, xmm6);\n-REGISTER_DECLARATION(XMMRegister, j_farg7, xmm7);\n+constexpr XMMRegister j_farg0 = xmm0;\n+constexpr XMMRegister j_farg1 = xmm1;\n+constexpr XMMRegister j_farg2 = xmm2;\n+constexpr XMMRegister j_farg3 = xmm3;\n+constexpr XMMRegister j_farg4 = xmm4;\n+constexpr XMMRegister j_farg5 = xmm5;\n+constexpr XMMRegister j_farg6 = xmm6;\n+constexpr XMMRegister j_farg7 = xmm7;\n@@ -132,2 +134,2 @@\n-REGISTER_DECLARATION(Register, rscratch1, r10);  \/\/ volatile\n-REGISTER_DECLARATION(Register, rscratch2, r11);  \/\/ volatile\n+constexpr Register rscratch1 = r10;  \/\/ volatile\n+constexpr Register rscratch2 = r11;  \/\/ volatile\n@@ -135,2 +137,2 @@\n-REGISTER_DECLARATION(Register, r12_heapbase, r12); \/\/ callee-saved\n-REGISTER_DECLARATION(Register, r15_thread, r15); \/\/ callee-saved\n+constexpr Register r12_heapbase = r12; \/\/ callee-saved\n+constexpr Register r15_thread   = r15; \/\/ callee-saved\n@@ -150,1 +152,1 @@\n-REGISTER_DECLARATION(Register, rbp_mh_SP_save, noreg);\n+constexpr Register rbp_mh_SP_save = noreg;\n@@ -356,1 +358,1 @@\n-      _target(NULL)\n+      _target(nullptr)\n@@ -758,1 +760,1 @@\n-                           int rip_relative_correction = 0);\n+                           int post_addr_length);\n@@ -764,1 +766,1 @@\n-                    int rip_relative_correction = 0);\n+                    int post_addr_length);\n@@ -769,1 +771,2 @@\n-                    RelocationHolder const& rspec);\n+                    RelocationHolder const& rspec,\n+                    int post_addr_length);\n@@ -774,1 +777,2 @@\n-                    RelocationHolder const& rspec);\n+                    RelocationHolder const& rspec,\n+                    int post_addr_length);\n@@ -777,1 +781,1 @@\n-                    int rip_relative_correction = 0);\n+                    int post_addr_length);\n@@ -782,1 +786,8 @@\n-                    RelocationHolder const& rspec);\n+                    RelocationHolder const& rspec,\n+                    int post_addr_length);\n+\n+  void emit_operand_helper(KRegister kreg,\n+                           int base_enc, int index_enc, Address::ScaleFactor scale,\n+                           int disp,\n+                           RelocationHolder const& rspec,\n+                           int post_addr_length);\n@@ -784,1 +795,10 @@\n-  void emit_operand(XMMRegister reg, Address adr);\n+  void emit_operand(KRegister kreg, Address adr,\n+                    int post_addr_length);\n+\n+  void emit_operand(KRegister kreg,\n+                    Register base, Register index, Address::ScaleFactor scale,\n+                    int disp,\n+                    RelocationHolder const& rspec,\n+                    int post_addr_length);\n+\n+  void emit_operand(XMMRegister reg, Address adr, int post_addr_length);\n@@ -788,0 +808,1 @@\n+  void emit_arith_operand_imm32(int op1, Register rm, Address adr, int32_t imm32);\n@@ -790,1 +811,1 @@\n-  #ifdef ASSERT\n+#ifdef ASSERT\n@@ -792,1 +813,1 @@\n-  #endif\n+#endif\n@@ -799,1 +820,3 @@\n-  bool reachable(AddressLiteral adr) NOT_LP64({ return true;});\n+  bool always_reachable(AddressLiteral adr) NOT_LP64( { return true; } );\n+  bool        reachable(AddressLiteral adr) NOT_LP64( { return true; } );\n+\n@@ -901,1 +924,1 @@\n-  void clear_attributes(void) { _attributes = NULL; }\n+  void clear_attributes(void) { _attributes = nullptr; }\n@@ -937,4 +960,0 @@\n-  \/\/ These are dummies to prevent surprise implicit conversions to Register\n-  void push(void* v);\n-  void pop(void* v);\n-\n@@ -1089,0 +1108,1 @@\n+  void cmpl_imm32(Address dst, int32_t imm32);\n@@ -1096,4 +1116,0 @@\n-  \/\/ these are dummies used to catch attempting to convert NULL to Register\n-  void cmpl(Register dst, void* junk); \/\/ dummy\n-  void cmpq(Register dst, void* junk); \/\/ dummy\n-\n@@ -1145,0 +1161,7 @@\n+  \/\/ Convert Halffloat to Single Precision Floating-Point value\n+  void vcvtps2ph(XMMRegister dst, XMMRegister src, int imm8, int vector_len);\n+  void vcvtph2ps(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evcvtps2ph(Address dst, KRegister mask, XMMRegister src, int imm8, int vector_len);\n+  void vcvtps2ph(Address dst, XMMRegister src, int imm8, int vector_len);\n+  void vcvtph2ps(XMMRegister dst, Address src, int vector_len);\n+\n@@ -1172,1 +1195,1 @@\n-  \/\/ Convert vector float and int\n+  \/\/ Convert vector float to int\/long\n@@ -1175,0 +1198,1 @@\n+  void evcvttps2qq(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1184,0 +1208,3 @@\n+  \/\/ Convert vector double to int\n+  void vcvttpd2dq(XMMRegister dst, XMMRegister src, int vector_len);\n+\n@@ -1202,0 +1229,3 @@\n+  \/\/ Evex casts with signed saturation\n+  void evpmovsqd(XMMRegister dst, XMMRegister src, int vector_len);\n+\n@@ -1211,3 +1241,0 @@\n-  void divl(Register src);\n-  void divq(Register src);\n-\n@@ -1369,1 +1396,1 @@\n-  void emit_operand32(Register reg, Address adr);\n+  void emit_operand32(Register reg, Address adr, int post_addr_length);\n@@ -1382,1 +1409,3 @@\n-  void idivl(Address src);\n+  void divl(Register src); \/\/ Unsigned division\n+\n+#ifdef _LP64\n@@ -1384,1 +1413,2 @@\n-  void idivq(Address src);\n+  void divq(Register src); \/\/ Unsigned division\n+#endif\n@@ -1456,0 +1486,1 @@\n+  void lzcntl(Register dst, Address src);\n@@ -1459,0 +1490,1 @@\n+  void lzcntq(Register dst, Address src);\n@@ -1484,0 +1516,1 @@\n+  void movddup(XMMRegister dst, Address src);\n@@ -1567,3 +1600,3 @@\n-  void evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len);\n-  void evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len);\n-  void evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqub(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evmovdqub(XMMRegister dst, Address src, int vector_len);\n+  void evmovdqub(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -1572,3 +1605,4 @@\n-  void evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len);\n-  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n-  void evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len);\n+\n+  void evmovdquw(XMMRegister dst, Address src, int vector_len);\n+  void evmovdquw(Address dst, XMMRegister src, int vector_len);\n+  void evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -1576,2 +1610,2 @@\n-  void evmovdqul(Address dst, XMMRegister src, int vector_len);\n-  void evmovdqul(XMMRegister dst, Address src, int vector_len);\n+  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+\n@@ -1579,2 +1613,3 @@\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n-  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evmovdqul(XMMRegister dst, Address src, int vector_len);\n+  void evmovdqul(Address dst, XMMRegister src, int vector_len);\n+\n@@ -1582,0 +1617,3 @@\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+\n@@ -1585,2 +1623,1 @@\n-  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n-  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+\n@@ -1588,0 +1625,2 @@\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -1598,6 +1637,0 @@\n-  \/\/ These dummies prevent using movl from converting a zero (like NULL) into Register\n-  \/\/ by giving the compiler two choices it can't resolve\n-\n-  void movl(Address  dst, void* junk);\n-  void movl(Register dst, void* junk);\n-\n@@ -1610,6 +1643,0 @@\n-\n-  \/\/ These dummies prevent using movq from converting a zero (like NULL) into Register\n-  \/\/ by giving the compiler two choices it can't resolve\n-\n-  void movq(Address  dst, void* dummy);\n-  void movq(Register dst, void* dummy);\n@@ -1638,1 +1665,0 @@\n-  void movslq(Register dst, void* src); \/\/ Dummy declaration to cause NULL to be ambiguous\n@@ -1649,0 +1675,5 @@\n+  void movups(XMMRegister dst, Address src);\n+  void vmovups(XMMRegister dst, Address src, int vector_len);\n+  void movups(Address dst, XMMRegister src);\n+  void vmovups(Address dst, XMMRegister src, int vector_len);\n+\n@@ -1720,0 +1751,1 @@\n+  void orq_imm32(Register dst, int32_t imm32);\n@@ -1745,0 +1777,1 @@\n+  void vpermps(XMMRegister dst,  XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1748,0 +1781,1 @@\n+  void vpermilps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1791,0 +1825,1 @@\n+  void evpcmpeqq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1804,0 +1839,7 @@\n+  void vpmaskmovq(XMMRegister dst, XMMRegister mask, Address src, int vector_len);\n+\n+\n+  void vmaskmovps(XMMRegister dst, Address src, XMMRegister mask, int vector_len);\n+  void vmaskmovpd(XMMRegister dst, Address src, XMMRegister mask, int vector_len);\n+  void vmaskmovps(Address dst, XMMRegister src, XMMRegister mask, int vector_len);\n+  void vmaskmovpd(Address dst, XMMRegister src, XMMRegister mask, int vector_len);\n@@ -1848,0 +1890,2 @@\n+  void evpmovzxbd(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evpmovzxbd(XMMRegister dst, Address src, int vector_len);\n@@ -1869,0 +1913,4 @@\n+  void evpmadd52luq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void evpmadd52luq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len);\n+  void evpmadd52huq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void evpmadd52huq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len);\n@@ -1908,0 +1956,2 @@\n+  void evpshufb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+\n@@ -1918,0 +1968,2 @@\n+  void vpshufhw(XMMRegister dst, XMMRegister src, int mode, int vector_len);\n+  void vpshuflw(XMMRegister dst, XMMRegister src, int mode, int vector_len);\n@@ -1920,4 +1972,4 @@\n-  void pshufps(XMMRegister, XMMRegister, int);\n-  void pshufpd(XMMRegister, XMMRegister, int);\n-  void vpshufps(XMMRegister, XMMRegister, XMMRegister, int, int);\n-  void vpshufpd(XMMRegister, XMMRegister, XMMRegister, int, int);\n+  void shufps(XMMRegister, XMMRegister, int);\n+  void shufpd(XMMRegister, XMMRegister, int);\n+  void vshufps(XMMRegister, XMMRegister, XMMRegister, int, int);\n+  void vshufpd(XMMRegister, XMMRegister, XMMRegister, int, int);\n@@ -1941,0 +1993,2 @@\n+  void evptestmd(KRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evptestnmd(KRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1944,0 +1998,1 @@\n+  void vtestps(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1966,0 +2021,5 @@\n+  void evpunpcklqdq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void evpunpcklqdq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len);\n+  void evpunpckhqdq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void evpunpckhqdq(XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len);\n+\n@@ -2002,0 +2062,2 @@\n+  void rorxl(Register dst, Register src, int imm8);\n+  void rorxl(Register dst, Address src, int imm8);\n@@ -2003,1 +2065,1 @@\n-  void rorxd(Register dst, Register src, int imm8);\n+  void rorxq(Register dst, Address src, int imm8);\n@@ -2066,0 +2128,4 @@\n+#ifdef _LP64\n+  void shldq(Register dst, Register src, int8_t imm8);\n+  void shrdq(Register dst, Register src, int8_t imm8);\n+#endif\n@@ -2139,0 +2205,1 @@\n+  void tzcntl(Register dst, Address src);\n@@ -2140,0 +2207,1 @@\n+  void tzcntq(Register dst, Address src);\n@@ -2216,0 +2284,4 @@\n+  void sarxl(Register dst, Register src1, Register src2);\n+  void sarxl(Register dst, Address src1, Register src2);\n+  void sarxq(Register dst, Register src1, Register src2);\n+  void sarxq(Register dst, Address src1, Register src2);\n@@ -2217,0 +2289,1 @@\n+  void shlxl(Register dst, Address src1, Register src2);\n@@ -2218,0 +2291,1 @@\n+  void shlxq(Register dst, Address src1, Register src2);\n@@ -2219,0 +2293,1 @@\n+  void shrxl(Register dst, Address src1, Register src2);\n@@ -2220,0 +2295,1 @@\n+  void shrxq(Register dst, Address src1, Register src2);\n@@ -2222,2 +2298,10 @@\n-  void pext(Register dst, Register src1, Register src2);\n-  void pdep(Register dst, Register src1, Register src2);\n+\n+  void pextl(Register dst, Register src1, Register src2);\n+  void pdepl(Register dst, Register src1, Register src2);\n+  void pextq(Register dst, Register src1, Register src2);\n+  void pdepq(Register dst, Register src1, Register src2);\n+  void pextl(Register dst, Register src1, Address src2);\n+  void pdepl(Register dst, Register src1, Address src2);\n+  void pextq(Register dst, Register src1, Address src2);\n+  void pdepq(Register dst, Register src1, Address src2);\n+\n@@ -2488,1 +2572,1 @@\n-  void vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2492,1 +2576,1 @@\n-  void vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void evpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -2585,1 +2669,2 @@\n-  void vpandq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpandq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpandq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -2595,1 +2680,2 @@\n-  void vporq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evporq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evporq(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -2609,0 +2695,1 @@\n+  void vpternlogq(XMMRegister dst, int imm8, XMMRegister src2, Address     src3, int vector_len);\n@@ -2722,0 +2809,2 @@\n+  void vzeroall();\n+\n@@ -2766,0 +2855,4 @@\n+  \/\/ floating point class tests\n+  void vfpclassss(KRegister kdst, XMMRegister src, uint8_t imm8);\n+  void vfpclasssd(KRegister kdst, XMMRegister src, uint8_t imm8);\n+\n@@ -2783,0 +2876,1 @@\n+  void gf2p8affineqb(XMMRegister dst, XMMRegister src, int imm8);\n@@ -2820,1 +2914,1 @@\n-      _current_assembler(NULL) { }\n+      _current_assembler(nullptr) { }\n@@ -2823,1 +2917,1 @@\n-    if (_current_assembler != NULL) {\n+    if (_current_assembler != nullptr) {\n@@ -2826,1 +2920,1 @@\n-    _current_assembler = NULL;\n+    _current_assembler = nullptr;\n@@ -2892,1 +2986,1 @@\n-    _embedded_opmask_register_specifier = (*mask).encoding() & 0x7;\n+    _embedded_opmask_register_specifier = mask->encoding() & 0x7;\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":207,"deletions":113,"binary":false,"changes":320,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"opto\/output.hpp\"\n@@ -36,0 +39,116 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#define STOP(error) stop(error)\n+#else\n+#define BLOCK_COMMENT(str) block_comment(str)\n+#define STOP(error) block_comment(error); stop(error)\n+#endif\n+\n+\/\/ C2 compiled method's prolog code.\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+\n+  \/\/ WARNING: Initial instruction MUST be 5 bytes or longer so that\n+  \/\/ NativeJump::patch_verified_entry will be able to patch out the entry\n+  \/\/ code safely. The push to verify stack depth is ok at 5 bytes,\n+  \/\/ the frame allocation can be either 3 or 6 bytes. So if we don't do\n+  \/\/ stack bang then we must use the 6 byte frame allocation even if\n+  \/\/ we have no frame. :-(\n+  assert(stack_bang_size >= framesize || stack_bang_size <= 0, \"stack bang size incorrect\");\n+\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return addr\n+  framesize -= wordSize;\n+  stack_bang_size -= wordSize;\n+\n+  \/\/ Calls to C2R adapters often do not accept exceptional returns.\n+  \/\/ We require that their callers must bang for them.  But be careful, because\n+  \/\/ some VM calls (such as call site linkage) can use several kilobytes of\n+  \/\/ stack.  But the stack safety zone should account for that.\n+  \/\/ See bugs 4446381, 4468289, 4497237.\n+  if (stack_bang_size > 0) {\n+    generate_stack_overflow_check(stack_bang_size);\n+\n+    \/\/ We always push rbp, so that on return to interpreter rbp, will be\n+    \/\/ restored correctly and we can correct the stack.\n+    push(rbp);\n+    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n+    if (PreserveFramePointer) {\n+      mov(rbp, rsp);\n+    }\n+    \/\/ Remove word for ebp\n+    framesize -= wordSize;\n+\n+    \/\/ Create frame\n+    if (framesize) {\n+      subptr(rsp, framesize);\n+    }\n+  } else {\n+    \/\/ Create frame (force generation of a 4 byte immediate value)\n+    subptr_imm32(rsp, framesize);\n+\n+    \/\/ Save RBP register now.\n+    framesize -= wordSize;\n+    movptr(Address(rsp, framesize), rbp);\n+    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n+    if (PreserveFramePointer) {\n+      movptr(rbp, rsp);\n+      if (framesize > 0) {\n+        addptr(rbp, framesize);\n+      }\n+    }\n+  }\n+\n+  if (VerifyStackAtCalls) { \/\/ Majik cookie to verify stack depth\n+    framesize -= wordSize;\n+    movptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n+  }\n+\n+#ifndef _LP64\n+  \/\/ If method sets FPU control word do it now\n+  if (fp_mode_24b) {\n+    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n+  }\n+  if (UseSSE >= 2 && VerifyFPU) {\n+    verify_FPU(0, \"FPU stack must be clean on entry\");\n+  }\n+#endif\n+\n+#ifdef ASSERT\n+  if (VerifyStackAtCalls) {\n+    Label L;\n+    push(rax);\n+    mov(rax, rsp);\n+    andptr(rax, StackAlignmentInBytes-1);\n+    cmpptr(rax, StackAlignmentInBytes-wordSize);\n+    pop(rax);\n+    jcc(Assembler::equal, L);\n+    STOP(\"Stack is not properly aligned!\");\n+    bind(L);\n+  }\n+#endif\n+\n+  if (!is_stub) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+ #ifdef _LP64\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n+      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+        Compile::current()->output()->add_stub(stub);\n+        slow_path = &stub->entry();\n+        continuation = &stub->continuation();\n+      }\n+      bs->nmethod_entry_barrier(this, slow_path, continuation);\n+    }\n+#else\n+    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+#endif\n+  }\n+}\n+\n@@ -92,1 +211,1 @@\n-    movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);\n+    movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()));\n@@ -112,1 +231,1 @@\n-  if (method_data != NULL) {\n+  if (method_data != nullptr) {\n@@ -126,1 +245,1 @@\n-  if (method_data != NULL) {\n+  if (method_data != nullptr) {\n@@ -144,1 +263,1 @@\n-  assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+  assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -154,1 +273,1 @@\n-    assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -240,1 +359,1 @@\n-    assert(stack_rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(stack_rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -286,2 +405,1 @@\n-  \/\/ Without cast to int32_t this style of movptr will destroy r10 which is typically obj.\n-  movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));\n+  movptr(Address(boxReg, 0), checked_cast<int32_t>(markWord::unused_mark().value()));\n@@ -301,1 +419,1 @@\n-    assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -444,0 +562,1 @@\n+    assert(cx1Reg == noreg, \"\");\n@@ -463,1 +582,1 @@\n-  Label IsInflated, DONE_LABEL;\n+  Label IsInflated, DONE_LABEL, NO_COUNT, COUNT;\n@@ -466,1 +585,1 @@\n-    load_klass(tmpReg, objReg, cx1Reg);\n+    load_klass(tmpReg, objReg, scrReg);\n@@ -491,1 +610,1 @@\n-    jcc(Assembler::equal, DONE_LABEL);           \/\/ Success\n+    jcc(Assembler::equal, COUNT);           \/\/ Success\n@@ -498,1 +617,1 @@\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n+    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n@@ -545,1 +664,1 @@\n-  \/\/ If we weren't able to swing _owner from NULL to the BasicLock\n+  \/\/ If we weren't able to swing _owner from null to the BasicLock\n@@ -547,1 +666,1 @@\n-  jccb  (Assembler::notZero, DONE_LABEL);\n+  jccb  (Assembler::notZero, NO_COUNT);\n@@ -568,1 +687,1 @@\n-  movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));\n+  movptr(Address(boxReg, 0), checked_cast<int32_t>(markWord::unused_mark().value()));\n@@ -570,1 +689,1 @@\n-  jcc(Assembler::equal, DONE_LABEL);           \/\/ CAS above succeeded; propagate ZF = 1 (success)\n+  jccb(Assembler::equal, COUNT);          \/\/ CAS above succeeded; propagate ZF = 1 (success)\n@@ -572,2 +691,2 @@\n-  cmpptr(r15_thread, rax);                     \/\/ Check if we are already the owner (recursive lock)\n-  jcc(Assembler::notEqual, DONE_LABEL);        \/\/ If not recursive, ZF = 0 at this point (fail)\n+  cmpptr(r15_thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n+  jccb(Assembler::notEqual, NO_COUNT);    \/\/ If not recursive, ZF = 0 at this point (fail)\n@@ -580,5 +699,0 @@\n-  \/\/ DONE_LABEL is a hot target - we'd really like to place it at the\n-  \/\/ start of cache line by padding with NOPs.\n-  \/\/ See the AMD and Intel software optimization manuals for the\n-  \/\/ most efficient \"long\" NOP encodings.\n-  \/\/ Unfortunately none of our alignment mechanisms suffice.\n@@ -587,1 +701,18 @@\n-  \/\/ At DONE_LABEL the icc ZFlag is set as follows ...\n+  \/\/ ZFlag == 1 count in fast path\n+  \/\/ ZFlag == 0 count in slow path\n+  jccb(Assembler::notZero, NO_COUNT); \/\/ jump if ZFlag == 0\n+\n+  bind(COUNT);\n+  \/\/ Count monitors in fast path\n+#ifndef _LP64\n+  get_thread(tmpReg);\n+  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n+#else \/\/ _LP64\n+  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+\n+  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n+\n+  bind(NO_COUNT);\n+\n+  \/\/ At NO_COUNT the icc ZFlag is set as follows ...\n@@ -629,1 +760,1 @@\n-  Label DONE_LABEL, Stacked, CheckSucc;\n+  Label DONE_LABEL, Stacked, COUNT, NO_COUNT;\n@@ -646,2 +777,2 @@\n-    cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   \/\/ Examine the displaced header\n-    jcc   (Assembler::zero, DONE_LABEL);                              \/\/ 0 indicates recursive stack-lock\n+    cmpptr(Address(boxReg, 0), NULL_WORD);                            \/\/ Examine the displaced header\n+    jcc   (Assembler::zero, COUNT);                                   \/\/ 0 indicates recursive stack-lock\n@@ -649,1 +780,1 @@\n-  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Examine the object's markword\n+  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));   \/\/ Examine the object's markword\n@@ -652,1 +783,1 @@\n-    jccb  (Assembler::zero, Stacked);\n+    jccb   (Assembler::zero, Stacked);\n@@ -688,2 +819,0 @@\n-  get_thread (boxReg);\n-\n@@ -699,1 +828,1 @@\n-  jccb  (Assembler::notZero, CheckSucc);\n+  jccb  (Assembler::notZero, DONE_LABEL);\n@@ -702,18 +831,0 @@\n-\n-  bind (Stacked);\n-  \/\/ It's not inflated and it's not recursively stack-locked.\n-  \/\/ It must be stack-locked.\n-  \/\/ Try to reset the header to displaced header.\n-  \/\/ The \"box\" value on the stack is stable, so we can reload\n-  \/\/ and be assured we observe the same value as above.\n-  movptr(tmpReg, Address(boxReg, 0));\n-  lock();\n-  cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-  \/\/ Intention fall-thru into DONE_LABEL\n-\n-  \/\/ DONE_LABEL is a hot target - we'd really like to place it at the\n-  \/\/ start of cache line by padding with NOPs.\n-  \/\/ See the AMD and Intel software optimization manuals for the\n-  \/\/ most efficient \"long\" NOP encodings.\n-  \/\/ Unfortunately none of our alignment mechanisms suffice.\n-  bind (CheckSucc);\n@@ -722,1 +833,1 @@\n-  Label LNotRecursive, LSuccess, LGoSlowPath;\n+  Label CheckSucc, LNotRecursive, LSuccess, LGoSlowPath;\n@@ -736,1 +847,1 @@\n-  movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);\n+  movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n@@ -746,1 +857,1 @@\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);\n+  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n@@ -751,1 +862,1 @@\n-  movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);\n+  movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n@@ -762,1 +873,1 @@\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);\n+  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n@@ -795,0 +906,1 @@\n+#endif\n@@ -800,0 +912,1 @@\n+    \/\/ Intentional fall-thru into DONE_LABEL\n@@ -801,1 +914,17 @@\n-#endif\n+\n+  \/\/ ZFlag == 1 count in fast path\n+  \/\/ ZFlag == 0 count in slow path\n+  jccb(Assembler::notZero, NO_COUNT);\n+\n+  bind(COUNT);\n+  \/\/ Count monitors in fast path\n+#ifndef _LP64\n+  get_thread(tmpReg);\n+  decrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n+#else \/\/ _LP64\n+  decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+\n+  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n+\n+  bind(NO_COUNT);\n@@ -808,1 +937,1 @@\n-void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {\n+void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src) {\n@@ -813,1 +942,1 @@\n-    andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);\n+    andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), noreg);\n@@ -816,1 +945,1 @@\n-    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), noreg);\n@@ -820,1 +949,1 @@\n-void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {\n+void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len) {\n@@ -822,1 +951,1 @@\n-    vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);\n+    vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, noreg);\n@@ -825,1 +954,1 @@\n-    vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);\n+    vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, noreg);\n@@ -829,1 +958,1 @@\n-void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {\n+void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src) {\n@@ -834,1 +963,1 @@\n-    andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);\n+    andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), noreg);\n@@ -837,1 +966,1 @@\n-    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), noreg);\n@@ -841,1 +970,1 @@\n-void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {\n+void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len) {\n@@ -843,1 +972,1 @@\n-    vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);\n+    vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, noreg);\n@@ -846,1 +975,1 @@\n-    vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);\n+    vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, noreg);\n@@ -1019,3 +1148,1 @@\n-void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst,\n-                                  XMMRegister zero, XMMRegister one,\n-                                  Register scratch) {\n+void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst, XMMRegister zero, XMMRegister one) {\n@@ -1033,1 +1160,1 @@\n-    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scratch);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), noreg);\n@@ -1041,1 +1168,1 @@\n-    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scratch);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), noreg);\n@@ -1300,1 +1427,1 @@\n-void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp) {\n@@ -1309,1 +1436,1 @@\n-  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, scratch);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, noreg);\n@@ -1315,1 +1442,1 @@\n-void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp) {\n@@ -1324,1 +1451,1 @@\n-  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, scratch);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, noreg);\n@@ -1469,2 +1596,1 @@\n-void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp,\n-                                         Register tmp, bool novlbwdq, int vlen_enc) {\n+void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, bool novlbwdq, int vlen_enc) {\n@@ -1474,1 +1600,1 @@\n-            Assembler::eq, true, vlen_enc, tmp);\n+            Assembler::eq, true, vlen_enc, noreg);\n@@ -1484,6 +1610,6 @@\n-  case 4:  movdl(dst, src);   break;\n-  case 8:  movq(dst, src);    break;\n-  case 16: movdqu(dst, src);  break;\n-  case 32: vmovdqu(dst, src); break;\n-  case 64: evmovdquq(dst, src, Assembler::AVX_512bit); break;\n-  default: ShouldNotReachHere();\n+    case 4:  movdl(dst, src);   break;\n+    case 8:  movq(dst, src);    break;\n+    case 16: movdqu(dst, src);  break;\n+    case 32: vmovdqu(dst, src); break;\n+    case 64: evmovdqul(dst, src, Assembler::AVX_512bit); break;\n+    default: ShouldNotReachHere();\n@@ -1494,0 +1620,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1502,10 +1630,24 @@\n-void C2_MacroAssembler::load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes) {\n-  ExternalAddress addr(StubRoutines::x86::vector_iota_indices());\n-  if (vlen_in_bytes == 4) {\n-    movdl(dst, addr);\n-  } else if (vlen_in_bytes == 8) {\n-    movq(dst, addr);\n-  } else if (vlen_in_bytes == 16) {\n-    movdqu(dst, addr, scratch);\n-  } else if (vlen_in_bytes == 32) {\n-    vmovdqu(dst, addr, scratch);\n+void C2_MacroAssembler::load_constant_vector(BasicType bt, XMMRegister dst, InternalAddress src, int vlen) {\n+  int vlen_enc = vector_length_encoding(vlen);\n+  if (VM_Version::supports_avx()) {\n+    if (bt == T_LONG) {\n+      if (VM_Version::supports_avx2()) {\n+        vpbroadcastq(dst, src, vlen_enc);\n+      } else {\n+        vmovddup(dst, src, vlen_enc);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      if (vlen_enc != Assembler::AVX_128bit) {\n+        vbroadcastsd(dst, src, vlen_enc, noreg);\n+      } else {\n+        vmovddup(dst, src, vlen_enc);\n+      }\n+    } else {\n+      if (VM_Version::supports_avx2() && is_integral_type(bt)) {\n+        vpbroadcastd(dst, src, vlen_enc);\n+      } else {\n+        vbroadcastss(dst, src, vlen_enc);\n+      }\n+    }\n+  } else if (VM_Version::supports_sse3()) {\n+    movddup(dst, src);\n@@ -1513,2 +1655,12 @@\n-    assert(vlen_in_bytes == 64, \"%d\", vlen_in_bytes);\n-    evmovdqub(dst, k0, addr, false \/*merge*\/, Assembler::AVX_512bit, scratch);\n+    movq(dst, src);\n+    if (vlen == 16) {\n+      punpcklqdq(dst, dst);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::load_iota_indices(XMMRegister dst, int vlen_in_bytes, BasicType bt) {\n+  \/\/ The iota indices are ordered by type B\/S\/I\/L\/F\/D, and the offset between two types is 64.\n+  int offset = exact_log2(type2aelembytes(bt)) << 6;\n+  if (is_floating_point_type(bt)) {\n+    offset += 128;\n@@ -1516,0 +1668,2 @@\n+  ExternalAddress addr(StubRoutines::x86::vector_iota_indices() + offset);\n+  load_vector(dst, addr, vlen_in_bytes);\n@@ -1568,1 +1722,1 @@\n-                            vpmullq(dst, dst, src, vector_len); break;\n+                            evpmullq(dst, dst, src, vector_len); break;\n@@ -1616,1 +1770,1 @@\n-    case Op_MulReductionVL: vpmullq(dst, src1, src2, vector_len); break;\n+    case Op_MulReductionVL: evpmullq(dst, src1, src2, vector_len); break;\n@@ -2006,2 +2160,2 @@\n-void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len) {\n-  MacroAssembler::evmovdqu(type, kmask, dst, src, vector_len);\n+void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len) {\n+  MacroAssembler::evmovdqu(type, kmask, dst, src, merge, vector_len);\n@@ -2010,2 +2164,2 @@\n-void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len) {\n-  MacroAssembler::evmovdqu(type, kmask, dst, src, vector_len);\n+void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len) {\n+  MacroAssembler::evmovdqu(type, kmask, dst, src, merge, vector_len);\n@@ -2014,0 +2168,33 @@\n+void C2_MacroAssembler::vmovmask(BasicType elem_bt, XMMRegister dst, Address src, XMMRegister mask,\n+                                 int vec_enc) {\n+  switch(elem_bt) {\n+    case T_INT:\n+    case T_FLOAT:\n+      vmaskmovps(dst, src, mask, vec_enc);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      vmaskmovpd(dst, src, mask, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(elem_bt));\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vmovmask(BasicType elem_bt, Address dst, XMMRegister src, XMMRegister mask,\n+                                 int vec_enc) {\n+  switch(elem_bt) {\n+    case T_INT:\n+    case T_FLOAT:\n+      vmaskmovps(dst, src, mask, vec_enc);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      vmaskmovpd(dst, src, mask, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(elem_bt));\n+      break;\n+  }\n+}\n@@ -2019,1 +2206,1 @@\n-  int permconst[] = {1, 14};\n+  const int permconst[] = {1, 14};\n@@ -2112,0 +2299,8 @@\n+void C2_MacroAssembler::movsxl(BasicType typ, Register dst) {\n+  if (typ == T_BYTE) {\n+    movsbl(dst, dst);\n+  } else if (typ == T_SHORT) {\n+    movswl(dst, dst);\n+  }\n+}\n+\n@@ -2123,4 +2318,1 @@\n-      if (typ == T_BYTE)\n-        movsbl(dst, dst);\n-      else if (typ == T_SHORT)\n-        movswl(dst, dst);\n+      movsxl(typ, dst);\n@@ -2130,0 +2322,1 @@\n+    movsxl(typ, dst);\n@@ -2133,1 +2326,1 @@\n-void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp, XMMRegister vtmp) {\n+void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, XMMRegister vtmp) {\n@@ -2145,1 +2338,1 @@\n-        pshufps(dst, dst, eindex);\n+        shufps(dst, dst, eindex);\n@@ -2147,1 +2340,1 @@\n-        vpshufps(dst, src, src, eindex, Assembler::AVX_128bit);\n+        vshufps(dst, src, src, eindex, Assembler::AVX_128bit);\n@@ -2162,2 +2355,2 @@\n-      assert((vtmp != xnoreg) && (tmp != noreg), \"required.\");\n-      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), tmp);\n+      assert(vtmp != xnoreg, \"required.\");\n+      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), noreg);\n@@ -2166,2 +2359,1 @@\n-      assert((tmp != noreg), \"required.\");\n-      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, tmp);\n+      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, noreg);\n@@ -2196,1 +2388,3 @@\n-void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch) {\n+void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral src2, int comparison, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n+\n@@ -2200,1 +2394,1 @@\n-      evpcmpb(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpb(kdmask, ksmask, src1, src2, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2204,1 +2398,1 @@\n-      evpcmpw(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpw(kdmask, ksmask, src1, src2, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2208,1 +2402,1 @@\n-      evpcmpd(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpd(kdmask, ksmask, src1, src2, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2212,1 +2406,1 @@\n-      evpcmpq(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpq(kdmask, ksmask, src1, src2, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2242,48 +2436,101 @@\n-void C2_MacroAssembler::vectortest(int bt, int vlen, XMMRegister src1, XMMRegister src2,\n-                                   XMMRegister vtmp1, XMMRegister vtmp2, KRegister mask) {\n-  switch(vlen) {\n-    case 4:\n-      assert(vtmp1 != xnoreg, \"required.\");\n-      \/\/ Broadcast lower 32 bits to 128 bits before ptest\n-      pshufd(vtmp1, src1, 0x0);\n-      if (bt == BoolTest::overflow) {\n-        assert(vtmp2 != xnoreg, \"required.\");\n-        pshufd(vtmp2, src2, 0x0);\n-      } else {\n-        assert(vtmp2 == xnoreg, \"required.\");\n-        vtmp2 = src2;\n-      }\n-      ptest(vtmp1, vtmp2);\n-     break;\n-    case 8:\n-      assert(vtmp1 != xnoreg, \"required.\");\n-      \/\/ Broadcast lower 64 bits to 128 bits before ptest\n-      pshufd(vtmp1, src1, 0x4);\n-      if (bt == BoolTest::overflow) {\n-        assert(vtmp2 != xnoreg, \"required.\");\n-        pshufd(vtmp2, src2, 0x4);\n-      } else {\n-        assert(vtmp2 == xnoreg, \"required.\");\n-        vtmp2 = src2;\n-      }\n-      ptest(vtmp1, vtmp2);\n-     break;\n-    case 16:\n-      assert((vtmp1 == xnoreg) && (vtmp2 == xnoreg), \"required.\");\n-      ptest(src1, src2);\n-      break;\n-    case 32:\n-      assert((vtmp1 == xnoreg) && (vtmp2 == xnoreg), \"required.\");\n-      vptest(src1, src2, Assembler::AVX_256bit);\n-      break;\n-    case 64:\n-      {\n-        assert((vtmp1 == xnoreg) && (vtmp2 == xnoreg), \"required.\");\n-        evpcmpeqb(mask, src1, src2, Assembler::AVX_512bit);\n-        if (bt == BoolTest::ne) {\n-          ktestql(mask, mask);\n-        } else {\n-          assert(bt == BoolTest::overflow, \"required\");\n-          kortestql(mask, mask);\n-        }\n-      }\n+void C2_MacroAssembler::vectortest(BasicType bt, XMMRegister src1, XMMRegister src2, XMMRegister vtmp, int vlen_in_bytes) {\n+  assert(vlen_in_bytes <= 32, \"\");\n+  int esize = type2aelembytes(bt);\n+  if (vlen_in_bytes == 32) {\n+    assert(vtmp == xnoreg, \"required.\");\n+    if (esize >= 4) {\n+      vtestps(src1, src2, AVX_256bit);\n+    } else {\n+      vptest(src1, src2, AVX_256bit);\n+    }\n+    return;\n+  }\n+  if (vlen_in_bytes < 16) {\n+    \/\/ Duplicate the lower part to fill the whole register,\n+    \/\/ Don't need to do so for src2\n+    assert(vtmp != xnoreg, \"required\");\n+    int shuffle_imm = (vlen_in_bytes == 4) ? 0x00 : 0x04;\n+    pshufd(vtmp, src1, shuffle_imm);\n+  } else {\n+    assert(vtmp == xnoreg, \"required\");\n+    vtmp = src1;\n+  }\n+  if (esize >= 4 && VM_Version::supports_avx()) {\n+    vtestps(vtmp, src2, AVX_128bit);\n+  } else {\n+    ptest(vtmp, src2);\n+  }\n+}\n+\n+void C2_MacroAssembler::vpadd(BasicType elem_bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vlen_enc) {\n+  assert(UseAVX >= 2, \"required\");\n+#ifdef ASSERT\n+  bool is_bw = ((elem_bt == T_BYTE) || (elem_bt == T_SHORT));\n+  bool is_bw_supported = VM_Version::supports_avx512bw();\n+  if (is_bw && !is_bw_supported) {\n+    assert(vlen_enc != Assembler::AVX_512bit, \"required\");\n+    assert((dst->encoding() < 16) && (src1->encoding() < 16) && (src2->encoding() < 16),\n+           \"XMM register should be 0-15\");\n+  }\n+#endif \/\/ ASSERT\n+  switch (elem_bt) {\n+    case T_BYTE: vpaddb(dst, src1, src2, vlen_enc); return;\n+    case T_SHORT: vpaddw(dst, src1, src2, vlen_enc); return;\n+    case T_INT: vpaddd(dst, src1, src2, vlen_enc); return;\n+    case T_FLOAT: vaddps(dst, src1, src2, vlen_enc); return;\n+    case T_LONG: vpaddq(dst, src1, src2, vlen_enc); return;\n+    case T_DOUBLE: vaddpd(dst, src1, src2, vlen_enc); return;\n+    default: fatal(\"Unsupported type %s\", type2name(elem_bt)); return;\n+  }\n+}\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::vpbroadcast(BasicType elem_bt, XMMRegister dst, Register src, int vlen_enc) {\n+  assert(UseAVX >= 2, \"required\");\n+  bool is_bw = ((elem_bt == T_BYTE) || (elem_bt == T_SHORT));\n+  bool is_vl = vlen_enc != Assembler::AVX_512bit;\n+  if ((UseAVX > 2) &&\n+      (!is_bw || VM_Version::supports_avx512bw()) &&\n+      (!is_vl || VM_Version::supports_avx512vl())) {\n+    switch (elem_bt) {\n+      case T_BYTE: evpbroadcastb(dst, src, vlen_enc); return;\n+      case T_SHORT: evpbroadcastw(dst, src, vlen_enc); return;\n+      case T_FLOAT: case T_INT: evpbroadcastd(dst, src, vlen_enc); return;\n+      case T_DOUBLE: case T_LONG: evpbroadcastq(dst, src, vlen_enc); return;\n+      default: fatal(\"Unsupported type %s\", type2name(elem_bt)); return;\n+    }\n+  } else {\n+    assert(vlen_enc != Assembler::AVX_512bit, \"required\");\n+    assert((dst->encoding() < 16),\"XMM register should be 0-15\");\n+    switch (elem_bt) {\n+      case T_BYTE: movdl(dst, src); vpbroadcastb(dst, dst, vlen_enc); return;\n+      case T_SHORT: movdl(dst, src); vpbroadcastw(dst, dst, vlen_enc); return;\n+      case T_INT: movdl(dst, src); vpbroadcastd(dst, dst, vlen_enc); return;\n+      case T_FLOAT: movdl(dst, src); vbroadcastss(dst, dst, vlen_enc); return;\n+      case T_LONG: movdq(dst, src); vpbroadcastq(dst, dst, vlen_enc); return;\n+      case T_DOUBLE: movdq(dst, src); vbroadcastsd(dst, dst, vlen_enc); return;\n+      default: fatal(\"Unsupported type %s\", type2name(elem_bt)); return;\n+    }\n+  }\n+}\n+#endif\n+\n+void C2_MacroAssembler::vconvert_b2x(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, int vlen_enc) {\n+  switch (to_elem_bt) {\n+    case T_SHORT:\n+      vpmovsxbw(dst, src, vlen_enc);\n+      break;\n+    case T_INT:\n+      vpmovsxbd(dst, src, vlen_enc);\n+      break;\n+    case T_FLOAT:\n+      vpmovsxbd(dst, src, vlen_enc);\n+      vcvtdq2ps(dst, dst, vlen_enc);\n+      break;\n+    case T_LONG:\n+      vpmovsxbq(dst, src, vlen_enc);\n+      break;\n+    case T_DOUBLE: {\n+      int mid_vlen_enc = (vlen_enc == Assembler::AVX_512bit) ? Assembler::AVX_256bit : Assembler::AVX_128bit;\n+      vpmovsxbd(dst, src, mid_vlen_enc);\n+      vcvtdq2pd(dst, dst, vlen_enc);\n@@ -2291,0 +2538,1 @@\n+    }\n@@ -2292,1 +2540,1 @@\n-      assert(false,\"Should not reach here.\");\n+      fatal(\"Unsupported type %s\", type2name(to_elem_bt));\n@@ -2566,2 +2814,2 @@\n-      andl(result, (os::vm_page_size()-1));\n-      cmpl(result, (os::vm_page_size()-16));\n+      andl(result, ((int)os::vm_page_size()-1));\n+      cmpl(result, ((int)os::vm_page_size()-16));\n@@ -2596,2 +2844,2 @@\n-    andl(result, (os::vm_page_size()-1));\n-    cmpl(result, (os::vm_page_size()-16));\n+    andl(result, ((int)os::vm_page_size()-1));\n+    cmpl(result, ((int)os::vm_page_size()-16));\n@@ -2985,0 +3233,189 @@\n+int C2_MacroAssembler::arrays_hashcode_elsize(BasicType eltype) {\n+  switch (eltype) {\n+  case T_BOOLEAN: return sizeof(jboolean);\n+  case T_BYTE:  return sizeof(jbyte);\n+  case T_SHORT: return sizeof(jshort);\n+  case T_CHAR:  return sizeof(jchar);\n+  case T_INT:   return sizeof(jint);\n+  default:\n+    ShouldNotReachHere();\n+    return -1;\n+  }\n+}\n+\n+void C2_MacroAssembler::arrays_hashcode_elload(Register dst, Address src, BasicType eltype) {\n+  switch (eltype) {\n+  \/\/ T_BOOLEAN used as surrogate for unsigned byte\n+  case T_BOOLEAN: movzbl(dst, src);   break;\n+  case T_BYTE:    movsbl(dst, src);   break;\n+  case T_SHORT:   movswl(dst, src);   break;\n+  case T_CHAR:    movzwl(dst, src);   break;\n+  case T_INT:     movl(dst, src);     break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::arrays_hashcode_elvload(XMMRegister dst, Address src, BasicType eltype) {\n+  load_vector(dst, src, arrays_hashcode_elsize(eltype) * 8);\n+}\n+\n+void C2_MacroAssembler::arrays_hashcode_elvload(XMMRegister dst, AddressLiteral src, BasicType eltype) {\n+  load_vector(dst, src, arrays_hashcode_elsize(eltype) * 8);\n+}\n+\n+void C2_MacroAssembler::arrays_hashcode_elvcast(XMMRegister dst, BasicType eltype) {\n+  const int vlen = Assembler::AVX_256bit;\n+  switch (eltype) {\n+  case T_BOOLEAN: vector_unsigned_cast(dst, dst, vlen, T_BYTE, T_INT);  break;\n+  case T_BYTE:      vector_signed_cast(dst, dst, vlen, T_BYTE, T_INT);  break;\n+  case T_SHORT:     vector_signed_cast(dst, dst, vlen, T_SHORT, T_INT); break;\n+  case T_CHAR:    vector_unsigned_cast(dst, dst, vlen, T_SHORT, T_INT); break;\n+  case T_INT:\n+    \/\/ do nothing\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::arrays_hashcode(Register ary1, Register cnt1, Register result,\n+                                        Register index, Register tmp2, Register tmp3, XMMRegister vnext,\n+                                        XMMRegister vcoef0, XMMRegister vcoef1, XMMRegister vcoef2, XMMRegister vcoef3,\n+                                        XMMRegister vresult0, XMMRegister vresult1, XMMRegister vresult2, XMMRegister vresult3,\n+                                        XMMRegister vtmp0, XMMRegister vtmp1, XMMRegister vtmp2, XMMRegister vtmp3,\n+                                        BasicType eltype) {\n+  ShortBranchVerifier sbv(this);\n+  assert(UseAVX >= 2, \"AVX2 intrinsics are required\");\n+  assert_different_registers(ary1, cnt1, result, index, tmp2, tmp3);\n+  assert_different_registers(vnext, vcoef0, vcoef1, vcoef2, vcoef3, vresult0, vresult1, vresult2, vresult3, vtmp0, vtmp1, vtmp2, vtmp3);\n+\n+  Label SHORT_UNROLLED_BEGIN, SHORT_UNROLLED_LOOP_BEGIN,\n+        SHORT_UNROLLED_LOOP_EXIT,\n+        UNROLLED_SCALAR_LOOP_BEGIN, UNROLLED_SCALAR_SKIP, UNROLLED_SCALAR_RESUME,\n+        UNROLLED_VECTOR_LOOP_BEGIN,\n+        END;\n+  switch (eltype) {\n+  case T_BOOLEAN: BLOCK_COMMENT(\"arrays_hashcode(unsigned byte) {\"); break;\n+  case T_CHAR:    BLOCK_COMMENT(\"arrays_hashcode(char) {\");          break;\n+  case T_BYTE:    BLOCK_COMMENT(\"arrays_hashcode(byte) {\");          break;\n+  case T_SHORT:   BLOCK_COMMENT(\"arrays_hashcode(short) {\");         break;\n+  case T_INT:     BLOCK_COMMENT(\"arrays_hashcode(int) {\");           break;\n+  default:        BLOCK_COMMENT(\"arrays_hashcode {\");                break;\n+  }\n+\n+  \/\/ For \"renaming\" for readibility of the code\n+  const XMMRegister vcoef[] = { vcoef0, vcoef1, vcoef2, vcoef3 },\n+                    vresult[] = { vresult0, vresult1, vresult2, vresult3 },\n+                    vtmp[] = { vtmp0, vtmp1, vtmp2, vtmp3 };\n+\n+  const int elsize = arrays_hashcode_elsize(eltype);\n+\n+  \/*\n+    if (cnt1 >= 2) {\n+      if (cnt1 >= 32) {\n+        UNROLLED VECTOR LOOP\n+      }\n+      UNROLLED SCALAR LOOP\n+    }\n+    SINGLE SCALAR\n+   *\/\n+\n+  cmpl(cnt1, 32);\n+  jcc(Assembler::less, SHORT_UNROLLED_BEGIN);\n+\n+  \/\/ cnt1 >= 32 && generate_vectorized_loop\n+  xorl(index, index);\n+\n+  \/\/ vresult = IntVector.zero(I256);\n+  for (int idx = 0; idx < 4; idx++) {\n+    vpxor(vresult[idx], vresult[idx]);\n+  }\n+  \/\/ vnext = IntVector.broadcast(I256, power_of_31_backwards[0]);\n+  Register bound = tmp2;\n+  Register next = tmp3;\n+  lea(tmp2, ExternalAddress(StubRoutines::x86::arrays_hashcode_powers_of_31() + (0 * sizeof(jint))));\n+  movl(next, Address(tmp2, 0));\n+  movdl(vnext, next);\n+  vpbroadcastd(vnext, vnext, Assembler::AVX_256bit);\n+\n+  \/\/ index = 0;\n+  \/\/ bound = cnt1 & ~(32 - 1);\n+  movl(bound, cnt1);\n+  andl(bound, ~(32 - 1));\n+  \/\/ for (; index < bound; index += 32) {\n+  bind(UNROLLED_VECTOR_LOOP_BEGIN);\n+  \/\/ result *= next;\n+  imull(result, next);\n+  \/\/ loop fission to upfront the cost of fetching from memory, OOO execution\n+  \/\/ can then hopefully do a better job of prefetching\n+  for (int idx = 0; idx < 4; idx++) {\n+    arrays_hashcode_elvload(vtmp[idx], Address(ary1, index, Address::times(elsize), 8 * idx * elsize), eltype);\n+  }\n+  \/\/ vresult = vresult * vnext + ary1[index+8*idx:index+8*idx+7];\n+  for (int idx = 0; idx < 4; idx++) {\n+    vpmulld(vresult[idx], vresult[idx], vnext, Assembler::AVX_256bit);\n+    arrays_hashcode_elvcast(vtmp[idx], eltype);\n+    vpaddd(vresult[idx], vresult[idx], vtmp[idx], Assembler::AVX_256bit);\n+  }\n+  \/\/ index += 32;\n+  addl(index, 32);\n+  \/\/ index < bound;\n+  cmpl(index, bound);\n+  jcc(Assembler::less, UNROLLED_VECTOR_LOOP_BEGIN);\n+  \/\/ }\n+\n+  lea(ary1, Address(ary1, bound, Address::times(elsize)));\n+  subl(cnt1, bound);\n+  \/\/ release bound\n+\n+  \/\/ vresult *= IntVector.fromArray(I256, power_of_31_backwards, 1);\n+  for (int idx = 0; idx < 4; idx++) {\n+    lea(tmp2, ExternalAddress(StubRoutines::x86::arrays_hashcode_powers_of_31() + ((8 * idx + 1) * sizeof(jint))));\n+    arrays_hashcode_elvload(vcoef[idx], Address(tmp2, 0), T_INT);\n+    vpmulld(vresult[idx], vresult[idx], vcoef[idx], Assembler::AVX_256bit);\n+  }\n+  \/\/ result += vresult.reduceLanes(ADD);\n+  for (int idx = 0; idx < 4; idx++) {\n+    reduceI(Op_AddReductionVI, 256\/(sizeof(jint) * 8), result, result, vresult[idx], vtmp[(idx * 2 + 0) % 4], vtmp[(idx * 2 + 1) % 4]);\n+  }\n+\n+  \/\/ } else if (cnt1 < 32) {\n+\n+  bind(SHORT_UNROLLED_BEGIN);\n+  \/\/ int i = 1;\n+  movl(index, 1);\n+  cmpl(index, cnt1);\n+  jcc(Assembler::greaterEqual, SHORT_UNROLLED_LOOP_EXIT);\n+\n+  \/\/ for (; i < cnt1 ; i += 2) {\n+  bind(SHORT_UNROLLED_LOOP_BEGIN);\n+  movl(tmp3, 961);\n+  imull(result, tmp3);\n+  arrays_hashcode_elload(tmp2, Address(ary1, index, Address::times(elsize), -elsize), eltype);\n+  movl(tmp3, tmp2);\n+  shll(tmp3, 5);\n+  subl(tmp3, tmp2);\n+  addl(result, tmp3);\n+  arrays_hashcode_elload(tmp3, Address(ary1, index, Address::times(elsize)), eltype);\n+  addl(result, tmp3);\n+  addl(index, 2);\n+  cmpl(index, cnt1);\n+  jccb(Assembler::less, SHORT_UNROLLED_LOOP_BEGIN);\n+\n+  \/\/ }\n+  \/\/ if (i >= cnt1) {\n+  bind(SHORT_UNROLLED_LOOP_EXIT);\n+  jccb(Assembler::greater, END);\n+  movl(tmp2, result);\n+  shll(result, 5);\n+  subl(result, tmp2);\n+  arrays_hashcode_elload(tmp3, Address(ary1, index, Address::times(elsize), -elsize), eltype);\n+  addl(result, tmp3);\n+  \/\/ }\n+  bind(END);\n+\n+  BLOCK_COMMENT(\"} \/\/ arrays_hashcode\");\n+\n+} \/\/ arrays_hashcode\n+\n@@ -4076,4 +4513,5 @@\n-void C2_MacroAssembler::vector_cast_float_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                                                            XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n-                                                            Register scratch, AddressLiteral float_sign_flip,\n-                                                            int vec_enc) {\n+void C2_MacroAssembler::vector_cast_float_to_int_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                                   XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                                                   Register rscratch, AddressLiteral float_sign_flip,\n+                                                                   int vec_enc) {\n+  assert(rscratch != noreg || always_reachable(float_sign_flip), \"missing\");\n@@ -4081,1 +4519,1 @@\n-  vmovdqu(xtmp1, float_sign_flip, scratch, vec_enc);\n+  vmovdqu(xtmp1, float_sign_flip, vec_enc, rscratch);\n@@ -4105,4 +4543,5 @@\n-void C2_MacroAssembler::vector_cast_float_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                                                             XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n-                                                             Register scratch, AddressLiteral float_sign_flip,\n-                                                             int vec_enc) {\n+void C2_MacroAssembler::vector_cast_float_to_int_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                                    XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                                    Register rscratch, AddressLiteral float_sign_flip,\n+                                                                    int vec_enc) {\n+  assert(rscratch != noreg || always_reachable(float_sign_flip), \"missing\");\n@@ -4110,1 +4549,1 @@\n-  evmovdqul(xtmp1, k0, float_sign_flip, false, vec_enc, scratch);\n+  evmovdqul(xtmp1, k0, float_sign_flip, false, vec_enc, rscratch);\n@@ -4126,0 +4565,45 @@\n+void C2_MacroAssembler::vector_cast_float_to_long_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                                     XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                                     Register rscratch, AddressLiteral double_sign_flip,\n+                                                                     int vec_enc) {\n+  assert(rscratch != noreg || always_reachable(double_sign_flip), \"missing\");\n+\n+  Label done;\n+  evmovdquq(xtmp1, k0, double_sign_flip, false, vec_enc, rscratch);\n+  Assembler::evpcmpeqq(ktmp1, k0, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmpps(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmpps(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::vector_cast_double_to_int_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                                     XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                                     Register rscratch, AddressLiteral float_sign_flip,\n+                                                                     int vec_enc) {\n+  assert(rscratch != noreg || always_reachable(float_sign_flip), \"missing\");\n+  Label done;\n+  evmovdquq(xtmp1, k0, float_sign_flip, false, vec_enc, rscratch);\n+  Assembler::evpcmpeqd(ktmp1, k0, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmppd(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdqul(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdqul(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n@@ -4134,4 +4618,6 @@\n-void C2_MacroAssembler::vector_cast_double_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                                                              XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n-                                                              Register scratch, AddressLiteral double_sign_flip,\n-                                                              int vec_enc) {\n+void C2_MacroAssembler::vector_cast_double_to_long_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                                      XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                                      Register rscratch, AddressLiteral double_sign_flip,\n+                                                                      int vec_enc) {\n+  assert(rscratch != noreg || always_reachable(double_sign_flip), \"missing\");\n+\n@@ -4139,1 +4625,1 @@\n-  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n+  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, rscratch);\n@@ -4155,0 +4641,76 @@\n+void C2_MacroAssembler::vector_crosslane_doubleword_pack_avx(XMMRegister dst, XMMRegister src, XMMRegister zero,\n+                                                             XMMRegister xtmp, int index, int vec_enc) {\n+   assert(vec_enc < Assembler::AVX_512bit, \"\");\n+   if (vec_enc == Assembler::AVX_256bit) {\n+     vextractf128_high(xtmp, src);\n+     vshufps(dst, src, xtmp, index, vec_enc);\n+   } else {\n+     vshufps(dst, src, zero, index, vec_enc);\n+   }\n+}\n+\n+void C2_MacroAssembler::vector_cast_double_to_int_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                                    XMMRegister xtmp3, XMMRegister xtmp4, XMMRegister xtmp5, Register rscratch,\n+                                                                    AddressLiteral float_sign_flip, int src_vec_enc) {\n+  assert(rscratch != noreg || always_reachable(float_sign_flip), \"missing\");\n+\n+  Label done;\n+  \/\/ Compare the destination lanes with float_sign_flip\n+  \/\/ value to get mask for all special values.\n+  movdqu(xtmp1, float_sign_flip, rscratch);\n+  vpcmpeqd(xtmp2, dst, xtmp1, Assembler::AVX_128bit);\n+  ptest(xtmp2, xtmp2);\n+  jccb(Assembler::equal, done);\n+\n+  \/\/ Flip float_sign_flip to get max integer value.\n+  vpcmpeqd(xtmp4, xtmp4, xtmp4, Assembler::AVX_128bit);\n+  pxor(xtmp1, xtmp4);\n+\n+  \/\/ Set detination lanes corresponding to unordered source lanes as zero.\n+  vpxor(xtmp4, xtmp4, xtmp4, src_vec_enc);\n+  vcmppd(xtmp3, src, src, Assembler::UNORD_Q, src_vec_enc);\n+\n+  \/\/ Shuffle mask vector and pack lower doubles word from each quadword lane.\n+  vector_crosslane_doubleword_pack_avx(xtmp3, xtmp3, xtmp4, xtmp5, 0x88, src_vec_enc);\n+  vblendvps(dst, dst, xtmp4, xtmp3, Assembler::AVX_128bit);\n+\n+  \/\/ Recompute the mask for remaining special value.\n+  pxor(xtmp2, xtmp3);\n+  \/\/ Extract mask corresponding to non-negative source lanes.\n+  vcmppd(xtmp3, src, xtmp4, Assembler::NLT_UQ, src_vec_enc);\n+\n+  \/\/ Shuffle mask vector and pack lower doubles word from each quadword lane.\n+  vector_crosslane_doubleword_pack_avx(xtmp3, xtmp3, xtmp4, xtmp5, 0x88, src_vec_enc);\n+  pand(xtmp3, xtmp2);\n+\n+  \/\/ Replace destination lanes holding special value(0x80000000) with max int\n+  \/\/ if corresponding source lane holds a +ve value.\n+  vblendvps(dst, dst, xtmp1, xtmp3, Assembler::AVX_128bit);\n+  bind(done);\n+}\n+\n+\n+void C2_MacroAssembler::vector_cast_int_to_subword(BasicType to_elem_bt, XMMRegister dst, XMMRegister zero,\n+                                                   XMMRegister xtmp, Register rscratch, int vec_enc) {\n+  switch(to_elem_bt) {\n+    case T_SHORT:\n+      assert(rscratch != noreg || always_reachable(ExternalAddress(StubRoutines::x86::vector_int_to_short_mask())), \"missing\");\n+      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_short_mask()), vec_enc, rscratch);\n+      vpackusdw(dst, dst, zero, vec_enc);\n+      if (vec_enc == Assembler::AVX_256bit) {\n+        vector_crosslane_doubleword_pack_avx(dst, dst, zero, xtmp, 0x44, vec_enc);\n+      }\n+      break;\n+    case  T_BYTE:\n+      assert(rscratch != noreg || always_reachable(ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask())), \"missing\");\n+      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), vec_enc, rscratch);\n+      vpackusdw(dst, dst, zero, vec_enc);\n+      if (vec_enc == Assembler::AVX_256bit) {\n+        vector_crosslane_doubleword_pack_avx(dst, dst, zero, xtmp, 0x44, vec_enc);\n+      }\n+      vpackuswb(dst, dst, zero, vec_enc);\n+      break;\n+    default: assert(false, \"%s\", type2name(to_elem_bt));\n+  }\n+}\n+\n@@ -4165,8 +4727,1 @@\n-void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n-                                            Register scratch, int vec_enc) {\n-  evcvttpd2qq(dst, src, vec_enc);\n-  vector_cast_double_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n-}\n-\n-void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+void C2_MacroAssembler::vector_castF2X_avx(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n@@ -4174,1 +4729,3 @@\n-                                           AddressLiteral float_sign_flip, Register scratch, int vec_enc) {\n+                                           AddressLiteral float_sign_flip, Register rscratch, int vec_enc) {\n+  int to_elem_sz = type2aelembytes(to_elem_bt);\n+  assert(to_elem_sz <= 4, \"\");\n@@ -4176,1 +4733,5 @@\n-  vector_cast_float_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, scratch, float_sign_flip, vec_enc);\n+  vector_cast_float_to_int_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, rscratch, float_sign_flip, vec_enc);\n+  if (to_elem_sz < 4) {\n+    vpxor(xtmp4, xtmp4, xtmp4, vec_enc);\n+    vector_cast_int_to_subword(to_elem_bt, dst, xtmp4, xtmp3, rscratch, vec_enc);\n+  }\n@@ -4179,3 +4740,5 @@\n-void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n-                                            Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_castF2X_evex(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                            XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                                            Register rscratch, int vec_enc) {\n+  int to_elem_sz = type2aelembytes(to_elem_bt);\n+  assert(to_elem_sz <= 4, \"\");\n@@ -4183,1 +4746,75 @@\n-  vector_cast_float_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, float_sign_flip, vec_enc);\n+  vector_cast_float_to_int_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, rscratch, float_sign_flip, vec_enc);\n+  switch(to_elem_bt) {\n+    case T_INT:\n+      break;\n+    case T_SHORT:\n+      evpmovdw(dst, dst, vec_enc);\n+      break;\n+    case T_BYTE:\n+      evpmovdb(dst, dst, vec_enc);\n+      break;\n+    default: assert(false, \"%s\", type2name(to_elem_bt));\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_castF2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                            Register rscratch, int vec_enc) {\n+  evcvttps2qq(dst, src, vec_enc);\n+  vector_cast_float_to_long_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, rscratch, double_sign_flip, vec_enc);\n+}\n+\n+\/\/ Handling for downcasting from double to integer or sub-word types on AVX2.\n+void C2_MacroAssembler::vector_castD2X_avx(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4, XMMRegister xtmp5,\n+                                           AddressLiteral float_sign_flip, Register rscratch, int vec_enc) {\n+  int to_elem_sz = type2aelembytes(to_elem_bt);\n+  assert(to_elem_sz < 8, \"\");\n+  vcvttpd2dq(dst, src, vec_enc);\n+  vector_cast_double_to_int_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, xtmp5, rscratch,\n+                                              float_sign_flip, vec_enc);\n+  if (to_elem_sz < 4) {\n+    \/\/ xtmp4 holds all zero lanes.\n+    vector_cast_int_to_subword(to_elem_bt, dst, xtmp4, xtmp5, rscratch, Assembler::AVX_128bit);\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_castD2X_evex(BasicType to_elem_bt, XMMRegister dst, XMMRegister src,\n+                                            XMMRegister xtmp1, XMMRegister xtmp2, KRegister ktmp1,\n+                                            KRegister ktmp2, AddressLiteral sign_flip,\n+                                            Register rscratch, int vec_enc) {\n+  if (VM_Version::supports_avx512dq()) {\n+    evcvttpd2qq(dst, src, vec_enc);\n+    vector_cast_double_to_long_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, rscratch, sign_flip, vec_enc);\n+    switch(to_elem_bt) {\n+      case T_LONG:\n+        break;\n+      case T_INT:\n+        evpmovsqd(dst, dst, vec_enc);\n+        break;\n+      case T_SHORT:\n+        evpmovsqd(dst, dst, vec_enc);\n+        evpmovdw(dst, dst, vec_enc);\n+        break;\n+      case T_BYTE:\n+        evpmovsqd(dst, dst, vec_enc);\n+        evpmovdb(dst, dst, vec_enc);\n+        break;\n+      default: assert(false, \"%s\", type2name(to_elem_bt));\n+    }\n+  } else {\n+    assert(type2aelembytes(to_elem_bt) <= 4, \"\");\n+    vcvttpd2dq(dst, src, vec_enc);\n+    vector_cast_double_to_int_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, rscratch, sign_flip, vec_enc);\n+    switch(to_elem_bt) {\n+      case T_INT:\n+        break;\n+      case T_SHORT:\n+        evpmovdw(dst, dst, vec_enc);\n+        break;\n+      case T_BYTE:\n+        evpmovdb(dst, dst, vec_enc);\n+        break;\n+      default: assert(false, \"%s\", type2name(to_elem_bt));\n+    }\n+  }\n@@ -4187,3 +4824,3 @@\n-void C2_MacroAssembler::vector_round_double_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                                 KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n-                                                 AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_round_double_evex(XMMRegister dst, XMMRegister src,\n+                                                 AddressLiteral double_sign_flip, AddressLiteral new_mxcsr, int vec_enc,\n+                                                 Register tmp, XMMRegister xtmp1, XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2) {\n@@ -4192,4 +4829,4 @@\n-  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-  ldmxcsr(new_mxcsr, scratch);\n-  mov64(scratch, julong_cast(0.5L));\n-  evpbroadcastq(xtmp1, scratch, vec_enc);\n+  ldmxcsr(new_mxcsr, tmp \/*rscratch*\/);\n+\n+  mov64(tmp, julong_cast(0.5L));\n+  evpbroadcastq(xtmp1, tmp, vec_enc);\n@@ -4198,2 +4835,4 @@\n-  vector_cast_double_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n-  ldmxcsr(mxcsr_std, scratch);\n+  vector_cast_double_to_long_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, tmp \/*rscratch*\/,\n+                                                double_sign_flip, vec_enc);;\n+\n+  ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()), tmp \/*rscratch*\/);\n@@ -4202,3 +4841,3 @@\n-void C2_MacroAssembler::vector_round_float_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                                KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n-                                                AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_round_float_evex(XMMRegister dst, XMMRegister src,\n+                                                AddressLiteral float_sign_flip, AddressLiteral new_mxcsr, int vec_enc,\n+                                                Register tmp, XMMRegister xtmp1, XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2) {\n@@ -4207,4 +4846,4 @@\n-  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-  ldmxcsr(new_mxcsr, scratch);\n-  movl(scratch, jint_cast(0.5));\n-  movq(xtmp1, scratch);\n+  ldmxcsr(new_mxcsr, tmp \/*rscratch*\/);\n+\n+  movl(tmp, jint_cast(0.5));\n+  movq(xtmp1, tmp);\n@@ -4214,2 +4853,4 @@\n-  vector_cast_float_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, float_sign_flip, vec_enc);\n-  ldmxcsr(mxcsr_std, scratch);\n+  vector_cast_float_to_int_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, tmp \/*rscratch*\/,\n+                                              float_sign_flip, vec_enc);\n+\n+  ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()), tmp \/*rscratch*\/);\n@@ -4218,3 +4859,3 @@\n-void C2_MacroAssembler::vector_round_float_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                               XMMRegister xtmp3, XMMRegister xtmp4, AddressLiteral float_sign_flip,\n-                                               AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_round_float_avx(XMMRegister dst, XMMRegister src,\n+                                               AddressLiteral float_sign_flip, AddressLiteral new_mxcsr, int vec_enc,\n+                                               Register tmp, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4) {\n@@ -4223,4 +4864,4 @@\n-  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-  ldmxcsr(new_mxcsr, scratch);\n-  movl(scratch, jint_cast(0.5));\n-  movq(xtmp1, scratch);\n+  ldmxcsr(new_mxcsr, tmp \/*rscratch*\/);\n+\n+  movl(tmp, jint_cast(0.5));\n+  movq(xtmp1, tmp);\n@@ -4230,2 +4871,3 @@\n-  vector_cast_float_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, scratch, float_sign_flip, vec_enc);\n-  ldmxcsr(mxcsr_std, scratch);\n+  vector_cast_float_to_int_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, tmp \/*rscratch*\/, float_sign_flip, vec_enc);\n+\n+  ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()), tmp \/*rscratch*\/);\n@@ -4233,1 +4875,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -4262,0 +4904,82 @@\n+void C2_MacroAssembler::vector_signed_cast(XMMRegister dst, XMMRegister src, int vlen_enc,\n+                                           BasicType from_elem_bt, BasicType to_elem_bt) {\n+  switch (from_elem_bt) {\n+    case T_BYTE:\n+      switch (to_elem_bt) {\n+        case T_SHORT: vpmovsxbw(dst, src, vlen_enc); break;\n+        case T_INT:   vpmovsxbd(dst, src, vlen_enc); break;\n+        case T_LONG:  vpmovsxbq(dst, src, vlen_enc); break;\n+        default: ShouldNotReachHere();\n+      }\n+      break;\n+    case T_SHORT:\n+      switch (to_elem_bt) {\n+        case T_INT:  vpmovsxwd(dst, src, vlen_enc); break;\n+        case T_LONG: vpmovsxwq(dst, src, vlen_enc); break;\n+        default: ShouldNotReachHere();\n+      }\n+      break;\n+    case T_INT:\n+      assert(to_elem_bt == T_LONG, \"\");\n+      vpmovsxdq(dst, src, vlen_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_mask_cast(XMMRegister dst, XMMRegister src,\n+                                         BasicType dst_bt, BasicType src_bt, int vlen) {\n+  int vlen_enc = vector_length_encoding(MAX2(type2aelembytes(src_bt), type2aelembytes(dst_bt)) * vlen);\n+  assert(vlen_enc != AVX_512bit, \"\");\n+\n+  int dst_bt_size = type2aelembytes(dst_bt);\n+  int src_bt_size = type2aelembytes(src_bt);\n+  if (dst_bt_size > src_bt_size) {\n+    switch (dst_bt_size \/ src_bt_size) {\n+      case 2: vpmovsxbw(dst, src, vlen_enc); break;\n+      case 4: vpmovsxbd(dst, src, vlen_enc); break;\n+      case 8: vpmovsxbq(dst, src, vlen_enc); break;\n+      default: ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(dst_bt_size < src_bt_size, \"\");\n+    switch (src_bt_size \/ dst_bt_size) {\n+      case 2: {\n+        if (vlen_enc == AVX_128bit) {\n+          vpacksswb(dst, src, src, vlen_enc);\n+        } else {\n+          vpacksswb(dst, src, src, vlen_enc);\n+          vpermq(dst, dst, 0x08, vlen_enc);\n+        }\n+        break;\n+      }\n+      case 4: {\n+        if (vlen_enc == AVX_128bit) {\n+          vpackssdw(dst, src, src, vlen_enc);\n+          vpacksswb(dst, dst, dst, vlen_enc);\n+        } else {\n+          vpackssdw(dst, src, src, vlen_enc);\n+          vpermq(dst, dst, 0x08, vlen_enc);\n+          vpacksswb(dst, dst, dst, AVX_128bit);\n+        }\n+        break;\n+      }\n+      case 8: {\n+        if (vlen_enc == AVX_128bit) {\n+          vpshufd(dst, src, 0x08, vlen_enc);\n+          vpackssdw(dst, dst, dst, vlen_enc);\n+          vpacksswb(dst, dst, dst, vlen_enc);\n+        } else {\n+          vpshufd(dst, src, 0x08, vlen_enc);\n+          vpermq(dst, dst, 0x08, vlen_enc);\n+          vpackssdw(dst, dst, dst, AVX_128bit);\n+          vpacksswb(dst, dst, dst, AVX_128bit);\n+        }\n+        break;\n+      }\n+      default: ShouldNotReachHere();\n+    }\n+  }\n+}\n+\n@@ -4289,1 +5013,1 @@\n-  pdep(rtmp1, src, rtmp1);\n+  pdepq(rtmp1, src, rtmp1);\n@@ -4306,1 +5030,1 @@\n-    pdep(rtmp1, rtmp2, rtmp1);\n+    pdepq(rtmp1, rtmp2, rtmp1);\n@@ -4447,1 +5171,1 @@\n-  pext(rtmp2, rtmp2, rtmp1);\n+  pextq(rtmp2, rtmp2, rtmp1);\n@@ -4475,1 +5199,1 @@\n-      fatal(\"Unsupported type\");\n+      fatal(\"Unsupported type %s\", type2name(bt));\n@@ -4501,1 +5225,1 @@\n-      fatal(\"Unsupported type\");\n+      fatal(\"Unsupported type %s\", type2name(bt));\n@@ -4508,0 +5232,42 @@\n+void C2_MacroAssembler::vector_signum_evex(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                                           KRegister ktmp1, int vec_enc) {\n+  if (opcode == Op_SignumVD) {\n+    vsubpd(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    evcmppd(ktmp1, k0, src, zero, Assembler::LT_OQ, vec_enc);\n+    evblendmpd(dst, ktmp1, one, dst, true, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    evcmppd(ktmp1, k0, src, zero, Assembler::EQ_UQ, vec_enc);\n+    evblendmpd(dst, ktmp1, dst, src, true, vec_enc);\n+  } else {\n+    assert(opcode == Op_SignumVF, \"\");\n+    vsubps(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    evcmpps(ktmp1, k0, src, zero, Assembler::LT_OQ, vec_enc);\n+    evblendmps(dst, ktmp1, one, dst, true, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    evcmpps(ktmp1, k0, src, zero, Assembler::EQ_UQ, vec_enc);\n+    evblendmps(dst, ktmp1, dst, src, true, vec_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_signum_avx(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                                          XMMRegister xtmp1, int vec_enc) {\n+  if (opcode == Op_SignumVD) {\n+    vsubpd(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    vblendvpd(dst, one, dst, src, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    vcmppd(xtmp1, src, zero, Assembler::EQ_UQ, vec_enc);\n+    vblendvpd(dst, dst, src, xtmp1, vec_enc);\n+  } else {\n+    assert(opcode == Op_SignumVF, \"\");\n+    vsubps(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    vblendvps(dst, one, dst, src, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    vcmpps(xtmp1, src, zero, Assembler::EQ_UQ, vec_enc);\n+    vblendvps(dst, dst, src, xtmp1, vec_enc);\n+  }\n+}\n+\n@@ -4539,1 +5305,2 @@\n-      default : ShouldNotReachHere(); break;\n+      fatal(\"Unsupported lane size %d\", lane_size);\n+      break;\n@@ -4549,1 +5316,2 @@\n-      default : ShouldNotReachHere(); break;\n+      fatal(\"Unsupported lane size %d\", lane_size);\n+      break;\n@@ -4591,1 +5359,1 @@\n-  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp, vec_enc);\n+  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), vec_enc, noreg);\n@@ -4644,1 +5412,2 @@\n-      ShouldNotReachHere();\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n@@ -4671,1 +5440,2 @@\n-      ShouldNotReachHere();\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n@@ -4694,1 +5464,1 @@\n-    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), vec_enc, noreg);\n@@ -4696,1 +5466,1 @@\n-    vpandq(dst, xtmp2, src, vec_enc);\n+    evpandq(dst, xtmp2, src, vec_enc);\n@@ -4707,4 +5477,2 @@\n-    vporq(xtmp2, dst, xtmp2, vec_enc);\n-    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n-\n-  } else if(!VM_Version::supports_avx512vlbw() && vec_enc == Assembler::AVX_512bit) {\n+    evporq(xtmp2, dst, xtmp2, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, vec_enc);\n@@ -4712,0 +5480,1 @@\n+  } else if(vec_enc == Assembler::AVX_512bit) {\n@@ -4714,1 +5483,0 @@\n-    vbroadcast(T_INT, xtmp1, 0x0F0F0F0F, rtmp, vec_enc);\n@@ -4717,5 +5485,1 @@\n-    vpandq(dst, xtmp1, src, vec_enc);\n-    vpsllq(dst, dst, 4, vec_enc);\n-    vpandn(xtmp2, xtmp1, src, vec_enc);\n-    vpsrlq(xtmp2, xtmp2, 4, vec_enc);\n-    vporq(xtmp1, dst, xtmp2, vec_enc);\n+    vector_swap_nbits(4, 0x0F0F0F0F, xtmp1, src, xtmp2, rtmp, vec_enc);\n@@ -4724,6 +5488,1 @@\n-    vbroadcast(T_INT, xtmp2, 0x33333333, rtmp, vec_enc);\n-    vpandq(dst, xtmp2, xtmp1, vec_enc);\n-    vpsllq(dst, dst, 2, vec_enc);\n-    vpandn(xtmp2, xtmp2, xtmp1, vec_enc);\n-    vpsrlq(xtmp2, xtmp2, 2, vec_enc);\n-    vporq(xtmp1, dst, xtmp2, vec_enc);\n+    vector_swap_nbits(2, 0x33333333, dst, xtmp1, xtmp2, rtmp, vec_enc);\n@@ -4732,6 +5491,2 @@\n-    vbroadcast(T_INT, xtmp2, 0x55555555, rtmp, vec_enc);\n-    vpandq(dst, xtmp2, xtmp1, vec_enc);\n-    vpsllq(dst, dst, 1, vec_enc);\n-    vpandn(xtmp2, xtmp2, xtmp1, vec_enc);\n-    vpsrlq(xtmp2, xtmp2, 1, vec_enc);\n-    vporq(xtmp1, dst, xtmp2, vec_enc);\n+    evmovdqul(xtmp1, k0, dst, true, vec_enc);\n+    vector_swap_nbits(1, 0x55555555, dst, xtmp1, xtmp2, rtmp, vec_enc);\n@@ -4739,0 +5494,1 @@\n+    evmovdqul(xtmp1, k0, dst, true, vec_enc);\n@@ -4740,2 +5496,1 @@\n-\n-    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), vec_enc, rtmp);\n@@ -4758,1 +5513,1 @@\n-    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, vec_enc);\n@@ -4762,2 +5517,5 @@\n-void C2_MacroAssembler::vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src,\n-                                                XMMRegister xtmp, AddressLiteral mask, Register rtmp, int vec_enc) {\n+void C2_MacroAssembler::vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src, AddressLiteral mask, int vec_enc,\n+                                                XMMRegister xtmp, Register rscratch) {\n+  assert(VM_Version::supports_gfni(), \"\");\n+  assert(rscratch != noreg || always_reachable(mask), \"missing\");\n+\n@@ -4766,2 +5524,1 @@\n-  assert(VM_Version::supports_gfni(), \"\");\n-  vpbroadcastq(xtmp, mask, vec_enc, rtmp);\n+  vpbroadcastq(xtmp, mask, vec_enc, rscratch);\n@@ -4769,1 +5526,11 @@\n-  vector_reverse_byte(bt, dst, xtmp, rtmp, vec_enc);\n+  vector_reverse_byte(bt, dst, xtmp, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_swap_nbits(int nbits, int bitmask, XMMRegister dst, XMMRegister src,\n+                                          XMMRegister xtmp1, Register rtmp, int vec_enc) {\n+  vbroadcast(T_INT, xtmp1, bitmask, rtmp, vec_enc);\n+  evpandq(dst, xtmp1, src, vec_enc);\n+  vpsllq(dst, dst, nbits, vec_enc);\n+  vpandn(xtmp1, xtmp1, src, vec_enc);\n+  vpsrlq(xtmp1, xtmp1, nbits, vec_enc);\n+  evporq(dst, dst, xtmp1, vec_enc);\n@@ -4776,1 +5543,0 @@\n-  evmovdqul(xtmp1, k0, src, true, vec_enc);\n@@ -4780,1 +5546,4 @@\n-      evprorq(xtmp1, k0, xtmp1, 32, true, vec_enc);\n+      evprorq(xtmp1, k0, src, 32, true, vec_enc);\n+      evprord(xtmp1, k0, xtmp1, 16, true, vec_enc);\n+      vector_swap_nbits(8, 0x00FF00FF, dst, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n@@ -4783,1 +5552,4 @@\n-      evprord(xtmp1, k0, xtmp1, 16, true, vec_enc);\n+      evprord(xtmp1, k0, src, 16, true, vec_enc);\n+      vector_swap_nbits(8, 0x00FF00FF, dst, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_CHAR:\n@@ -4786,6 +5558,1 @@\n-      vbroadcast(T_INT, dst, 0x00FF00FF, rtmp, vec_enc);\n-      vpandq(xtmp2, dst, xtmp1, vec_enc);\n-      vpsllq(xtmp2, xtmp2, 8, vec_enc);\n-      vpandn(xtmp1, dst, xtmp1, vec_enc);\n-      vpsrlq(dst, xtmp1, 8, vec_enc);\n-      vporq(dst, dst, xtmp2, vec_enc);\n+      vector_swap_nbits(8, 0x00FF00FF, dst, src, xtmp2, rtmp, vec_enc);\n@@ -4797,1 +5564,1 @@\n-      fatal(\"Unsupported type\");\n+      fatal(\"Unsupported type %s\", type2name(bt));\n@@ -4802,1 +5569,1 @@\n-void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc) {\n+void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, int vec_enc) {\n@@ -4815,1 +5582,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), vec_enc, noreg);\n@@ -4818,1 +5585,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), vec_enc, noreg);\n@@ -4820,0 +5587,1 @@\n+    case T_CHAR:\n@@ -4821,1 +5589,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), vec_enc, noreg);\n@@ -4824,1 +5592,1 @@\n-      fatal(\"Unsupported type\");\n+      fatal(\"Unsupported type %s\", type2name(bt));\n@@ -4870,1 +5638,2 @@\n-      ShouldNotReachHere();\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n@@ -4996,1 +5765,2 @@\n-      ShouldNotReachHere();\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n@@ -5015,17 +5785,1 @@\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-void C2_MacroAssembler::vpadd(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc) {\n-  switch(bt) {\n-    case T_BYTE:\n-      vpaddb(dst, src1, src2, vec_enc);\n-      break;\n-    case T_SHORT:\n-      vpaddw(dst, src1, src2, vec_enc);\n-      break;\n-    case T_INT:\n-      vpaddd(dst, src1, src2, vec_enc);\n-      break;\n-    case T_LONG:\n-      vpaddq(dst, src1, src2, vec_enc);\n+      fatal(\"Unsupported type %s\", type2name(bt));\n@@ -5033,2 +5787,0 @@\n-    default:\n-      ShouldNotReachHere();\n@@ -5155,0 +5907,84 @@\n+void C2_MacroAssembler::reverseI(Register dst, Register src, XMMRegister xtmp1,\n+                                 XMMRegister xtmp2, Register rtmp) {\n+  if(VM_Version::supports_gfni()) {\n+    \/\/ Galois field instruction based bit reversal based on following algorithm.\n+    \/\/ http:\/\/0x80.pl\/articles\/avx512-galois-field-for-bit-shuffling.html\n+    mov64(rtmp, 0x8040201008040201L);\n+    movq(xtmp1, src);\n+    movq(xtmp2, rtmp);\n+    gf2p8affineqb(xtmp1, xtmp2, 0);\n+    movq(dst, xtmp1);\n+  } else {\n+    \/\/ Swap even and odd numbered bits.\n+    movl(rtmp, src);\n+    andl(rtmp, 0x55555555);\n+    shll(rtmp, 1);\n+    movl(dst, src);\n+    andl(dst, 0xAAAAAAAA);\n+    shrl(dst, 1);\n+    orl(dst, rtmp);\n+\n+    \/\/ Swap LSB and MSB 2 bits of each nibble.\n+    movl(rtmp, dst);\n+    andl(rtmp, 0x33333333);\n+    shll(rtmp, 2);\n+    andl(dst, 0xCCCCCCCC);\n+    shrl(dst, 2);\n+    orl(dst, rtmp);\n+\n+    \/\/ Swap LSB and MSB 4 bits of each byte.\n+    movl(rtmp, dst);\n+    andl(rtmp, 0x0F0F0F0F);\n+    shll(rtmp, 4);\n+    andl(dst, 0xF0F0F0F0);\n+    shrl(dst, 4);\n+    orl(dst, rtmp);\n+  }\n+  bswapl(dst);\n+}\n+\n+void C2_MacroAssembler::reverseL(Register dst, Register src, XMMRegister xtmp1,\n+                                 XMMRegister xtmp2, Register rtmp1, Register rtmp2) {\n+  if(VM_Version::supports_gfni()) {\n+    \/\/ Galois field instruction based bit reversal based on following algorithm.\n+    \/\/ http:\/\/0x80.pl\/articles\/avx512-galois-field-for-bit-shuffling.html\n+    mov64(rtmp1, 0x8040201008040201L);\n+    movq(xtmp1, src);\n+    movq(xtmp2, rtmp1);\n+    gf2p8affineqb(xtmp1, xtmp2, 0);\n+    movq(dst, xtmp1);\n+  } else {\n+    \/\/ Swap even and odd numbered bits.\n+    movq(rtmp1, src);\n+    mov64(rtmp2, 0x5555555555555555L);\n+    andq(rtmp1, rtmp2);\n+    shlq(rtmp1, 1);\n+    movq(dst, src);\n+    notq(rtmp2);\n+    andq(dst, rtmp2);\n+    shrq(dst, 1);\n+    orq(dst, rtmp1);\n+\n+    \/\/ Swap LSB and MSB 2 bits of each nibble.\n+    movq(rtmp1, dst);\n+    mov64(rtmp2, 0x3333333333333333L);\n+    andq(rtmp1, rtmp2);\n+    shlq(rtmp1, 2);\n+    notq(rtmp2);\n+    andq(dst, rtmp2);\n+    shrq(dst, 2);\n+    orq(dst, rtmp1);\n+\n+    \/\/ Swap LSB and MSB 4 bits of each byte.\n+    movq(rtmp1, dst);\n+    mov64(rtmp2, 0x0F0F0F0F0F0F0F0FL);\n+    andq(rtmp1, rtmp2);\n+    shlq(rtmp1, 4);\n+    notq(rtmp2);\n+    andq(dst, rtmp2);\n+    shrq(dst, 4);\n+    orq(dst, rtmp1);\n+  }\n+  bswapq(dst);\n+}\n+\n@@ -5235,0 +6071,58 @@\n+\n+void C2_MacroAssembler::rearrange_bytes(XMMRegister dst, XMMRegister shuffle, XMMRegister src, XMMRegister xtmp1,\n+                                        XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, KRegister ktmp,\n+                                        int vlen_enc) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  \/\/ Byte shuffles are inlane operations and indices are determined using\n+  \/\/ lower 4 bit of each shuffle lane, thus all shuffle indices are\n+  \/\/ normalized to index range 0-15. This makes sure that all the multiples\n+  \/\/ of an index value are placed at same relative position in 128 bit\n+  \/\/ lane i.e. elements corresponding to shuffle indices 16, 32 and 64\n+  \/\/ will be 16th element in their respective 128 bit lanes.\n+  movl(rtmp, 16);\n+  evpbroadcastb(xtmp1, rtmp, vlen_enc);\n+\n+  \/\/ Compute a mask for shuffle vector by comparing indices with expression INDEX < 16,\n+  \/\/ Broadcast first 128 bit lane across entire vector, shuffle the vector lanes using\n+  \/\/ original shuffle indices and move the shuffled lanes corresponding to true\n+  \/\/ mask to destination vector.\n+  evpcmpb(ktmp, k0, shuffle, xtmp1, Assembler::lt, true, vlen_enc);\n+  evshufi64x2(xtmp2, src, src, 0x0, vlen_enc);\n+  evpshufb(dst, ktmp, xtmp2, shuffle, false, vlen_enc);\n+\n+  \/\/ Perform above steps with lane comparison expression as INDEX >= 16 && INDEX < 32\n+  \/\/ and broadcasting second 128 bit lane.\n+  evpcmpb(ktmp, k0, shuffle,  xtmp1, Assembler::nlt, true, vlen_enc);\n+  vpsllq(xtmp2, xtmp1, 0x1, vlen_enc);\n+  evpcmpb(ktmp, ktmp, shuffle, xtmp2, Assembler::lt, true, vlen_enc);\n+  evshufi64x2(xtmp3, src, src, 0x55, vlen_enc);\n+  evpshufb(dst, ktmp, xtmp3, shuffle, true, vlen_enc);\n+\n+  \/\/ Perform above steps with lane comparison expression as INDEX >= 32 && INDEX < 48\n+  \/\/ and broadcasting third 128 bit lane.\n+  evpcmpb(ktmp, k0, shuffle,  xtmp2, Assembler::nlt, true, vlen_enc);\n+  vpaddb(xtmp1, xtmp1, xtmp2, vlen_enc);\n+  evpcmpb(ktmp, ktmp, shuffle,  xtmp1, Assembler::lt, true, vlen_enc);\n+  evshufi64x2(xtmp3, src, src, 0xAA, vlen_enc);\n+  evpshufb(dst, ktmp, xtmp3, shuffle, true, vlen_enc);\n+\n+  \/\/ Perform above steps with lane comparison expression as INDEX >= 48 && INDEX < 64\n+  \/\/ and broadcasting third 128 bit lane.\n+  evpcmpb(ktmp, k0, shuffle,  xtmp1, Assembler::nlt, true, vlen_enc);\n+  vpsllq(xtmp2, xtmp2, 0x1, vlen_enc);\n+  evpcmpb(ktmp, ktmp, shuffle,  xtmp2, Assembler::lt, true, vlen_enc);\n+  evshufi64x2(xtmp3, src, src, 0xFF, vlen_enc);\n+  evpshufb(dst, ktmp, xtmp3, shuffle, true, vlen_enc);\n+}\n+\n+void C2_MacroAssembler::vector_rearrange_int_float(BasicType bt, XMMRegister dst,\n+                                                   XMMRegister shuffle, XMMRegister src, int vlen_enc) {\n+  if (vlen_enc == AVX_128bit) {\n+    vpermilps(dst, src, shuffle, vlen_enc);\n+  } else if (bt == T_INT) {\n+    vpermd(dst, shuffle, src, vlen_enc);\n+  } else {\n+    assert(bt == T_FLOAT, \"\");\n+    vpermps(dst, shuffle, src, vlen_enc);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":1205,"deletions":311,"binary":false,"changes":1516,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,3 @@\n+  \/\/ C2 compiled method's prolog code.\n+  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+\n@@ -67,4 +70,4 @@\n-  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr);\n-  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);\n-  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr);\n-  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);\n+  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src);\n+  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len);\n+  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src);\n+  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len);\n@@ -87,3 +90,1 @@\n-  void signum_fp(int opcode, XMMRegister dst,\n-                 XMMRegister zero, XMMRegister one,\n-                 Register scratch);\n+  void signum_fp(int opcode, XMMRegister dst, XMMRegister zero, XMMRegister one);\n@@ -118,2 +119,2 @@\n-  void varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch);\n-  void evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch);\n+  void varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp);\n+  void evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp);\n@@ -127,2 +128,2 @@\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len);\n@@ -134,1 +135,2 @@\n-  void get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp = noreg, XMMRegister vtmp = xnoreg);\n+  void get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, XMMRegister vtmp = xnoreg);\n+  void movsxl(BasicType typ, Register dst);\n@@ -137,2 +139,7 @@\n-  void vectortest(int bt, int vlen, XMMRegister src1, XMMRegister src2,\n-                  XMMRegister vtmp1 = xnoreg, XMMRegister vtmp2 = xnoreg, KRegister mask = knoreg);\n+  void vectortest(BasicType bt, XMMRegister src1, XMMRegister src2, XMMRegister vtmp, int vlen_in_bytes);\n+\n+ \/\/ Covert B2X\n+ void vconvert_b2x(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, int vlen_enc);\n+#ifdef _LP64\n+ void vpbroadcast(BasicType elem_bt, XMMRegister dst, Register src, int vlen_enc);\n+#endif\n@@ -141,2 +148,2 @@\n-  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch = rscratch1);\n-  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, XMMRegister src2, int comparison, int vector_len);\n+  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, XMMRegister    src2, int comparison, int vector_len);\n+  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral src2, int comparison, int vector_len, Register rscratch = noreg);\n@@ -145,0 +152,3 @@\n+  void load_vector(XMMRegister dst, Address        src, int vlen_in_bytes);\n+  void load_vector(XMMRegister dst, AddressLiteral src, int vlen_in_bytes, Register rscratch = noreg);\n+\n@@ -146,1 +156,1 @@\n-  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, Register tmp, bool novlbwdq, int vlen_enc);\n+  void load_vector_mask(KRegister   dst, XMMRegister src, XMMRegister xtmp, bool novlbwdq, int vlen_enc);\n@@ -148,3 +158,2 @@\n-  void load_vector(XMMRegister dst, Address src, int vlen_in_bytes);\n-  void load_vector(XMMRegister dst, AddressLiteral src, int vlen_in_bytes, Register rscratch = rscratch1);\n-  void load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes);\n+  void load_constant_vector(BasicType bt, XMMRegister dst, InternalAddress src, int vlen);\n+  void load_iota_indices(XMMRegister dst, int vlen_in_bytes, BasicType bt);\n@@ -285,0 +294,13 @@\n+  void arrays_hashcode(Register str1, Register cnt1, Register result,\n+                       Register tmp1, Register tmp2, Register tmp3, XMMRegister vnext,\n+                       XMMRegister vcoef0, XMMRegister vcoef1, XMMRegister vcoef2, XMMRegister vcoef3,\n+                       XMMRegister vresult0, XMMRegister vresult1, XMMRegister vresult2, XMMRegister vresult3,\n+                       XMMRegister vtmp0, XMMRegister vtmp1, XMMRegister vtmp2, XMMRegister vtmp3,\n+                       BasicType eltype);\n+\n+  \/\/ helper functions for arrays_hashcode\n+  int arrays_hashcode_elsize(BasicType eltype);\n+  void arrays_hashcode_elload(Register dst, Address src, BasicType eltype);\n+  void arrays_hashcode_elvload(XMMRegister dst, Address src, BasicType eltype);\n+  void arrays_hashcode_elvload(XMMRegister dst, AddressLiteral src, BasicType eltype);\n+  void arrays_hashcode_elvcast(XMMRegister dst, BasicType eltype);\n@@ -300,3 +322,2 @@\n-  void vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                          XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n-                          AddressLiteral float_sign_flip, Register scratch, int vec_enc);\n+  void vector_unsigned_cast(XMMRegister dst, XMMRegister src, int vlen_enc,\n+                            BasicType from_elem_bt, BasicType to_elem_bt);\n@@ -304,3 +325,9 @@\n-  void vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                           KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n-                           Register scratch, int vec_enc);\n+  void vector_signed_cast(XMMRegister dst, XMMRegister src, int vlen_enc,\n+                          BasicType from_elem_bt, BasicType to_elem_bt);\n+\n+  void vector_cast_int_to_subword(BasicType to_elem_bt, XMMRegister dst, XMMRegister zero,\n+                                  XMMRegister xtmp, Register rscratch, int vec_enc);\n+\n+  void vector_castF2X_avx(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                          XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                          AddressLiteral float_sign_flip, Register rscratch, int vec_enc);\n@@ -308,0 +335,3 @@\n+  void vector_castF2X_evex(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                           XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                           Register rscratch, int vec_enc);\n@@ -309,1 +339,1 @@\n-  void vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+  void vector_castF2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n@@ -311,1 +341,1 @@\n-                           Register scratch, int vec_enc);\n+                           Register rscratch, int vec_enc);\n@@ -313,2 +343,3 @@\n-  void vector_unsigned_cast(XMMRegister dst, XMMRegister src, int vlen_enc,\n-                            BasicType from_elem_bt, BasicType to_elem_bt);\n+  void vector_castD2X_evex(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                           XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2, AddressLiteral sign_flip,\n+                           Register rscratch, int vec_enc);\n@@ -316,3 +347,3 @@\n-  void vector_cast_double_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                             KRegister ktmp1, KRegister ktmp2, Register scratch, AddressLiteral double_sign_flip,\n-                                             int vec_enc);\n+  void vector_castD2X_avx(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                          XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4, XMMRegister xtmp5,\n+                          AddressLiteral float_sign_flip, Register rscratch, int vec_enc);\n@@ -320,7 +351,3 @@\n-  void vector_cast_float_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, Register scratch, AddressLiteral float_sign_flip,\n-                                            int vec_enc);\n-  void vector_cast_float_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n-                                           Register scratch, AddressLiteral float_sign_flip,\n-                                           int vec_enc);\n+  void vector_cast_double_to_int_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                   XMMRegister xtmp3, XMMRegister xtmp4, XMMRegister xtmp5, Register rscratch,\n+                                                   AddressLiteral float_sign_flip, int vec_enc);\n@@ -329,4 +356,3 @@\n-#ifdef _LP64\n-  void vector_round_double_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n-                                AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+  void vector_cast_double_to_int_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                    KRegister ktmp1, KRegister ktmp2, Register rscratch, AddressLiteral float_sign_flip,\n+                                                    int vec_enc);\n@@ -334,3 +360,3 @@\n-  void vector_round_float_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                               KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n-                               AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+  void vector_cast_double_to_long_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                     KRegister ktmp1, KRegister ktmp2, Register rscratch, AddressLiteral double_sign_flip,\n+                                                     int vec_enc);\n@@ -338,4 +364,3 @@\n-  void vector_round_float_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                              XMMRegister xtmp3, XMMRegister xtmp4, AddressLiteral float_sign_flip,\n-                              AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n-#endif\n+  void vector_cast_float_to_int_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                   KRegister ktmp1, KRegister ktmp2, Register rscratch, AddressLiteral float_sign_flip,\n+                                                   int vec_enc);\n@@ -343,2 +368,3 @@\n-  void evpternlog(XMMRegister dst, int func, KRegister mask, XMMRegister src2, XMMRegister src3,\n-                  bool merge, BasicType bt, int vlen_enc);\n+  void vector_cast_float_to_long_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                    KRegister ktmp1, KRegister ktmp2, Register rscratch, AddressLiteral double_sign_flip,\n+                                                    int vec_enc);\n@@ -346,2 +372,3 @@\n-  void evpternlog(XMMRegister dst, int func, KRegister mask, XMMRegister src2, Address src3,\n-                  bool merge, BasicType bt, int vlen_enc);\n+  void vector_cast_float_to_int_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                  XMMRegister xtmp4, Register rscratch, AddressLiteral float_sign_flip,\n+                                                  int vec_enc);\n@@ -349,2 +376,2 @@\n-  void vector_reverse_bit(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                          XMMRegister xtmp2, Register rtmp, int vec_enc);\n+  void vector_crosslane_doubleword_pack_avx(XMMRegister dst, XMMRegister src, XMMRegister zero,\n+                                            XMMRegister xtmp, int index, int vec_enc);\n@@ -352,2 +379,1 @@\n-  void vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp,\n-                               AddressLiteral mask, Register rtmp, int vec_enc);\n+  void vector_mask_cast(XMMRegister dst, XMMRegister src, BasicType dst_bt, BasicType src_bt, int vlen);\n@@ -355,1 +381,10 @@\n-  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc);\n+#ifdef _LP64\n+  void vector_round_double_evex(XMMRegister dst, XMMRegister src, AddressLiteral double_sign_flip, AddressLiteral new_mxcsr, int vec_enc,\n+                                Register tmp, XMMRegister xtmp1, XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2);\n+\n+  void vector_round_float_evex(XMMRegister dst, XMMRegister src, AddressLiteral double_sign_flip, AddressLiteral new_mxcsr, int vec_enc,\n+                               Register tmp, XMMRegister xtmp1, XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2);\n+\n+  void vector_round_float_avx(XMMRegister dst, XMMRegister src, AddressLiteral float_sign_flip, AddressLiteral new_mxcsr, int vec_enc,\n+                              Register tmp, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4);\n+#endif \/\/ _LP64\n@@ -362,0 +397,4 @@\n+  void reverseI(Register dst, Register src, XMMRegister xtmp1,\n+                XMMRegister xtmp2, Register rtmp);\n+  void reverseL(Register dst, Register src, XMMRegister xtmp1,\n+                XMMRegister xtmp2, Register rtmp1, Register rtmp2);\n@@ -367,0 +406,14 @@\n+  void evpternlog(XMMRegister dst, int func, KRegister mask, XMMRegister src2, XMMRegister src3,\n+                  bool merge, BasicType bt, int vlen_enc);\n+\n+  void evpternlog(XMMRegister dst, int func, KRegister mask, XMMRegister src2, Address src3,\n+                  bool merge, BasicType bt, int vlen_enc);\n+\n+  void vector_reverse_bit(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                          XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src, AddressLiteral mask, int vec_enc,\n+                               XMMRegister xtmp, Register rscratch = noreg);\n+\n+  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, int vec_enc);\n+\n@@ -390,1 +443,0 @@\n-\n@@ -418,0 +470,3 @@\n+  void vector_swap_nbits(int nbits, int bitmask, XMMRegister dst, XMMRegister src,\n+                         XMMRegister xtmp1, Register rtmp, int vec_enc);\n+\n@@ -421,0 +476,16 @@\n+  void vector_signum_avx(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                         XMMRegister xtmp1, int vec_enc);\n+\n+  void vector_signum_evex(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                          KRegister ktmp1, int vec_enc);\n+\n+  void vmovmask(BasicType elem_bt, XMMRegister dst, Address src, XMMRegister mask, int vec_enc);\n+\n+  void vmovmask(BasicType elem_bt, Address dst, XMMRegister src, XMMRegister mask, int vec_enc);\n+\n+  void rearrange_bytes(XMMRegister dst, XMMRegister shuffle, XMMRegister src, XMMRegister xtmp1,\n+                       XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, KRegister ktmp, int vlen_enc);\n+\n+  void vector_rearrange_int_float(BasicType bt, XMMRegister dst, XMMRegister shuffle,\n+                                  XMMRegister src, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":133,"deletions":62,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,0 @@\n-#include \"jvm.h\"\n@@ -31,0 +30,1 @@\n+#include \"crc32c.h\"\n@@ -37,0 +37,1 @@\n+#include \"jvm.h\"\n@@ -43,1 +44,1 @@\n-#include \"runtime\/flags\/flagSetting.hpp\"\n+#include \"runtime\/continuation.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -52,2 +54,0 @@\n-#include \"runtime\/thread.hpp\"\n-#include \"crc32c.h\"\n@@ -70,1 +70,1 @@\n-static Assembler::Condition reverse[] = {\n+static const Assembler::Condition reverse[] = {\n@@ -104,1 +104,2 @@\n-Address MacroAssembler::as_Address(ArrayAddress adr) {\n+Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n+  assert(rscratch == noreg, \"\");\n@@ -127,1 +128,2 @@\n-void MacroAssembler::cmpoop(Register src1, jobject obj) {\n+void MacroAssembler::cmpoop(Register src1, jobject obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -163,2 +165,3 @@\n-void MacroAssembler::jump(ArrayAddress entry) {\n-  jmp(as_Address(entry));\n+void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n+  assert(rscratch == noreg, \"not needed\");\n+  jmp(as_Address(entry, noreg));\n@@ -194,1 +197,1 @@\n-    mov_literal32(dst, (int32_t)src.target(), src.rspec());\n+  mov_literal32(dst, (int32_t)src.target(), src.rspec());\n@@ -197,1 +200,3 @@\n-void MacroAssembler::lea(Address dst, AddressLiteral adr) {\n+void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n+  assert(rscratch == noreg, \"not needed\");\n+\n@@ -200,1 +205,1 @@\n-  mov_literal32(dst, (int32_t) adr.target(), adr.rspec());\n+  mov_literal32(dst, (int32_t)adr.target(), adr.rspec());\n@@ -300,1 +305,2 @@\n-void MacroAssembler::movoop(Address dst, jobject obj) {\n+void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -308,1 +314,2 @@\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {\n+void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -312,3 +319,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n-  \/\/ scratch register is not used,\n-  \/\/ it is defined to match parameters of 64-bit version of this method.\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -322,2 +327,3 @@\n-void MacroAssembler::movptr(ArrayAddress dst, Register src) {\n-  movl(as_Address(dst), src);\n+void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n+  movl(as_Address(dst, noreg), src);\n@@ -327,1 +333,1 @@\n-  movl(dst, as_Address(src));\n+  movl(dst, as_Address(src, noreg));\n@@ -330,2 +336,2 @@\n-\/\/ src should NEVER be a real pointer. Use AddressLiteral for true pointers\n-void MacroAssembler::movptr(Address dst, intptr_t src) {\n+void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -335,1 +341,2 @@\n-void MacroAssembler::pushoop(jobject obj) {\n+void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -339,1 +346,2 @@\n-void MacroAssembler::pushklass(Metadata* obj) {\n+void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -343,1 +351,2 @@\n-void MacroAssembler::pushptr(AddressLiteral src) {\n+void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -396,1 +405,1 @@\n-  FlagSetting fs(Debugging, true);\n+  DebuggingContext debugging{};\n@@ -436,2 +445,2 @@\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr());\n+  ExternalAddress message((address)msg);\n+  pushptr(message.addr(), noreg);\n@@ -448,2 +457,2 @@\n-  ExternalAddress message((address) msg);\n-  pushptr(message.addr());\n+  ExternalAddress message((address)msg);\n+  pushptr(message.addr(), noreg);\n@@ -479,1 +488,1 @@\n-  return Address((int32_t)(intptr_t)(adr.target() - pc()), adr.target(), adr.reloc());\n+  return Address(checked_cast<int32_t>(adr.target() - pc()), adr.target(), adr.reloc());\n@@ -483,1 +492,1 @@\n-Address MacroAssembler::as_Address(ArrayAddress adr) {\n+Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n@@ -485,1 +494,1 @@\n-  lea(rscratch1, base);\n+  lea(rscratch, base);\n@@ -488,1 +497,1 @@\n-  Address array(rscratch1, index._index, index._scale, index._disp);\n+  Address array(rscratch, index._index, index._scale, index._disp);\n@@ -506,3 +515,1 @@\n-  {\n-    call(RuntimeAddress(entry_point));\n-  }\n+  call(RuntimeAddress(entry_point));\n@@ -513,3 +520,1 @@\n-  {\n-    call(RuntimeAddress(entry_point));\n-  }\n+  call(RuntimeAddress(entry_point));\n@@ -526,1 +531,1 @@\n-void MacroAssembler::cmp64(Register src1, AddressLiteral src2) {\n+void MacroAssembler::cmp64(Register src1, AddressLiteral src2, Register rscratch) {\n@@ -528,0 +533,1 @@\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n@@ -532,2 +538,2 @@\n-    lea(rscratch1, src2);\n-    Assembler::cmpq(src1, Address(rscratch1, 0));\n+    lea(rscratch, src2);\n+    Assembler::cmpq(src1, Address(rscratch, 0));\n@@ -555,1 +561,1 @@\n-  cmp64(rax, ExternalAddress((address) &min_long));\n+  cmp64(rax, ExternalAddress((address) &min_long), rdx \/*rscratch*\/);\n@@ -590,1 +596,3 @@\n-void MacroAssembler::incrementq(AddressLiteral dst) {\n+void MacroAssembler::incrementq(AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -594,2 +602,2 @@\n-    lea(rscratch1, dst);\n-    incrementq(Address(rscratch1, 0));\n+    lea(rscratch, dst);\n+    incrementq(Address(rscratch, 0));\n@@ -617,2 +625,2 @@\n-void MacroAssembler::jump(ArrayAddress entry) {\n-  lea(rscratch1, entry.base());\n+void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n+  lea(rscratch, entry.base());\n@@ -621,1 +629,1 @@\n-  dispatch._base = rscratch1;\n+  dispatch._base = rscratch;\n@@ -631,1 +639,1 @@\n-    mov_literal64(dst, (intptr_t)src.target(), src.rspec());\n+  mov_literal64(dst, (intptr_t)src.target(), src.rspec());\n@@ -634,3 +642,3 @@\n-void MacroAssembler::lea(Address dst, AddressLiteral adr) {\n-  mov_literal64(rscratch1, (intptr_t)adr.target(), adr.rspec());\n-  movptr(dst, rscratch1);\n+void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n+  lea(rscratch, adr);\n+  movptr(dst, rscratch);\n@@ -653,3 +661,3 @@\n-void MacroAssembler::movoop(Address dst, jobject obj) {\n-  mov_literal64(rscratch1, (intptr_t)obj, oop_Relocation::spec_for_immediate());\n-  movq(dst, rscratch1);\n+void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n+  mov_literal64(rscratch, (intptr_t)obj, oop_Relocation::spec_for_immediate());\n+  movq(dst, rscratch);\n@@ -662,3 +670,3 @@\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {\n-  mov_literal64(rscratch1, (intptr_t)obj, metadata_Relocation::spec_for_immediate());\n-  movq(dst, rscratch1);\n+void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n+  mov_literal64(rscratch, (intptr_t)obj, metadata_Relocation::spec_for_immediate());\n+  movq(dst, rscratch);\n@@ -667,1 +675,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -674,2 +682,2 @@\n-      lea(scratch, src);\n-      movq(dst, Address(scratch, 0));\n+      lea(dst, src);\n+      movq(dst, Address(dst, 0));\n@@ -680,2 +688,2 @@\n-void MacroAssembler::movptr(ArrayAddress dst, Register src) {\n-  movq(as_Address(dst), src);\n+void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n+  movq(as_Address(dst, rscratch), src);\n@@ -685,1 +693,1 @@\n-  movq(dst, as_Address(src));\n+  movq(dst, as_Address(src, dst \/*rscratch*\/));\n@@ -689,1 +697,1 @@\n-void MacroAssembler::movptr(Address dst, intptr_t src) {\n+void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n@@ -693,2 +701,2 @@\n-    mov64(rscratch1, src);\n-    movq(dst, rscratch1);\n+    mov64(rscratch, src);\n+    movq(dst, rscratch);\n@@ -698,12 +706,3 @@\n-\/\/ These are mostly for initializing NULL\n-void MacroAssembler::movptr(Address dst, int32_t src) {\n-  movslq(dst, src);\n-}\n-\n-void MacroAssembler::movptr(Register dst, int32_t src) {\n-  mov64(dst, (intptr_t)src);\n-}\n-\n-void MacroAssembler::pushoop(jobject obj) {\n-  movoop(rscratch1, obj);\n-  push(rscratch1);\n+void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n+  movoop(rscratch, obj);\n+  push(rscratch);\n@@ -712,3 +711,3 @@\n-void MacroAssembler::pushklass(Metadata* obj) {\n-  mov_metadata(rscratch1, obj);\n-  push(rscratch1);\n+void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n+  mov_metadata(rscratch, obj);\n+  push(rscratch);\n@@ -717,2 +716,2 @@\n-void MacroAssembler::pushptr(AddressLiteral src) {\n-  lea(rscratch1, src);\n+void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n+  lea(rscratch, src);\n@@ -720,1 +719,1 @@\n-    push(rscratch1);\n+    push(rscratch);\n@@ -722,1 +721,1 @@\n-    pushq(Address(rscratch1, 0));\n+    pushq(Address(rscratch, 0));\n@@ -732,22 +731,3 @@\n-                                         address  last_java_pc) {\n-  vzeroupper();\n-  \/\/ determine last_java_sp register\n-  if (!last_java_sp->is_valid()) {\n-    last_java_sp = rsp;\n-  }\n-\n-  \/\/ last_java_fp is optional\n-  if (last_java_fp->is_valid()) {\n-    movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()),\n-           last_java_fp);\n-  }\n-\n-  \/\/ last_java_pc is optional\n-  if (last_java_pc != NULL) {\n-    Address java_pc(r15_thread,\n-                    JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset());\n-    lea(rscratch1, InternalAddress(last_java_pc));\n-    movptr(java_pc, rscratch1);\n-  }\n-\n-  movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), last_java_sp);\n+                                         address  last_java_pc,\n+                                         Register rscratch) {\n+  set_last_Java_frame(r15_thread, last_java_sp, last_java_fp, last_java_pc, rscratch);\n@@ -798,0 +778,1 @@\n+\n@@ -799,2 +780,2 @@\n-  lea(rax, ExternalAddress(CAST_FROM_FN_PTR(address, warning)));\n-  call(rax);\n+  call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));\n+\n@@ -853,1 +834,1 @@\n-  FlagSetting fs(Debugging, true);\n+  DebuggingContext debugging{};\n@@ -915,1 +896,1 @@\n-void MacroAssembler::long_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -926,2 +907,3 @@\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n-      movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+      assert(dst.is_single_reg(), \"not a stack pair: (%s, %s), (%s, %s)\",\n+             src.first()->name(), src.second()->name(), dst.first()->name(), dst.second()->name());\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -931,1 +913,1 @@\n-    movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n+    movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -934,2 +916,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -940,1 +922,1 @@\n-void MacroAssembler::double_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -953,1 +935,1 @@\n-      movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+      movdbl(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -957,1 +939,1 @@\n-    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n+    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -960,2 +942,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -967,1 +949,1 @@\n-void MacroAssembler::float_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -975,2 +957,2 @@\n-      movl(rax, Address(rbp, reg2offset_in(src.first())));\n-      movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movl(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movptr(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -980,1 +962,1 @@\n-      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n+      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -985,1 +967,1 @@\n-    movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+    movflt(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -999,1 +981,1 @@\n-void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -1003,2 +985,2 @@\n-      movslq(rax, Address(rbp, reg2offset_in(src.first())));\n-      movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movslq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -1007,1 +989,1 @@\n-      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n+      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -1013,1 +995,1 @@\n-    movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -1056,1 +1038,1 @@\n-  \/\/ See if oop is NULL if it is we need no handle\n+  \/\/ See if oop is null if it is we need no handle\n@@ -1067,1 +1049,1 @@\n-    cmpptr(Address(rbp, reg2offset_in(src.first())), (int32_t)NULL_WORD);\n+    cmpptr(Address(rbp, reg2offset_in(src.first())), NULL_WORD);\n@@ -1069,1 +1051,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -1073,2 +1055,2 @@\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+    \/\/ Oop is in a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-null\n@@ -1097,1 +1079,1 @@\n-    \/\/ Store oop in handle area, may be NULL\n+    \/\/ Store oop in handle area, may be null\n@@ -1103,1 +1085,1 @@\n-    cmpptr(rOop, (int32_t)NULL_WORD);\n+    cmpptr(rOop, NULL_WORD);\n@@ -1105,1 +1087,1 @@\n-    \/\/ conditionally move a NULL from the handle area where it was just stored\n+    \/\/ conditionally move a null from the handle area where it was just stored\n@@ -1131,1 +1113,3 @@\n-void MacroAssembler::addsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::addsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1135,2 +1119,2 @@\n-    lea(rscratch1, src);\n-    Assembler::addsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::addsd(dst, Address(rscratch, 0));\n@@ -1140,1 +1124,3 @@\n-void MacroAssembler::addss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::addss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1144,2 +1130,2 @@\n-    lea(rscratch1, src);\n-    addss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    addss(dst, Address(rscratch, 0));\n@@ -1149,1 +1135,3 @@\n-void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1153,2 +1141,2 @@\n-    lea(rscratch1, src);\n-    Assembler::addpd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::addpd(dst, Address(rscratch, 0));\n@@ -1181,1 +1169,21 @@\n-void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::push_f(XMMRegister r) {\n+  subptr(rsp, wordSize);\n+  movflt(Address(rsp, 0), r);\n+}\n+\n+void MacroAssembler::pop_f(XMMRegister r) {\n+  movflt(r, Address(rsp, 0));\n+  addptr(rsp, wordSize);\n+}\n+\n+void MacroAssembler::push_d(XMMRegister r) {\n+  subptr(rsp, 2 * wordSize);\n+  movdbl(Address(rsp, 0), r);\n+}\n+\n+void MacroAssembler::pop_d(XMMRegister r) {\n+  movdbl(r, Address(rsp, 0));\n+  addptr(rsp, 2 * Interpreter::stackElementSize);\n+}\n+\n+void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n@@ -1184,0 +1192,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1187,2 +1197,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::andpd(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::andpd(dst, Address(rscratch, 0));\n@@ -1192,1 +1202,1 @@\n-void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register rscratch) {\n@@ -1195,0 +1205,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1198,2 +1210,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::andps(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::andps(dst, Address(rscratch, 0));\n@@ -1207,0 +1219,13 @@\n+#ifdef _LP64\n+void MacroAssembler::andq(Register dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    andq(dst, as_Address(src));\n+  } else {\n+    lea(rscratch, src);\n+    andq(dst, Address(rscratch, 0));\n+  }\n+}\n+#endif\n+\n@@ -1212,1 +1237,3 @@\n-void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register scr) {\n+void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(counter_addr), \"missing\");\n+\n@@ -1216,2 +1243,2 @@\n-    lea(scr, counter_addr);\n-    atomic_incl(Address(scr, 0));\n+    lea(rscratch, counter_addr);\n+    atomic_incl(Address(rscratch, 0));\n@@ -1227,1 +1254,3 @@\n-void MacroAssembler::atomic_incq(AddressLiteral counter_addr, Register scr) {\n+void MacroAssembler::atomic_incq(AddressLiteral counter_addr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(counter_addr), \"missing\");\n+\n@@ -1231,2 +1260,2 @@\n-    lea(scr, counter_addr);\n-    atomic_incq(Address(scr, 0));\n+    lea(rscratch, counter_addr);\n+    atomic_incq(Address(rscratch, 0));\n@@ -1246,3 +1275,3 @@\n-  movl(Address(tmp, (-os::vm_page_size())), size );\n-  subptr(tmp, os::vm_page_size());\n-  subl(size, os::vm_page_size());\n+  movl(Address(tmp, (-(int)os::vm_page_size())), size );\n+  subptr(tmp, (int)os::vm_page_size());\n+  subl(size, (int)os::vm_page_size());\n@@ -1257,1 +1286,1 @@\n-  for (int i = 1; i < ((int)StackOverflow::stack_shadow_zone_size() \/ os::vm_page_size()); i++) {\n+  for (int i = 1; i < ((int)StackOverflow::stack_shadow_zone_size() \/ (int)os::vm_page_size()); i++) {\n@@ -1260,1 +1289,1 @@\n-    movptr(Address(tmp, (-i*os::vm_page_size())), size );\n+    movptr(Address(tmp, (-i*(int)os::vm_page_size())), size );\n@@ -1265,4 +1294,4 @@\n-    \/\/ testing if reserved zone needs to be enabled\n-    Label no_reserved_zone_enabling;\n-    Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-    NOT_LP64(get_thread(rsi);)\n+  \/\/ testing if reserved zone needs to be enabled\n+  Label no_reserved_zone_enabling;\n+  Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n+  NOT_LP64(get_thread(rsi);)\n@@ -1270,2 +1299,2 @@\n-    cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));\n-    jcc(Assembler::below, no_reserved_zone_enabling);\n+  cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));\n+  jcc(Assembler::below, no_reserved_zone_enabling);\n@@ -1273,3 +1302,3 @@\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);\n-    jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));\n-    should_not_reach_here();\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);\n+  jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));\n+  should_not_reach_here();\n@@ -1277,1 +1306,1 @@\n-    bind(no_reserved_zone_enabling);\n+  bind(no_reserved_zone_enabling);\n@@ -1298,1 +1327,3 @@\n-void MacroAssembler::call(AddressLiteral entry) {\n+void MacroAssembler::call(AddressLiteral entry, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(entry), \"missing\");\n+\n@@ -1302,2 +1333,2 @@\n-    lea(rscratch1, entry);\n-    Assembler::call(rscratch1);\n+    lea(rscratch, entry);\n+    Assembler::call(rscratch);\n@@ -1313,0 +1344,7 @@\n+void MacroAssembler::emit_static_call_stub() {\n+  \/\/ Static stub relocation also tags the Method* in the code-stream.\n+  mov_metadata(rbx, (Metadata*) nullptr);  \/\/ Method is zapped till fixup time.\n+  \/\/ This is recognized as unresolved by relocs\/nativeinst\/ic code.\n+  jump(RuntimeAddress(pc()));\n+}\n+\n@@ -1526,1 +1564,1 @@\n-  set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);\n+  set_last_Java_frame(java_thread, last_java_sp, rbp, nullptr, rscratch1);\n@@ -1561,1 +1599,1 @@\n-    cmpptr(Address(java_thread, Thread::pending_exception_offset()), (int32_t) NULL_WORD);\n+    cmpptr(Address(java_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -1638,0 +1676,14 @@\n+void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {\n+  LP64_ONLY(assert(arg_0 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  pass_arg3(this, arg_3);\n+  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  pass_arg2(this, arg_2);\n+  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n+  pass_arg1(this, arg_1);\n+  pass_arg0(this, arg_0);\n+  call_VM_leaf(entry_point, 3);\n+}\n+\n@@ -1692,1 +1744,3 @@\n-void MacroAssembler::cmp32(AddressLiteral src1, int32_t imm) {\n+void MacroAssembler::cmp32(AddressLiteral src1, int32_t imm, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src1), \"missing\");\n+\n@@ -1696,2 +1750,2 @@\n-    lea(rscratch1, src1);\n-    cmpl(Address(rscratch1, 0), imm);\n+    lea(rscratch, src1);\n+    cmpl(Address(rscratch, 0), imm);\n@@ -1701,1 +1755,1 @@\n-void MacroAssembler::cmp32(Register src1, AddressLiteral src2) {\n+void MacroAssembler::cmp32(Register src1, AddressLiteral src2, Register rscratch) {\n@@ -1703,0 +1757,2 @@\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n+\n@@ -1706,2 +1762,2 @@\n-    lea(rscratch1, src2);\n-    cmpl(src1, Address(rscratch1, 0));\n+    lea(rscratch, src2);\n+    cmpl(src1, Address(rscratch, 0));\n@@ -1764,1 +1820,3 @@\n-void MacroAssembler::cmp8(AddressLiteral src1, int imm) {\n+void MacroAssembler::cmp8(AddressLiteral src1, int imm, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src1), \"missing\");\n+\n@@ -1768,2 +1826,2 @@\n-    lea(rscratch1, src1);\n-    cmpb(Address(rscratch1, 0), imm);\n+    lea(rscratch, src1);\n+    cmpb(Address(rscratch, 0), imm);\n@@ -1773,1 +1831,1 @@\n-void MacroAssembler::cmpptr(Register src1, AddressLiteral src2) {\n+void MacroAssembler::cmpptr(Register src1, AddressLiteral src2, Register rscratch) {\n@@ -1775,0 +1833,2 @@\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n+\n@@ -1776,2 +1836,2 @@\n-    movptr(rscratch1, src2);\n-    Assembler::cmpq(src1, rscratch1);\n+    movptr(rscratch, src2);\n+    Assembler::cmpq(src1, rscratch);\n@@ -1781,2 +1841,2 @@\n-    lea(rscratch1, src2);\n-    Assembler::cmpq(src1, Address(rscratch1, 0));\n+    lea(rscratch, src2);\n+    Assembler::cmpq(src1, Address(rscratch, 0));\n@@ -1785,0 +1845,1 @@\n+  assert(rscratch == noreg, \"not needed\");\n@@ -1786,1 +1847,1 @@\n-    cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());\n+    cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n@@ -1793,1 +1854,1 @@\n-void MacroAssembler::cmpptr(Address src1, AddressLiteral src2) {\n+void MacroAssembler::cmpptr(Address src1, AddressLiteral src2, Register rscratch) {\n@@ -1797,2 +1858,2 @@\n-  movptr(rscratch1, src2);\n-  Assembler::cmpq(src1, rscratch1);\n+  movptr(rscratch, src2);\n+  Assembler::cmpq(src1, rscratch);\n@@ -1800,1 +1861,2 @@\n-  cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());\n+  assert(rscratch == noreg, \"not needed\");\n+  cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n@@ -1813,3 +1875,3 @@\n-void MacroAssembler::cmpoop(Register src1, jobject src2) {\n-  movoop(rscratch1, src2);\n-  cmpptr(src1, rscratch1);\n+void MacroAssembler::cmpoop(Register src1, jobject src2, Register rscratch) {\n+  movoop(rscratch, src2);\n+  cmpptr(src1, rscratch);\n@@ -1819,1 +1881,3 @@\n-void MacroAssembler::locked_cmpxchgptr(Register reg, AddressLiteral adr) {\n+void MacroAssembler::locked_cmpxchgptr(Register reg, AddressLiteral adr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(adr), \"missing\");\n+\n@@ -1824,1 +1888,1 @@\n-    lea(rscratch1, adr);\n+    lea(rscratch, adr);\n@@ -1826,1 +1890,1 @@\n-    cmpxchgptr(reg, Address(rscratch1, 0));\n+    cmpxchgptr(reg, Address(rscratch, 0));\n@@ -1834,1 +1898,3 @@\n-void MacroAssembler::comisd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::comisd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1838,2 +1904,2 @@\n-    lea(rscratch1, src);\n-    Assembler::comisd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::comisd(dst, Address(rscratch, 0));\n@@ -1843,1 +1909,3 @@\n-void MacroAssembler::comiss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::comiss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1847,2 +1915,2 @@\n-    lea(rscratch1, src);\n-    Assembler::comiss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::comiss(dst, Address(rscratch, 0));\n@@ -1853,1 +1921,3 @@\n-void MacroAssembler::cond_inc32(Condition cond, AddressLiteral counter_addr) {\n+void MacroAssembler::cond_inc32(Condition cond, AddressLiteral counter_addr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(counter_addr), \"missing\");\n+\n@@ -1858,1 +1928,1 @@\n-  atomic_incl(counter_addr);\n+  atomic_incl(counter_addr, rscratch);\n@@ -1918,1 +1988,1 @@\n-  assert (shift_value > 0, \"illegal shift value\");\n+  assert(shift_value > 0, \"illegal shift value\");\n@@ -1934,1 +2004,3 @@\n-void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1938,2 +2010,2 @@\n-    lea(rscratch1, src);\n-    Assembler::divsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::divsd(dst, Address(rscratch, 0));\n@@ -1943,1 +2015,3 @@\n-void MacroAssembler::divss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::divss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1947,2 +2021,2 @@\n-    lea(rscratch1, src);\n-    Assembler::divss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::divss(dst, Address(rscratch, 0));\n@@ -1957,0 +2031,14 @@\n+void MacroAssembler::post_call_nop() {\n+  if (!Continuations::enabled()) {\n+    return;\n+  }\n+  InstructionMark im(this);\n+  relocate(post_call_nop_Relocation::spec());\n+  InlineSkippedInstructionsCounter skipCounter(this);\n+  emit_int8((int8_t)0x0f);\n+  emit_int8((int8_t)0x1f);\n+  emit_int8((int8_t)0x84);\n+  emit_int8((int8_t)0x00);\n+  emit_int32(0x00);\n+}\n+\n@@ -1962,5 +2050,5 @@\n-    emit_int8(0x26); \/\/ es:\n-    emit_int8(0x2e); \/\/ cs:\n-    emit_int8(0x64); \/\/ fs:\n-    emit_int8(0x65); \/\/ gs:\n-    emit_int8((unsigned char)0x90);\n+    emit_int8((int8_t)0x26); \/\/ es:\n+    emit_int8((int8_t)0x2e); \/\/ cs:\n+    emit_int8((int8_t)0x64); \/\/ fs:\n+    emit_int8((int8_t)0x65); \/\/ gs:\n+    emit_int8((int8_t)0x90);\n@@ -2045,1 +2133,1 @@\n-  Assembler::fldcw(as_Address(src));\n+  fldcw(as_Address(src));\n@@ -2079,1 +2167,2 @@\n-void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2083,2 +2172,2 @@\n-    lea(rscratch1, src);\n-    Assembler::mulpd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::mulpd(dst, Address(rscratch, 0));\n@@ -2200,1 +2289,3 @@\n-void MacroAssembler::incrementl(AddressLiteral dst) {\n+void MacroAssembler::incrementl(AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2204,2 +2295,2 @@\n-    lea(rscratch1, dst);\n-    incrementl(Address(rscratch1, 0));\n+    lea(rscratch, dst);\n+    incrementl(Address(rscratch, 0));\n@@ -2209,2 +2300,2 @@\n-void MacroAssembler::incrementl(ArrayAddress dst) {\n-  incrementl(as_Address(dst));\n+void MacroAssembler::incrementl(ArrayAddress dst, Register rscratch) {\n+  incrementl(as_Address(dst, rscratch));\n@@ -2229,1 +2320,3 @@\n-void MacroAssembler::jump(AddressLiteral dst) {\n+void MacroAssembler::jump(AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2233,2 +2326,2 @@\n-    lea(rscratch1, dst);\n-    jmp(rscratch1);\n+    lea(rscratch, dst);\n+    jmp(rscratch);\n@@ -2238,1 +2331,3 @@\n-void MacroAssembler::jump_cc(Condition cc, AddressLiteral dst) {\n+void MacroAssembler::jump_cc(Condition cc, AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2261,2 +2356,2 @@\n-    lea(rscratch1, dst);\n-    Assembler::jmp(rscratch1);\n+    lea(rscratch, dst);\n+    Assembler::jmp(rscratch);\n@@ -2267,3 +2362,2 @@\n-void MacroAssembler::fld_x(AddressLiteral src) {\n-  Assembler::fld_x(as_Address(src));\n-}\n+void MacroAssembler::ldmxcsr(AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2271,1 +2365,0 @@\n-void MacroAssembler::ldmxcsr(AddressLiteral src, Register scratchReg) {\n@@ -2275,2 +2368,2 @@\n-    lea(scratchReg, src);\n-    Assembler::ldmxcsr(Address(scratchReg, 0));\n+    lea(rscratch, src);\n+    Assembler::ldmxcsr(Address(rscratch, 0));\n@@ -2380,1 +2473,3 @@\n-void MacroAssembler::mov32(AddressLiteral dst, Register src) {\n+void MacroAssembler::mov32(AddressLiteral dst, Register src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2384,2 +2479,2 @@\n-    lea(rscratch1, dst);\n-    movl(Address(rscratch1, 0), src);\n+    lea(rscratch, dst);\n+    movl(Address(rscratch, 0), src);\n@@ -2393,2 +2488,2 @@\n-    lea(rscratch1, src);\n-    movl(dst, Address(rscratch1, 0));\n+    lea(dst, src);\n+    movl(dst, Address(dst, 0));\n@@ -2436,3 +2531,2 @@\n-void MacroAssembler::movbyte(ArrayAddress dst, int src) {\n-  movb(as_Address(dst), src);\n-}\n+void MacroAssembler::movdl(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2440,1 +2534,0 @@\n-void MacroAssembler::movdl(XMMRegister dst, AddressLiteral src) {\n@@ -2444,2 +2537,2 @@\n-    lea(rscratch1, src);\n-    movdl(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    movdl(dst, Address(rscratch, 0));\n@@ -2449,1 +2542,3 @@\n-void MacroAssembler::movq(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movq(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2453,2 +2548,2 @@\n-    lea(rscratch1, src);\n-    movq(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    movq(dst, Address(rscratch, 0));\n@@ -2458,1 +2553,3 @@\n-void MacroAssembler::movdbl(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movdbl(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2466,1 +2563,1 @@\n-    lea(rscratch1, src);\n+    lea(rscratch, src);\n@@ -2468,1 +2565,1 @@\n-      movsd (dst, Address(rscratch1, 0));\n+      movsd (dst, Address(rscratch, 0));\n@@ -2470,1 +2567,1 @@\n-      movlpd(dst, Address(rscratch1, 0));\n+      movlpd(dst, Address(rscratch, 0));\n@@ -2475,1 +2572,3 @@\n-void MacroAssembler::movflt(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movflt(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2479,2 +2578,2 @@\n-    lea(rscratch1, src);\n-    movss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    movss(dst, Address(rscratch, 0));\n@@ -2501,0 +2600,4 @@\n+void MacroAssembler::movptr(Address dst, int32_t src) {\n+  LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src));\n+}\n+\n@@ -2502,2 +2605,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2507,2 +2610,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2512,2 +2615,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2516,1 +2619,3 @@\n-void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg) {\n+void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2520,2 +2625,2 @@\n-    lea(scratchReg, src);\n-    movdqu(dst, Address(scratchReg, 0));\n+    lea(rscratch, src);\n+    movdqu(dst, Address(rscratch, 0));\n@@ -2526,2 +2631,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2531,2 +2636,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2536,2 +2641,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2540,1 +2645,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2545,2 +2652,2 @@\n-    lea(scratch_reg, src);\n-    vmovdqu(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    vmovdqu(dst, Address(rscratch, 0));\n@@ -2550,2 +2657,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len) {\n-  assert(vector_len <= AVX_512bit, \"unexpected vector length\");\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2553,1 +2661,1 @@\n-    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+    evmovdquq(dst, src, AVX_512bit, rscratch);\n@@ -2555,1 +2663,1 @@\n-    vmovdqu(dst, src, scratch_reg);\n+    vmovdqu(dst, src, rscratch);\n@@ -2557,1 +2665,1 @@\n-    movdqu(dst, src, scratch_reg);\n+    movdqu(dst, src, rscratch);\n@@ -2606,1 +2714,3 @@\n-void MacroAssembler::kmovql(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::kmovql(KRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2610,2 +2720,2 @@\n-    lea(scratch_reg, src);\n-    kmovql(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    kmovql(dst, Address(rscratch, 0));\n@@ -2615,1 +2725,3 @@\n-void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2619,2 +2731,2 @@\n-    lea(scratch_reg, src);\n-    kmovwl(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    kmovwl(dst, Address(rscratch, 0));\n@@ -2625,1 +2737,3 @@\n-                               int vector_len, Register scratch_reg) {\n+                               int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2627,5 +2741,1 @@\n-    if (mask == k0) {\n-      Assembler::evmovdqub(dst, as_Address(src), merge, vector_len);\n-    } else {\n-      Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n-    }\n+    Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n@@ -2633,6 +2743,2 @@\n-    lea(scratch_reg, src);\n-    if (mask == k0) {\n-      Assembler::evmovdqub(dst, Address(scratch_reg, 0), merge, vector_len);\n-    } else {\n-      Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n-    }\n+    lea(rscratch, src);\n+    Assembler::evmovdqub(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2643,1 +2749,3 @@\n-                               int vector_len, Register scratch_reg) {\n+                               int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2647,2 +2755,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdquw(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdquw(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2652,2 +2760,3 @@\n-void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register scratch_reg) {\n+void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2657,2 +2766,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdqul(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdqul(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2662,2 +2771,3 @@\n-void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register scratch_reg) {\n+void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2667,2 +2777,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdquq(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdquq(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2673,0 +2783,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2681,1 +2793,3 @@\n-void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2685,2 +2799,2 @@\n-    lea(rscratch1, src);\n-    Assembler::movdqa(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::movdqa(dst, Address(rscratch, 0));\n@@ -2690,1 +2804,3 @@\n-void MacroAssembler::movsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2694,2 +2810,2 @@\n-    lea(rscratch1, src);\n-    Assembler::movsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::movsd(dst, Address(rscratch, 0));\n@@ -2699,1 +2815,3 @@\n-void MacroAssembler::movss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2703,2 +2821,13 @@\n-    lea(rscratch1, src);\n-    Assembler::movss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::movss(dst, Address(rscratch, 0));\n+  }\n+}\n+\n+void MacroAssembler::movddup(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::movddup(dst, as_Address(src));\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::movddup(dst, Address(rscratch, 0));\n@@ -2709,0 +2838,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2717,1 +2848,3 @@\n-void MacroAssembler::mulsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::mulsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2721,2 +2854,2 @@\n-    lea(rscratch1, src);\n-    Assembler::mulsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::mulsd(dst, Address(rscratch, 0));\n@@ -2726,1 +2859,3 @@\n-void MacroAssembler::mulss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::mulss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2730,2 +2865,2 @@\n-    lea(rscratch1, src);\n-    Assembler::mulss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::mulss(dst, Address(rscratch, 0));\n@@ -2737,1 +2872,1 @@\n-    \/\/ provoke OS NULL exception if reg = NULL by\n+    \/\/ provoke OS null exception if reg is null by\n@@ -2746,1 +2881,1 @@\n-    \/\/ will provoke OS NULL exception if reg = NULL\n+    \/\/ will provoke OS null exception if reg is null\n@@ -2757,1 +2892,1 @@\n-  const char* buf = NULL;\n+  const char* buf = nullptr;\n@@ -2816,0 +2951,103 @@\n+void MacroAssembler::push_cont_fastpath() {\n+  if (!Continuations::enabled()) return;\n+\n+#ifndef _LP64\n+  Register rthread = rax;\n+  Register rrealsp = rbx;\n+  push(rthread);\n+  push(rrealsp);\n+\n+  get_thread(rthread);\n+\n+  \/\/ The code below wants the original RSP.\n+  \/\/ Move it back after the pushes above.\n+  movptr(rrealsp, rsp);\n+  addptr(rrealsp, 2*wordSize);\n+#else\n+  Register rthread = r15_thread;\n+  Register rrealsp = rsp;\n+#endif\n+\n+  Label done;\n+  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::belowEqual, done);\n+  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), rrealsp);\n+  bind(done);\n+\n+#ifndef _LP64\n+  pop(rrealsp);\n+  pop(rthread);\n+#endif\n+}\n+\n+void MacroAssembler::pop_cont_fastpath() {\n+  if (!Continuations::enabled()) return;\n+\n+#ifndef _LP64\n+  Register rthread = rax;\n+  Register rrealsp = rbx;\n+  push(rthread);\n+  push(rrealsp);\n+\n+  get_thread(rthread);\n+\n+  \/\/ The code below wants the original RSP.\n+  \/\/ Move it back after the pushes above.\n+  movptr(rrealsp, rsp);\n+  addptr(rrealsp, 2*wordSize);\n+#else\n+  Register rthread = r15_thread;\n+  Register rrealsp = rsp;\n+#endif\n+\n+  Label done;\n+  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::below, done);\n+  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), 0);\n+  bind(done);\n+\n+#ifndef _LP64\n+  pop(rrealsp);\n+  pop(rthread);\n+#endif\n+}\n+\n+void MacroAssembler::inc_held_monitor_count() {\n+#ifndef _LP64\n+  Register thread = rax;\n+  push(thread);\n+  get_thread(thread);\n+  incrementl(Address(thread, JavaThread::held_monitor_count_offset()));\n+  pop(thread);\n+#else \/\/ LP64\n+  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+}\n+\n+void MacroAssembler::dec_held_monitor_count() {\n+#ifndef _LP64\n+  Register thread = rax;\n+  push(thread);\n+  get_thread(thread);\n+  decrementl(Address(thread, JavaThread::held_monitor_count_offset()));\n+  pop(thread);\n+#else \/\/ LP64\n+  decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+}\n+\n+#ifdef ASSERT\n+void MacroAssembler::stop_if_in_cont(Register cont, const char* name) {\n+#ifdef _LP64\n+  Label no_cont;\n+  movptr(cont, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  testl(cont, cont);\n+  jcc(Assembler::zero, no_cont);\n+  stop(name);\n+  bind(no_cont);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+#endif\n+\n@@ -2868,1 +3106,2 @@\n-                                         address  last_java_pc) {\n+                                         address  last_java_pc,\n+                                         Register rscratch) {\n@@ -2879,2 +3118,0 @@\n-\n-\n@@ -2885,7 +3122,4 @@\n-\n-\n-  if (last_java_pc != NULL) {\n-    lea(Address(java_thread,\n-                 JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset()),\n-        InternalAddress(last_java_pc));\n-\n+  if (last_java_pc != nullptr) {\n+    Address java_pc(java_thread,\n+                    JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset());\n+    lea(java_pc, InternalAddress(last_java_pc), rscratch);\n@@ -2923,0 +3157,16 @@\n+void MacroAssembler::testl(Address dst, int32_t imm32) {\n+  if (imm32 >= 0 && is8bit(imm32)) {\n+    testb(dst, imm32);\n+  } else {\n+    Assembler::testl(dst, imm32);\n+  }\n+}\n+\n+void MacroAssembler::testl(Register dst, int32_t imm32) {\n+  if (imm32 >= 0 && is8bit(imm32) && dst->has_byte_register()) {\n+    testb(dst, imm32);\n+  } else {\n+    Assembler::testl(dst, imm32);\n+  }\n+}\n+\n@@ -2924,1 +3174,1 @@\n-  assert(reachable(src), \"Address should be reachable\");\n+  assert(always_reachable(src), \"Address should be reachable\");\n@@ -2928,0 +3178,20 @@\n+#ifdef _LP64\n+\n+void MacroAssembler::testq(Address dst, int32_t imm32) {\n+  if (imm32 >= 0) {\n+    testl(dst, imm32);\n+  } else {\n+    Assembler::testq(dst, imm32);\n+  }\n+}\n+\n+void MacroAssembler::testq(Register dst, int32_t imm32) {\n+  if (imm32 >= 0) {\n+    testl(dst, imm32);\n+  } else {\n+    Assembler::testq(dst, imm32);\n+  }\n+}\n+\n+#endif\n+\n@@ -2968,8 +3238,2 @@\n-void MacroAssembler::sqrtsd(XMMRegister dst, AddressLiteral src) {\n-  if (reachable(src)) {\n-    Assembler::sqrtsd(dst, as_Address(src));\n-  } else {\n-    lea(rscratch1, src);\n-    Assembler::sqrtsd(dst, Address(rscratch1, 0));\n-  }\n-}\n+void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2977,1 +3241,0 @@\n-void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src) {\n@@ -2981,2 +3244,2 @@\n-    lea(rscratch1, src);\n-    Assembler::sqrtss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::sqrtss(dst, Address(rscratch, 0));\n@@ -2986,1 +3249,3 @@\n-void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2990,2 +3255,2 @@\n-    lea(rscratch1, src);\n-    Assembler::subsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::subsd(dst, Address(rscratch, 0));\n@@ -2995,1 +3260,3 @@\n-void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {\n+void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2999,2 +3266,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);\n+    lea(rscratch, src);\n+    Assembler::roundsd(dst, Address(rscratch, 0), rmode);\n@@ -3004,1 +3271,3 @@\n-void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::subss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3008,2 +3277,2 @@\n-    lea(rscratch1, src);\n-    Assembler::subss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::subss(dst, Address(rscratch, 0));\n@@ -3013,1 +3282,3 @@\n-void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3017,2 +3288,2 @@\n-    lea(rscratch1, src);\n-    Assembler::ucomisd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::ucomisd(dst, Address(rscratch, 0));\n@@ -3022,1 +3293,3 @@\n-void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3026,2 +3299,2 @@\n-    lea(rscratch1, src);\n-    Assembler::ucomiss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::ucomiss(dst, Address(rscratch, 0));\n@@ -3031,1 +3304,3 @@\n-void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3037,2 +3312,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorpd(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorpd(dst, Address(rscratch, 0));\n@@ -3059,1 +3334,3 @@\n-void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3065,2 +3342,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorps(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorps(dst, Address(rscratch, 0));\n@@ -3070,1 +3347,3 @@\n-void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3077,2 +3356,2 @@\n-    lea(rscratch1, src);\n-    Assembler::pshufb(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::pshufb(dst, Address(rscratch, 0));\n@@ -3084,1 +3363,3 @@\n-void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3088,2 +3369,2 @@\n-    lea(rscratch1, src);\n-    vaddsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vaddsd(dst, nds, Address(rscratch, 0));\n@@ -3093,1 +3374,3 @@\n-void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3097,2 +3380,2 @@\n-    lea(rscratch1, src);\n-    vaddss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vaddss(dst, nds, Address(rscratch, 0));\n@@ -3104,0 +3387,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3114,0 +3399,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3122,1 +3409,1 @@\n-void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {\n+void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch) {\n@@ -3124,1 +3411,3 @@\n-  vandps(dst, nds, negate_field, vector_len);\n+  assert(rscratch != noreg || always_reachable(negate_field), \"missing\");\n+\n+  vandps(dst, nds, negate_field, vector_len, rscratch);\n@@ -3127,1 +3416,1 @@\n-void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {\n+void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch) {\n@@ -3129,1 +3418,3 @@\n-  vandpd(dst, nds, negate_field, vector_len);\n+  assert(rscratch != noreg || always_reachable(negate_field), \"missing\");\n+\n+  vandpd(dst, nds, negate_field, vector_len, rscratch);\n@@ -3152,1 +3443,3 @@\n-void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3156,2 +3449,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpand(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3161,3 +3454,9 @@\n-void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(((dst->encoding() < 16 && src->encoding() < 16) || VM_Version::supports_avx512vlbw()),\"XMM register should be 0-15\");\n-  Assembler::vpbroadcastw(dst, src, vector_len);\n+void MacroAssembler::vpbroadcastd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::vpbroadcastd(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpbroadcastd(dst, Address(rscratch, 0), vector_len);\n+  }\n@@ -3167,0 +3466,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3176,0 +3477,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3184,0 +3487,11 @@\n+void MacroAssembler::vbroadcastss(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::vbroadcastss(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vbroadcastss(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n@@ -3194,2 +3508,3 @@\n-void MacroAssembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds,\n-                               AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3199,2 +3514,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpeqd(kdst, mask, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpeqd(kdst, mask, nds, Address(rscratch, 0), vector_len);\n@@ -3205,1 +3520,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3209,2 +3526,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpd(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpd(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3215,1 +3532,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3219,2 +3538,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpq(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpq(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3225,1 +3544,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3229,2 +3550,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpb(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpb(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3235,1 +3556,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3239,2 +3562,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpw(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpw(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3309,1 +3632,1 @@\n-void MacroAssembler::vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n@@ -3311,0 +3634,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3314,2 +3639,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpmulld(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpmulld(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3405,1 +3730,3 @@\n-void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3409,2 +3736,2 @@\n-    lea(scratch_reg, src);\n-    vandpd(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vandpd(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3414,1 +3741,3 @@\n-void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3418,2 +3747,2 @@\n-    lea(scratch_reg, src);\n-    vandps(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vandps(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3424,1 +3753,3 @@\n-                            bool merge, int vector_len, Register scratch_reg) {\n+                            bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3428,2 +3759,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpord(dst, mask, nds, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpord(dst, mask, nds, Address(rscratch, 0), merge, vector_len);\n@@ -3433,1 +3764,3 @@\n-void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3437,2 +3770,2 @@\n-    lea(rscratch1, src);\n-    vdivsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vdivsd(dst, nds, Address(rscratch, 0));\n@@ -3442,1 +3775,3 @@\n-void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3446,2 +3781,2 @@\n-    lea(rscratch1, src);\n-    vdivss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vdivss(dst, nds, Address(rscratch, 0));\n@@ -3451,1 +3786,3 @@\n-void MacroAssembler::vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3455,2 +3792,2 @@\n-    lea(rscratch1, src);\n-    vmulsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vmulsd(dst, nds, Address(rscratch, 0));\n@@ -3460,1 +3797,3 @@\n-void MacroAssembler::vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3464,2 +3803,2 @@\n-    lea(rscratch1, src);\n-    vmulss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vmulss(dst, nds, Address(rscratch, 0));\n@@ -3469,1 +3808,3 @@\n-void MacroAssembler::vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3473,2 +3814,2 @@\n-    lea(rscratch1, src);\n-    vsubsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vsubsd(dst, nds, Address(rscratch, 0));\n@@ -3478,1 +3819,3 @@\n-void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3482,2 +3825,2 @@\n-    lea(rscratch1, src);\n-    vsubss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vsubss(dst, nds, Address(rscratch, 0));\n@@ -3487,1 +3830,1 @@\n-void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n@@ -3489,1 +3832,3 @@\n-  vxorps(dst, nds, src, Assembler::AVX_128bit);\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  vxorps(dst, nds, src, Assembler::AVX_128bit, rscratch);\n@@ -3492,1 +3837,1 @@\n-void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n@@ -3494,1 +3839,3 @@\n-  vxorpd(dst, nds, src, Assembler::AVX_128bit);\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  vxorpd(dst, nds, src, Assembler::AVX_128bit, rscratch);\n@@ -3497,1 +3844,3 @@\n-void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3501,2 +3850,2 @@\n-    lea(scratch_reg, src);\n-    vxorpd(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vxorpd(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3506,1 +3855,3 @@\n-void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3510,2 +3861,2 @@\n-    lea(scratch_reg, src);\n-    vxorps(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vxorps(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3515,1 +3866,3 @@\n-void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3520,2 +3873,2 @@\n-      lea(scratch_reg, src);\n-      Assembler::vpxor(dst, nds, Address(scratch_reg, 0), vector_len);\n+      lea(rscratch, src);\n+      Assembler::vpxor(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3523,3 +3876,2 @@\n-  }\n-  else {\n-    MacroAssembler::vxorpd(dst, nds, src, vector_len, scratch_reg);\n+  } else {\n+    MacroAssembler::vxorpd(dst, nds, src, vector_len, rscratch);\n@@ -3529,1 +3881,3 @@\n-void MacroAssembler::vpermd(XMMRegister dst,  XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpermd(XMMRegister dst,  XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3533,2 +3887,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpermd(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpermd(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3538,3 +3892,3 @@\n-void MacroAssembler::clear_jweak_tag(Register possibly_jweak) {\n-  const int32_t inverted_jweak_mask = ~static_cast<int32_t>(JNIHandles::weak_tag_mask);\n-  STATIC_ASSERT(inverted_jweak_mask == -2); \/\/ otherwise check this code\n+void MacroAssembler::clear_jobject_tag(Register possibly_non_local) {\n+  const int32_t inverted_mask = ~static_cast<int32_t>(JNIHandles::tag_mask);\n+  STATIC_ASSERT(inverted_mask == -4); \/\/ otherwise check this code\n@@ -3542,1 +3896,1 @@\n-  andptr(possibly_jweak, inverted_jweak_mask);\n+  andptr(possibly_non_local, inverted_mask);\n@@ -3549,1 +3903,1 @@\n-  Label done, not_weak;\n+  Label done, tagged, weak_tagged;\n@@ -3551,3 +3905,19 @@\n-  jcc(Assembler::zero, done);                \/\/ Use NULL as-is.\n-  testptr(value, JNIHandles::weak_tag_mask); \/\/ Test for jweak tag.\n-  jcc(Assembler::zero, not_weak);\n+  jcc(Assembler::zero, done);           \/\/ Use null as-is.\n+  testptr(value, JNIHandles::tag_mask); \/\/ Test for tag.\n+  jcc(Assembler::notZero, tagged);\n+\n+  \/\/ Resolve local handle\n+  access_load_at(T_OBJECT, IN_NATIVE | AS_RAW, value, Address(value, 0), tmp, thread);\n+  verify_oop(value);\n+  jmp(done);\n+\n+  bind(tagged);\n+  testptr(value, JNIHandles::TypeTag::weak_global); \/\/ Test for weak tag.\n+  jcc(Assembler::notZero, weak_tagged);\n+\n+  \/\/ Resolve global handle\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp, thread);\n+  verify_oop(value);\n+  jmp(done);\n+\n+  bind(weak_tagged);\n@@ -3556,1 +3926,1 @@\n-                 value, Address(value, -JNIHandles::weak_tag_value), tmp, thread);\n+                 value, Address(value, -JNIHandles::TypeTag::weak_global), tmp, thread);\n@@ -3558,4 +3928,25 @@\n-  jmp(done);\n-  bind(not_weak);\n-  \/\/ Resolve (untagged) jobject.\n-  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);\n+\n+  bind(done);\n+}\n+\n+void MacroAssembler::resolve_global_jobject(Register value,\n+                                            Register thread,\n+                                            Register tmp) {\n+  assert_different_registers(value, thread, tmp);\n+  Label done;\n+\n+  testptr(value, value);\n+  jcc(Assembler::zero, done);           \/\/ Use null as-is.\n+\n+#ifdef ASSERT\n+  {\n+    Label valid_global_tag;\n+    testptr(value, JNIHandles::TypeTag::global); \/\/ Test for global tag.\n+    jcc(Assembler::notZero, valid_global_tag);\n+    stop(\"non global jobject using resolve_global_jobject\");\n+    bind(valid_global_tag);\n+  }\n+#endif\n+\n+  \/\/ Resolve global handle\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp, thread);\n@@ -3563,0 +3954,1 @@\n+\n@@ -3623,1 +4015,1 @@\n-  int num_xmm_registers = XMMRegisterImpl::available_xmm_registers();\n+  int num_xmm_registers = XMMRegister::available_xmm_registers();\n@@ -3664,1 +4056,1 @@\n-  gp_area_size = align_up(gp_registers.size() * RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size,\n+  gp_area_size = align_up(gp_registers.size() * Register::max_slots_per_register * VMRegImpl::stack_slot_size,\n@@ -3757,1 +4149,1 @@\n-    int register_push_size = set.size() * RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size;\n+    int register_push_size = set.size() * Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n@@ -3767,1 +4159,1 @@\n-    spill_offset += RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size;\n+    spill_offset += Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n@@ -3773,1 +4165,1 @@\n-  int gp_reg_size = RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size;\n+  int gp_reg_size = Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n@@ -3793,10 +4185,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register thread, Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n@@ -3898,1 +4280,1 @@\n-  \/\/ for (scan = klass->itable(); scan->interface() != NULL; scan += scan_step) {\n+  \/\/ for (scan = klass->itable(); scan->interface() != nullptr; scan += scan_step) {\n@@ -3956,2 +4338,2 @@\n-  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, NULL);\n-  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, NULL);\n+  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, nullptr);\n+  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, nullptr);\n@@ -3980,4 +4362,4 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -4079,3 +4461,3 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -4099,1 +4481,1 @@\n-  if (super_klass != rax || UseCompressedOops) {\n+  if (super_klass != rax) {\n@@ -4137,1 +4519,1 @@\n-    assert(!pushed_rdi, \"rdi must be left non-NULL\");\n+    assert(!pushed_rdi, \"rdi must be left non-null\");\n@@ -4158,1 +4540,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -4161,1 +4543,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -4163,1 +4545,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -4209,0 +4591,7 @@\n+  BLOCK_COMMENT(\"verify_oop {\");\n+#ifdef _LP64\n+  push(rscratch1);\n+#endif\n+  push(rax);                          \/\/ save rax\n+  push(reg);                          \/\/ pass register argument\n+\n@@ -4210,1 +4599,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -4217,10 +4606,2 @@\n-  BLOCK_COMMENT(\"verify_oop {\");\n-#ifdef _LP64\n-  push(rscratch1);                    \/\/ save r10, trashed by movptr()\n-#endif\n-  push(rax);                          \/\/ save rax,\n-  push(reg);                          \/\/ pass register argument\n-  \/\/ avoid using pushptr, as it modifies scratch registers\n-  \/\/ and our contract is not to modify anything\n-  movptr(rax, buffer.addr());\n-  push(rax);\n+  pushptr(buffer.addr(), rscratch1);\n+\n@@ -4237,0 +4618,2 @@\n+    \/\/ Only pcmpeq has dependency breaking treatment (i.e the execution can begin without\n+    \/\/ waiting for the previous result on dst), not vpcmpeqd, so just use vpternlog\n@@ -4238,0 +4621,2 @@\n+  } else if (VM_Version::supports_avx()) {\n+    vpcmpeqd(dst, dst, dst, vector_len);\n@@ -4239,2 +4624,2 @@\n-    assert(UseAVX > 0, \"\");\n-    vpcmpeqb(dst, dst, dst, vector_len);\n+    assert(VM_Version::supports_sse2(), \"\");\n+    pcmpeqd(dst, dst);\n@@ -4268,10 +4653,1 @@\n-  \/\/ Address adjust(addr.base(), addr.index(), addr.scale(), addr.disp() + BytesPerWord);\n-  \/\/ Pass register number to verify_oop_subroutine\n-  const char* b = NULL;\n-  {\n-    ResourceMark rm;\n-    stringStream ss;\n-    ss.print(\"verify_oop_addr: %s (%s:%d)\", s, file, line);\n-    b = code_string(ss.as_string());\n-  }\n-  push(rscratch1);                    \/\/ save r10, trashed by movptr()\n+  push(rscratch1);\n@@ -4280,1 +4656,1 @@\n-  push(rax);                          \/\/ save rax,\n+  push(rax); \/\/ save rax,\n@@ -4292,0 +4668,8 @@\n+  \/\/ Pass register number to verify_oop_subroutine\n+  const char* b = nullptr;\n+  {\n+    ResourceMark rm;\n+    stringStream ss;\n+    ss.print(\"verify_oop_addr: %s (%s:%d)\", s, file, line);\n+    b = code_string(ss.as_string());\n+  }\n@@ -4293,5 +4677,1 @@\n-  \/\/ pass msg argument\n-  \/\/ avoid using pushptr, as it modifies scratch registers\n-  \/\/ and our contract is not to modify anything\n-  movptr(rax, buffer.addr());\n-  push(rax);\n+  pushptr(buffer.addr(), rscratch1);\n@@ -4358,1 +4738,1 @@\n-        rc = NULL; \/\/ silence compiler warnings\n+        rc = nullptr; \/\/ silence compiler warnings\n@@ -4369,1 +4749,1 @@\n-        pc = NULL; \/\/ silence compiler warnings\n+        pc = nullptr; \/\/ silence compiler warnings\n@@ -4491,1 +4871,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -4674,1 +5054,1 @@\n-  pushptr(msg.addr());\n+  pushptr(msg.addr(), noreg);\n@@ -4689,1 +5069,1 @@\n-void MacroAssembler::restore_cpu_control_state_after_jni() {\n+void MacroAssembler::restore_cpu_control_state_after_jni(Register rscratch) {\n@@ -4694,1 +5074,1 @@\n-      ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()));\n+      ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()), rscratch);\n@@ -4785,1 +5165,1 @@\n-  decorators = AccessInternal::decorator_fixup(decorators);\n+  decorators = AccessInternal::decorator_fixup(decorators, type);\n@@ -4794,1 +5174,1 @@\n-void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,\n+void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register val,\n@@ -4797,1 +5177,1 @@\n-  decorators = AccessInternal::decorator_fixup(decorators);\n+  decorators = AccessInternal::decorator_fixup(decorators, type);\n@@ -4800,1 +5180,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, val, tmp1, tmp2, tmp3);\n@@ -4802,1 +5182,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+    bs->store_at(this, decorators, type, dst, val, tmp1, tmp2, tmp3);\n@@ -4817,1 +5197,1 @@\n-void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,\n+void MacroAssembler::store_heap_oop(Address dst, Register val, Register tmp1,\n@@ -4819,1 +5199,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, val, tmp1, tmp2, tmp3);\n@@ -4822,1 +5202,1 @@\n-\/\/ Used for storing NULLs.\n+\/\/ Used for storing nulls.\n@@ -4838,1 +5218,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4841,2 +5221,1 @@\n-    const auto src2 = ExternalAddress((address)CompressedOops::ptrs_base_addr());\n-    assert(!src2.is_lval(), \"should not be lval\");\n+    ExternalAddress src2(CompressedOops::ptrs_base_addr());\n@@ -4847,1 +5226,1 @@\n-    cmpptr(r12_heapbase, src2);\n+    cmpptr(r12_heapbase, src2, rscratch1);\n@@ -4864,1 +5243,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -4889,1 +5268,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -4913,1 +5292,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -4926,1 +5305,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -4944,1 +5323,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4951,1 +5330,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -4955,1 +5334,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -4962,1 +5341,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4975,1 +5354,1 @@\n-      if (CompressedOops::base() != NULL) {\n+      if (CompressedOops::base() != nullptr) {\n@@ -4980,1 +5359,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -4989,1 +5368,1 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  if (CompressedKlassPointers::base() != nullptr) {\n@@ -5001,1 +5380,1 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  if (CompressedKlassPointers::base() != nullptr) {\n@@ -5024,1 +5403,1 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  if (CompressedKlassPointers::base() != nullptr) {\n@@ -5038,1 +5417,1 @@\n-  if (CompressedKlassPointers::base() == NULL &&\n+  if (CompressedKlassPointers::base() == nullptr &&\n@@ -5044,1 +5423,1 @@\n-    if (CompressedKlassPointers::base() != NULL) {\n+    if (CompressedKlassPointers::base() != nullptr) {\n@@ -5061,2 +5440,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5070,2 +5449,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5079,1 +5458,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5087,1 +5466,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5095,2 +5474,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5104,2 +5483,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5113,1 +5492,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5121,1 +5500,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5129,2 +5508,2 @@\n-    if (Universe::heap() != NULL) {\n-      if (CompressedOops::base() == NULL) {\n+    if (Universe::heap() != nullptr) {\n+      if (CompressedOops::base() == nullptr) {\n@@ -5136,1 +5515,1 @@\n-      movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));\n+      movptr(r12_heapbase, ExternalAddress(CompressedOops::ptrs_base_addr()));\n@@ -5143,89 +5522,0 @@\n-\/\/ C2 compiled method's prolog code.\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n-\n-  \/\/ WARNING: Initial instruction MUST be 5 bytes or longer so that\n-  \/\/ NativeJump::patch_verified_entry will be able to patch out the entry\n-  \/\/ code safely. The push to verify stack depth is ok at 5 bytes,\n-  \/\/ the frame allocation can be either 3 or 6 bytes. So if we don't do\n-  \/\/ stack bang then we must use the 6 byte frame allocation even if\n-  \/\/ we have no frame. :-(\n-  assert(stack_bang_size >= framesize || stack_bang_size <= 0, \"stack bang size incorrect\");\n-\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return addr\n-  framesize -= wordSize;\n-  stack_bang_size -= wordSize;\n-\n-  \/\/ Calls to C2R adapters often do not accept exceptional returns.\n-  \/\/ We require that their callers must bang for them.  But be careful, because\n-  \/\/ some VM calls (such as call site linkage) can use several kilobytes of\n-  \/\/ stack.  But the stack safety zone should account for that.\n-  \/\/ See bugs 4446381, 4468289, 4497237.\n-  if (stack_bang_size > 0) {\n-    generate_stack_overflow_check(stack_bang_size);\n-\n-    \/\/ We always push rbp, so that on return to interpreter rbp, will be\n-    \/\/ restored correctly and we can correct the stack.\n-    push(rbp);\n-    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n-    if (PreserveFramePointer) {\n-      mov(rbp, rsp);\n-    }\n-    \/\/ Remove word for ebp\n-    framesize -= wordSize;\n-\n-    \/\/ Create frame\n-    if (framesize) {\n-      subptr(rsp, framesize);\n-    }\n-  } else {\n-    \/\/ Create frame (force generation of a 4 byte immediate value)\n-    subptr_imm32(rsp, framesize);\n-\n-    \/\/ Save RBP register now.\n-    framesize -= wordSize;\n-    movptr(Address(rsp, framesize), rbp);\n-    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n-    if (PreserveFramePointer) {\n-      movptr(rbp, rsp);\n-      if (framesize > 0) {\n-        addptr(rbp, framesize);\n-      }\n-    }\n-  }\n-\n-  if (VerifyStackAtCalls) { \/\/ Majik cookie to verify stack depth\n-    framesize -= wordSize;\n-    movptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n-  }\n-\n-#ifndef _LP64\n-  \/\/ If method sets FPU control word do it now\n-  if (fp_mode_24b) {\n-    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    verify_FPU(0, \"FPU stack must be clean on entry\");\n-  }\n-#endif\n-\n-#ifdef ASSERT\n-  if (VerifyStackAtCalls) {\n-    Label L;\n-    push(rax);\n-    mov(rax, rsp);\n-    andptr(rax, StackAlignmentInBytes-1);\n-    cmpptr(rax, StackAlignmentInBytes-wordSize);\n-    pop(rax);\n-    jcc(Assembler::equal, L);\n-    STOP(\"Stack is not properly aligned!\");\n-    bind(L);\n-  }\n-#endif\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n-}\n-\n@@ -5307,0 +5597,2 @@\n+  const int fill64_per_loop = 4;\n+  const int max_unrolled_fill64 = 8;\n@@ -5310,1 +5602,17 @@\n-  for (int i = 0; i < vector64_count; i++) {\n+  int start64 = 0;\n+  if (vector64_count > max_unrolled_fill64) {\n+    Label LOOP;\n+    Register index = rtmp;\n+\n+    start64 = vector64_count - (vector64_count % fill64_per_loop);\n+\n+    movl(index, 0);\n+    BIND(LOOP);\n+    for (int i = 0; i < fill64_per_loop; i++) {\n+      fill64(Address(base, index, Address::times_1, i * 64), xtmp, use64byteVector);\n+    }\n+    addl(index, fill64_per_loop * 64);\n+    cmpl(index, start64 * 64);\n+    jccb(Assembler::less, LOOP);\n+  }\n+  for (int i = start64; i < vector64_count; i++) {\n@@ -5322,1 +5630,1 @@\n-        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_128bit);\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_128bit);\n@@ -5327,1 +5635,1 @@\n-        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_256bit);\n@@ -5330,1 +5638,1 @@\n-        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -5336,1 +5644,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -5338,1 +5646,1 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -5346,1 +5654,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -5348,2 +5656,2 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n-          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, Assembler::AVX_128bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, false, Assembler::AVX_128bit);\n@@ -5356,1 +5664,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -5358,1 +5666,1 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -5361,1 +5669,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, true, Assembler::AVX_256bit);\n@@ -6333,1 +6641,1 @@\n-    evmovdqub(rymm0, Address(obja, result), false, Assembler::AVX_512bit);\n+    evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);\n@@ -7165,1 +7473,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32), rscratch1);\n@@ -7179,1 +7487,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16), rscratch1);\n@@ -7188,1 +7496,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16), rscratch1);\n@@ -7198,1 +7506,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr()));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr()), rscratch1);\n@@ -7977,5 +8285,7 @@\n-  BIND(L_wordByWord);\n-  jcc(Assembler::greaterEqual, L_byteByByteProlog);\n-    crc32(in_out, Address(in1, 0), 4);\n-    addq(in1, 4);\n-    jmp(L_wordByWord);\n+  jccb(Assembler::greaterEqual, L_byteByByteProlog);\n+  align(16);\n+  BIND(L_wordByWord);\n+    crc32(in_out, Address(in1, 0), 8);\n+    addq(in1, 8);\n+    cmpq(in1, tmp1);\n+    jcc(Assembler::less, L_wordByWord);\n@@ -7988,1 +8298,0 @@\n-  BIND(L_byteByByte);\n@@ -7991,0 +8300,1 @@\n+  BIND(L_byteByByte);\n@@ -7994,1 +8304,2 @@\n-    jmp(L_byteByByte);\n+    cmpl(tmp2, in2);\n+    jcc(Assembler::lessEqual, L_byteByByte);\n@@ -8174,1 +8485,1 @@\n-    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), \/*merge*\/ false, Assembler::AVX_512bit);\n+    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), Assembler::AVX_512bit);\n@@ -8344,1 +8655,1 @@\n-    evmovdquw(Address(dst, len, Address::times_2), tmp1, \/*merge*\/ false, Assembler::AVX_512bit);\n+    evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);\n@@ -8446,1 +8757,1 @@\n-void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len) {\n+void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len) {\n@@ -8450,1 +8761,1 @@\n-      evmovdqub(dst, kmask, src, false, vector_len);\n+      evmovdqub(dst, kmask, src, merge, vector_len);\n@@ -8454,1 +8765,1 @@\n-      evmovdquw(dst, kmask, src, false, vector_len);\n+      evmovdquw(dst, kmask, src, merge, vector_len);\n@@ -8458,1 +8769,1 @@\n-      evmovdqul(dst, kmask, src, false, vector_len);\n+      evmovdqul(dst, kmask, src, merge, vector_len);\n@@ -8462,1 +8773,1 @@\n-      evmovdquq(dst, kmask, src, false, vector_len);\n+      evmovdquq(dst, kmask, src, merge, vector_len);\n@@ -8470,1 +8781,1 @@\n-void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len) {\n+void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len) {\n@@ -8474,1 +8785,1 @@\n-      evmovdqub(dst, kmask, src, true, vector_len);\n+      evmovdqub(dst, kmask, src, merge, vector_len);\n@@ -8478,1 +8789,1 @@\n-      evmovdquw(dst, kmask, src, true, vector_len);\n+      evmovdquw(dst, kmask, src, merge, vector_len);\n@@ -8482,1 +8793,1 @@\n-      evmovdqul(dst, kmask, src, true, vector_len);\n+      evmovdqul(dst, kmask, src, merge, vector_len);\n@@ -8486,1 +8797,1 @@\n-      evmovdquq(dst, kmask, src, true, vector_len);\n+      evmovdquq(dst, kmask, src, merge, vector_len);\n@@ -8762,20 +9073,0 @@\n-void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src1, KRegister src2) {\n-   masklen = masklen < 8 ? 8 : masklen;\n-   ktest(masklen, src1, src2);\n-   setb(Assembler::notZero, dst);\n-   movzbl(dst, dst);\n-}\n-\n-void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch) {\n-  if (masklen < 8) {\n-    knotbl(kscratch, src2);\n-    kortestbl(src1, kscratch);\n-    setb(Assembler::carrySet, dst);\n-    movzbl(dst, dst);\n-  } else {\n-    ktest(masklen, src1, src2);\n-    setb(Assembler::carrySet, dst);\n-    movzbl(dst, dst);\n-  }\n-}\n-\n@@ -8867,0 +9158,34 @@\n+\n+void MacroAssembler::evpandq(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    evpandq(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    evpandq(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evporq(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    evporq(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    evporq(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n+void MacroAssembler::vpternlogq(XMMRegister dst, int imm8, XMMRegister src2, AddressLiteral src3, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src3), \"missing\");\n+\n+  if (reachable(src3)) {\n+    vpternlogq(dst, imm8, src2, as_Address(src3), vector_len);\n+  } else {\n+    lea(rscratch, src3);\n+    vpternlogq(dst, imm8, src2, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n@@ -8875,1 +9200,1 @@\n-  evmovdqu(bt, mask, dst, xmm, vec_enc);\n+  evmovdqu(bt, mask, dst, xmm, true, vec_enc);\n@@ -8883,1 +9208,1 @@\n-  BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n+  const BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n@@ -8899,1 +9224,1 @@\n-  BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n+  const BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n@@ -8904,1 +9229,1 @@\n-void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n+void MacroAssembler::fill32(Address dst, XMMRegister xmm) {\n@@ -8906,1 +9231,1 @@\n-  vmovdqu(Address(dst, disp), xmm);\n+  vmovdqu(dst, xmm);\n@@ -8909,1 +9234,5 @@\n-void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n+  fill32(Address(dst, disp), xmm);\n+}\n+\n+void MacroAssembler::fill64(Address dst, XMMRegister xmm, bool use64byteVector) {\n@@ -8911,3 +9240,2 @@\n-  BasicType type[] = {T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    fill32(dst, disp, xmm);\n-    fill32(dst, disp + 32, xmm);\n+    fill32(dst, xmm);\n+    fill32(dst.plus_disp(32), xmm);\n@@ -8916,1 +9244,1 @@\n-    evmovdquq(Address(dst, disp), xmm, Assembler::AVX_512bit);\n+    evmovdquq(dst, xmm, Assembler::AVX_512bit);\n@@ -8920,0 +9248,4 @@\n+void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+  fill64(Address(dst, disp), xmm, use64byteVector);\n+}\n+\n@@ -9000,1 +9332,1 @@\n-      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_256bit);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, true, Assembler::AVX_256bit);\n@@ -9070,1 +9402,1 @@\n-      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_512bit);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, true, Assembler::AVX_512bit);\n@@ -9291,1 +9623,1 @@\n-    MacroAssembler* masm, const bool* flag_addr, bool value) {\n+    MacroAssembler* masm, const bool* flag_addr, bool value, Register rscratch) {\n@@ -9293,1 +9625,1 @@\n-  _masm->cmp8(ExternalAddress((address)flag_addr), value);\n+  _masm->cmp8(ExternalAddress((address)flag_addr), value, rscratch);\n@@ -9341,0 +9673,16 @@\n+\n+void MacroAssembler::check_stack_alignment(Register sp, const char* msg, unsigned bias, Register tmp) {\n+  Label L_stack_ok;\n+  if (bias == 0) {\n+    testptr(sp, 2 * wordSize - 1);\n+  } else {\n+    \/\/ lea(tmp, Address(rsp, bias);\n+    mov(tmp, sp);\n+    addptr(tmp, bias);\n+    testptr(tmp, 2 * wordSize - 1);\n+  }\n+  jcc(Assembler::equal, L_stack_ok);\n+  block_comment(msg);\n+  stop(msg);\n+  bind(L_stack_ok);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":1095,"deletions":747,"binary":false,"changes":1842,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -92,1 +92,1 @@\n-  Address as_Address(ArrayAddress adr);\n+  Address as_Address(ArrayAddress adr, Register rscratch);\n@@ -94,1 +94,1 @@\n-  \/\/ Support for NULL-checks\n+  \/\/ Support for null-checks\n@@ -96,1 +96,1 @@\n-  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n+  \/\/ Generates code that causes a null OS exception if the content of reg is null.\n@@ -122,1 +122,1 @@\n-                file == NULL ? \"<NULL>\" : file, line);\n+                file == nullptr ? \"<null>\" : file, line);\n@@ -165,0 +165,5 @@\n+  void incrementl(AddressLiteral dst, Register rscratch = noreg);\n+  void incrementl(ArrayAddress   dst, Register rscratch);\n+\n+  void incrementq(AddressLiteral dst, Register rscratch = noreg);\n+\n@@ -172,1 +177,1 @@\n-  void movflt(XMMRegister dst, AddressLiteral src);\n+  void movflt(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -184,1 +189,1 @@\n-  void movdbl(XMMRegister dst, AddressLiteral src);\n+  void movdbl(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -192,2 +197,7 @@\n-  void incrementl(AddressLiteral dst);\n-  void incrementl(ArrayAddress dst);\n+  void flt_to_flt16(Register dst, XMMRegister src, XMMRegister tmp) {\n+    \/\/ Use separate tmp XMM register because caller may\n+    \/\/ requires src XMM register to be unchanged (as in x86.ad).\n+    vcvtps2ph(tmp, src, 0x04, Assembler::AVX_128bit);\n+    movdl(dst, tmp);\n+    movswl(dst, dst);\n+  }\n@@ -195,1 +205,4 @@\n-  void incrementq(AddressLiteral dst);\n+  void flt16_to_flt(XMMRegister dst, Register src) {\n+    movdl(dst, src);\n+    vcvtph2ps(dst, dst, Assembler::AVX_128bit);\n+  }\n@@ -203,0 +216,1 @@\n+  void post_call_nop();\n@@ -217,4 +231,5 @@\n-  void move32_64(VMRegPair src, VMRegPair dst);\n-  void long_move(VMRegPair src, VMRegPair dst);\n-  void float_move(VMRegPair src, VMRegPair dst);\n-  void double_move(VMRegPair src, VMRegPair dst);\n+  \/\/ bias in bytes\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n@@ -297,0 +312,3 @@\n+  void call_VM_leaf(address entry_point,\n+                    Register arg_1, Register arg_2, Register arg_3, Register arg_4);\n+\n@@ -309,1 +327,2 @@\n-                           address last_java_pc);\n+                           address  last_java_pc,\n+                           Register rscratch);\n@@ -314,1 +333,2 @@\n-                           address last_java_pc);\n+                           address  last_java_pc,\n+                           Register rscratch);\n@@ -322,1 +342,1 @@\n-  void clear_jweak_tag(Register possibly_jweak);\n+  void clear_jobject_tag(Register possibly_non_local);\n@@ -324,0 +344,1 @@\n+  void resolve_global_jobject(Register value, Register thread, Register tmp);\n@@ -335,1 +356,1 @@\n-  void resolve_oop_handle(Register result, Register tmp = rscratch2);\n+  void resolve_oop_handle(Register result, Register tmp);\n@@ -337,1 +358,1 @@\n-  void load_mirror(Register mirror, Register method, Register tmp = rscratch2);\n+  void load_mirror(Register mirror, Register method, Register tmp);\n@@ -348,1 +369,1 @@\n-  void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,\n+  void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register val,\n@@ -355,1 +376,1 @@\n-  void store_heap_oop(Address dst, Register src, Register tmp1 = noreg,\n+  void store_heap_oop(Address dst, Register val, Register tmp1 = noreg,\n@@ -358,1 +379,1 @@\n-  \/\/ Used for storing NULL. All other oop constants should be\n+  \/\/ Used for storing null. All other oop constants should be\n@@ -366,1 +387,1 @@\n-  \/\/ converting a zero (like NULL) into a Register by giving\n+  \/\/ converting a zero (like null) into a Register by giving\n@@ -524,0 +545,8 @@\n+  void push_cont_fastpath();\n+  void pop_cont_fastpath();\n+\n+  void inc_held_monitor_count();\n+  void dec_held_monitor_count();\n+\n+  DEBUG_ONLY(void stop_if_in_cont(Register cont_reg, const char* name);)\n+\n@@ -557,8 +586,0 @@\n-  void eden_allocate(\n-    Register thread,                   \/\/ Current thread\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n@@ -593,1 +614,1 @@\n-  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n+  \/\/ One of the three labels can be null, meaning take the fall-through.\n@@ -626,2 +647,2 @@\n-                      Label* L_fast_path = NULL,\n-                      Label* L_slow_path = NULL);\n+                      Label* L_fast_path = nullptr,\n+                      Label* L_slow_path = nullptr);\n@@ -660,1 +681,1 @@\n-  void restore_cpu_control_state_after_jni();\n+  void restore_cpu_control_state_after_jni(Register rscratch);\n@@ -704,1 +725,1 @@\n-  Condition negate_condition(Condition cond);\n+  static Condition negate_condition(Condition cond);\n@@ -721,2 +742,2 @@\n-    if (src.is_constant()) addptr(dst, (int) src.as_constant());\n-    else                   addptr(dst,       src.as_register());\n+    if (src.is_constant()) addptr(dst, src.as_constant());\n+    else                   addptr(dst, src.as_register());\n@@ -728,1 +749,6 @@\n-  void cmp8(AddressLiteral src1, int imm);\n+#ifdef _LP64\n+  using Assembler::andq;\n+  void andq(Register dst, AddressLiteral src, Register rscratch = noreg);\n+#endif\n+\n+  void cmp8(AddressLiteral src1, int imm, Register rscratch = noreg);\n@@ -733,1 +759,1 @@\n-  void cmp32(AddressLiteral src1, int32_t imm);\n+  void cmp32(AddressLiteral src1, int32_t imm, Register rscratch = noreg);\n@@ -735,1 +761,1 @@\n-  void cmp32(Register src1, AddressLiteral src2);\n+  void cmp32(Register src1, AddressLiteral src2, Register rscratch = noreg);\n@@ -747,1 +773,1 @@\n-  void cmpoop(Register dst, jobject obj);\n+  void cmpoop(Register dst, jobject obj, Register rscratch);\n@@ -750,1 +776,1 @@\n-  void cmpptr(Address src1, AddressLiteral src2);\n+  void cmpptr(Address src1, AddressLiteral src2, Register rscratch);\n@@ -752,1 +778,1 @@\n-  void cmpptr(Register src1, AddressLiteral src2);\n+  void cmpptr(Register src1, AddressLiteral src2, Register rscratch = noreg);\n@@ -762,1 +788,1 @@\n-  void cmp64(Register src1, AddressLiteral src);\n+  void cmp64(Register src1, AddressLiteral src, Register rscratch = noreg);\n@@ -766,2 +792,1 @@\n-  void locked_cmpxchgptr(Register reg, AddressLiteral adr);\n-\n+  void locked_cmpxchgptr(Register reg, AddressLiteral adr, Register rscratch = noreg);\n@@ -810,1 +835,1 @@\n-  void cond_inc32(Condition cond, AddressLiteral counter_addr);\n+  void cond_inc32(Condition cond, AddressLiteral counter_addr, Register rscratch = noreg);\n@@ -813,1 +838,1 @@\n-  void atomic_incl(AddressLiteral counter_addr, Register scr = rscratch1);\n+  void atomic_incl(AddressLiteral counter_addr, Register rscratch = noreg);\n@@ -816,1 +841,1 @@\n-  void atomic_incq(AddressLiteral counter_addr, Register scr = rscratch1);\n+  void atomic_incq(AddressLiteral counter_addr, Register rscratch = noreg);\n@@ -818,1 +843,1 @@\n-  void atomic_incptr(AddressLiteral counter_addr, Register scr = rscratch1) { LP64_ONLY(atomic_incq(counter_addr, scr)) NOT_LP64(atomic_incl(counter_addr, scr)) ; }\n+  void atomic_incptr(AddressLiteral counter_addr, Register rscratch = noreg) { LP64_ONLY(atomic_incq(counter_addr, rscratch)) NOT_LP64(atomic_incl(counter_addr, rscratch)) ; }\n@@ -821,0 +846,1 @@\n+  void lea(Register dst, Address        adr) { Assembler::lea(dst, adr); }\n@@ -822,2 +848,1 @@\n-  void lea(Address dst, AddressLiteral adr);\n-  void lea(Register dst, Address adr) { Assembler::lea(dst, adr); }\n+  void lea(Address  dst, AddressLiteral adr, Register rscratch);\n@@ -830,1 +855,6 @@\n-  void testl(Register dst, AddressLiteral src);\n+  void testl(Address dst, int32_t imm32);\n+  void testl(Register dst, int32_t imm32);\n+  void testl(Register dst, AddressLiteral src); \/\/ requires reachable address\n+  using Assembler::testq;\n+  void testq(Address dst, int32_t imm32);\n+  void testq(Register dst, int32_t imm32);\n@@ -853,1 +883,1 @@\n-  void call(AddressLiteral entry);\n+  void call(AddressLiteral entry, Register rscratch = rax);\n@@ -858,0 +888,2 @@\n+  void emit_static_call_stub();\n+\n@@ -863,2 +895,3 @@\n-  void jump(AddressLiteral dst);\n-  void jump_cc(Condition cc, AddressLiteral dst);\n+  void jump(AddressLiteral dst, Register rscratch = noreg);\n+\n+  void jump_cc(Condition cc, AddressLiteral dst, Register rscratch = noreg);\n@@ -869,1 +902,1 @@\n-  void jump(ArrayAddress entry);\n+  void jump(ArrayAddress entry, Register rscratch);\n@@ -873,3 +906,8 @@\n-  void andpd(XMMRegister dst, Address src) { Assembler::andpd(dst, src); }\n-  void andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void andpd(XMMRegister dst, XMMRegister src) { Assembler::andpd(dst, src); }\n+  void push_f(XMMRegister r);\n+  void pop_f(XMMRegister r);\n+  void push_d(XMMRegister r);\n+  void pop_d(XMMRegister r);\n+\n+  void andpd(XMMRegister dst, XMMRegister    src) { Assembler::andpd(dst, src); }\n+  void andpd(XMMRegister dst, Address        src) { Assembler::andpd(dst, src); }\n+  void andpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -877,3 +915,3 @@\n-  void andps(XMMRegister dst, XMMRegister src) { Assembler::andps(dst, src); }\n-  void andps(XMMRegister dst, Address src) { Assembler::andps(dst, src); }\n-  void andps(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void andps(XMMRegister dst, XMMRegister    src) { Assembler::andps(dst, src); }\n+  void andps(XMMRegister dst, Address        src) { Assembler::andps(dst, src); }\n+  void andps(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -881,3 +919,3 @@\n-  void comiss(XMMRegister dst, XMMRegister src) { Assembler::comiss(dst, src); }\n-  void comiss(XMMRegister dst, Address src) { Assembler::comiss(dst, src); }\n-  void comiss(XMMRegister dst, AddressLiteral src);\n+  void comiss(XMMRegister dst, XMMRegister    src) { Assembler::comiss(dst, src); }\n+  void comiss(XMMRegister dst, Address        src) { Assembler::comiss(dst, src); }\n+  void comiss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -885,3 +923,3 @@\n-  void comisd(XMMRegister dst, XMMRegister src) { Assembler::comisd(dst, src); }\n-  void comisd(XMMRegister dst, Address src) { Assembler::comisd(dst, src); }\n-  void comisd(XMMRegister dst, AddressLiteral src);\n+  void comisd(XMMRegister dst, XMMRegister    src) { Assembler::comisd(dst, src); }\n+  void comisd(XMMRegister dst, Address        src) { Assembler::comisd(dst, src); }\n+  void comisd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -890,1 +928,1 @@\n-  void fadd_s(Address src)        { Assembler::fadd_s(src); }\n+  void fadd_s(Address        src) { Assembler::fadd_s(src); }\n@@ -893,1 +931,1 @@\n-  void fldcw(Address src) { Assembler::fldcw(src); }\n+  void fldcw(Address        src) { Assembler::fldcw(src); }\n@@ -896,2 +934,2 @@\n-  void fld_s(int index)   { Assembler::fld_s(index); }\n-  void fld_s(Address src) { Assembler::fld_s(src); }\n+  void fld_s(int index)          { Assembler::fld_s(index); }\n+  void fld_s(Address        src) { Assembler::fld_s(src); }\n@@ -900,1 +938,1 @@\n-  void fld_d(Address src) { Assembler::fld_d(src); }\n+  void fld_d(Address        src) { Assembler::fld_d(src); }\n@@ -903,3 +941,2 @@\n-  void fmul_s(Address src)        { Assembler::fmul_s(src); }\n-  void fmul_s(AddressLiteral src) { Assembler::fmul_s(as_Address(src)); }\n-#endif \/\/ _LP64\n+  void fld_x(Address        src) { Assembler::fld_x(src); }\n+  void fld_x(AddressLiteral src) { Assembler::fld_x(as_Address(src)); }\n@@ -907,2 +944,3 @@\n-  void fld_x(Address src) { Assembler::fld_x(src); }\n-  void fld_x(AddressLiteral src);\n+  void fmul_s(Address        src) { Assembler::fmul_s(src); }\n+  void fmul_s(AddressLiteral src) { Assembler::fmul_s(as_Address(src)); }\n+#endif \/\/ !_LP64\n@@ -911,1 +949,1 @@\n-  void ldmxcsr(AddressLiteral src, Register scratchReg = rscratch1);\n+  void ldmxcsr(AddressLiteral src, Register rscratch = noreg);\n@@ -944,14 +982,0 @@\n-  void gfmul(XMMRegister tmp0, XMMRegister t);\n-  void schoolbookAAD(int i, Register subkeyH, XMMRegister data, XMMRegister tmp0,\n-                     XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3);\n-  void generateHtbl_one_block(Register htbl);\n-  void generateHtbl_eight_blocks(Register htbl);\n- public:\n-  void sha256_AVX2(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n-                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n-                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n-                   bool multi_block, XMMRegister shuf_mask);\n-  void avx_ghash(Register state, Register htbl, Register data, Register blocks);\n-#endif\n-#ifdef _LP64\n- private:\n@@ -968,0 +992,4 @@\n+  void sha256_AVX2(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n+                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n+                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n+                   bool multi_block, XMMRegister shuf_mask);\n@@ -972,21 +1000,1 @@\n-private:\n-  void roundEnc(XMMRegister key, int rnum);\n-  void lastroundEnc(XMMRegister key, int rnum);\n-  void roundDec(XMMRegister key, int rnum);\n-  void lastroundDec(XMMRegister key, int rnum);\n-  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask);\n-  void gfmul_avx512(XMMRegister ghash, XMMRegister hkey);\n-  void generateHtbl_48_block_zmm(Register htbl, Register avx512_subkeyHtbl);\n-  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n-                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n-                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n-                                  bool final_reduction, int index, XMMRegister counter_inc_mask);\n-public:\n-  void aesecb_encrypt(Register source_addr, Register dest_addr, Register key, Register len);\n-  void aesecb_decrypt(Register source_addr, Register dest_addr, Register key, Register len);\n-  void aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n-                      Register len_reg, Register used, Register used_addr, Register saved_encCounter_start);\n-  void aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n-                      Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter);\n-\n-#endif\n+#endif \/\/ _LP64\n@@ -1018,17 +1026,8 @@\n-#ifdef _LP64\n-  void fast_log(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1, Register tmp2);\n-\n-  void fast_log10(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                  XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                  Register rax, Register rcx, Register rdx, Register r11);\n-\n-  void fast_pow(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n-                XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register rax, Register rcx,\n-                Register rdx, Register tmp1, Register tmp2, Register tmp3, Register tmp4);\n-\n-  void fast_sin(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rbx, Register rcx, Register rdx, Register tmp1, Register tmp2,\n-                Register tmp3, Register tmp4);\n+#ifndef _LP64\n+ private:\n+  \/\/ Initialized in macroAssembler_x86_constants.cpp\n+  static address ONES;\n+  static address L_2IL0FLOATPACKET_0;\n+  static address PI4_INV;\n+  static address PI4X3;\n+  static address PI4X4;\n@@ -1036,9 +1035,1 @@\n-  void fast_cos(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1,\n-                Register tmp2, Register tmp3, Register tmp4);\n-  void fast_tan(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1,\n-                Register tmp2, Register tmp3, Register tmp4);\n-#else\n+ public:\n@@ -1079,1 +1070,1 @@\n-#endif\n+#endif \/\/ !_LP64\n@@ -1085,4 +1076,4 @@\n-  void movss(XMMRegister dst, XMMRegister src) { Assembler::movss(dst, src); }\n-  void movss(Address dst, XMMRegister src)     { Assembler::movss(dst, src); }\n-  void movss(XMMRegister dst, Address src)     { Assembler::movss(dst, src); }\n-  void movss(XMMRegister dst, AddressLiteral src);\n+  void movss(Address     dst, XMMRegister    src) { Assembler::movss(dst, src); }\n+  void movss(XMMRegister dst, XMMRegister    src) { Assembler::movss(dst, src); }\n+  void movss(XMMRegister dst, Address        src) { Assembler::movss(dst, src); }\n+  void movss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1090,2 +1081,2 @@\n-  void movlpd(XMMRegister dst, Address src)    {Assembler::movlpd(dst, src); }\n-  void movlpd(XMMRegister dst, AddressLiteral src);\n+  void movlpd(XMMRegister dst, Address        src) {Assembler::movlpd(dst, src); }\n+  void movlpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1095,3 +1086,7 @@\n-  void addsd(XMMRegister dst, XMMRegister src)    { Assembler::addsd(dst, src); }\n-  void addsd(XMMRegister dst, Address src)        { Assembler::addsd(dst, src); }\n-  void addsd(XMMRegister dst, AddressLiteral src);\n+  void addsd(XMMRegister dst, XMMRegister    src) { Assembler::addsd(dst, src); }\n+  void addsd(XMMRegister dst, Address        src) { Assembler::addsd(dst, src); }\n+  void addsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void addss(XMMRegister dst, XMMRegister    src) { Assembler::addss(dst, src); }\n+  void addss(XMMRegister dst, Address        src) { Assembler::addss(dst, src); }\n+  void addss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1099,3 +1094,3 @@\n-  void addss(XMMRegister dst, XMMRegister src)    { Assembler::addss(dst, src); }\n-  void addss(XMMRegister dst, Address src)        { Assembler::addss(dst, src); }\n-  void addss(XMMRegister dst, AddressLiteral src);\n+  void addpd(XMMRegister dst, XMMRegister    src) { Assembler::addpd(dst, src); }\n+  void addpd(XMMRegister dst, Address        src) { Assembler::addpd(dst, src); }\n+  void addpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1103,3 +1098,5 @@\n-  void addpd(XMMRegister dst, XMMRegister src)    { Assembler::addpd(dst, src); }\n-  void addpd(XMMRegister dst, Address src)        { Assembler::addpd(dst, src); }\n-  void addpd(XMMRegister dst, AddressLiteral src);\n+  using Assembler::vbroadcastsd;\n+  void vbroadcastsd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n+  using Assembler::vbroadcastss;\n+  void vbroadcastss(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1107,3 +1104,3 @@\n-  void divsd(XMMRegister dst, XMMRegister src)    { Assembler::divsd(dst, src); }\n-  void divsd(XMMRegister dst, Address src)        { Assembler::divsd(dst, src); }\n-  void divsd(XMMRegister dst, AddressLiteral src);\n+  void divsd(XMMRegister dst, XMMRegister    src) { Assembler::divsd(dst, src); }\n+  void divsd(XMMRegister dst, Address        src) { Assembler::divsd(dst, src); }\n+  void divsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1111,3 +1108,3 @@\n-  void divss(XMMRegister dst, XMMRegister src)    { Assembler::divss(dst, src); }\n-  void divss(XMMRegister dst, Address src)        { Assembler::divss(dst, src); }\n-  void divss(XMMRegister dst, AddressLiteral src);\n+  void divss(XMMRegister dst, XMMRegister    src) { Assembler::divss(dst, src); }\n+  void divss(XMMRegister dst, Address        src) { Assembler::divss(dst, src); }\n+  void divss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1116,18 +1113,18 @@\n-  void movdqu(Address     dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, Address src);\n-  void movdqu(XMMRegister dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg = rscratch1);\n-\n-  void kmovwl(KRegister dst, Register src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(Register dst, KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, Address src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void kmovwl(Address dst,  KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, KRegister src) { Assembler::kmovwl(dst, src); }\n-\n-  void kmovql(KRegister dst, KRegister src) { Assembler::kmovql(dst, src); }\n-  void kmovql(KRegister dst, Register src) { Assembler::kmovql(dst, src); }\n-  void kmovql(Register dst, KRegister src) { Assembler::kmovql(dst, src); }\n-  void kmovql(KRegister dst, Address src) { Assembler::kmovql(dst, src); }\n-  void kmovql(Address  dst, KRegister src) { Assembler::kmovql(dst, src); }\n-  void kmovql(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void movdqu(Address     dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, Address        src);\n+  void movdqu(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void kmovwl(Register  dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(Address   dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Register       src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Address        src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void kmovql(KRegister dst, KRegister      src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, Register       src) { Assembler::kmovql(dst, src); }\n+  void kmovql(Register  dst, KRegister      src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, Address        src) { Assembler::kmovql(dst, src); }\n+  void kmovql(Address   dst, KRegister      src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1143,6 +1140,2 @@\n-  \/\/ AVX Unaligned forms\n-  void vmovdqu(Address     dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, Address src);\n-  void vmovdqu(XMMRegister dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len);\n+  using Assembler::movddup;\n+  void movddup(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1150,0 +1143,9 @@\n+  using Assembler::vmovddup;\n+  void vmovddup(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n+  \/\/ AVX Unaligned forms\n+  void vmovdqu(Address     dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, Address        src);\n+  void vmovdqu(XMMRegister dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src,                 Register rscratch = noreg);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1152,18 +1154,27 @@\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len);\n-\n-  void evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n-  void evmovdqub(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n-\n-  void evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n-  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n-  void evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n-  void evmovdquw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n-  void evmovdquw(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n-\n-  void evmovdqul(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n-  void evmovdqul(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+  void evmovdqu(BasicType type, KRegister kmask, Address     dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address     src, bool merge, int vector_len);\n+\n+  void evmovdqub(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+\n+  void evmovdqub(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0)  {\n+      Assembler::evmovdqub(dst, mask, src, merge, vector_len);\n+    }\n+  }\n+  void evmovdqub(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n+\n+  void evmovdquw(Address     dst, XMMRegister src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+  void evmovdquw(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+\n+  void evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0) {\n+      Assembler::evmovdquw(dst, mask, src, merge, vector_len);\n+    }\n+  }\n+  void evmovdquw(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n+  void evmovdquw(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n+\n@@ -1171,2 +1182,3 @@\n-     if (dst->encoding() == src->encoding()) return;\n-     Assembler::evmovdqul(dst, src, vector_len);\n+     if (dst->encoding() != src->encoding()) {\n+       Assembler::evmovdqul(dst, src, vector_len);\n+     }\n@@ -1174,2 +1186,3 @@\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(Address     dst, XMMRegister src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+  void evmovdqul(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+\n@@ -1177,8 +1190,8 @@\n-    if (dst->encoding() == src->encoding() && mask == k0) return;\n-    Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n-   }\n-  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n-\n-  void evmovdquq(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n-  void evmovdquq(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n-  void evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch);\n+    if (dst->encoding() != src->encoding() || mask != k0)  {\n+      Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n+    }\n+  }\n+  void evmovdqul(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n+\n@@ -1186,2 +1199,3 @@\n-    if (dst->encoding() == src->encoding()) return;\n-    Assembler::evmovdquq(dst, src, vector_len);\n+    if (dst->encoding() != src->encoding()) {\n+      Assembler::evmovdquq(dst, src, vector_len);\n+    }\n@@ -1189,2 +1203,4 @@\n-  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n-  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, Address        src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n+  void evmovdquq(Address     dst, XMMRegister    src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n+  void evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n@@ -1192,2 +1208,3 @@\n-    if (dst->encoding() == src->encoding() && mask == k0) return;\n-    Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+    if (dst->encoding() != src->encoding() || mask != k0) {\n+      Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+    }\n@@ -1195,1 +1212,3 @@\n-  void evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdquq(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n@@ -1198,8 +1217,3 @@\n-  void movdqa(XMMRegister dst, Address src)       { Assembler::movdqa(dst, src); }\n-  void movdqa(XMMRegister dst, XMMRegister src)   { Assembler::movdqa(dst, src); }\n-  void movdqa(XMMRegister dst, AddressLiteral src);\n-\n-  void movsd(XMMRegister dst, XMMRegister src) { Assembler::movsd(dst, src); }\n-  void movsd(Address dst, XMMRegister src)     { Assembler::movsd(dst, src); }\n-  void movsd(XMMRegister dst, Address src)     { Assembler::movsd(dst, src); }\n-  void movsd(XMMRegister dst, AddressLiteral src);\n+  void movdqa(XMMRegister dst, XMMRegister    src) { Assembler::movdqa(dst, src); }\n+  void movdqa(XMMRegister dst, Address        src) { Assembler::movdqa(dst, src); }\n+  void movdqa(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1207,2 +1221,4 @@\n-  using Assembler::vmovddup;\n-  void vmovddup(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n+  void movsd(Address     dst, XMMRegister    src) { Assembler::movsd(dst, src); }\n+  void movsd(XMMRegister dst, XMMRegister    src) { Assembler::movsd(dst, src); }\n+  void movsd(XMMRegister dst, Address        src) { Assembler::movsd(dst, src); }\n+  void movsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1210,3 +1226,3 @@\n-  void mulpd(XMMRegister dst, XMMRegister src)    { Assembler::mulpd(dst, src); }\n-  void mulpd(XMMRegister dst, Address src)        { Assembler::mulpd(dst, src); }\n-  void mulpd(XMMRegister dst, AddressLiteral src);\n+  void mulpd(XMMRegister dst, XMMRegister    src) { Assembler::mulpd(dst, src); }\n+  void mulpd(XMMRegister dst, Address        src) { Assembler::mulpd(dst, src); }\n+  void mulpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1214,3 +1230,3 @@\n-  void mulsd(XMMRegister dst, XMMRegister src)    { Assembler::mulsd(dst, src); }\n-  void mulsd(XMMRegister dst, Address src)        { Assembler::mulsd(dst, src); }\n-  void mulsd(XMMRegister dst, AddressLiteral src);\n+  void mulsd(XMMRegister dst, XMMRegister    src) { Assembler::mulsd(dst, src); }\n+  void mulsd(XMMRegister dst, Address        src) { Assembler::mulsd(dst, src); }\n+  void mulsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1218,3 +1234,3 @@\n-  void mulss(XMMRegister dst, XMMRegister src)    { Assembler::mulss(dst, src); }\n-  void mulss(XMMRegister dst, Address src)        { Assembler::mulss(dst, src); }\n-  void mulss(XMMRegister dst, AddressLiteral src);\n+  void mulss(XMMRegister dst, XMMRegister    src) { Assembler::mulss(dst, src); }\n+  void mulss(XMMRegister dst, Address        src) { Assembler::mulss(dst, src); }\n+  void mulss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1245,7 +1261,3 @@\n-  void sqrtsd(XMMRegister dst, XMMRegister src)    { Assembler::sqrtsd(dst, src); }\n-  void sqrtsd(XMMRegister dst, Address src)        { Assembler::sqrtsd(dst, src); }\n-  void sqrtsd(XMMRegister dst, AddressLiteral src);\n-\n-  void roundsd(XMMRegister dst, XMMRegister src, int32_t rmode)    { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, Address src, int32_t rmode)        { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg);\n+  void roundsd(XMMRegister dst, XMMRegister    src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, Address        src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch = noreg);\n@@ -1253,3 +1265,3 @@\n-  void sqrtss(XMMRegister dst, XMMRegister src)    { Assembler::sqrtss(dst, src); }\n-  void sqrtss(XMMRegister dst, Address src)        { Assembler::sqrtss(dst, src); }\n-  void sqrtss(XMMRegister dst, AddressLiteral src);\n+  void sqrtss(XMMRegister dst, XMMRegister     src) { Assembler::sqrtss(dst, src); }\n+  void sqrtss(XMMRegister dst, Address         src) { Assembler::sqrtss(dst, src); }\n+  void sqrtss(XMMRegister dst, AddressLiteral  src, Register rscratch = noreg);\n@@ -1257,3 +1269,3 @@\n-  void subsd(XMMRegister dst, XMMRegister src)    { Assembler::subsd(dst, src); }\n-  void subsd(XMMRegister dst, Address src)        { Assembler::subsd(dst, src); }\n-  void subsd(XMMRegister dst, AddressLiteral src);\n+  void subsd(XMMRegister dst, XMMRegister    src) { Assembler::subsd(dst, src); }\n+  void subsd(XMMRegister dst, Address        src) { Assembler::subsd(dst, src); }\n+  void subsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1261,3 +1273,3 @@\n-  void subss(XMMRegister dst, XMMRegister src)    { Assembler::subss(dst, src); }\n-  void subss(XMMRegister dst, Address src)        { Assembler::subss(dst, src); }\n-  void subss(XMMRegister dst, AddressLiteral src);\n+  void subss(XMMRegister dst, XMMRegister    src) { Assembler::subss(dst, src); }\n+  void subss(XMMRegister dst, Address        src) { Assembler::subss(dst, src); }\n+  void subss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1265,3 +1277,3 @@\n-  void ucomiss(XMMRegister dst, XMMRegister src) { Assembler::ucomiss(dst, src); }\n-  void ucomiss(XMMRegister dst, Address src)     { Assembler::ucomiss(dst, src); }\n-  void ucomiss(XMMRegister dst, AddressLiteral src);\n+  void ucomiss(XMMRegister dst, XMMRegister    src) { Assembler::ucomiss(dst, src); }\n+  void ucomiss(XMMRegister dst, Address        src) { Assembler::ucomiss(dst, src); }\n+  void ucomiss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1269,3 +1281,3 @@\n-  void ucomisd(XMMRegister dst, XMMRegister src) { Assembler::ucomisd(dst, src); }\n-  void ucomisd(XMMRegister dst, Address src)     { Assembler::ucomisd(dst, src); }\n-  void ucomisd(XMMRegister dst, AddressLiteral src);\n+  void ucomisd(XMMRegister dst, XMMRegister    src) { Assembler::ucomisd(dst, src); }\n+  void ucomisd(XMMRegister dst, Address        src) { Assembler::ucomisd(dst, src); }\n+  void ucomisd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1274,3 +1286,3 @@\n-  void xorpd(XMMRegister dst, XMMRegister src);\n-  void xorpd(XMMRegister dst, Address src)     { Assembler::xorpd(dst, src); }\n-  void xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorpd(XMMRegister dst, XMMRegister    src);\n+  void xorpd(XMMRegister dst, Address        src) { Assembler::xorpd(dst, src); }\n+  void xorpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1279,3 +1291,3 @@\n-  void xorps(XMMRegister dst, XMMRegister src);\n-  void xorps(XMMRegister dst, Address src)     { Assembler::xorps(dst, src); }\n-  void xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorps(XMMRegister dst, XMMRegister    src);\n+  void xorps(XMMRegister dst, Address        src) { Assembler::xorps(dst, src); }\n+  void xorps(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1284,3 +1296,3 @@\n-  void pshufb(XMMRegister dst, XMMRegister src) { Assembler::pshufb(dst, src); }\n-  void pshufb(XMMRegister dst, Address src)     { Assembler::pshufb(dst, src); }\n-  void pshufb(XMMRegister dst, AddressLiteral src);\n+  void pshufb(XMMRegister dst, XMMRegister    src) { Assembler::pshufb(dst, src); }\n+  void pshufb(XMMRegister dst, Address        src) { Assembler::pshufb(dst, src); }\n+  void pshufb(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1289,3 +1301,3 @@\n-  void vaddsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vaddsd(dst, nds, src); }\n-  void vaddsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vaddsd(dst, nds, src); }\n-  void vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vaddsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vaddsd(dst, nds, src); }\n+  void vaddsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vaddsd(dst, nds, src); }\n+  void vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1293,3 +1305,3 @@\n-  void vaddss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vaddss(dst, nds, src); }\n-  void vaddss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vaddss(dst, nds, src); }\n-  void vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vaddss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vaddss(dst, nds, src); }\n+  void vaddss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vaddss(dst, nds, src); }\n+  void vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1297,2 +1309,2 @@\n-  void vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len);\n-  void vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len);\n+  void vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch = noreg);\n+  void vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch = noreg);\n@@ -1300,3 +1312,3 @@\n-  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n-  void vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n-  void vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, Address        src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1305,1 +1317,1 @@\n-  void vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpaddw(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -1307,3 +1319,3 @@\n-  void vpaddd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n-  void vpaddd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n-  void vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch);\n+  void vpaddd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n+  void vpaddd(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n+  void vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1311,12 +1323,3 @@\n-  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n-\n-  void vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len);\n-  void vpbroadcastw(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastw(dst, src, vector_len); }\n-\n-  using Assembler::vbroadcastsd;\n-  void vbroadcastsd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n-  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n-  void vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n-  void vpbroadcastq(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1324,0 +1327,2 @@\n+  using Assembler::vpbroadcastd;\n+  void vpbroadcastd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1325,0 +1330,2 @@\n+  using Assembler::vpbroadcastq;\n+  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1329,1 +1336,1 @@\n-  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1332,16 +1339,19 @@\n-  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpd(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n-  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpq(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n-  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpb(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n-  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpw(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpd(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n+\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpq(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n+\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpb(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n+\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpw(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n@@ -1355,1 +1365,1 @@\n-  void vpmovzxbw(XMMRegister dst, Address src, int vector_len);\n+  void vpmovzxbw(XMMRegister dst, Address     src, int vector_len);\n@@ -1361,8 +1371,5 @@\n-  void vpmullw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n-  void vpmulld(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n-    Assembler::vpmulld(dst, nds, src, vector_len);\n-  };\n-  void vpmulld(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-    Assembler::vpmulld(dst, nds, src, vector_len);\n-  }\n-  void vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+  void vpmullw(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n+\n+  void vpmulld(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpmulld(dst, nds, src, vector_len); }\n+  void vpmulld(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpmulld(dst, nds, src, vector_len); }\n+  void vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1371,1 +1378,1 @@\n-  void vpsubb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpsubb(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -1374,1 +1381,1 @@\n-  void vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpsubw(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -1377,1 +1384,1 @@\n-  void vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len);\n+  void vpsraw(XMMRegister dst, XMMRegister nds, int         shift, int vector_len);\n@@ -1380,1 +1387,1 @@\n-  void evpsraq(XMMRegister dst, XMMRegister nds, int shift, int vector_len);\n+  void evpsraq(XMMRegister dst, XMMRegister nds, int         shift, int vector_len);\n@@ -1469,3 +1476,3 @@\n-  void vandpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vandpd(dst, nds, src, vector_len); }\n-  void vandpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len)     { Assembler::vandpd(dst, nds, src, vector_len); }\n-  void vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vandpd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vandpd(dst, nds, src, vector_len); }\n+  void vandpd(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vandpd(dst, nds, src, vector_len); }\n+  void vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1473,3 +1480,3 @@\n-  void vandps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vandps(dst, nds, src, vector_len); }\n-  void vandps(XMMRegister dst, XMMRegister nds, Address src, int vector_len)     { Assembler::vandps(dst, nds, src, vector_len); }\n-  void vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vandps(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vandps(dst, nds, src, vector_len); }\n+  void vandps(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vandps(dst, nds, src, vector_len); }\n+  void vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1477,1 +1484,1 @@\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n@@ -1479,3 +1486,3 @@\n-  void vdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vdivsd(dst, nds, src); }\n-  void vdivsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vdivsd(dst, nds, src); }\n-  void vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vdivsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vdivsd(dst, nds, src); }\n+  void vdivsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vdivsd(dst, nds, src); }\n+  void vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1483,3 +1490,3 @@\n-  void vdivss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vdivss(dst, nds, src); }\n-  void vdivss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vdivss(dst, nds, src); }\n-  void vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vdivss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vdivss(dst, nds, src); }\n+  void vdivss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vdivss(dst, nds, src); }\n+  void vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1487,3 +1494,3 @@\n-  void vmulsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vmulsd(dst, nds, src); }\n-  void vmulsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vmulsd(dst, nds, src); }\n-  void vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vmulsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vmulsd(dst, nds, src); }\n+  void vmulsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vmulsd(dst, nds, src); }\n+  void vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1491,3 +1498,3 @@\n-  void vmulss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vmulss(dst, nds, src); }\n-  void vmulss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vmulss(dst, nds, src); }\n-  void vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vmulss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vmulss(dst, nds, src); }\n+  void vmulss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vmulss(dst, nds, src); }\n+  void vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1495,3 +1502,3 @@\n-  void vsubsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vsubsd(dst, nds, src); }\n-  void vsubsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vsubsd(dst, nds, src); }\n-  void vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vsubsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vsubsd(dst, nds, src); }\n+  void vsubsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vsubsd(dst, nds, src); }\n+  void vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1499,3 +1506,3 @@\n-  void vsubss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vsubss(dst, nds, src); }\n-  void vsubss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vsubss(dst, nds, src); }\n-  void vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vsubss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vsubss(dst, nds, src); }\n+  void vsubss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vsubss(dst, nds, src); }\n+  void vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1503,2 +1510,2 @@\n-  void vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n-  void vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n+  void vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1508,3 +1515,3 @@\n-  void vxorpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n-  void vxorpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n-  void vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vxorpd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n+  void vxorpd(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n+  void vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1512,3 +1519,3 @@\n-  void vxorps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n-  void vxorps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n-  void vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vxorps(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n+  void vxorps(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n+  void vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1528,1 +1535,1 @@\n-  void vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1540,2 +1547,2 @@\n-  void vpermd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpermd(dst, nds, src, vector_len); }\n-  void vpermd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+  void vpermd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpermd(dst, nds, src, vector_len); }\n+  void vpermd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1771,2 +1778,8 @@\n-  void alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch);\n-  void anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n+  using Assembler::evpandq;\n+  void evpandq(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n+  using Assembler::evporq;\n+  void evporq(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n+  using Assembler::vpternlogq;\n+  void vpternlogq(XMMRegister dst, int imm8, XMMRegister src2, AddressLiteral src3, int vector_len, Register rscratch = noreg);\n@@ -1783,1 +1796,1 @@\n-  void movoop(Address dst, jobject obj);\n+  void movoop(Address  dst, jobject obj, Register rscratch);\n@@ -1786,19 +1799,11 @@\n-  void mov_metadata(Address dst, Metadata* obj);\n-\n-  void movptr(ArrayAddress dst, Register src);\n-  \/\/ can this do an lea?\n-  void movptr(Register dst, ArrayAddress src);\n-\n-  void movptr(Register dst, Address src);\n-\n-#ifdef _LP64\n-  void movptr(Register dst, AddressLiteral src, Register scratch=rscratch1);\n-#else\n-  void movptr(Register dst, AddressLiteral src, Register scratch=noreg); \/\/ Scratch reg is ignored in 32-bit\n-#endif\n-\n-  void movptr(Register dst, intptr_t src);\n-  void movptr(Register dst, Register src);\n-  void movptr(Address dst, intptr_t src);\n-\n-  void movptr(Address dst, Register src);\n+  void mov_metadata(Address  dst, Metadata* obj, Register rscratch);\n+\n+  void movptr(Register     dst, Register       src);\n+  void movptr(Register     dst, Address        src);\n+  void movptr(Register     dst, AddressLiteral src);\n+  void movptr(Register     dst, ArrayAddress   src);\n+  void movptr(Register     dst, intptr_t       src);\n+  void movptr(Address      dst, Register       src);\n+  void movptr(Address      dst, int32_t        imm);\n+  void movptr(Address      dst, intptr_t       src, Register rscratch);\n+  void movptr(ArrayAddress dst, Register       src, Register rscratch);\n@@ -1811,11 +1816,0 @@\n-#ifdef _LP64\n-  \/\/ Generally the next two are only used for moving NULL\n-  \/\/ Although there are situations in initializing the mark word where\n-  \/\/ they could be used. They are dangerous.\n-\n-  \/\/ They only exist on LP64 so that int32_t and intptr_t are not the same\n-  \/\/ and we have ambiguous declarations.\n-\n-  void movptr(Address dst, int32_t imm32);\n-  void movptr(Register dst, int32_t imm32);\n-#endif \/\/ _LP64\n@@ -1824,5 +1818,2 @@\n-  void mov32(AddressLiteral dst, Register src);\n-  void mov32(Register dst, AddressLiteral src);\n-\n-  \/\/ to avoid hiding movb\n-  void movbyte(ArrayAddress dst, int src);\n+  void mov32(Register       dst, AddressLiteral src);\n+  void mov32(AddressLiteral dst, Register        src, Register rscratch = noreg);\n@@ -1833,0 +1824,2 @@\n+  void movdl(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n@@ -1834,2 +1827,1 @@\n-  void movdl(XMMRegister dst, AddressLiteral src);\n-  void movq(XMMRegister dst, AddressLiteral src);\n+  void movq(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1838,1 +1830,1 @@\n-  void pushptr(AddressLiteral src);\n+  void pushptr(AddressLiteral src, Register rscratch);\n@@ -1843,2 +1835,2 @@\n-  void pushoop(jobject obj);\n-  void pushklass(Metadata* obj);\n+  void pushoop(jobject obj, Register rscratch);\n+  void pushklass(Metadata* obj, Register rscratch);\n@@ -1852,3 +1844,0 @@\n-  \/\/ C2 compiled method's prolog code.\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n-\n@@ -1927,1 +1916,0 @@\n-  void updateBytesAdler32(Register adler32, Register buf, Register length, XMMRegister shuf0, XMMRegister shuf1, ExternalAddress scale);\n@@ -1995,0 +1983,2 @@\n+  void fill32(Address dst, XMMRegister xmm);\n+\n@@ -1997,0 +1987,2 @@\n+  void fill64(Address dst, XMMRegister xmm, bool use64byteVector = false);\n+\n@@ -2010,27 +2002,1 @@\n-#if COMPILER2_OR_JVMCI\n-  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n-                                    Register to, Register count, int shift,\n-                                    Register index, Register temp,\n-                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                             Register to, Register start_index, Register end_index,\n-                                             Register count, int shift, Register temp,\n-                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0,\n-                         bool use64byteVector = false);\n-\n-  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0);\n-\n-  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  int shift = Address::times_1, int offset = 0);\n-\n-  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  bool conjoint, int shift = Address::times_1, int offset = 0,\n-                  bool use64byteVector = false);\n-\n+#ifdef COMPILER2_OR_JVMCI\n@@ -2039,2 +2005,0 @@\n-\n-\n@@ -2045,0 +2009,3 @@\n+\n+  void check_stack_alignment(Register sp, const char* msg, unsigned bias = 0, Register tmp = noreg);\n+\n@@ -2061,1 +2028,1 @@\n-   SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value);\n+   SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value, Register rscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":412,"deletions":445,"binary":false,"changes":857,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -45,1 +46,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -68,0 +68,16 @@\n+ATTRIBUTE_ALIGNED(16) static const uint32_t KEY_SHUFFLE_MASK[] = {\n+    0x00010203UL, 0x04050607UL, 0x08090A0BUL, 0x0C0D0E0FUL,\n+};\n+\n+ATTRIBUTE_ALIGNED(16) static const uint32_t COUNTER_SHUFFLE_MASK[] = {\n+    0x0C0D0E0FUL, 0x08090A0BUL, 0x04050607UL, 0x00010203UL,\n+};\n+\n+ATTRIBUTE_ALIGNED(16) static const uint32_t GHASH_BYTE_SWAP_MASK[] = {\n+    0x0C0D0E0FUL, 0x08090A0BUL, 0x04050607UL, 0x00010203UL,\n+};\n+\n+ATTRIBUTE_ALIGNED(16) static const uint32_t GHASH_LONG_SWAP_MASK[] = {\n+    0x0B0A0908UL, 0x0F0E0D0CUL, 0x03020100UL, 0x07060504UL,\n+};\n+\n@@ -179,1 +195,1 @@\n-      __ cmpptr(Address(rcx, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ cmpptr(Address(rcx, Thread::pending_exception_offset()), NULL_WORD);\n@@ -346,4 +362,4 @@\n-    __ movptr(Address(rcx, Thread::pending_exception_offset()), rax          );\n-    __ lea(Address(rcx, Thread::exception_file_offset   ()),\n-           ExternalAddress((address)__FILE__));\n-    __ movl(Address(rcx, Thread::exception_line_offset   ()), __LINE__ );\n+    __ movptr(Address(rcx, Thread::pending_exception_offset()), rax);\n+    __ lea(Address(rcx, Thread::exception_file_offset()),\n+           ExternalAddress((address)__FILE__), noreg);\n+    __ movl(Address(rcx, Thread::exception_line_offset()), __LINE__ );\n@@ -351,1 +367,1 @@\n-    assert(StubRoutines::_call_stub_return_address != NULL, \"_call_stub_return_address must have been generated before\");\n+    assert(StubRoutines::_call_stub_return_address != nullptr, \"_call_stub_return_address must have been generated before\");\n@@ -391,1 +407,1 @@\n-      __ cmpptr(Address(thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+      __ cmpptr(Address(thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -643,0 +659,1 @@\n+    \/\/ B\n@@ -659,0 +676,90 @@\n+\n+    \/\/ W\n+    __ emit_data(0x00010000, relocInfo::none, 0);\n+    __ emit_data(0x00030002, relocInfo::none, 0);\n+    __ emit_data(0x00050004, relocInfo::none, 0);\n+    __ emit_data(0x00070006, relocInfo::none, 0);\n+    __ emit_data(0x00090008, relocInfo::none, 0);\n+    __ emit_data(0x000B000A, relocInfo::none, 0);\n+    __ emit_data(0x000D000C, relocInfo::none, 0);\n+    __ emit_data(0x000F000E, relocInfo::none, 0);\n+    __ emit_data(0x00110010, relocInfo::none, 0);\n+    __ emit_data(0x00130012, relocInfo::none, 0);\n+    __ emit_data(0x00150014, relocInfo::none, 0);\n+    __ emit_data(0x00170016, relocInfo::none, 0);\n+    __ emit_data(0x00190018, relocInfo::none, 0);\n+    __ emit_data(0x001B001A, relocInfo::none, 0);\n+    __ emit_data(0x001D001C, relocInfo::none, 0);\n+    __ emit_data(0x001F001E, relocInfo::none, 0);\n+\n+    \/\/ D\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000001, relocInfo::none, 0);\n+    __ emit_data(0x00000002, relocInfo::none, 0);\n+    __ emit_data(0x00000003, relocInfo::none, 0);\n+    __ emit_data(0x00000004, relocInfo::none, 0);\n+    __ emit_data(0x00000005, relocInfo::none, 0);\n+    __ emit_data(0x00000006, relocInfo::none, 0);\n+    __ emit_data(0x00000007, relocInfo::none, 0);\n+    __ emit_data(0x00000008, relocInfo::none, 0);\n+    __ emit_data(0x00000009, relocInfo::none, 0);\n+    __ emit_data(0x0000000A, relocInfo::none, 0);\n+    __ emit_data(0x0000000B, relocInfo::none, 0);\n+    __ emit_data(0x0000000C, relocInfo::none, 0);\n+    __ emit_data(0x0000000D, relocInfo::none, 0);\n+    __ emit_data(0x0000000E, relocInfo::none, 0);\n+    __ emit_data(0x0000000F, relocInfo::none, 0);\n+\n+    \/\/ Q\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000001, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000002, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000003, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000004, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000005, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000006, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000007, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+\n+    \/\/ D - FP\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 0.0f\n+    __ emit_data(0x3F800000, relocInfo::none, 0); \/\/ 1.0f\n+    __ emit_data(0x40000000, relocInfo::none, 0); \/\/ 2.0f\n+    __ emit_data(0x40400000, relocInfo::none, 0); \/\/ 3.0f\n+    __ emit_data(0x40800000, relocInfo::none, 0); \/\/ 4.0f\n+    __ emit_data(0x40A00000, relocInfo::none, 0); \/\/ 5.0f\n+    __ emit_data(0x40C00000, relocInfo::none, 0); \/\/ 6.0f\n+    __ emit_data(0x40E00000, relocInfo::none, 0); \/\/ 7.0f\n+    __ emit_data(0x41000000, relocInfo::none, 0); \/\/ 8.0f\n+    __ emit_data(0x41100000, relocInfo::none, 0); \/\/ 9.0f\n+    __ emit_data(0x41200000, relocInfo::none, 0); \/\/ 10.0f\n+    __ emit_data(0x41300000, relocInfo::none, 0); \/\/ 11.0f\n+    __ emit_data(0x41400000, relocInfo::none, 0); \/\/ 12.0f\n+    __ emit_data(0x41500000, relocInfo::none, 0); \/\/ 13.0f\n+    __ emit_data(0x41600000, relocInfo::none, 0); \/\/ 14.0f\n+    __ emit_data(0x41700000, relocInfo::none, 0); \/\/ 15.0f\n+\n+    \/\/ Q - FP\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 0.0d\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 1.0d\n+    __ emit_data(0x3FF00000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 2.0d\n+    __ emit_data(0x40000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 3.0d\n+    __ emit_data(0x40080000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 4.0d\n+    __ emit_data(0x40100000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 5.0d\n+    __ emit_data(0x40140000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 6.0d\n+    __ emit_data(0x40180000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 7.0d\n+    __ emit_data(0x401c0000, relocInfo::none, 0);\n@@ -891,1 +998,1 @@\n-    __ jcc(Assembler::zero, exit);               \/\/ if obj is NULL it is ok\n+    __ jcc(Assembler::zero, exit);               \/\/ if obj is null it is ok\n@@ -904,1 +1011,1 @@\n-    __ jcc(Assembler::zero, error);              \/\/ if klass is NULL it is broken\n+    __ jcc(Assembler::zero, error);              \/\/ if klass is null it is broken\n@@ -1030,1 +1137,1 @@\n-    if (entry != NULL) {\n+    if (entry != nullptr) {\n@@ -1207,1 +1314,1 @@\n-    if (entry != NULL) {\n+    if (entry != nullptr) {\n@@ -1465,1 +1572,1 @@\n-    if (label_ptr != NULL)  __ jcc(assembler_con, *(label_ptr));        \\\n+    if (label_ptr != nullptr)  __ jcc(assembler_con, *(label_ptr));        \\\n@@ -1471,1 +1578,1 @@\n-    \/\/                                  L_success, L_failure, NULL);\n+    \/\/                                  L_success, L_failure, null);\n@@ -1500,2 +1607,2 @@\n-    if (L_success == NULL) { BLOCK_COMMENT(\"L_success:\"); }\n-    if (L_failure == NULL) { BLOCK_COMMENT(\"L_failure:\"); }\n+    if (L_success == nullptr) { BLOCK_COMMENT(\"L_success:\"); }\n+    if (L_failure == nullptr) { BLOCK_COMMENT(\"L_failure:\"); }\n@@ -1555,1 +1662,1 @@\n-    if (entry != NULL) {\n+    if (entry != nullptr) {\n@@ -1623,1 +1730,1 @@\n-                        &L_store_element, NULL);\n+                        &L_store_element, nullptr);\n@@ -1830,1 +1937,1 @@\n-    \/\/ (5) src klass and dst klass should be the same and not NULL.\n+    \/\/ (5) src klass and dst klass should be the same and not null.\n@@ -1842,1 +1949,1 @@\n-    \/\/  if (src == NULL) return -1;\n+    \/\/  if (src == null) return -1;\n@@ -1852,1 +1959,1 @@\n-    \/\/  if (dst == NULL) return -1;\n+    \/\/  if (dst == nullptr) return -1;\n@@ -1867,1 +1974,1 @@\n-    \/\/  if (src->klass() == NULL) return -1;\n+    \/\/  if (src->klass() == nullptr) return -1;\n@@ -1874,1 +1981,1 @@\n-    \/\/  assert(src->klass() != NULL);\n+    \/\/  assert(src->klass() != nullptr);\n@@ -1878,1 +1985,1 @@\n-      __ jccb(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n+      __ jccb(Assembler::notZero, L2);   \/\/ it is broken if klass is null\n@@ -1882,1 +1989,1 @@\n-      __ cmpptr(dst_klass_addr, (int32_t)NULL_WORD);\n+      __ cmpptr(dst_klass_addr, NULL_WORD);\n@@ -2051,1 +2158,1 @@\n-                          rdi_temp, NULL, &L_fail_array_check);\n+                          rdi_temp, nullptr, &L_fail_array_check);\n@@ -2115,1 +2222,1 @@\n-                               NULL, \"arrayof_jbyte_arraycopy\");\n+                               nullptr, \"arrayof_jbyte_arraycopy\");\n@@ -2128,1 +2235,1 @@\n-                               NULL, \"arrayof_jshort_arraycopy\");\n+                               nullptr, \"arrayof_jshort_arraycopy\");\n@@ -2157,1 +2264,1 @@\n-                               NULL, \"oop_arraycopy_uninit\",\n+                               nullptr, \"oop_arraycopy_uninit\",\n@@ -2186,1 +2293,1 @@\n-        generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL, \/*dest_uninitialized*\/true);\n+        generate_checkcast_copy(\"checkcast_arraycopy_uninit\", nullptr, \/*dest_uninitialized*\/true);\n@@ -2208,9 +2315,2 @@\n-  address generate_key_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x00010203, relocInfo::none, 0 );\n-    __ emit_data(0x04050607, relocInfo::none, 0 );\n-    __ emit_data(0x08090a0b, relocInfo::none, 0 );\n-    __ emit_data(0x0c0d0e0f, relocInfo::none, 0 );\n-    return start;\n+  address key_shuffle_mask_addr() {\n+    return (address)KEY_SHUFFLE_MASK;\n@@ -2219,9 +2319,2 @@\n-  address generate_counter_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x0c0d0e0f, relocInfo::none, 0);\n-    __ emit_data(0x08090a0b, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    return start;\n+  address counter_shuffle_mask_addr() {\n+    return (address)COUNTER_SHUFFLE_MASK;\n@@ -2232,1 +2325,1 @@\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {\n+  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n@@ -2234,1 +2327,1 @@\n-    if (xmm_shuf_mask != NULL) {\n+    if (xmm_shuf_mask != xnoreg) {\n@@ -2237,1 +2330,1 @@\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+      __ pshufb(xmmdst, ExternalAddress(key_shuffle_mask_addr()));\n@@ -2243,1 +2336,1 @@\n-  void aes_enc_key(XMMRegister xmmdst, XMMRegister xmmtmp, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {\n+  void aes_enc_key(XMMRegister xmmdst, XMMRegister xmmtmp, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n@@ -2250,1 +2343,1 @@\n-  void aes_dec_key(XMMRegister xmmdst, XMMRegister xmmtmp, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {\n+  void aes_dec_key(XMMRegister xmmdst, XMMRegister xmmtmp, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n@@ -2318,1 +2411,1 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n@@ -2417,1 +2510,1 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n@@ -2550,1 +2643,1 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n@@ -2718,1 +2811,1 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n@@ -2937,1 +3030,1 @@\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()));\n+    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n@@ -2941,1 +3034,1 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n@@ -2967,2 +3060,2 @@\n-      __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-      __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()));\n+      __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n+      __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n@@ -3020,2 +3113,2 @@\n-      __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-      __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()));\n+      __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n+      __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n@@ -3106,1 +3199,1 @@\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()));\n+    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n@@ -3294,10 +3387,2 @@\n-  address generate_ghash_long_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x0b0a0908, relocInfo::none, 0);\n-    __ emit_data(0x0f0e0d0c, relocInfo::none, 0);\n-    __ emit_data(0x03020100, relocInfo::none, 0);\n-    __ emit_data(0x07060504, relocInfo::none, 0);\n-\n-  return start;\n+  address ghash_long_swap_mask_addr() {\n+    return (address)GHASH_LONG_SWAP_MASK;\n@@ -3307,9 +3392,2 @@\n-  address generate_ghash_byte_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x0c0d0e0f, relocInfo::none, 0);\n-    __ emit_data(0x08090a0b, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-  return start;\n+  address ghash_byte_swap_mask_addr() {\n+    return (address)GHASH_BYTE_SWAP_MASK;\n@@ -3354,1 +3432,1 @@\n-    __ pshufb(xmm_temp0, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    __ pshufb(xmm_temp0, ExternalAddress(ghash_long_swap_mask_addr()));\n@@ -3357,1 +3435,1 @@\n-    __ pshufb(xmm_temp1, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    __ pshufb(xmm_temp1, ExternalAddress(ghash_long_swap_mask_addr()));\n@@ -3361,1 +3439,1 @@\n-    __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+    __ pshufb(xmm_temp2, ExternalAddress(ghash_byte_swap_mask_addr()));\n@@ -3447,1 +3525,1 @@\n-    __ pshufb(xmm_temp6, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    __ pshufb(xmm_temp6, ExternalAddress(ghash_long_swap_mask_addr()));\n@@ -3534,3 +3612,3 @@\n-    const Register empty = 0; \/\/ will never be used, in order not\n-                              \/\/ to change a signature for crc32c_IPL_Alg2_Alt2\n-                              \/\/ between 64\/32 I'm just keeping it here\n+    const Register empty = noreg; \/\/ will never be used, in order not\n+                                  \/\/ to change a signature for crc32c_IPL_Alg2_Alt2\n+                                  \/\/ between 64\/32 I'm just keeping it here\n@@ -3573,12 +3651,0 @@\n-    const XMMRegister x0  = xmm0;\n-    const XMMRegister x1  = xmm1;\n-    const XMMRegister x2  = xmm2;\n-    const XMMRegister x3  = xmm3;\n-\n-    const XMMRegister x4  = xmm4;\n-    const XMMRegister x5  = xmm5;\n-    const XMMRegister x6  = xmm6;\n-    const XMMRegister x7  = xmm7;\n-\n-    const Register tmp   = rbx;\n-\n@@ -3587,1 +3653,2 @@\n-    __ fast_exp(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+    __ fast_exp(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+                rax, rcx, rdx, rbx);\n@@ -3600,12 +3667,0 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-   const XMMRegister x2 = xmm2;\n-   const XMMRegister x3 = xmm3;\n-\n-   const XMMRegister x4 = xmm4;\n-   const XMMRegister x5 = xmm5;\n-   const XMMRegister x6 = xmm6;\n-   const XMMRegister x7 = xmm7;\n-\n-   const Register tmp = rbx;\n-\n@@ -3614,1 +3669,2 @@\n-   __ fast_log(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+   __ fast_log(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+               rax, rcx, rdx, rbx);\n@@ -3627,12 +3683,0 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-   const XMMRegister x2 = xmm2;\n-   const XMMRegister x3 = xmm3;\n-\n-   const XMMRegister x4 = xmm4;\n-   const XMMRegister x5 = xmm5;\n-   const XMMRegister x6 = xmm6;\n-   const XMMRegister x7 = xmm7;\n-\n-   const Register tmp = rbx;\n-\n@@ -3641,1 +3685,2 @@\n-   __ fast_log10(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+   __ fast_log10(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+               rax, rcx, rdx, rbx);\n@@ -3654,12 +3699,0 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-   const XMMRegister x2 = xmm2;\n-   const XMMRegister x3 = xmm3;\n-\n-   const XMMRegister x4 = xmm4;\n-   const XMMRegister x5 = xmm5;\n-   const XMMRegister x6 = xmm6;\n-   const XMMRegister x7 = xmm7;\n-\n-   const Register tmp = rbx;\n-\n@@ -3668,1 +3701,2 @@\n-   __ fast_pow(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+   __ fast_pow(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+               rax, rcx, rdx, rbx);\n@@ -3693,4 +3727,1 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-\n-   __ libm_sincos_huge(x0, x1, rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n+   __ libm_sincos_huge(xmm0, xmm1, rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n@@ -3708,10 +3739,0 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-   const XMMRegister x2 = xmm2;\n-   const XMMRegister x3 = xmm3;\n-\n-   const XMMRegister x4 = xmm4;\n-   const XMMRegister x5 = xmm5;\n-   const XMMRegister x6 = xmm6;\n-   const XMMRegister x7 = xmm7;\n-\n@@ -3720,1 +3741,2 @@\n-   __ fast_sin(x0, x1, x2, x3, x4, x5, x6, x7, rax, rbx, rdx);\n+   __ fast_sin(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+               rax, rbx, rdx);\n@@ -3733,12 +3755,0 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-   const XMMRegister x2 = xmm2;\n-   const XMMRegister x3 = xmm3;\n-\n-   const XMMRegister x4 = xmm4;\n-   const XMMRegister x5 = xmm5;\n-   const XMMRegister x6 = xmm6;\n-   const XMMRegister x7 = xmm7;\n-\n-   const Register tmp = rbx;\n-\n@@ -3747,1 +3757,2 @@\n-   __ fast_cos(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+   __ fast_cos(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+               rax, rcx, rdx, rbx);\n@@ -3760,4 +3771,1 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-\n-   __ libm_tancot_huge(x0, x1, rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n+   __ libm_tancot_huge(xmm0, xmm1, rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n@@ -3775,12 +3783,0 @@\n-   const XMMRegister x0 = xmm0;\n-   const XMMRegister x1 = xmm1;\n-   const XMMRegister x2 = xmm2;\n-   const XMMRegister x3 = xmm3;\n-\n-   const XMMRegister x4 = xmm4;\n-   const XMMRegister x5 = xmm5;\n-   const XMMRegister x6 = xmm6;\n-   const XMMRegister x7 = xmm7;\n-\n-   const Register tmp = rbx;\n-\n@@ -3789,1 +3785,2 @@\n-   __ fast_tan(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+   __ fast_tan(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+               rax, rcx, rdx, rbx);\n@@ -3955,1 +3952,1 @@\n-    __ set_last_Java_frame(java_thread, rsp, rbp, NULL);\n+    __ set_last_Java_frame(java_thread, rsp, rbp, nullptr, noreg);\n@@ -3976,1 +3973,1 @@\n-    __ cmpptr(Address(java_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+    __ cmpptr(Address(java_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -4010,0 +4007,80 @@\n+  address generate_cont_thaw() {\n+    if (!Continuations::enabled()) return nullptr;\n+    Unimplemented();\n+    return nullptr;\n+  }\n+\n+  address generate_cont_returnBarrier() {\n+    if (!Continuations::enabled()) return nullptr;\n+    Unimplemented();\n+    return nullptr;\n+  }\n+\n+  address generate_cont_returnBarrier_exception() {\n+    if (!Continuations::enabled()) return nullptr;\n+    Unimplemented();\n+    return nullptr;\n+  }\n+\n+#if INCLUDE_JFR\n+\n+  static void jfr_prologue(address the_pc, MacroAssembler* masm) {\n+    Register java_thread = rdi;\n+    __ get_thread(java_thread);\n+    __ set_last_Java_frame(java_thread, rsp, rbp, the_pc, noreg);\n+    __ movptr(Address(rsp, 0), java_thread);\n+  }\n+\n+  \/\/ The handle is dereferenced through a load barrier.\n+  static void jfr_epilogue(MacroAssembler* masm) {\n+    Register java_thread = rdi;\n+    __ get_thread(java_thread);\n+    __ reset_last_Java_frame(java_thread, true);\n+    __ resolve_global_jobject(rax, java_thread, rdx);\n+  }\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  static RuntimeStub* generate_jfr_write_checkpoint() {\n+    enum layout {\n+      FPUState_off         = 0,\n+      rbp_off              = FPUStateSizeInWords,\n+      rdi_off,\n+      rsi_off,\n+      rcx_off,\n+      rbx_off,\n+      saved_argument_off,\n+      saved_argument_off2, \/\/ 2nd half of double\n+      framesize\n+    };\n+\n+    int insts_size = 1024;\n+    int locs_size = 64;\n+    CodeBuffer code(\"jfr_write_checkpoint\", insts_size, locs_size);\n+    OopMapSet* oop_maps = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    MacroAssembler* _masm = masm;\n+\n+    address start = __ pc();\n+    __ enter();\n+    int frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+    jfr_prologue(the_pc, _masm);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+    jfr_epilogue(_masm);\n+    __ leave();\n+    __ ret(0);\n+\n+    OopMap* map = new OopMap(framesize, 1); \/\/ rbp\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+      RuntimeStub::new_runtime_stub(\"jfr_write_checkpoint\", &code, frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps, false);\n+    return stub;\n+  }\n+\n+#endif \/\/ INCLUDE_JFR\n+\n@@ -4013,1 +4090,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -4030,0 +4107,5 @@\n+    \/\/ Initialize table for copy memory (arraycopy) check.\n+    if (UnsafeCopyMemory::_table == nullptr) {\n+      UnsafeCopyMemory::create_table(16);\n+    }\n+\n@@ -4054,9 +4136,0 @@\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||\n-          vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos) ||\n-          vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::x86::_L_2il0floatpacket_0_adr = (address)StubRoutines::x86::_L_2il0floatpacket_0;\n-        StubRoutines::x86::_Pi4Inv_adr = (address)StubRoutines::x86::_Pi4Inv;\n-        StubRoutines::x86::_Pi4x3_adr = (address)StubRoutines::x86::_Pi4x3;\n-        StubRoutines::x86::_Pi4x4_adr = (address)StubRoutines::x86::_Pi4x4;\n-        StubRoutines::x86::_ones_adr = (address)StubRoutines::x86::_ones;\n-      }\n@@ -4097,1 +4170,11 @@\n-  void generate_all() {\n+  void generate_continuation_stubs() {\n+    \/\/ Continuation stubs:\n+    StubRoutines::_cont_thaw          = generate_cont_thaw();\n+    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n+\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+  }\n+\n+  void generate_final_stubs() {\n@@ -4106,2 +4189,16 @@\n-    \/\/------------------------------------------------------------------------------------------------------------------------\n-    \/\/ entry points that are platform specific\n+    \/\/ support for verify_oop (must happen after universe_init)\n+    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n+\n+    \/\/ arraycopy stubs used by compilers\n+    generate_arraycopy_stubs();\n+\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != nullptr) {\n+      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+    }\n+  }\n+\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n+\n+    \/\/ entry points that are C2\/JVMCI specific\n@@ -4143,6 +4240,0 @@\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n@@ -4151,2 +4242,0 @@\n-      StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ might be needed by the others\n-\n@@ -4160,1 +4249,0 @@\n-      StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n@@ -4183,2 +4271,0 @@\n-      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-      StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n@@ -4187,5 +4273,1 @@\n-\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -4196,6 +4278,18 @@\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n-      generate_initial();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -4205,6 +4299,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 8\n-void StubGenerator_generate(CodeBuffer* code, bool all) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, all);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":321,"deletions":231,"binary":false,"changes":552,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,2 +27,1 @@\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"ci\/ciUtilities.hpp\"\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -34,7 +33,1 @@\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"nativeInst_x86.hpp\"\n-#include \"oops\/instanceOop.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/objArrayKlass.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -43,2 +36,1 @@\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -46,2 +38,1 @@\n-#include \"runtime\/stubCodeGenerator.hpp\"\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n@@ -51,0 +42,1 @@\n+#include \"opto\/c2_globals.hpp\"\n@@ -58,0 +50,3 @@\n+#if INCLUDE_JFR\n+#include \"jfr\/support\/jfrIntrinsics.hpp\"\n+#endif\n@@ -59,1 +54,0 @@\n-\/\/ Declaration and definition of StubGenerator (no .hpp file).\n@@ -65,1 +59,0 @@\n-#define a__ ((Assembler*)_masm)->\n@@ -71,1 +64,1 @@\n-#endif\n+#endif \/\/ PRODUCT\n@@ -74,134 +67,70 @@\n-const int MXCSR_MASK = 0xFFC0;  \/\/ Mask out any pending exceptions\n-\n-\/\/ Stub Code definitions\n-\n-class StubGenerator: public StubCodeGenerator {\n- private:\n-\n-#ifdef PRODUCT\n-#define inc_counter_np(counter) ((void)0)\n-#else\n-  void inc_counter_np_(int& counter) {\n-    \/\/ This can destroy rscratch1 if counter is far from the code cache\n-    __ incrementl(ExternalAddress((address)&counter));\n-  }\n-#define inc_counter_np(counter) \\\n-  BLOCK_COMMENT(\"inc_counter \" #counter); \\\n-  inc_counter_np_(counter);\n-#endif\n-\n-  \/\/ Call stubs are used to call Java from C\n-  \/\/\n-  \/\/ Linux Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    c_rarg4:   (interpreter) entry point              address\n-  \/\/    c_rarg5:   parameters                             intptr_t*\n-  \/\/    16(rbp): parameter size (in words)              int\n-  \/\/    24(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -12 [ argument word 1      ]\n-  \/\/ -11 [ saved r15            ] <--- rsp_after_call\n-  \/\/ -10 [ saved r14            ]\n-  \/\/  -9 [ saved r13            ]\n-  \/\/  -8 [ saved r12            ]\n-  \/\/  -7 [ saved rbx            ]\n-  \/\/  -6 [ call wrapper         ]\n-  \/\/  -5 [ result               ]\n-  \/\/  -4 [ result type          ]\n-  \/\/  -3 [ method               ]\n-  \/\/  -2 [ entry point          ]\n-  \/\/  -1 [ parameters           ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ parameter size       ]\n-  \/\/   3 [ thread               ]\n-  \/\/\n-  \/\/ Windows Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    48(rbp): (interpreter) entry point              address\n-  \/\/    56(rbp): parameters                             intptr_t*\n-  \/\/    64(rbp): parameter size (in words)              int\n-  \/\/    72(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -60 [ argument word 1      ]\n-  \/\/ -59 [ saved xmm31          ] <--- rsp after_call\n-  \/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n-  \/\/ -27 [ saved xmm15          ]\n-  \/\/     [ saved xmm7-xmm14     ]\n-  \/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n-  \/\/  -7 [ saved r15            ]\n-  \/\/  -6 [ saved r14            ]\n-  \/\/  -5 [ saved r13            ]\n-  \/\/  -4 [ saved r12            ]\n-  \/\/  -3 [ saved rdi            ]\n-  \/\/  -2 [ saved rsi            ]\n-  \/\/  -1 [ saved rbx            ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ call wrapper         ]\n-  \/\/   3 [ result               ]\n-  \/\/   4 [ result type          ]\n-  \/\/   5 [ method               ]\n-  \/\/   6 [ entry point          ]\n-  \/\/   7 [ parameters           ]\n-  \/\/   8 [ parameter size       ]\n-  \/\/   9 [ thread               ]\n-  \/\/\n-  \/\/    Windows reserves the callers stack space for arguments 1-4.\n-  \/\/    We spill c_rarg0-c_rarg3 to this space.\n-  \/\/ Call stub stack layout word offsets from rbp\n-  enum call_stub_layout {\n-#ifdef _WIN64\n-    xmm_save_first     = 6,  \/\/ save from xmm6\n-    xmm_save_last      = 31, \/\/ to xmm31\n-    xmm_save_base      = -9,\n-    rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n-    r15_off            = -7,\n-    r14_off            = -6,\n-    r13_off            = -5,\n-    r12_off            = -4,\n-    rdi_off            = -3,\n-    rsi_off            = -2,\n-    rbx_off            = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    call_wrapper_off   =  2,\n-    result_off         =  3,\n-    result_type_off    =  4,\n-    method_off         =  5,\n-    entry_point_off    =  6,\n-    parameters_off     =  7,\n-    parameter_size_off =  8,\n-    thread_off         =  9\n-#else\n-    rsp_after_call_off = -12,\n-    mxcsr_off          = rsp_after_call_off,\n-    r15_off            = -11,\n-    r14_off            = -10,\n-    r13_off            = -9,\n-    r12_off            = -8,\n-    rbx_off            = -7,\n-    call_wrapper_off   = -6,\n-    result_off         = -5,\n-    result_type_off    = -4,\n-    method_off         = -3,\n-    entry_point_off    = -2,\n-    parameters_off     = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    parameter_size_off =  2,\n-    thread_off         =  3\n-#endif\n-  };\n+\/\/\n+\/\/ Linux Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    c_rarg4:   (interpreter) entry point              address\n+\/\/    c_rarg5:   parameters                             intptr_t*\n+\/\/    16(rbp): parameter size (in words)              int\n+\/\/    24(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -12 [ argument word 1      ]\n+\/\/ -11 [ saved r15            ] <--- rsp_after_call\n+\/\/ -10 [ saved r14            ]\n+\/\/  -9 [ saved r13            ]\n+\/\/  -8 [ saved r12            ]\n+\/\/  -7 [ saved rbx            ]\n+\/\/  -6 [ call wrapper         ]\n+\/\/  -5 [ result               ]\n+\/\/  -4 [ result type          ]\n+\/\/  -3 [ method               ]\n+\/\/  -2 [ entry point          ]\n+\/\/  -1 [ parameters           ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ parameter size       ]\n+\/\/   3 [ thread               ]\n+\/\/\n+\/\/ Windows Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    48(rbp): (interpreter) entry point              address\n+\/\/    56(rbp): parameters                             intptr_t*\n+\/\/    64(rbp): parameter size (in words)              int\n+\/\/    72(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -60 [ argument word 1      ]\n+\/\/ -59 [ saved xmm31          ] <--- rsp after_call\n+\/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n+\/\/ -27 [ saved xmm15          ]\n+\/\/     [ saved xmm7-xmm14     ]\n+\/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n+\/\/  -7 [ saved r15            ]\n+\/\/  -6 [ saved r14            ]\n+\/\/  -5 [ saved r13            ]\n+\/\/  -4 [ saved r12            ]\n+\/\/  -3 [ saved rdi            ]\n+\/\/  -2 [ saved rsi            ]\n+\/\/  -1 [ saved rbx            ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ call wrapper         ]\n+\/\/   3 [ result               ]\n+\/\/   4 [ result type          ]\n+\/\/   5 [ method               ]\n+\/\/   6 [ entry point          ]\n+\/\/   7 [ parameters           ]\n+\/\/   8 [ parameter size       ]\n+\/\/   9 [ thread               ]\n+\/\/\n+\/\/    Windows reserves the callers stack space for arguments 1-4.\n+\/\/    We spill c_rarg0-c_rarg3 to this space.\n@@ -210,0 +139,1 @@\n+\/\/ Call stub stack layout word offsets from rbp\n@@ -211,38 +141,83 @@\n-  Address xmm_save(int reg) {\n-    assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n-    return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n-  }\n-#endif\n-\n-  address generate_call_stub(address& return_address) {\n-    assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n-           (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n-           \"adjust this code\");\n-    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n-    address start = __ pc();\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-\n-    const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n-    const Address result        (rbp, result_off         * wordSize);\n-    const Address result_type   (rbp, result_type_off    * wordSize);\n-    const Address method        (rbp, method_off         * wordSize);\n-    const Address entry_point   (rbp, entry_point_off    * wordSize);\n-    const Address parameters    (rbp, parameters_off     * wordSize);\n-    const Address parameter_size(rbp, parameter_size_off * wordSize);\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address thread        (rbp, thread_off         * wordSize);\n-\n-    const Address r15_save(rbp, r15_off * wordSize);\n-    const Address r14_save(rbp, r14_off * wordSize);\n-    const Address r13_save(rbp, r13_off * wordSize);\n-    const Address r12_save(rbp, r12_off * wordSize);\n-    const Address rbx_save(rbp, rbx_off * wordSize);\n-\n-    \/\/ stub code\n-    __ enter();\n-    __ subptr(rsp, -rsp_after_call_off * wordSize);\n-\n-    \/\/ save register parameters\n+enum call_stub_layout {\n+  xmm_save_first     = 6,  \/\/ save from xmm6\n+  xmm_save_last      = 31, \/\/ to xmm31\n+  xmm_save_base      = -9,\n+  rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n+  r15_off            = -7,\n+  r14_off            = -6,\n+  r13_off            = -5,\n+  r12_off            = -4,\n+  rdi_off            = -3,\n+  rsi_off            = -2,\n+  rbx_off            = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  call_wrapper_off   =  2,\n+  result_off         =  3,\n+  result_type_off    =  4,\n+  method_off         =  5,\n+  entry_point_off    =  6,\n+  parameters_off     =  7,\n+  parameter_size_off =  8,\n+  thread_off         =  9\n+};\n+\n+static Address xmm_save(int reg) {\n+  assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n+  return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n+}\n+#else \/\/ !_WIN64\n+enum call_stub_layout {\n+  rsp_after_call_off = -12,\n+  mxcsr_off          = rsp_after_call_off,\n+  r15_off            = -11,\n+  r14_off            = -10,\n+  r13_off            = -9,\n+  r12_off            = -8,\n+  rbx_off            = -7,\n+  call_wrapper_off   = -6,\n+  result_off         = -5,\n+  result_type_off    = -4,\n+  method_off         = -3,\n+  entry_point_off    = -2,\n+  parameters_off     = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  parameter_size_off =  2,\n+  thread_off         =  3\n+};\n+#endif \/\/ _WIN64\n+\n+address StubGenerator::generate_call_stub(address& return_address) {\n+\n+  assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n+         (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n+         \"adjust this code\");\n+  StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n+  address start = __ pc();\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+\n+  const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n+  const Address result        (rbp, result_off         * wordSize);\n+  const Address result_type   (rbp, result_type_off    * wordSize);\n+  const Address method        (rbp, method_off         * wordSize);\n+  const Address entry_point   (rbp, entry_point_off    * wordSize);\n+  const Address parameters    (rbp, parameters_off     * wordSize);\n+  const Address parameter_size(rbp, parameter_size_off * wordSize);\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address thread        (rbp, thread_off         * wordSize);\n+\n+  const Address r15_save(rbp, r15_off * wordSize);\n+  const Address r14_save(rbp, r14_off * wordSize);\n+  const Address r13_save(rbp, r13_off * wordSize);\n+  const Address r12_save(rbp, r12_off * wordSize);\n+  const Address rbx_save(rbp, rbx_off * wordSize);\n+\n+  \/\/ stub code\n+  __ enter();\n+  __ subptr(rsp, -rsp_after_call_off * wordSize);\n+\n+  \/\/ save register parameters\n@@ -250,2 +225,2 @@\n-    __ movptr(parameters,   c_rarg5); \/\/ parameters\n-    __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n+  __ movptr(parameters,   c_rarg5); \/\/ parameters\n+  __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n@@ -254,4 +229,4 @@\n-    __ movptr(method,       c_rarg3); \/\/ method\n-    __ movl(result_type,  c_rarg2);   \/\/ result type\n-    __ movptr(result,       c_rarg1); \/\/ result\n-    __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n+  __ movptr(method,       c_rarg3); \/\/ method\n+  __ movl(result_type,  c_rarg2);   \/\/ result type\n+  __ movptr(result,       c_rarg1); \/\/ result\n+  __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n@@ -259,6 +234,6 @@\n-    \/\/ save regs belonging to calling function\n-    __ movptr(rbx_save, rbx);\n-    __ movptr(r12_save, r12);\n-    __ movptr(r13_save, r13);\n-    __ movptr(r14_save, r14);\n-    __ movptr(r15_save, r15);\n+  \/\/ save regs belonging to calling function\n+  __ movptr(rbx_save, rbx);\n+  __ movptr(r12_save, r12);\n+  __ movptr(r13_save, r13);\n+  __ movptr(r14_save, r14);\n+  __ movptr(r15_save, r15);\n@@ -267,3 +242,7 @@\n-    int last_reg = 15;\n-    if (UseAVX > 2) {\n-      last_reg = 31;\n+  int last_reg = 15;\n+  if (UseAVX > 2) {\n+    last_reg = 31;\n+  }\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n@@ -271,8 +250,3 @@\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(xmm_save(i), as_XMMRegister(i));\n-      }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(xmm_save(i), as_XMMRegister(i));\n@@ -280,0 +254,1 @@\n+  }\n@@ -281,2 +256,2 @@\n-    const Address rdi_save(rbp, rdi_off * wordSize);\n-    const Address rsi_save(rbp, rsi_off * wordSize);\n+  const Address rdi_save(rbp, rdi_off * wordSize);\n+  const Address rsi_save(rbp, rsi_off * wordSize);\n@@ -284,2 +259,2 @@\n-    __ movptr(rsi_save, rsi);\n-    __ movptr(rdi_save, rdi);\n+  __ movptr(rsi_save, rsi);\n+  __ movptr(rdi_save, rdi);\n@@ -287,12 +262,12 @@\n-    const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n-    {\n-      Label skip_ldmx;\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ cmp32(rax, mxcsr_std);\n-      __ jcc(Assembler::equal, skip_ldmx);\n-      __ ldmxcsr(mxcsr_std);\n-      __ bind(skip_ldmx);\n-    }\n+  const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n+  {\n+    Label skip_ldmx;\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, skip_ldmx);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+    __ bind(skip_ldmx);\n+  }\n@@ -301,3 +276,3 @@\n-    \/\/ Load up thread register\n-    __ movptr(r15_thread, thread);\n-    __ reinit_heapbase();\n+  \/\/ Load up thread register\n+  __ movptr(r15_thread, thread);\n+  __ reinit_heapbase();\n@@ -306,8 +281,8 @@\n-    \/\/ make sure we have no pending exceptions\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure we have no pending exceptions\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n+    __ bind(L);\n+  }\n@@ -316,49 +291,49 @@\n-    \/\/ pass parameters if any\n-    BLOCK_COMMENT(\"pass parameters if any\");\n-    Label parameters_done;\n-    __ movl(c_rarg3, parameter_size);\n-    __ testl(c_rarg3, c_rarg3);\n-    __ jcc(Assembler::zero, parameters_done);\n-\n-    Label loop;\n-    __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n-    __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n-    __ BIND(loop);\n-    __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n-    __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n-    __ decrementl(c_rarg1);             \/\/ decrement counter\n-    __ push(rax);                       \/\/ pass parameter\n-    __ jcc(Assembler::notZero, loop);\n-\n-    \/\/ call Java function\n-    __ BIND(parameters_done);\n-    __ movptr(rbx, method);             \/\/ get Method*\n-    __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n-    __ mov(r13, rsp);                   \/\/ set sender sp\n-    BLOCK_COMMENT(\"call Java function\");\n-    __ call(c_rarg1);\n-\n-    BLOCK_COMMENT(\"call_stub_return_address:\");\n-    return_address = __ pc();\n-\n-    \/\/ store result depending on type (everything that is not\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(c_rarg0, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(c_rarg1, result_type);\n-    __ cmpl(c_rarg1, T_OBJECT);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(c_rarg1, T_LONG);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(c_rarg1, T_FLOAT);\n-    __ jcc(Assembler::equal, is_float);\n-    __ cmpl(c_rarg1, T_DOUBLE);\n-    __ jcc(Assembler::equal, is_double);\n-\n-    \/\/ handle T_INT case\n-    __ movl(Address(c_rarg0, 0), rax);\n-\n-    __ BIND(exit);\n-\n-    \/\/ pop parameters\n-    __ lea(rsp, rsp_after_call);\n+  \/\/ pass parameters if any\n+  BLOCK_COMMENT(\"pass parameters if any\");\n+  Label parameters_done;\n+  __ movl(c_rarg3, parameter_size);\n+  __ testl(c_rarg3, c_rarg3);\n+  __ jcc(Assembler::zero, parameters_done);\n+\n+  Label loop;\n+  __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n+  __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n+  __ BIND(loop);\n+  __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n+  __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n+  __ decrementl(c_rarg1);             \/\/ decrement counter\n+  __ push(rax);                       \/\/ pass parameter\n+  __ jcc(Assembler::notZero, loop);\n+\n+  \/\/ call Java function\n+  __ BIND(parameters_done);\n+  __ movptr(rbx, method);             \/\/ get Method*\n+  __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n+  __ mov(r13, rsp);                   \/\/ set sender sp\n+  BLOCK_COMMENT(\"call Java function\");\n+  __ call(c_rarg1);\n+\n+  BLOCK_COMMENT(\"call_stub_return_address:\");\n+  return_address = __ pc();\n+\n+  \/\/ store result depending on type (everything that is not\n+  \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(c_rarg0, result);\n+  Label is_long, is_float, is_double, exit;\n+  __ movl(c_rarg1, result_type);\n+  __ cmpl(c_rarg1, T_OBJECT);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(c_rarg1, T_LONG);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(c_rarg1, T_FLOAT);\n+  __ jcc(Assembler::equal, is_float);\n+  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ jcc(Assembler::equal, is_double);\n+\n+  \/\/ handle T_INT case\n+  __ movl(Address(c_rarg0, 0), rax);\n+\n+  __ BIND(exit);\n+\n+  \/\/ pop parameters\n+  __ lea(rsp, rsp_after_call);\n@@ -367,17 +342,17 @@\n-    \/\/ verify that threads correspond\n-    {\n-     Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::call_stub: threads must correspond\");\n-      __ bind(L3);\n-    }\n+  \/\/ verify that threads correspond\n+  {\n+   Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::call_stub: threads must correspond\");\n+    __ bind(L3);\n+  }\n@@ -386,1 +361,3 @@\n-    \/\/ restore regs belonging to calling function\n+  __ pop_cont_fastpath();\n+\n+  \/\/ restore regs belonging to calling function\n@@ -388,9 +365,4 @@\n-    \/\/ emit the restores for xmm regs\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(as_XMMRegister(i), xmm_save(i));\n-      }\n+  \/\/ emit the restores for xmm regs\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n@@ -398,0 +370,5 @@\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(as_XMMRegister(i), xmm_save(i));\n+    }\n+  }\n@@ -399,5 +376,5 @@\n-    __ movptr(r15, r15_save);\n-    __ movptr(r14, r14_save);\n-    __ movptr(r13, r13_save);\n-    __ movptr(r12, r12_save);\n-    __ movptr(rbx, rbx_save);\n+  __ movptr(r15, r15_save);\n+  __ movptr(r14, r14_save);\n+  __ movptr(r13, r13_save);\n+  __ movptr(r12, r12_save);\n+  __ movptr(rbx, rbx_save);\n@@ -406,2 +383,2 @@\n-    __ movptr(rdi, rdi_save);\n-    __ movptr(rsi, rsi_save);\n+  __ movptr(rdi, rdi_save);\n+  __ movptr(rsi, rsi_save);\n@@ -409,1 +386,1 @@\n-    __ ldmxcsr(mxcsr_save);\n+  __ ldmxcsr(mxcsr_save);\n@@ -412,2 +389,2 @@\n-    \/\/ restore rsp\n-    __ addptr(rsp, -rsp_after_call_off * wordSize);\n+  \/\/ restore rsp\n+  __ addptr(rsp, -rsp_after_call_off * wordSize);\n@@ -415,4 +392,4 @@\n-    \/\/ return\n-    __ vzeroupper();\n-    __ pop(rbp);\n-    __ ret(0);\n+  \/\/ return\n+  __ vzeroupper();\n+  __ pop(rbp);\n+  __ ret(0);\n@@ -420,4 +397,4 @@\n-    \/\/ handle return types different from T_INT\n-    __ BIND(is_long);\n-    __ movq(Address(c_rarg0, 0), rax);\n-    __ jmp(exit);\n+  \/\/ handle return types different from T_INT\n+  __ BIND(is_long);\n+  __ movq(Address(c_rarg0, 0), rax);\n+  __ jmp(exit);\n@@ -425,3 +402,3 @@\n-    __ BIND(is_float);\n-    __ movflt(Address(c_rarg0, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_float);\n+  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ jmp(exit);\n@@ -429,3 +406,3 @@\n-    __ BIND(is_double);\n-    __ movdbl(Address(c_rarg0, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_double);\n+  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ jmp(exit);\n@@ -433,2 +410,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -436,11 +413,11 @@\n-  \/\/ Return point for a Java call if there's an exception thrown in\n-  \/\/ Java code.  The exception is caught and transformed into a\n-  \/\/ pending exception stored in JavaThread that can be tested from\n-  \/\/ within the VM.\n-  \/\/\n-  \/\/ Note: Usually the parameters are removed by the callee. In case\n-  \/\/ of an exception crossing an activation frame boundary, that is\n-  \/\/ not the case if the callee is compiled code => need to setup the\n-  \/\/ rsp.\n-  \/\/\n-  \/\/ rax: exception oop\n+\/\/ Return point for a Java call if there's an exception thrown in\n+\/\/ Java code.  The exception is caught and transformed into a\n+\/\/ pending exception stored in JavaThread that can be tested from\n+\/\/ within the VM.\n+\/\/\n+\/\/ Note: Usually the parameters are removed by the callee. In case\n+\/\/ of an exception crossing an activation frame boundary, that is\n+\/\/ not the case if the callee is compiled code => need to setup the\n+\/\/ rsp.\n+\/\/\n+\/\/ rax: exception oop\n@@ -448,3 +425,3 @@\n-  address generate_catch_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_catch_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n+  address start = __ pc();\n@@ -452,3 +429,3 @@\n-    \/\/ same as in generate_call_stub():\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-    const Address thread        (rbp, thread_off         * wordSize);\n+  \/\/ same as in generate_call_stub():\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+  const Address thread        (rbp, thread_off         * wordSize);\n@@ -457,17 +434,17 @@\n-    \/\/ verify that threads correspond\n-    {\n-      Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n-      __ bind(L3);\n-    }\n+  \/\/ verify that threads correspond\n+  {\n+    Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n+    __ bind(L3);\n+  }\n@@ -476,2 +453,2 @@\n-    \/\/ set pending exception\n-    __ verify_oop(rax);\n+  \/\/ set pending exception\n+  __ verify_oop(rax);\n@@ -479,4 +456,4 @@\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n-    __ lea(rscratch1, ExternalAddress((address)__FILE__));\n-    __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n-    __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n+  __ lea(rscratch1, ExternalAddress((address)__FILE__));\n+  __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n+  __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n@@ -484,4 +461,4 @@\n-    \/\/ complete return to VM\n-    assert(StubRoutines::_call_stub_return_address != NULL,\n-           \"_call_stub_return_address must have been generated before\");\n-    __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n+  \/\/ complete return to VM\n+  assert(StubRoutines::_call_stub_return_address != nullptr,\n+         \"_call_stub_return_address must have been generated before\");\n+  __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n@@ -489,2 +466,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -492,10 +469,10 @@\n-  \/\/ Continuation point for runtime calls returning with a pending\n-  \/\/ exception.  The pending exception check happened in the runtime\n-  \/\/ or native call stub.  The pending exception in Thread is\n-  \/\/ converted into a Java-level exception.\n-  \/\/\n-  \/\/ Contract with Java-level exception handlers:\n-  \/\/ rax: exception\n-  \/\/ rdx: throwing pc\n-  \/\/\n-  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n+\/\/ Continuation point for runtime calls returning with a pending\n+\/\/ exception.  The pending exception check happened in the runtime\n+\/\/ or native call stub.  The pending exception in Thread is\n+\/\/ converted into a Java-level exception.\n+\/\/\n+\/\/ Contract with Java-level exception handlers:\n+\/\/ rax: exception\n+\/\/ rdx: throwing pc\n+\/\/\n+\/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n@@ -503,3 +480,3 @@\n-  address generate_forward_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_forward_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n+  address start = __ pc();\n@@ -507,8 +484,8 @@\n-    \/\/ Upon entry, the sp points to the return address returning into\n-    \/\/ Java (interpreted or compiled) code; i.e., the return address\n-    \/\/ becomes the throwing pc.\n-    \/\/\n-    \/\/ Arguments pushed before the runtime call are still on the stack\n-    \/\/ but the exception handler will reset the stack pointer ->\n-    \/\/ ignore them.  A potential result in registers can be ignored as\n-    \/\/ well.\n+  \/\/ Upon entry, the sp points to the return address returning into\n+  \/\/ Java (interpreted or compiled) code; i.e., the return address\n+  \/\/ becomes the throwing pc.\n+  \/\/\n+  \/\/ Arguments pushed before the runtime call are still on the stack\n+  \/\/ but the exception handler will reset the stack pointer ->\n+  \/\/ ignore them.  A potential result in registers can be ignored as\n+  \/\/ well.\n@@ -517,8 +494,8 @@\n-    \/\/ make sure this code is only executed if there is a pending exception\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t) NULL);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure this code is only executed if there is a pending exception\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n+    __ bind(L);\n+  }\n@@ -527,7 +504,7 @@\n-    \/\/ compute exception handler into rbx\n-    __ movptr(c_rarg0, Address(rsp, 0));\n-    BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n-                         SharedRuntime::exception_handler_for_return_address),\n-                    r15_thread, c_rarg0);\n-    __ mov(rbx, rax);\n+  \/\/ compute exception handler into rbx\n+  __ movptr(c_rarg0, Address(rsp, 0));\n+  BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                       SharedRuntime::exception_handler_for_return_address),\n+                  r15_thread, c_rarg0);\n+  __ mov(rbx, rax);\n@@ -535,4 +512,4 @@\n-    \/\/ setup rax & rdx, remove return address & clear pending exception\n-    __ pop(rdx);\n-    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  \/\/ setup rax & rdx, remove return address & clear pending exception\n+  __ pop(rdx);\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -541,8 +518,8 @@\n-    \/\/ make sure exception is set\n-    {\n-      Label L;\n-      __ testptr(rax, rax);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure exception is set\n+  {\n+    Label L;\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n+    __ bind(L);\n+  }\n@@ -551,9 +528,6 @@\n-    \/\/ continue at exception handler (return address removed)\n-    \/\/ rax: exception\n-    \/\/ rbx: exception handler\n-    \/\/ rdx: throwing pc\n-    __ verify_oop(rax);\n-    __ jmp(rbx);\n-\n-    return start;\n-  }\n+  \/\/ continue at exception handler (return address removed)\n+  \/\/ rax: exception\n+  \/\/ rbx: exception handler\n+  \/\/ rdx: throwing pc\n+  __ verify_oop(rax);\n+  __ jmp(rbx);\n@@ -561,10 +535,2 @@\n-  \/\/ Support for intptr_t OrderAccess::fence()\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/\n-  \/\/ Result:\n-  address generate_orderaccess_fence() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n-    address start = __ pc();\n-    __ membar(Assembler::StoreLoad);\n-    __ ret(0);\n+  return start;\n+}\n@@ -572,2 +538,8 @@\n-    return start;\n-  }\n+\/\/ Support for intptr_t OrderAccess::fence()\n+\/\/\n+\/\/ Arguments :\n+\/\/\n+\/\/ Result:\n+address StubGenerator::generate_orderaccess_fence() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n+  address start = __ pc();\n@@ -575,0 +547,2 @@\n+  __ membar(Assembler::StoreLoad);\n+  __ ret(0);\n@@ -576,11 +550,2 @@\n-  \/\/ Support for intptr_t get_previous_sp()\n-  \/\/\n-  \/\/ This routine is used to find the previous stack pointer for the\n-  \/\/ caller.\n-  address generate_get_previous_sp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n-    address start = __ pc();\n-\n-    __ movptr(rax, rsp);\n-    __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n-    __ ret(0);\n+  return start;\n+}\n@@ -588,34 +553,7 @@\n-    return start;\n-  }\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Support for void verify_mxcsr()\n-  \/\/\n-  \/\/ This routine is used with -Xcheck:jni to verify that native\n-  \/\/ JNI code does not return to Java code without restoring the\n-  \/\/ MXCSR register to our expected state.\n-\n-  address generate_verify_mxcsr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n-    address start = __ pc();\n-\n-    const Address mxcsr_save(rsp, 0);\n-\n-    if (CheckJNICalls) {\n-      Label ok_ret;\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ push(rax);\n-      __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      __ cmp32(rax, mxcsr_std);\n-      __ jcc(Assembler::equal, ok_ret);\n-\n-      __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n-\n-      __ ldmxcsr(mxcsr_std);\n-\n-      __ bind(ok_ret);\n-      __ addptr(rsp, wordSize);\n-      __ pop(rax);\n-    }\n+\/\/ Support for intptr_t get_previous_sp()\n+\/\/\n+\/\/ This routine is used to find the previous stack pointer for the\n+\/\/ caller.\n+address StubGenerator::generate_get_previous_sp() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n+  address start = __ pc();\n@@ -624,1 +562,3 @@\n-    __ ret(0);\n+  __ movptr(rax, rsp);\n+  __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n+  __ ret(0);\n@@ -626,2 +566,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -629,3 +569,6 @@\n-  address generate_f2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Support for void verify_mxcsr()\n+\/\/\n+\/\/ This routine is used with -Xcheck:jni to verify that native\n+\/\/ JNI code does not return to Java code without restoring the\n+\/\/ MXCSR register to our expected state.\n@@ -633,1 +576,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_verify_mxcsr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n+  address start = __ pc();\n@@ -635,1 +580,1 @@\n-    Label L;\n+  const Address mxcsr_save(rsp, 0);\n@@ -637,0 +582,3 @@\n+  if (CheckJNICalls) {\n+    Label ok_ret;\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n@@ -638,15 +586,6 @@\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg3, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmovl(Assembler::positive, c_rarg3, rax);\n+    __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, ok_ret);\n@@ -654,2 +593,1 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+    __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n@@ -657,3 +595,4 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+\n+    __ bind(ok_ret);\n+    __ addptr(rsp, wordSize);\n@@ -661,0 +600,1 @@\n+  }\n@@ -662,1 +602,1 @@\n-    __ ret(0);\n+  __ ret(0);\n@@ -664,2 +604,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -667,4 +607,3 @@\n-  address generate_f2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n-    address start = __ pc();\n+address StubGenerator::generate_f2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n@@ -672,1 +611,1 @@\n-    Label L;\n+  address start = __ pc();\n@@ -674,16 +613,1 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg3, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmov(Assembler::positive, c_rarg3, rax);\n+  Label L;\n@@ -691,2 +615,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -694,4 +620,11 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg3, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmovl(Assembler::positive, c_rarg3, rax);\n@@ -699,1 +632,2 @@\n-    __ ret(0);\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -701,2 +635,4 @@\n-    return start;\n-  }\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -704,3 +640,1 @@\n-  address generate_d2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ ret(0);\n@@ -708,1 +642,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -710,1 +645,4 @@\n-    Label L;\n+address StubGenerator::generate_f2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+  address start = __ pc();\n@@ -712,24 +650,1 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg2, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmov(Assembler::positive, c_rarg2, rax);\n+  Label L;\n@@ -737,2 +652,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg2);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -740,5 +657,11 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg3, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmov(Assembler::positive, c_rarg3, rax);\n@@ -746,1 +669,2 @@\n-    __ ret(0);\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -748,2 +672,4 @@\n-    return start;\n-  }\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -751,3 +677,1 @@\n-  address generate_d2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ ret(0);\n@@ -755,1 +679,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -757,1 +682,43 @@\n-    Label L;\n+address StubGenerator::generate_d2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg2, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmov(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movptr(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -759,24 +726,2 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg2, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmovq(Assembler::positive, c_rarg2, rax);\n+  return start;\n+}\n@@ -784,2 +729,43 @@\n-    __ bind(L);\n-    __ movq(inout, c_rarg2);\n+address StubGenerator::generate_d2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg2, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmovq(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movq(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -787,5 +773,2 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  return start;\n+}\n@@ -793,1 +776,4 @@\n-    __ ret(0);\n+address StubGenerator::generate_count_leading_zeros_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -795,2 +781,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -798,14 +790,2 @@\n-  address generate_count_leading_zeros_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -813,14 +793,4 @@\n-  address generate_popcount_avx_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_popcount_avx_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -828,14 +798,8 @@\n-  address generate_iota_indices(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0706050403020100, relocInfo::none);\n-    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n-    __ emit_data64(0x1716151413121110, relocInfo::none);\n-    __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n-    __ emit_data64(0x2726252423222120, relocInfo::none);\n-    __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n-    __ emit_data64(0x3736353433323130, relocInfo::none);\n-    __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n@@ -843,14 +807,2 @@\n-  address generate_vector_reverse_bit_lut(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -858,14 +810,60 @@\n-  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_iota_indices(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+  \/\/ B\n+  __ emit_data64(0x0706050403020100, relocInfo::none);\n+  __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+  __ emit_data64(0x1716151413121110, relocInfo::none);\n+  __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n+  __ emit_data64(0x2726252423222120, relocInfo::none);\n+  __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n+  __ emit_data64(0x3736353433323130, relocInfo::none);\n+  __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n+  \/\/ W\n+  __ emit_data64(0x0003000200010000, relocInfo::none);\n+  __ emit_data64(0x0007000600050004, relocInfo::none);\n+  __ emit_data64(0x000B000A00090008, relocInfo::none);\n+  __ emit_data64(0x000F000E000D000C, relocInfo::none);\n+  __ emit_data64(0x0013001200110010, relocInfo::none);\n+  __ emit_data64(0x0017001600150014, relocInfo::none);\n+  __ emit_data64(0x001B001A00190018, relocInfo::none);\n+  __ emit_data64(0x001F001E001D001C, relocInfo::none);\n+  \/\/ D\n+  __ emit_data64(0x0000000100000000, relocInfo::none);\n+  __ emit_data64(0x0000000300000002, relocInfo::none);\n+  __ emit_data64(0x0000000500000004, relocInfo::none);\n+  __ emit_data64(0x0000000700000006, relocInfo::none);\n+  __ emit_data64(0x0000000900000008, relocInfo::none);\n+  __ emit_data64(0x0000000B0000000A, relocInfo::none);\n+  __ emit_data64(0x0000000D0000000C, relocInfo::none);\n+  __ emit_data64(0x0000000F0000000E, relocInfo::none);\n+  \/\/ Q\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000005, relocInfo::none);\n+  __ emit_data64(0x0000000000000006, relocInfo::none);\n+  __ emit_data64(0x0000000000000007, relocInfo::none);\n+  \/\/ D - FP\n+  __ emit_data64(0x3F80000000000000, relocInfo::none); \/\/ 0.0f, 1.0f\n+  __ emit_data64(0x4040000040000000, relocInfo::none); \/\/ 2.0f, 3.0f\n+  __ emit_data64(0x40A0000040800000, relocInfo::none); \/\/ 4.0f, 5.0f\n+  __ emit_data64(0x40E0000040C00000, relocInfo::none); \/\/ 6.0f, 7.0f\n+  __ emit_data64(0x4110000041000000, relocInfo::none); \/\/ 8.0f, 9.0f\n+  __ emit_data64(0x4130000041200000, relocInfo::none); \/\/ 10.0f, 11.0f\n+  __ emit_data64(0x4150000041400000, relocInfo::none); \/\/ 12.0f, 13.0f\n+  __ emit_data64(0x4170000041600000, relocInfo::none); \/\/ 14.0f, 15.0f\n+  \/\/ Q - FP\n+  __ emit_data64(0x0000000000000000, relocInfo::none); \/\/ 0.0d\n+  __ emit_data64(0x3FF0000000000000, relocInfo::none); \/\/ 1.0d\n+  __ emit_data64(0x4000000000000000, relocInfo::none); \/\/ 2.0d\n+  __ emit_data64(0x4008000000000000, relocInfo::none); \/\/ 3.0d\n+  __ emit_data64(0x4010000000000000, relocInfo::none); \/\/ 4.0d\n+  __ emit_data64(0x4014000000000000, relocInfo::none); \/\/ 5.0d\n+  __ emit_data64(0x4018000000000000, relocInfo::none); \/\/ 6.0d\n+  __ emit_data64(0x401c000000000000, relocInfo::none); \/\/ 7.0d\n+  return start;\n+}\n@@ -873,14 +871,4 @@\n-  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_bit_lut(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -888,14 +876,8 @@\n-  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n@@ -903,10 +885,2 @@\n-  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -914,4 +888,4 @@\n-  address generate_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -919,2 +893,8 @@\n-    __ emit_data64( mask, relocInfo::none );\n-    __ emit_data64( mask, relocInfo::none );\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n@@ -922,2 +902,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -925,16 +905,4 @@\n-  address generate_vector_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -942,4 +910,8 @@\n-  address generate_vector_byte_perm_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n@@ -947,8 +919,2 @@\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000005, relocInfo::none);\n-    __ emit_data64(0x0000000000000007, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000006, relocInfo::none);\n+  return start;\n+}\n@@ -956,2 +922,4 @@\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -959,16 +927,8 @@\n-  address generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n@@ -976,30 +936,2 @@\n-  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n-                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n-                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n-                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n-                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n-    __ emit_data(val0, relocInfo::none, 0);\n-    __ emit_data(val1, relocInfo::none, 0);\n-    __ emit_data(val2, relocInfo::none, 0);\n-    __ emit_data(val3, relocInfo::none, 0);\n-    if (len >= Assembler::AVX_256bit) {\n-      __ emit_data(val4, relocInfo::none, 0);\n-      __ emit_data(val5, relocInfo::none, 0);\n-      __ emit_data(val6, relocInfo::none, 0);\n-      __ emit_data(val7, relocInfo::none, 0);\n-      if (len >= Assembler::AVX_512bit) {\n-        __ emit_data(val8, relocInfo::none, 0);\n-        __ emit_data(val9, relocInfo::none, 0);\n-        __ emit_data(val10, relocInfo::none, 0);\n-        __ emit_data(val11, relocInfo::none, 0);\n-        __ emit_data(val12, relocInfo::none, 0);\n-        __ emit_data(val13, relocInfo::none, 0);\n-        __ emit_data(val14, relocInfo::none, 0);\n-        __ emit_data(val15, relocInfo::none, 0);\n-      }\n-    }\n+  return start;\n+}\n@@ -1007,2 +939,4 @@\n-    return start;\n-  }\n+address StubGenerator::generate_vector_byte_shuffle_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1010,41 +944,7 @@\n-  \/\/ Non-destructive plausibility checks for oops\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/    all args on stack!\n-  \/\/\n-  \/\/ Stack after saving c_rarg3:\n-  \/\/    [tos + 0]: saved c_rarg3\n-  \/\/    [tos + 1]: saved c_rarg2\n-  \/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n-  \/\/    [tos + 3]: saved flags\n-  \/\/    [tos + 4]: return address\n-  \/\/  * [tos + 5]: error message (char*)\n-  \/\/  * [tos + 6]: object to verify (oop)\n-  \/\/  * [tos + 7]: saved rax - saved by caller and bashed\n-  \/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n-  \/\/  * = popped on exit\n-  address generate_verify_oop() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n-    address start = __ pc();\n-\n-    Label exit, error;\n-\n-    __ pushf();\n-    __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));\n-\n-    __ push(r12);\n-\n-    \/\/ save c_rarg2 and c_rarg3\n-    __ push(c_rarg2);\n-    __ push(c_rarg3);\n-\n-    enum {\n-           \/\/ After previous pushes.\n-           oop_to_verify = 6 * wordSize,\n-           saved_rax     = 7 * wordSize,\n-           saved_r10     = 8 * wordSize,\n-\n-           \/\/ Before the call to MacroAssembler::debug(), see below.\n-           return_addr   = 16 * wordSize,\n-           error_msg     = 17 * wordSize\n-    };\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+\n+  return start;\n+}\n@@ -1052,2 +952,4 @@\n-    \/\/ get object\n-    __ movptr(rax, Address(rsp, oop_to_verify));\n+address StubGenerator::generate_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1055,3 +957,2 @@\n-    \/\/ make sure object is 'reasonable'\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n+  __ emit_data64( mask, relocInfo::none );\n+  __ emit_data64( mask, relocInfo::none );\n@@ -1059,7 +960,2 @@\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      \/\/ Check if metadata bits indicate a bad oop\n-      __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ jcc(Assembler::notZero, error);\n-    }\n-#endif\n+  return start;\n+}\n@@ -1067,7 +963,4 @@\n-    \/\/ Check if the oop is in the right area of memory\n-    __ movptr(c_rarg2, rax);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n-    __ andptr(c_rarg2, c_rarg3);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n-    __ cmpptr(c_rarg2, c_rarg3);\n-    __ jcc(Assembler::notZero, error);\n+address StubGenerator::generate_vector_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1075,50 +968,8 @@\n-    \/\/ make sure klass is 'reasonable', which is not zero.\n-    __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n-\n-    \/\/ return if everything seems ok\n-    __ bind(exit);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ restore c_rarg3\n-    __ pop(c_rarg2);                             \/\/ restore c_rarg2\n-    __ pop(r12);                                 \/\/ restore r12\n-    __ popf();                                   \/\/ restore flags\n-    __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n-\n-    \/\/ handle errors\n-    __ bind(error);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n-    __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n-    __ pop(r12);                                 \/\/ get saved r12 back\n-    __ popf();                                   \/\/ get saved flags off stack --\n-                                                 \/\/ will be ignored\n-\n-    __ pusha();                                  \/\/ push registers\n-                                                 \/\/ (rip is already\n-                                                 \/\/ already pushed)\n-    \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n-    \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n-    \/\/ pushed all the registers, so now the stack looks like:\n-    \/\/     [tos +  0] 16 saved registers\n-    \/\/     [tos + 16] return address\n-    \/\/   * [tos + 17] error message (char*)\n-    \/\/   * [tos + 18] object to verify (oop)\n-    \/\/   * [tos + 19] saved rax - saved by caller and bashed\n-    \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n-    \/\/   * = popped on exit\n-\n-    __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n-    __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n-    __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n-    __ mov(r12, rsp);                               \/\/ remember rsp\n-    __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-    __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n-    BLOCK_COMMENT(\"call MacroAssembler::debug\");\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n-    __ hlt();\n-    return start;\n-  }\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1126,19 +977,2 @@\n-  \/\/\n-  \/\/ Verify that a register contains clean 32-bits positive value\n-  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    Rint  -  32-bits value\n-  \/\/    Rtmp  -  scratch\n-  \/\/\n-  void assert_clean_int(Register Rint, Register Rtmp) {\n-#ifdef ASSERT\n-    Label L;\n-    assert_different_registers(Rtmp, Rint);\n-    __ movslq(Rtmp, Rint);\n-    __ cmpq(Rtmp, Rint);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"high 32-bits of int value are not 0\");\n-    __ bind(L);\n-#endif\n-  }\n+  return start;\n+}\n@@ -1146,34 +980,62 @@\n-  \/\/  Generate overlap test for array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/     c_rarg0 - from\n-  \/\/     c_rarg1 - to\n-  \/\/     c_rarg2 - element count\n-  \/\/\n-  \/\/  Output:\n-  \/\/     rax   - &from[element count - 1]\n-  \/\/\n-  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n-    assert(no_overlap_target != NULL, \"must be generated\");\n-    array_overlap_test(no_overlap_target, NULL, sf);\n-  }\n-  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n-    array_overlap_test(NULL, &L_no_overlap, sf);\n-  }\n-  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n-    const Register from     = c_rarg0;\n-    const Register to       = c_rarg1;\n-    const Register count    = c_rarg2;\n-    const Register end_from = rax;\n-\n-    __ cmpptr(to, from);\n-    __ lea(end_from, Address(from, count, sf, 0));\n-    if (NOLp == NULL) {\n-      ExternalAddress no_overlap(no_overlap_target);\n-      __ jump_cc(Assembler::belowEqual, no_overlap);\n-      __ cmpptr(to, end_from);\n-      __ jump_cc(Assembler::aboveEqual, no_overlap);\n-    } else {\n-      __ jcc(Assembler::belowEqual, (*NOLp));\n-      __ cmpptr(to, end_from);\n-      __ jcc(Assembler::aboveEqual, (*NOLp));\n+address StubGenerator::generate_vector_byte_perm_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000005, relocInfo::none);\n+  __ emit_data64(0x0000000000000007, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000006, relocInfo::none);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                   int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                   int32_t val4, int32_t val5, int32_t val6, int32_t val7,\n+                                   int32_t val8, int32_t val9, int32_t val10, int32_t val11,\n+                                   int32_t val12, int32_t val13, int32_t val14, int32_t val15) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+\n+  assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n+  __ emit_data(val0, relocInfo::none, 0);\n+  __ emit_data(val1, relocInfo::none, 0);\n+  __ emit_data(val2, relocInfo::none, 0);\n+  __ emit_data(val3, relocInfo::none, 0);\n+  if (len >= Assembler::AVX_256bit) {\n+    __ emit_data(val4, relocInfo::none, 0);\n+    __ emit_data(val5, relocInfo::none, 0);\n+    __ emit_data(val6, relocInfo::none, 0);\n+    __ emit_data(val7, relocInfo::none, 0);\n+    if (len >= Assembler::AVX_512bit) {\n+      __ emit_data(val8, relocInfo::none, 0);\n+      __ emit_data(val9, relocInfo::none, 0);\n+      __ emit_data(val10, relocInfo::none, 0);\n+      __ emit_data(val11, relocInfo::none, 0);\n+      __ emit_data(val12, relocInfo::none, 0);\n+      __ emit_data(val13, relocInfo::none, 0);\n+      __ emit_data(val14, relocInfo::none, 0);\n+      __ emit_data(val15, relocInfo::none, 0);\n@@ -1182,0 +1044,2 @@\n+  return start;\n+}\n@@ -1183,12 +1047,98 @@\n-  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n-  \/\/\n-  \/\/ Outputs:\n-  \/\/    rdi - rcx\n-  \/\/    rsi - rdx\n-  \/\/    rdx - r8\n-  \/\/    rcx - r9\n-  \/\/\n-  \/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n-  \/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n-  \/\/\n-  DEBUG_ONLY(bool regs_in_thread;)\n+\/\/ Non-destructive plausibility checks for oops\n+\/\/\n+\/\/ Arguments:\n+\/\/    all args on stack!\n+\/\/\n+\/\/ Stack after saving c_rarg3:\n+\/\/    [tos + 0]: saved c_rarg3\n+\/\/    [tos + 1]: saved c_rarg2\n+\/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n+\/\/    [tos + 3]: saved flags\n+\/\/    [tos + 4]: return address\n+\/\/  * [tos + 5]: error message (char*)\n+\/\/  * [tos + 6]: object to verify (oop)\n+\/\/  * [tos + 7]: saved rax - saved by caller and bashed\n+\/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n+\/\/  * = popped on exit\n+address StubGenerator::generate_verify_oop() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n+  address start = __ pc();\n+\n+  Label exit, error;\n+\n+  __ pushf();\n+  __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()), rscratch1);\n+\n+  __ push(r12);\n+\n+  \/\/ save c_rarg2 and c_rarg3\n+  __ push(c_rarg2);\n+  __ push(c_rarg3);\n+\n+  enum {\n+    \/\/ After previous pushes.\n+    oop_to_verify = 6 * wordSize,\n+    saved_rax     = 7 * wordSize,\n+    saved_r10     = 8 * wordSize,\n+\n+    \/\/ Before the call to MacroAssembler::debug(), see below.\n+    return_addr   = 16 * wordSize,\n+    error_msg     = 17 * wordSize\n+  };\n+\n+  \/\/ get object\n+  __ movptr(rax, Address(rsp, oop_to_verify));\n+\n+  \/\/ make sure object is 'reasonable'\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, exit); \/\/ if obj is null it is OK\n+\n+   BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+   bs_asm->check_oop(_masm, rax, c_rarg2, c_rarg3, error);\n+\n+  \/\/ return if everything seems ok\n+  __ bind(exit);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ restore c_rarg3\n+  __ pop(c_rarg2);                             \/\/ restore c_rarg2\n+  __ pop(r12);                                 \/\/ restore r12\n+  __ popf();                                   \/\/ restore flags\n+  __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n+\n+  \/\/ handle errors\n+  __ bind(error);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n+  __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n+  __ pop(r12);                                 \/\/ get saved r12 back\n+  __ popf();                                   \/\/ get saved flags off stack --\n+                                               \/\/ will be ignored\n+\n+  __ pusha();                                  \/\/ push registers\n+                                               \/\/ (rip is already\n+                                               \/\/ already pushed)\n+  \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n+  \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n+  \/\/ pushed all the registers, so now the stack looks like:\n+  \/\/     [tos +  0] 16 saved registers\n+  \/\/     [tos + 16] return address\n+  \/\/   * [tos + 17] error message (char*)\n+  \/\/   * [tos + 18] object to verify (oop)\n+  \/\/   * [tos + 19] saved rax - saved by caller and bashed\n+  \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n+  \/\/   * = popped on exit\n+\n+  __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n+  __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n+  __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n+  __ mov(r12, rsp);                               \/\/ remember rsp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n+  BLOCK_COMMENT(\"call MacroAssembler::debug\");\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n+  __ hlt();\n+\n+  return start;\n+}\n@@ -1196,4 +1146,16 @@\n-  void setup_arg_regs(int nargs = 3) {\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-    assert(nargs == 3 || nargs == 4, \"else fix\");\n+\n+\/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+\/\/\n+\/\/ Outputs:\n+\/\/    rdi - rcx\n+\/\/    rsi - rdx\n+\/\/    rdx - r8\n+\/\/    rcx - r9\n+\/\/\n+\/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n+\/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n+\/\/\n+void StubGenerator::setup_arg_regs(int nargs) {\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+  assert(nargs == 3 || nargs == 4, \"else fix\");\n@@ -1201,11 +1163,13 @@\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    if (nargs >= 4)\n-      __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n-    __ movptr(saved_rdi, rdi);\n-    __ movptr(saved_rsi, rsi);\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-    if (nargs >= 4)\n-      __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  if (nargs == 4) {\n+    __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n+  }\n+  __ movptr(saved_rdi, rdi);\n+  __ movptr(saved_rsi, rsi);\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+  if (nargs == 4) {\n+    __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n+  }\n@@ -1213,2 +1177,2 @@\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n@@ -1216,2 +1180,2 @@\n-    DEBUG_ONLY(regs_in_thread = false;)\n-  }\n+  DEBUG_ONLY(_regs_in_thread = false;)\n+}\n@@ -1219,4 +1183,5 @@\n-  void restore_arg_regs() {\n-    assert(!regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n+\n+void StubGenerator::restore_arg_regs() {\n+  assert(!_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n@@ -1224,2 +1189,2 @@\n-    __ movptr(rdi, saved_rdi);\n-    __ movptr(rsi, saved_rsi);\n+  __ movptr(rdi, saved_rdi);\n+  __ movptr(rsi, saved_rsi);\n@@ -1227,1 +1192,2 @@\n-  }\n+}\n+\n@@ -1229,4 +1195,5 @@\n-  \/\/ This is used in places where r10 is a scratch register, and can\n-  \/\/ be adapted if r9 is needed also.\n-  void setup_arg_regs_using_thread() {\n-    const Register saved_r15 = r9;\n+\/\/ This is used in places where r10 is a scratch register, and can\n+\/\/ be adapted if r9 is needed also.\n+void StubGenerator::setup_arg_regs_using_thread(int nargs) {\n+  const Register saved_r15 = r9;\n+  assert(nargs == 3 || nargs == 4, \"else fix\");\n@@ -1234,10 +1201,16 @@\n-    __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n-    __ get_thread(r15_thread);\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n-\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n+  if (nargs == 4) {\n+    __ mov(rax, r9);       \/\/ r9 is also saved_r15\n+  }\n+  __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n+  __ get_thread(r15_thread);\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n+\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+  if (nargs == 4) {\n+    __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n+  }\n@@ -1245,2 +1218,2 @@\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n@@ -1248,2 +1221,2 @@\n-    DEBUG_ONLY(regs_in_thread = true;)\n-  }\n+  DEBUG_ONLY(_regs_in_thread = true;)\n+}\n@@ -1251,2120 +1224,3 @@\n-  void restore_arg_regs_using_thread() {\n-    assert(regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ get_thread(r15_thread);\n-    __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n-    __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n-    __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n-#endif\n-  }\n-\n-  \/\/ Copy big chunks forward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   end_from     - source arrays end address\n-  \/\/   end_to       - destination array end address\n-  \/\/   qword_count  - 64-bits element count, negative\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_forward(Register end_from, Register end_to,\n-                             Register qword_count, Register to,\n-                             Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-        __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-        __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-      }\n-\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 8);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-      __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-      __ jccb(Assembler::greater, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n-      }\n-      __ addptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 4);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-    }\n-    __ subptr(qword_count, 4);\n-    __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n-\n-  \/\/ Copy big chunks backward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   from         - source arrays address\n-  \/\/   dest         - destination array address\n-  \/\/   qword_count  - 64-bits element count\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_backward(Register from, Register dest,\n-                              Register qword_count, Register to,\n-                              Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-        __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-        __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-      }\n-\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 8);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-\n-      __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-      __ jccb(Assembler::less, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      }\n-      __ subptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 24));\n-      __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 16));\n-      __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  8));\n-      __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  0));\n-      __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 4);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-    }\n-    __ addptr(qword_count, 4);\n-    __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n-\n-#ifndef PRODUCT\n-    int& get_profile_ctr(int shift) {\n-      if ( 0 == shift)\n-        return SharedRuntime::_jbyte_array_copy_ctr;\n-      else if(1 == shift)\n-        return SharedRuntime::_jshort_array_copy_ctr;\n-      else if(2 == shift)\n-        return SharedRuntime::_jint_array_copy_ctr;\n-      else\n-        return SharedRuntime::_jlong_array_copy_ctr;\n-    }\n-#endif\n-\n-  void setup_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                        \/\/ r9 and r10 may be used to save non-volatile registers\n-    } else {\n-      setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    }\n-  }\n-\n-  void restore_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      restore_arg_regs();\n-    } else {\n-      restore_arg_regs_using_thread();\n-    }\n-  }\n-\n-#if COMPILER2_OR_JVMCI\n-  \/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n-  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n-  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n-  \/\/   default configuration.\n-  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n-  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n-  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n-  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n-  \/\/   copy performs better.\n-  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n-  \/\/   64 byte vector registers (ZMMs).\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n-  \/\/\n-\n-  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             bool aligned, bool is_oop, bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = r11;\n-    const Register temp3       = rax;\n-    const Register temp4       = rcx;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-    setup_argument_regs(type);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    {\n-      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n-      int loop_size[]        = { 192,     96,       48,      24};\n-      int threshold[]        = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n-      \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n-      __ mov64(temp4, 0);\n-      __ movq(temp1, count);\n-\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n-\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if (avx3threshold != 0) {\n-        __ cmpq(count, threshold[shift]);\n-        if (MaxVectorSize == 64) {\n-          \/\/ Copy using 64 byte vectors.\n-          __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-        } else {\n-          assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n-          \/\/ REP MOVS offer a faster copy path.\n-          __ jcc(Assembler::greaterEqual, L_repmovs);\n-        }\n-      }\n-\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 32);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n-\n-        __ BIND(L_main_pre_loop);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n-\n-        __ addq(temp1, loop_size[shift]);\n-\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n-\n-        __ BIND(L_repmovs);\n-          __ movq(temp2, temp1);\n-          \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n-          __ movq(temp3, to);\n-          __ movq(to,  from);\n-          __ movq(from, temp3);\n-          \/\/ Save to\/from for restoration post rep_mov.\n-          __ movq(temp1, to);\n-          __ movq(temp3, from);\n-          if(shift < 3) {\n-            __ shrq(temp2, 3-shift);     \/\/ quad word count\n-          }\n-          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n-          __ rep_mov();\n-          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n-          if(shift) {\n-            __ shrq(temp2, shift);       \/\/ type specific count.\n-          }\n-          \/\/ Restore original addresses in to\/from.\n-          __ movq(to, temp3);\n-          __ movq(from, temp1);\n-          __ movq(temp4, temp2);\n-          __ movq(temp1, count);\n-          __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n-          __ jmp(L_tail);\n-      }\n-\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 64);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        __ addq(temp1, loop_size[shift]);\n-        \/\/ Zero length check.\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                        temp4, temp3, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n-\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if (is_oop) {\n-      __ movq(r11, shift == 3 ? count : to);\n-    }\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             address nooverlap_target, bool aligned, bool is_oop,\n-                                             bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-\n-    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = rcx;\n-    const Register temp3       = r11;\n-    const Register temp4       = rax;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n-\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-    setup_argument_regs(type);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-    {\n-      \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n-      int loop_size[]   = { 192,     96,       48,      24};\n-      int threshold[]   = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count.\n-      __ movq(temp1, count);\n-\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ mov64(temp2, 0);\n-      __ movq(temp3, temp1);\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                               temp4, use64byteVector, L_entry, L_exit);\n-\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n-        __ cmpq(temp1, threshold[shift]);\n-        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-      }\n-\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n-\n-        __ BIND(L_main_pre_loop);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n-           __ subptr(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n-\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n-      }\n-\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n-           __ subq(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        \/\/ Zero length check.\n-        __ cmpq(temp1, 0);\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ mov64(temp2, 0);\n-        __ movq(temp3, temp1);\n-        __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                                 temp4, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if(is_oop) {\n-      __ movq(r11, count);\n-    }\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_byte_copy().\n-  \/\/\n-  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n-                                                 aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    Label L_copy_byte, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count); \/\/ make the count negative\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(byte_count, 4);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(byte_count, 2);\n-      __ jccb(Assembler::zero, L_copy_byte);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 2);\n-      __ addptr(end_to, 2);\n-\n-      \/\/ Check for and copy trailing byte\n-    __ BIND(L_copy_byte);\n-      __ testl(byte_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movb(rax, Address(end_from, 8));\n-      __ movb(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n-                                      address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_1);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.\n-\n-      \/\/ Check for and copy trailing byte\n-      __ testl(byte_count, 1);\n-      __ jcc(Assembler::zero, L_copy_2_bytes);\n-      __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-      __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-      __ decrement(byte_count); \/\/ Adjust for possible trailing word\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(byte_count, 2);\n-      __ jcc(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-      __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(byte_count, 4);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_short_copy().\n-  \/\/\n-  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n-                                                 aligned, false, false);\n-    }\n-#endif\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Original 'dest' is trashed, so we can't use it as a\n-      \/\/ base register for a possible trailing word copy\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(word_count, 2);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n-  }\n-\n-  address generate_fill(BasicType t, bool aligned, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    const Register to       = c_rarg0;  \/\/ destination array address\n-    const Register value    = c_rarg1;  \/\/ value\n-    const Register count    = c_rarg2;  \/\/ elements count\n-    __ mov(r11, count);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n-\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n-                                       address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_2);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing word\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-      __ movw(Address(to, word_count, Address::times_2, -2), rax);\n-\n-     \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(word_count, 2);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_int_oop_copy().\n-  \/\/\n-  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n-                                         const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    inc_counter_np(SharedRuntime::_jint_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n-                                         address *entry, const char *name,\n-                                         bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_4);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ no registers are destroyed by this call\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing dword\n-      __ testl(dword_count, 1);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-      __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    }\n-    restore_arg_regs_using_thread();\n-    inc_counter_np(SharedRuntime::_jint_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-\n-  __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    inc_counter_np(SharedRuntime::_jint_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n- \/\/ Side Effects:\n-  \/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n-  \/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n-  \/\/\n-  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = rcx;  \/\/ destination array end address\n-    const Register saved_count = r11;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    if (is_oop) {\n-      inc_counter_np(SharedRuntime::_oop_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    } else {\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    }\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n-                                          address nooverlap_target, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register saved_count = rcx;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_8);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    if (is_oop) {\n-      inc_counter_np(SharedRuntime::_oop_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    } else {\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    }\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ Helper for generating a dynamic type check.\n-  \/\/ Smashes no registers.\n-  void generate_type_check(Register sub_klass,\n-                           Register super_check_offset,\n-                           Register super_klass,\n-                           Label& L_success) {\n-    assert_different_registers(sub_klass, super_check_offset, super_klass);\n-\n-    BLOCK_COMMENT(\"type_check:\");\n-\n-    Label L_miss;\n-\n-    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n-                                     super_check_offset);\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n-\n-    \/\/ Fall through on failure!\n-    __ BIND(L_miss);\n-  }\n-\n-  \/\/\n-  \/\/  Generate checkcasting array copy stub\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/    c_rarg3   - size_t ckoff (super_check_offset)\n-  \/\/ not Win64\n-  \/\/    c_rarg4   - oop ckval (super_klass)\n-  \/\/ Win64\n-  \/\/    rsp+40    - oop ckval (super_klass)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_checkcast_copy(const char *name, address *entry,\n-                                  bool dest_uninitialized = false) {\n-\n-    Label L_load_element, L_store_element, L_do_card_marks, L_done;\n-\n-    \/\/ Input registers (after setup_arg_regs)\n-    const Register from        = rdi;   \/\/ source array address\n-    const Register to          = rsi;   \/\/ destination array address\n-    const Register length      = rdx;   \/\/ elements count\n-    const Register ckoff       = rcx;   \/\/ super_check_offset\n-    const Register ckval       = r8;    \/\/ super_klass\n-\n-    \/\/ Registers used as temps (r13, r14 are save-on-entry)\n-    const Register end_from    = from;  \/\/ source array end address\n-    const Register end_to      = r13;   \/\/ destination array end address\n-    const Register count       = rdx;   \/\/ -(count_remaining)\n-    const Register r14_length  = r14;   \/\/ saved copy of length\n-    \/\/ End pointers are inclusive, and if length is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    const Register rax_oop    = rax;    \/\/ actual oop copied\n-    const Register r11_klass  = r11;    \/\/ oop._klass\n-\n-    \/\/---------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the two arrays are subtypes of Object[] but the\n-    \/\/ destination array type is not equal to or a supertype\n-    \/\/ of the source type.  Each element must be separately\n-    \/\/ checked.\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef ASSERT\n-    \/\/ caller guarantees that the arrays really are different\n-    \/\/ otherwise, we would have to make conjoint checks\n-    { Label L;\n-      array_overlap_test(L, TIMES_OOP);\n-      __ stop(\"checkcast_copy within a single array\");\n-      __ bind(L);\n-    }\n-#endif \/\/ASSERT\n-\n-    setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n-                       \/\/ ckoff => rcx, ckval => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last argument (#4) is on stack on Win64\n-    __ movptr(ckval, Address(rsp, 6 * wordSize));\n-#endif\n-\n-    \/\/ Caller of this entry point must set up the argument registers.\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-      saved_r13_offset,\n-      saved_r14_offset,\n-      saved_r10_offset,\n-      saved_rbp_offset\n-    };\n-    __ subptr(rsp, saved_rbp_offset * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-    __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n-\n-#ifdef ASSERT\n-      Label L2;\n-      __ get_thread(r14);\n-      __ cmpptr(r15_thread, r14);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-#endif \/\/ ASSERT\n-\n-    \/\/ check that int operands are properly extended to size_t\n-    assert_clean_int(length, rax);\n-    assert_clean_int(ckoff, rax);\n-\n-#ifdef ASSERT\n-    BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n-    \/\/ The ckoff and ckval must be mutually consistent,\n-    \/\/ even though caller generates both.\n-    { Label L;\n-      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ cmpl(ckoff, Address(ckval, sco_offset));\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"super_check_offset inconsistent\");\n-      __ bind(L);\n-    }\n-#endif \/\/ASSERT\n-\n-    \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n-    Address end_from_addr(from, length, TIMES_OOP, 0);\n-    Address   end_to_addr(to,   length, TIMES_OOP, 0);\n-    \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n-    Address from_element_addr(end_from, count, TIMES_OOP, 0);\n-    Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-\n-    BasicType type = T_OBJECT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    \/\/ Copy from low to high addresses, indexed from the end of each array.\n-    __ lea(end_from, end_from_addr);\n-    __ lea(end_to,   end_to_addr);\n-    __ movptr(r14_length, length);        \/\/ save a copy of the length\n-    assert(length == count, \"\");          \/\/ else fix next line:\n-    __ negptr(count);                     \/\/ negate and test the length\n-    __ jcc(Assembler::notZero, L_load_element);\n-\n-    \/\/ Empty array:  Nothing to do.\n-    __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n-    __ jmp(L_done);\n-\n-    \/\/ ======== begin loop ========\n-    \/\/ (Loop is rotated; its entry is L_load_element.)\n-    \/\/ Loop control:\n-    \/\/   for (count = -count; count != 0; count++)\n-    \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_store_element);\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n-    __ increment(count);               \/\/ increment the count toward zero\n-    __ jcc(Assembler::zero, L_do_card_marks);\n-\n-    \/\/ ======== loop entry is here ========\n-    __ BIND(L_load_element);\n-    __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n-    __ testptr(rax_oop, rax_oop);\n-    __ jcc(Assembler::zero, L_store_element);\n-\n-    __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n-    generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n-    \/\/ ======== end loop ========\n-\n-    \/\/ It was a real error; we must depend on the caller to finish the job.\n-    \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n-    \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n-    \/\/ and report their number to the caller.\n-    assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n-    Label L_post_barrier;\n-    __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n-    __ movptr(rax, r14_length);       \/\/ save the value\n-    __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n-    __ jccb(Assembler::notZero, L_post_barrier);\n-    __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n-\n-    \/\/ Come here on success only.\n-    __ BIND(L_do_card_marks);\n-    __ xorptr(rax, rax);              \/\/ return 0 on success\n-\n-    __ BIND(L_post_barrier);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n-\n-    \/\/ Common exit point (success or failure).\n-    __ BIND(L_done);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/\n-  \/\/  Generate 'unsafe' array copy stub\n-  \/\/  Though just as safe as the other stubs, it takes an unscaled\n-  \/\/  size_t argument instead of an element count.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ Examines the alignment of the operands and dispatches\n-  \/\/ to a long, int, short, or byte copy loop.\n-  \/\/\n-  address generate_unsafe_copy(const char *name,\n-                               address byte_copy_entry, address short_copy_entry,\n-                               address int_copy_entry, address long_copy_entry) {\n-\n-    Label L_long_aligned, L_int_aligned, L_short_aligned;\n-\n-    \/\/ Input registers (before setup_arg_regs)\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register size        = c_rarg2;  \/\/ byte count (size_t)\n-\n-    \/\/ Register used as a temp\n-    const Register bits        = rax;      \/\/ test copy of low bits\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ bump this on entry, not on exit:\n-    inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);\n-\n-    __ mov(bits, from);\n-    __ orptr(bits, to);\n-    __ orptr(bits, size);\n-\n-    __ testb(bits, BytesPerLong-1);\n-    __ jccb(Assembler::zero, L_long_aligned);\n-\n-    __ testb(bits, BytesPerInt-1);\n-    __ jccb(Assembler::zero, L_int_aligned);\n-\n-    __ testb(bits, BytesPerShort-1);\n-    __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n-\n-    __ BIND(L_short_aligned);\n-    __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-    __ BIND(L_int_aligned);\n-    __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-    __ BIND(L_long_aligned);\n-    __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    return start;\n-  }\n-\n-  \/\/ Perform range checks on the proposed arraycopy.\n-  \/\/ Kills temp, but nothing else.\n-  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n-  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n-                              Register src_pos, \/\/ source position (c_rarg1)\n-                              Register dst,     \/\/ destination array oo (c_rarg2)\n-                              Register dst_pos, \/\/ destination position (c_rarg3)\n-                              Register length,\n-                              Register temp,\n-                              Label& L_failed) {\n-    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n-\n-    \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, src_pos);             \/\/ src_pos + length\n-    __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, dst_pos);             \/\/ dst_pos + length\n-    __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n-    \/\/ Move with sign extension can be used since they are positive.\n-    __ movslq(src_pos, src_pos);\n-    __ movslq(dst_pos, dst_pos);\n-\n-    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n-  }\n-\n-  \/\/\n-  \/\/  Generate generic array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0    -  src oop\n-  \/\/    c_rarg1    -  src_pos (32-bits)\n-  \/\/    c_rarg2    -  dst oop\n-  \/\/    c_rarg3    -  dst_pos (32-bits)\n-  \/\/ not Win64\n-  \/\/    c_rarg4    -  element count (32-bits)\n-  \/\/ Win64\n-  \/\/    rsp+40     -  element count (32-bits)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_generic_copy(const char *name,\n-                                address byte_copy_entry, address short_copy_entry,\n-                                address int_copy_entry, address oop_copy_entry,\n-                                address long_copy_entry, address checkcast_copy_entry) {\n-\n-    Label L_failed, L_failed_0, L_objArray;\n-    Label L_copy_shorts, L_copy_ints, L_copy_longs;\n-\n-    \/\/ Input registers\n-    const Register src        = c_rarg0;  \/\/ source array oop\n-    const Register src_pos    = c_rarg1;  \/\/ source position\n-    const Register dst        = c_rarg2;  \/\/ destination array oop\n-    const Register dst_pos    = c_rarg3;  \/\/ destination position\n-#ifndef _WIN64\n-    const Register length     = c_rarg4;\n-    const Register rklass_tmp = r9;  \/\/ load_klass\n-#else\n-    const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n-    const Register rklass_tmp = rdi;  \/\/ load_klass\n-#endif\n-\n-    { int modulus = CodeEntryAlignment;\n-      int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n-      int advance = target - (__ offset() % modulus);\n-      if (advance < 0)  advance += modulus;\n-      if (advance > 0)  __ nop(advance);\n-    }\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n-    __ BIND(L_failed_0);\n-    __ jmp(L_failed);\n-    assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n-\n-    __ align(CodeEntryAlignment);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-    __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n-#endif\n-\n-    \/\/ bump this on entry, not on exit:\n-    inc_counter_np(SharedRuntime::_generic_array_copy_ctr);\n-\n-    \/\/-----------------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the following conditions are met:\n-    \/\/\n-    \/\/ (1) src and dst must not be null.\n-    \/\/ (2) src_pos must not be negative.\n-    \/\/ (3) dst_pos must not be negative.\n-    \/\/ (4) length  must not be negative.\n-    \/\/ (5) src klass and dst klass should be the same and not NULL.\n-    \/\/ (6) src and dst should be arrays.\n-    \/\/ (7) src_pos + length must not exceed length of src.\n-    \/\/ (8) dst_pos + length must not exceed length of dst.\n-    \/\/\n-\n-    \/\/  if (src == NULL) return -1;\n-    __ testptr(src, src);         \/\/ src oop\n-    size_t j1off = __ offset();\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (src_pos < 0) return -1;\n-    __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (dst == NULL) return -1;\n-    __ testptr(dst, dst);         \/\/ dst oop\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (dst_pos < 0) return -1;\n-    __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n-    size_t j4off = __ offset();\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/ The first four tests are very dense code,\n-    \/\/ but not quite dense enough to put four\n-    \/\/ jumps in a 16-byte instruction fetch buffer.\n-    \/\/ That's good, because some branch predicters\n-    \/\/ do not like jumps so close together.\n-    \/\/ Make sure of this.\n-    guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n-\n-    \/\/ registers used as temp\n-    const Register r11_length    = r11; \/\/ elements count to copy\n-    const Register r10_src_klass = r10; \/\/ array klass\n-\n-    \/\/  if (length < 0) return -1;\n-    __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n-    __ testl(r11_length, r11_length);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    __ load_klass(r10_src_klass, src, rklass_tmp);\n-#ifdef ASSERT\n-    \/\/  assert(src->klass() != NULL);\n-    {\n-      BLOCK_COMMENT(\"assert klasses not null {\");\n-      Label L1, L2;\n-      __ testptr(r10_src_klass, r10_src_klass);\n-      __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n-      __ bind(L1);\n-      __ stop(\"broken null klass\");\n-      __ bind(L2);\n-      __ load_klass(rax, dst, rklass_tmp);\n-      __ cmpq(rax, 0);\n-      __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n-      BLOCK_COMMENT(\"} assert klasses not null done\");\n-    }\n-#endif\n-\n-    \/\/ Load layout helper (32-bits)\n-    \/\/\n-    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-    \/\/ 32        30    24            16              8     2                 0\n-    \/\/\n-    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-    \/\/\n-\n-    const int lh_offset = in_bytes(Klass::layout_helper_offset());\n-\n-    \/\/ Handle objArrays completely differently...\n-    const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-    __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n-    __ jcc(Assembler::equal, L_objArray);\n-\n-    \/\/  if (src->klass() != dst->klass()) return -1;\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax);\n-    __ jcc(Assembler::notEqual, L_failed);\n-\n-    const Register rax_lh = rax;  \/\/ layout helper\n-    __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n-\n-    \/\/  if (!src->is_Array()) return -1;\n-    __ cmpl(rax_lh, Klass::_lh_neutral_value);\n-    __ jcc(Assembler::greaterEqual, L_failed);\n-\n-    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n-#ifdef ASSERT\n-    {\n-      BLOCK_COMMENT(\"assert primitive array {\");\n-      Label L;\n-      __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L);\n-      __ stop(\"must be a primitive array\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert primitive array done\");\n-    }\n-#endif\n-\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n-\n-    \/\/ TypeArrayKlass\n-    \/\/\n-    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-    \/\/\n-\n-    const Register r10_offset = r10;    \/\/ array offset\n-    const Register rax_elsize = rax_lh; \/\/ element size\n-\n-    __ movl(r10_offset, rax_lh);\n-    __ shrl(r10_offset, Klass::_lh_header_size_shift);\n-    __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n-    __ addptr(src, r10_offset);           \/\/ src array offset\n-    __ addptr(dst, r10_offset);           \/\/ dst array offset\n-    BLOCK_COMMENT(\"choose copy loop based on element size\");\n-    __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n-\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-\n-    \/\/ next registers should be set before the jump to corresponding stub\n-    const Register from     = c_rarg0;  \/\/ source array address\n-    const Register to       = c_rarg1;  \/\/ destination array address\n-    const Register count    = c_rarg2;  \/\/ elements count\n-\n-    \/\/ 'from', 'to', 'count' registers should be set in such order\n-    \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n-\n-    __ cmpl(rax_elsize, 0);\n-    __ jccb(Assembler::notEqual, L_copy_shorts);\n-    __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(byte_copy_entry));\n-\n-  __ BIND(L_copy_shorts);\n-    __ cmpl(rax_elsize, LogBytesPerShort);\n-    __ jccb(Assembler::notEqual, L_copy_ints);\n-    __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-  __ BIND(L_copy_ints);\n-    __ cmpl(rax_elsize, LogBytesPerInt);\n-    __ jccb(Assembler::notEqual, L_copy_longs);\n-    __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-  __ BIND(L_copy_longs);\n-#ifdef ASSERT\n-    {\n-      BLOCK_COMMENT(\"assert long copy {\");\n-      Label L;\n-      __ cmpl(rax_elsize, LogBytesPerLong);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"must be long copy, but elsize is wrong\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert long copy done\");\n-    }\n-#endif\n-    __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    \/\/ ObjArrayKlass\n-  __ BIND(L_objArray);\n-    \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n-\n-    Label L_plain_copy, L_checkcast_copy;\n-    \/\/  test array classes for subtyping\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n-    __ jcc(Assembler::notEqual, L_checkcast_copy);\n-\n-    \/\/ Identically typed arrays can be copied without element-wise checks.\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n-\n-    __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-  __ BIND(L_plain_copy);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ jump(RuntimeAddress(oop_copy_entry));\n-\n-  __ BIND(L_checkcast_copy);\n-    \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n-    {\n-      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-      __ cmpl(Address(rax, lh_offset), objArray_lh);\n-      __ jcc(Assembler::notEqual, L_failed);\n-\n-      \/\/ It is safe to examine both src.length and dst.length.\n-      arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                             rax, L_failed);\n-\n-      const Register r11_dst_klass = r11;\n-      __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n-\n-      \/\/ Marshal the base address arguments now, freeing registers.\n-      __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ movl(count, length);           \/\/ length (reloaded)\n-      Register sco_temp = c_rarg3;      \/\/ this register is free now\n-      assert_different_registers(from, to, count, sco_temp,\n-                                 r11_dst_klass, r10_src_klass);\n-      assert_clean_int(count, sco_temp);\n-\n-      \/\/ Generate the type check.\n-      const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n-      generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n-\n-      \/\/ Fetch destination element klass from the ObjArrayKlass header.\n-      int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-      __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n-      __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n+void StubGenerator::restore_arg_regs_using_thread() {\n+  assert(_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_r15 = r9;\n@@ -3373,1 +1229,4 @@\n-      __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+  __ get_thread(r15_thread);\n+  __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n+  __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n+  __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n@@ -3375,0 +1234,1 @@\n+}\n@@ -3376,155 +1236,7 @@\n-      \/\/ the checkcast_copy loop needs two extra arguments:\n-      assert(c_rarg3 == sco_temp, \"#3 already in place\");\n-      \/\/ Set up arguments for checkcast_copy_entry.\n-      setup_arg_regs(4);\n-      __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n-      __ jump(RuntimeAddress(checkcast_copy_entry));\n-    }\n-\n-  __ BIND(L_failed);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ xorptr(rax, rax);\n-    __ notptr(rax); \/\/ return -1\n-    __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  address generate_data_cache_writeback() {\n-    const Register src        = c_rarg0;  \/\/ source address\n-\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n-\n-    address start = __ pc();\n-    __ enter();\n-    __ cache_wb(Address(src, 0));\n-    __ leave();\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  address generate_data_cache_writeback_sync() {\n-    const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n-\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n-\n-    \/\/ pre wbsync is a no-op\n-    \/\/ post wbsync translates to an sfence\n-\n-    Label skip;\n-    address start = __ pc();\n-    __ enter();\n-    __ cmpl(is_pre, 0);\n-    __ jcc(Assembler::notEqual, skip);\n-    __ cache_wbsync(false);\n-    __ bind(skip);\n-    __ leave();\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  void generate_arraycopy_stubs() {\n-    address entry;\n-    address entry_jbyte_arraycopy;\n-    address entry_jshort_arraycopy;\n-    address entry_jint_arraycopy;\n-    address entry_oop_arraycopy;\n-    address entry_jlong_arraycopy;\n-    address entry_checkcast_arraycopy;\n-\n-    StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n-                                                                           \"jbyte_disjoint_arraycopy\");\n-    StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n-                                                                           \"jbyte_arraycopy\");\n-\n-    StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n-                                                                            \"jshort_disjoint_arraycopy\");\n-    StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n-                                                                            \"jshort_arraycopy\");\n-\n-    StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n-                                                                              \"jint_disjoint_arraycopy\");\n-    StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n-                                                                              &entry_jint_arraycopy, \"jint_arraycopy\");\n-\n-    StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n-                                                                               \"jlong_disjoint_arraycopy\");\n-    StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n-                                                                               &entry_jlong_arraycopy, \"jlong_arraycopy\");\n-\n-\n-    if (UseCompressedOops) {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                              \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                              &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                                     \"oop_disjoint_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                                     NULL, \"oop_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-    } else {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                               \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                               &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                                      \"oop_disjoint_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                                      NULL, \"oop_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-    }\n-    StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n-    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n-                                                                        \/*dest_uninitialized*\/true);\n-\n-    StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n-                                                              entry_jbyte_arraycopy,\n-                                                              entry_jshort_arraycopy,\n-                                                              entry_jint_arraycopy,\n-                                                              entry_jlong_arraycopy);\n-    StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n-                                                               entry_jbyte_arraycopy,\n-                                                               entry_jshort_arraycopy,\n-                                                               entry_jint_arraycopy,\n-                                                               entry_oop_arraycopy,\n-                                                               entry_jlong_arraycopy,\n-                                                               entry_checkcast_arraycopy);\n-\n-    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n-\n-    \/\/ We don't generate specialized code for HeapWord-aligned source\n-    \/\/ arrays, so just use the code we've already generated\n-    StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n-\n-    StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n-\n-    StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n-\n-    StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n-    StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n-    StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n+void StubGenerator::setup_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                      \/\/ r9 and r10 may be used to save non-volatile registers\n+  } else {\n+    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                   \/\/ r9 is used to save r15_thread\n@@ -3533,0 +1245,1 @@\n+}\n@@ -3534,18 +1247,5 @@\n-  \/\/ AES intrinsic stubs\n-  enum {AESBlockSize = 16};\n-\n-  address generate_key_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64( 0x0405060700010203, relocInfo::none );\n-    __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n-    return start;\n-  }\n-  address generate_counter_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n+void StubGenerator::restore_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    restore_arg_regs();\n+  } else {\n+    restore_arg_regs_using_thread();\n@@ -3554,0 +1254,1 @@\n+}\n@@ -3555,10 +1256,2 @@\n-  \/\/ Utility routine for loading a 128-bit key word in little endian format\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != NULL) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n-  }\n+address StubGenerator::generate_data_cache_writeback() {\n+  const Register src        = c_rarg0;  \/\/ source address\n@@ -3566,11 +1259,1 @@\n-  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n-    __ pextrq(reg, xmmdst, 0x0);\n-    __ addq(reg, inc_delta);\n-    __ pinsrq(xmmdst, reg, 0x0);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-    __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n-    __ addq(reg, 0x01);\n-    __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n-    __ BIND(next_block);          \/\/ next instruction\n-  }\n+  __ align(CodeEntryAlignment);\n@@ -3578,89 +1261,1 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_encryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n-\n-    \/\/ For encryption, the java expanded key ordering is just what we need\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-\n-    load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n-    __ pxor(xmm_result, xmm_temp1);\n-\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenclast(xmm_result, xmm_temp2);\n-    __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n@@ -3668,2 +1263,1 @@\n-    return start;\n-  }\n+  address start = __ pc();\n@@ -3671,0 +1265,4 @@\n+  __ enter();\n+  __ cache_wb(Address(src, 0));\n+  __ leave();\n+  __ ret(0);\n@@ -3672,90 +1270,2 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_decryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));\n-\n-    \/\/ for decryption java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ pxor  (xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    \/\/ for decryption the aesdeclast operation is always on key+0x00\n-    __ aesdeclast(xmm_result, xmm_temp3);\n-    __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -3763,2 +1273,2 @@\n-    return start;\n-  }\n+address StubGenerator::generate_data_cache_writeback_sync() {\n+  const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n@@ -3766,0 +1276,1 @@\n+  __ align(CodeEntryAlignment);\n@@ -3767,45 +1278,1 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_encryptAESCrypt() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n-    address start = __ pc();\n-\n-    Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n-#endif\n-    const Register pos         = rax;\n-\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_temp   = xmm1;\n-    \/\/ keys 0-10 preloaded into xmm2-xmm12\n-    const int XMM_REG_NUM_KEY_FIRST = 2;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n-    const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n-    const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n-    const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n@@ -3813,6 +1280,2 @@\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-#else\n-    __ push(len_reg); \/\/ Save\n-#endif\n+  \/\/ pre wbsync is a no-op\n+  \/\/ post wbsync translates to an sfence\n@@ -3820,31 +1283,2 @@\n-    const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rax, 44);\n-    __ jcc(Assembler::notEqual, L_key_192_256);\n-\n-    \/\/ 128 bit code follows here\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_128);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key10);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_128);\n+  Label skip;\n+  address start = __ pc();\n@@ -3852,2 +1286,7 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+  __ enter();\n+  __ cmpl(is_pre, 0);\n+  __ jcc(Assembler::notEqual, skip);\n+  __ cache_wbsync(false);\n+  __ bind(skip);\n+  __ leave();\n+  __ ret(0);\n@@ -3855,7 +1294,2 @@\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -3863,49 +1297,24 @@\n-    __ BIND(L_key_192_256);\n-    \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n-    load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n-    __ cmpl(rax, 52);\n-    __ jcc(Assembler::notEqual, L_key_256);\n-\n-    \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_192);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key12);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_192);\n-    __ jmp(L_exit);\n-\n-    __ BIND(L_key_256);\n-    \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n-    load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_256);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    load_key(xmm_temp, key, 0xe0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_256);\n-    __ jmp(L_exit);\n-\n-    return start;\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+address StubGenerator::generate_md5_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  const Register buf_param = r15;\n+  const Address state_param(rsp, 0 * wordSize);\n+  const Address ofs_param  (rsp, 1 * wordSize    );\n+  const Address limit_param(rsp, 1 * wordSize + 4);\n+\n+  __ enter();\n+  __ push(rbx);\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(r15);\n+  __ subptr(rsp, 2 * wordSize);\n+\n+  __ movptr(buf_param, c_rarg0);\n+  __ movptr(state_param, c_rarg1);\n+  if (multi_block) {\n+    __ movl(ofs_param, c_rarg2);\n+    __ movl(limit_param, c_rarg3);\n@@ -3913,0 +1322,1 @@\n+  __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n@@ -3914,51 +1324,7 @@\n-  \/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n-#endif\n-    const Register pos         = rax;\n-\n-    const int PARALLEL_FACTOR = 4;\n-    const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n-\n-    Label L_exit;\n-    Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n-\n-    \/\/ keys 0-10 preloaded into xmm5-xmm15\n-    const int XMM_REG_NUM_KEY_FIRST = 5;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ addptr(rsp, 2 * wordSize);\n+  __ pop(r15);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rbx);\n+  __ leave();\n+  __ ret(0);\n@@ -3966,77 +1332,2 @@\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-#else\n-    __ push(len_reg); \/\/ Save\n-#endif\n-    __ push(rbx);\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n-\n-    const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n-\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm0;\n-    const XMMRegister xmm_result1 = xmm2;\n-    const XMMRegister xmm_result2 = xmm3;\n-    const XMMRegister xmm_result3 = xmm4;\n-\n-    __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n-\n-    __ xorptr(pos, pos);\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n-\n-#define DoFour(opc, src_reg)           \\\n-  __ opc(xmm_result0, src_reg);         \\\n-  __ opc(xmm_result1, src_reg);         \\\n-  __ opc(xmm_result2, src_reg);         \\\n-  __ opc(xmm_result3, src_reg);         \\\n-\n-    for (int k = 0; k < 3; ++k) {\n-      __ BIND(L_multiBlock_loopTopHead[k]);\n-      if (k != 0) {\n-        __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-        __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n-      }\n-      if (k == 1) {\n-        __ subptr(rsp, 6 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      } else if (k == 2) {\n-        __ subptr(rsp, 10 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n-        __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n-        __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      }\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-      __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n-\n-      if  (k != 0) {\n-        __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n-      }\n+  return start;\n+}\n@@ -4044,4 +1335,4 @@\n-      __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n-      __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+address StubGenerator::generate_upper_word_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n+  address start = __ pc();\n@@ -4049,27 +1340,2 @@\n-      DoFour(pxor, xmm_key_first);\n-      if (k == 0) {\n-        for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 1) {\n-        for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 2) {\n-        for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n-        DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      }\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n@@ -4077,12 +1343,2 @@\n-      \/\/ for each result, xor with the r vector of previous cipher block\n-      __ pxor(xmm_result0, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ pxor(xmm_result1, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ pxor(xmm_result2, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ pxor(xmm_result3, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n-      if (k != 0) {\n-        __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n-      }\n+  return start;\n+}\n@@ -4090,54 +1346,4 @@\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ registers used in the non-parallelized loops\n-      \/\/ xmm register assignments for the loops below\n-      const XMMRegister xmm_result = xmm0;\n-      const XMMRegister xmm_prev_block_cipher_save = xmm2;\n-      const XMMRegister xmm_key11 = xmm3;\n-      const XMMRegister xmm_key12 = xmm4;\n-      const XMMRegister key_tmp = xmm4;\n-\n-      __ BIND(L_singleBlock_loopTopHead[k]);\n-      if (k == 1) {\n-        __ addptr(rsp, 6 * wordSize);\n-      } else if (k == 2) {\n-        __ addptr(rsp, 10 * wordSize);\n-      }\n-      __ cmpptr(len_reg, 0); \/\/ any blocks left??\n-      __ jcc(Assembler::equal, L_exit);\n-      __ BIND(L_singleBlock_loopTopHead2[k]);\n-      if (k == 1) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n-      }\n-      if (k == 2) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n-      }\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlock_loopTop[k]);\n-      __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n-      __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n-      __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n-      for (int rnum = 1; rnum <= 9 ; rnum++) {\n-          __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-      }\n-      if (k == 1) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        __ aesdec(xmm_result, xmm_key12);\n-      }\n-      if (k == 2) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        load_key(key_tmp, key, 0xc0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xd0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xe0);\n-        __ aesdec(xmm_result, key_tmp);\n-      }\n+address StubGenerator::generate_shuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n+  address start = __ pc();\n@@ -4145,12 +1351,2 @@\n-      __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n-      __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n-      __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n-      \/\/ no need to store r to memory until we exit\n-      __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n-      __ addptr(pos, AESBlockSize);\n-      __ subptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n-      if (k != 2) {\n-        __ jmp(L_exit);\n-      }\n-    } \/\/for 128\/192\/256\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n@@ -4158,11 +1354,1 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-    __ pop(rbx);\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+  return start;\n@@ -4171,68 +1357,6 @@\n-  address generate_electronicCodeBook_encryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_encrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n- }\n-\n-  address generate_electronicCodeBook_decryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_decrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n-  address generate_md5_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    const Register buf_param = r15;\n-    const Address state_param(rsp, 0 * wordSize);\n-    const Address ofs_param  (rsp, 1 * wordSize    );\n-    const Address limit_param(rsp, 1 * wordSize + 4);\n-\n-    __ enter();\n-    __ push(rbx);\n-    __ push(rdi);\n-    __ push(rsi);\n-    __ push(r15);\n-    __ subptr(rsp, 2 * wordSize);\n-\n-    __ movptr(buf_param, c_rarg0);\n-    __ movptr(state_param, c_rarg1);\n-    if (multi_block) {\n-      __ movl(ofs_param, c_rarg2);\n-      __ movl(limit_param, c_rarg3);\n-    }\n-    __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n-\n-    __ addptr(rsp, 2 * wordSize);\n-    __ pop(r15);\n-    __ pop(rsi);\n-    __ pop(rdi);\n-    __ pop(rbx);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+address StubGenerator::generate_sha1_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4240,8 +1364,4 @@\n-  address generate_upper_word_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n-    return start;\n-  }\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4249,8 +1369,4 @@\n-  address generate_shuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n-  }\n+  const XMMRegister abcd = xmm0;\n+  const XMMRegister e0 = xmm1;\n+  const XMMRegister e1 = xmm2;\n+  const XMMRegister msg0 = xmm3;\n@@ -4258,6 +1374,4 @@\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha1_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+  const XMMRegister msg1 = xmm4;\n+  const XMMRegister msg2 = xmm5;\n+  const XMMRegister msg3 = xmm6;\n+  const XMMRegister shuf_mask = xmm7;\n@@ -4265,4 +1379,1 @@\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n+  __ enter();\n@@ -4270,4 +1381,1 @@\n-    const XMMRegister abcd = xmm0;\n-    const XMMRegister e0 = xmm1;\n-    const XMMRegister e1 = xmm2;\n-    const XMMRegister msg0 = xmm3;\n+  __ subptr(rsp, 4 * wordSize);\n@@ -4275,4 +1383,2 @@\n-    const XMMRegister msg1 = xmm4;\n-    const XMMRegister msg2 = xmm5;\n-    const XMMRegister msg3 = xmm6;\n-    const XMMRegister shuf_mask = xmm7;\n+  __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n+    buf, state, ofs, limit, rsp, multi_block);\n@@ -4280,1 +1386,1 @@\n-    __ enter();\n+  __ addptr(rsp, 4 * wordSize);\n@@ -4282,1 +1388,2 @@\n-    __ subptr(rsp, 4 * wordSize);\n+  __ leave();\n+  __ ret(0);\n@@ -4284,2 +1391,2 @@\n-    __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n-      buf, state, ofs, limit, rsp, multi_block);\n+  return start;\n+}\n@@ -4287,1 +1394,4 @@\n-    __ addptr(rsp, 4 * wordSize);\n+address StubGenerator::generate_pshuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n+  address start = __ pc();\n@@ -4289,4 +1399,2 @@\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n@@ -4294,5 +1402,2 @@\n-  address generate_pshuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n@@ -4300,113 +1405,10 @@\n-\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n-      __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n-      \/\/ _SHUF_00BA\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      \/\/ _SHUF_DC00\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n-  address generate_pshuffle_byte_flip_mask_sha512() {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n-    address start = __ pc();\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n-      __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-      __ emit_data64(0x1011121314151617, relocInfo::none);\n-      __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n-      __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n-      __ emit_data64(0x0000000000000000, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-    }\n-\n-    return start;\n-  }\n-\n-\/\/ ofs and limit are use for multi-block byte array.\n-\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    const XMMRegister shuf_mask = xmm8;\n-\n-    __ enter();\n-\n-    __ subptr(rsp, 4 * wordSize);\n-\n-    if (VM_Version::supports_sha()) {\n-      __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    } else if (VM_Version::supports_avx2()) {\n-      __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    }\n-    __ addptr(rsp, 4 * wordSize);\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address generate_sha512_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_avx2(), \"\");\n-    assert(VM_Version::supports_bmi2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    const XMMRegister shuf_mask = xmm8;\n-\n-    __ enter();\n-\n-    __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-    buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+    \/\/ _SHUF_00BA\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    \/\/ _SHUF_DC00\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n@@ -4415,96 +1417,2 @@\n-  address ghash_polynomial512_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n-    __ emit_data64(0x0000000100000000, relocInfo::none);\n-    return start;\n-}\n-\n-  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n-  \/\/ Windows regs            |  Linux regs\n-  \/\/ in = c_rarg0 (rcx)      |  c_rarg0 (rsi)\n-  \/\/ len = c_rarg1 (rdx)     |  c_rarg1 (rdi)\n-  \/\/ ct = c_rarg2 (r8)       |  c_rarg2 (rdx)\n-  \/\/ out = c_rarg3 (r9)      |  c_rarg3 (rcx)\n-  \/\/ key = r10               |  c_rarg4 (r8)\n-  \/\/ state = r13             |  c_rarg5 (r9)\n-  \/\/ subkeyHtbl = r14        |  r11\n-  \/\/ counter = rsi           |  r12\n-  \/\/ return - number of processed bytes\n-  address generate_galoisCounterMode_AESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register in = c_rarg0;\n-    const Register len = c_rarg1;\n-    const Register ct = c_rarg2;\n-    const Register out = c_rarg3;\n-    \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-    const Register key = c_rarg4;\n-    const Register state = c_rarg5;\n-    const Address subkeyH_mem(rbp, 2 * wordSize);\n-    const Register subkeyHtbl = r11;\n-    const Register avx512_subkeyHtbl = r13;\n-    const Address counter_mem(rbp, 3 * wordSize);\n-    const Register counter = r12;\n-#else\n-    const Address key_mem(rbp, 6 * wordSize);\n-    const Register key = r10;\n-    const Address state_mem(rbp, 7 * wordSize);\n-    const Register state = r13;\n-    const Address subkeyH_mem(rbp, 8 * wordSize);\n-    const Register subkeyHtbl = r14;\n-    const Register avx512_subkeyHtbl = r12;\n-    const Address counter_mem(rbp, 9 * wordSize);\n-    const Register counter = rsi;\n-#endif\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ push(rsi);\n-    __ movptr(key, key_mem);\n-    __ movptr(state, state_mem);\n-#endif\n-    __ movptr(subkeyHtbl, subkeyH_mem);\n-    __ movptr(counter, counter_mem);\n-\/\/ Save rbp and rsp\n-    __ push(rbp);\n-    __ movq(rbp, rsp);\n-\/\/ Align stack\n-    __ andq(rsp, -64);\n-    __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n-    __ movptr(avx512_subkeyHtbl, rsp);\n-\n-    __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n-    __ vzeroupper();\n-\n-    __ movq(rsp, rbp);\n-    __ pop(rbp);\n-\n-    \/\/ Restore state before leaving routine\n-#ifdef _WIN64\n-    __ pop(rsi);\n-#endif\n-    __ pop(rbx);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n+  return start;\n+}\n@@ -4512,4 +1420,5 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-     return start;\n-  }\n+\/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+address StubGenerator::generate_pshuffle_byte_flip_mask_sha512() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n+  address start = __ pc();\n@@ -4517,9 +1426,2 @@\n-  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n-  address counter_mask_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n@@ -4527,26 +1429,3 @@\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n+    __ emit_data64(0x1011121314151617, relocInfo::none);\n+    __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n@@ -4554,17 +1433,2 @@\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n@@ -4573,56 +1437,2 @@\n- \/\/ Vector AES Counter implementation\n-  address generate_counterMode_VectorAESCrypt()  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address r8\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-    \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n-#else\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n-#endif\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#else\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#endif\n-    __ push(rbx);\n-    __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n-    __ vzeroupper();\n-    \/\/ Restore state before leaving routine\n-    __ pop(rbx);\n-#ifdef _WIN64\n-    __ movl(rax, len_mem); \/\/ return length\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n+  return start;\n+}\n@@ -4630,3 +1440,35 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+address StubGenerator::generate_sha256_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n+\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n+\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n+\n+  const XMMRegister shuf_mask = xmm8;\n+\n+  __ enter();\n+\n+  __ subptr(rsp, 4 * wordSize);\n+\n+  if (VM_Version::supports_sha()) {\n+    __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  } else if (VM_Version::supports_avx2()) {\n+    __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n@@ -4634,0 +1476,4 @@\n+  __ addptr(rsp, 4 * wordSize);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -4635,85 +1481,2 @@\n-  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - counter vector byte array address\n-  \/\/   Linux\n-  \/\/     c_rarg4   -          input length\n-  \/\/     c_rarg5   -          saved encryptedCounter start\n-  \/\/     rbp + 6 * wordSize - saved used length\n-  \/\/   Windows\n-  \/\/     rbp + 6 * wordSize - input length\n-  \/\/     rbp + 7 * wordSize - saved encryptedCounter start\n-  \/\/     rbp + 8 * wordSize - saved used length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_counterMode_AESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-                                      \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n-#else\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n-#endif\n-    const Register pos = rax;\n-\n-    const int PARALLEL_FACTOR = 6;\n-    const XMMRegister xmm_counter_shuf_mask = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n-    const XMMRegister xmm_curr_counter = xmm2;\n-\n-    const XMMRegister xmm_key_tmp0 = xmm3;\n-    const XMMRegister xmm_key_tmp1 = xmm4;\n-\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm5;\n-    const XMMRegister xmm_result1 = xmm6;\n-    const XMMRegister xmm_result2 = xmm7;\n-    const XMMRegister xmm_result3 = xmm8;\n-    const XMMRegister xmm_result4 = xmm9;\n-    const XMMRegister xmm_result5 = xmm10;\n-\n-    const XMMRegister xmm_from0 = xmm11;\n-    const XMMRegister xmm_from1 = xmm12;\n-    const XMMRegister xmm_from2 = xmm13;\n-    const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n-    const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n-    const XMMRegister xmm_from5 = xmm4;\n-\n-    \/\/for key_128, key_192, key_256\n-    const int rounds[3] = {10, 12, 14};\n-    Label L_exit_preLoop, L_preLoop_start;\n-    Label L_multiBlock_loopTop[3];\n-    Label L_singleBlockLoopTop[3];\n-    Label L__incCounter[3][6]; \/\/for 6 blocks\n-    Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n-    Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n-    Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n-\n-    Label L_exit;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n@@ -4721,20 +1484,6 @@\n-#ifdef _WIN64\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-        saved_r13_offset,\n-        saved_r14_offset\n-    };\n-    __ subptr(rsp, 2 * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#else\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#endif\n+address StubGenerator::generate_sha512_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4742,75 +1491,4 @@\n-    __ push(rbx); \/\/ Save RBX\n-    __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n-    __ movptr(pos, 0);\n-\n-    \/\/ Use the partially used encrpyted counter from last invocation\n-    __ BIND(L_preLoop_start);\n-    __ cmpptr(used, 16);\n-    __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit_preLoop);\n-      __ movb(rbx, Address(saved_encCounter_start, used));\n-      __ xorb(rbx, Address(from, pos));\n-      __ movb(Address(to, pos), rbx);\n-      __ addptr(pos, 1);\n-      __ addptr(used, 1);\n-      __ subptr(len_reg, 1);\n-\n-    __ jmp(L_preLoop_start);\n-\n-    __ BIND(L_exit_preLoop);\n-    __ movl(Address(used_addr, 0), used);\n-\n-    \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n-\n-#define CTR_DoSix(opc, src_reg)                \\\n-    __ opc(xmm_result0, src_reg);              \\\n-    __ opc(xmm_result1, src_reg);              \\\n-    __ opc(xmm_result2, src_reg);              \\\n-    __ opc(xmm_result3, src_reg);              \\\n-    __ opc(xmm_result4, src_reg);              \\\n-    __ opc(xmm_result5, src_reg);\n-\n-    \/\/ k == 0 :  generate code for key_128\n-    \/\/ k == 1 :  generate code for key_192\n-    \/\/ k == 2 :  generate code for key_256\n-    for (int k = 0; k < 3; ++k) {\n-      \/\/multi blocks starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n-      __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-\n-      \/\/load, then increase counters\n-      CTR_DoSix(movdqa, xmm_curr_counter);\n-      inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n-      inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n-      inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n-      inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n-      inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n-      inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n-      CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n-      CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n-\n-      \/\/load two ROUND_KEYs at a time\n-      for (int i = 1; i < rounds[k]; ) {\n-        load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n-        load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n-        CTR_DoSix(aesenc, xmm_key_tmp1);\n-        i++;\n-        if (i != rounds[k]) {\n-          CTR_DoSix(aesenc, xmm_key_tmp0);\n-        } else {\n-          CTR_DoSix(aesenclast, xmm_key_tmp0);\n-        }\n-        i++;\n-      }\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4818,105 +1496,8 @@\n-      \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n-      __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-      __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n-      __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n-\n-      __ pxor(xmm_result0, xmm_from0);\n-      __ pxor(xmm_result1, xmm_from1);\n-      __ pxor(xmm_result2, xmm_from2);\n-      __ pxor(xmm_result3, xmm_from3);\n-      __ pxor(xmm_result4, xmm_from4);\n-      __ pxor(xmm_result5, xmm_from5);\n-\n-      \/\/ store 6 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-      __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n-      __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ singleBlock starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlockLoopTop[k]);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-      __ movdqa(xmm_result0, xmm_curr_counter);\n-      inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n-      __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n-      __ pxor(xmm_result0, xmm_key_tmp0);\n-      for (int i = 1; i < rounds[k]; i++) {\n-        load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n-        __ aesenc(xmm_result0, xmm_key_tmp0);\n-      }\n-      load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n-      __ aesenclast(xmm_result0, xmm_key_tmp0);\n-      __ cmpptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::less, L_processTail_insr[k]);\n-        __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-        __ pxor(xmm_result0, xmm_from0);\n-        __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-        __ addptr(pos, AESBlockSize);\n-        __ subptr(len_reg, AESBlockSize);\n-        __ jmp(L_singleBlockLoopTop[k]);\n-      __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n-        __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n-          __ subptr(pos,8);\n-          __ pinsrq(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_4_insr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n-          __ subptr(pos,4);\n-          __ pslldq(xmm_from0, 4);\n-          __ pinsrd(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_2_insr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n-          __ subptr(pos, 2);\n-          __ pslldq(xmm_from0, 2);\n-          __ pinsrw(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_1_insr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n-          __ subptr(pos, 1);\n-          __ pslldq(xmm_from0, 1);\n-          __ pinsrb(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_exit_insr[k]);\n-\n-        __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n-        __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n-\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n-          __ pextrq(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 8);\n-          __ addptr(pos, 8);\n-        __ BIND(L_processTail_4_extr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n-          __ pextrd(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 4);\n-          __ addptr(pos, 4);\n-        __ BIND(L_processTail_2_extr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n-          __ pextrw(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 2);\n-          __ addptr(pos, 2);\n-        __ BIND(L_processTail_1_extr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n-          __ pextrb(Address(to, pos), xmm_result0, 0);\n-\n-        __ BIND(L_processTail_exit_extr[k]);\n-        __ movl(Address(used_addr, 0), len_reg);\n-        __ jmp(L_exit);\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n@@ -4924,1 +1505,1 @@\n-    }\n+  const XMMRegister shuf_mask = xmm8;\n@@ -4926,16 +1507,1 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n-    __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n-    __ pop(rbx); \/\/ pop the saved RBX.\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ addptr(rsp, 2 * wordSize);\n-#else\n-    __ pop(rax); \/\/ return 'len'\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ enter();\n@@ -4943,10 +1509,2 @@\n-void roundDec(XMMRegister xmm_reg) {\n-  __ vaesdec(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n-}\n+  __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+  buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n@@ -4954,9 +1512,5 @@\n-void roundDeclast(XMMRegister xmm_reg) {\n-  __ vaesdeclast(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n@@ -4965,8 +1519,15 @@\n-  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != NULL) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n-    __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);\n+address StubGenerator::base64_shuffle_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+  __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+  __ emit_data64(0x1617151613141213, relocInfo::none);\n+  __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+  __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+  __ emit_data64(0x2829272825262425, relocInfo::none);\n+  __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n@@ -4974,1 +1535,2 @@\n-  }\n+  return start;\n+}\n@@ -4976,17 +1538,4 @@\n-address generate_cipherBlockChaining_decryptVectorAESCrypt() {\n-    assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-    \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg = r11;      \/\/ pick the volatile windows register\n-#endif\n+address StubGenerator::base64_avx2_shuffle_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n+  address start = __ pc();\n@@ -4994,2 +1543,4 @@\n-    Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n-          Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n+  __ emit_data64(0x0809070805060405, relocInfo::none);\n+  __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n@@ -4997,1 +1548,2 @@\n-    __ enter();\n+  return start;\n+}\n@@ -4999,8 +1551,4 @@\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-#else\n-    __ push(len_reg); \/\/ Save\n-#endif\n-    __ push(rbx);\n-    __ vzeroupper();\n+address StubGenerator::base64_avx2_input_mask_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n+  address start = __ pc();\n@@ -5008,241 +1556,4 @@\n-    \/\/ Temporary variable declaration for swapping key bytes\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-    \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-    const Register rounds = rbx;\n-    __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    const XMMRegister IV = xmm0;\n-    \/\/ Load IV and broadcast value to 512-bits\n-    __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n-\n-    \/\/ Temporary variables for storing round keys\n-    const XMMRegister RK0 = xmm30;\n-    const XMMRegister RK1 = xmm9;\n-    const XMMRegister RK2 = xmm18;\n-    const XMMRegister RK3 = xmm19;\n-    const XMMRegister RK4 = xmm20;\n-    const XMMRegister RK5 = xmm21;\n-    const XMMRegister RK6 = xmm22;\n-    const XMMRegister RK7 = xmm23;\n-    const XMMRegister RK8 = xmm24;\n-    const XMMRegister RK9 = xmm25;\n-    const XMMRegister RK10 = xmm26;\n-\n-     \/\/ Load and shuffle key\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 1*16 here and hit 0*16 last\n-    ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n-\n-    \/\/ Variables for storing source cipher text\n-    const XMMRegister S0 = xmm10;\n-    const XMMRegister S1 = xmm11;\n-    const XMMRegister S2 = xmm12;\n-    const XMMRegister S3 = xmm13;\n-    const XMMRegister S4 = xmm14;\n-    const XMMRegister S5 = xmm15;\n-    const XMMRegister S6 = xmm16;\n-    const XMMRegister S7 = xmm17;\n-\n-    \/\/ Variables for storing decrypted text\n-    const XMMRegister B0 = xmm1;\n-    const XMMRegister B1 = xmm2;\n-    const XMMRegister B2 = xmm3;\n-    const XMMRegister B3 = xmm4;\n-    const XMMRegister B4 = xmm5;\n-    const XMMRegister B5 = xmm6;\n-    const XMMRegister B6 = xmm7;\n-    const XMMRegister B7 = xmm8;\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::greater, KEY_192);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_192);\n-    const XMMRegister RK11 = xmm27;\n-    const XMMRegister RK12 = xmm28;\n-    ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n-    ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::greater, KEY_256);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_256);\n-    const XMMRegister RK13 = xmm29;\n-    const XMMRegister RK14 = xmm31;\n-    ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n-    ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n-\n-    __ BIND(Loop);\n-    __ cmpl(len_reg, 512);\n-    __ jcc(Assembler::below, Lcbc_dec_rem);\n-    __ BIND(Loop1);\n-    __ subl(len_reg, 512);\n-    __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n-    __ leaq(from, Address(from, 8 * 64));\n-\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n-\n-    __ evalignq(IV, S0, IV, 0x06);\n-    __ evalignq(S0, S1, S0, 0x06);\n-    __ evalignq(S1, S2, S1, 0x06);\n-    __ evalignq(S2, S3, S2, 0x06);\n-    __ evalignq(S3, S4, S3, 0x06);\n-    __ evalignq(S4, S5, S4, 0x06);\n-    __ evalignq(S5, S6, S5, 0x06);\n-    __ evalignq(S6, S7, S6, 0x06);\n-\n-    roundDec(RK2);\n-    roundDec(RK3);\n-    roundDec(RK4);\n-    roundDec(RK5);\n-    roundDec(RK6);\n-    roundDec(RK7);\n-    roundDec(RK8);\n-    roundDec(RK9);\n-    roundDec(RK10);\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, L_128);\n-    roundDec(RK11);\n-    roundDec(RK12);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, L_192);\n-    roundDec(RK13);\n-    roundDec(RK14);\n-\n-    __ BIND(L_256);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_128);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_192);\n-    roundDeclast(RK0);\n-\n-    __ BIND(Loop2);\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n-    __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n-    __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n-    __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n-    __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n-    __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n-\n-    __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n-    __ leaq(to, Address(to, 8 * 64));\n-    __ jmp(Loop);\n-\n-    __ BIND(Lcbc_dec_rem);\n-    __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_loop);\n-    __ subl(len_reg, 16);\n-    __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n-\n-    __ movdqu(S0, Address(from, 0));\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_last);\n-    __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n-\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n-    __ movdqu(Address(to, 0), B0);\n-    __ leaq(from, Address(from, 16));\n-    __ leaq(to, Address(to, 16));\n-    __ jmp(Lcbc_dec_rem_loop);\n-\n-    __ BIND(Lcbc_dec_ret);\n-    __ movdqu(Address(rvec, 0), IV);\n-\n-    \/\/ Zero out the round keys\n-    __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n-    __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n-    __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n-    __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n-    __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n-    __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n-    __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n-    __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n-    __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n-    __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n-    __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n-    __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_exit);\n-    __ vzeroupper();\n-    __ pop(rbx);\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n+  __ emit_data64(0x8000000000000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n@@ -5250,8 +1561,1 @@\n-\/\/ Polynomial x^128+x^127+x^126+x^121+1\n-address ghash_polynomial_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0xc200000000000000, relocInfo::none);\n-    return start;\n+  return start;\n@@ -5260,8 +1564,4 @@\n-address ghash_shufflemask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    return start;\n-}\n+address StubGenerator::base64_avx2_lut_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n+  address start = __ pc();\n@@ -5269,19 +1569,10 @@\n-\/\/ Ghash single and multi block operations using AVX instructions\n-address generate_avx_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n-\n-    \/\/ arguments\n-    const Register state = c_rarg0;\n-    const Register htbl = c_rarg1;\n-    const Register data = c_rarg2;\n-    const Register blocks = c_rarg3;\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ avx_ghash(state, htbl, data, blocks);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+\n+  \/\/ URL LUT\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n@@ -5289,8 +1580,26 @@\n-  \/\/ byte swap x86 long\n-  address generate_ghash_long_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n-    __ emit_data64(0x0706050403020100, relocInfo::none );\n-  }\n+}\n+\n+address StubGenerator::base64_encoding_table_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x2f2b393837363534, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x5f2d393837363534, relocInfo::none);\n@@ -5299,8 +1608,37 @@\n-  \/\/ byte swap x86 byte array\n-  address generate_ghash_byte_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n-    __ emit_data64(0x0001020304050607, relocInfo::none );\n-  }\n+}\n+\n+\/\/ Code for generating Base64 encoding.\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+\/\/ boolean isURL) {\n+address StubGenerator::generate_base64_encodeBlock()\n+{\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0;       \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2;   \/\/ end offset\n+  const Register dest = c_rarg3;   \/\/ destination array\n+\n+#ifndef _WIN64\n+  const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n+#else\n+  const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10; \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+#endif\n@@ -5309,6 +1647,3 @@\n-  \/* Single and multi-block ghash operations *\/\n-  address generate_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-    Label L_ghash_loop, L_exit;\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n+  const Register length = r14;\n+  const Register encode_table = r13;\n+  Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n@@ -5316,4 +1651,4 @@\n-    const Register state        = c_rarg0;\n-    const Register subkeyH      = c_rarg1;\n-    const Register data         = c_rarg2;\n-    const Register blocks       = c_rarg3;\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ jcc(Assembler::lessEqual, L_exit);\n@@ -5321,11 +1656,6 @@\n-    const XMMRegister xmm_temp0 = xmm0;\n-    const XMMRegister xmm_temp1 = xmm1;\n-    const XMMRegister xmm_temp2 = xmm2;\n-    const XMMRegister xmm_temp3 = xmm3;\n-    const XMMRegister xmm_temp4 = xmm4;\n-    const XMMRegister xmm_temp5 = xmm5;\n-    const XMMRegister xmm_temp6 = xmm6;\n-    const XMMRegister xmm_temp7 = xmm7;\n-    const XMMRegister xmm_temp8 = xmm8;\n-    const XMMRegister xmm_temp9 = xmm9;\n-    const XMMRegister xmm_temp10 = xmm10;\n+  \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+  \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+  \/\/ sure not to read past the end of the input buffer.\n+  if (VM_Version::supports_avx512_vbmi()) {\n+    __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+    __ jcc(Assembler::below, L_not512);\n@@ -5333,1 +1663,4 @@\n-    __ enter();\n+    __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+    __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ addptr(encode_table, isURL);\n+    __ shrl(isURL, 6); \/\/ restore isURL\n@@ -5335,1 +1668,7 @@\n-    __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n+    __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n+    __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+    __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+\n+    __ align32();\n+    __ BIND(L_vbmiLoop);\n@@ -5337,2 +1676,2 @@\n-    __ movdqu(xmm_temp0, Address(state, 0));\n-    __ pshufb(xmm_temp0, xmm_temp10);\n+    __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ subl(length, 48);\n@@ -5340,0 +1679,4 @@\n+    \/\/ Put the input bytes into the proper lanes for writing, then\n+    \/\/ encode them.\n+    __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+    __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n@@ -5341,3 +1684,2 @@\n-    __ BIND(L_ghash_loop);\n-    __ movdqu(xmm_temp2, Address(data, 0));\n-    __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+    \/\/ Write to destination\n+    __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n@@ -5345,2 +1687,4 @@\n-    __ movdqu(xmm_temp1, Address(subkeyH, 0));\n-    __ pshufb(xmm_temp1, xmm_temp10);\n+    __ addptr(dest, 64);\n+    __ addptr(source, 48);\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n@@ -5348,1 +1692,2 @@\n-    __ pxor(xmm_temp0, xmm_temp2);\n+    __ vzeroupper();\n+  }\n@@ -5350,0 +1695,35 @@\n+  __ BIND(L_not512);\n+  if (VM_Version::supports_avx2()) {\n+    \/*\n+    ** This AVX2 encoder is based off the paper at:\n+    **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+    **\n+    ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+    ** output bytes.\n+    **\n+    *\/\n+    \/\/ Lengths under 32 bytes are done with scalar routine\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n+\n+    \/\/ Set up supporting constant table data\n+    __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n+    \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+    __ movl(rax, 0x0fc0fc00);\n+    __ movdl(xmm8, rax);\n+    __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n+    __ vpbroadcastd(xmm8, xmm8, Assembler::AVX_256bit);\n+\n+    \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+    \/\/ bits\n+    __ movl(rax, 0x04000040);\n+\n+    __ subl(length, 24);\n+    __ movdl(xmm7, rax);\n+    __ vpbroadcastd(xmm7, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ For the first load, we mask off reading of the first 4\n+    \/\/ bytes into the register. This is so we can get 4 3-byte\n+    \/\/ chunks into each lane of the register, avoiding having to\n+    \/\/ handle end conditions.  We then shuffle these bytes into a\n+    \/\/ specific order so that manipulation is easier.\n@@ -5351,1 +1731,1 @@\n-    \/\/ Multiply with the hash key\n+    \/\/ The initial read loads the XMM register like this:\n@@ -5353,36 +1733,5 @@\n-    __ movdqu(xmm_temp3, xmm_temp0);\n-    __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n-    __ movdqu(xmm_temp4, xmm_temp0);\n-    __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n-\n-    __ movdqu(xmm_temp5, xmm_temp0);\n-    __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n-    __ movdqu(xmm_temp6, xmm_temp0);\n-    __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n-\n-    __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n-\n-    __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n-    __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n-    __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n-    __ pxor(xmm_temp3, xmm_temp5);\n-    __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n-                                        \/\/ of the carry-less multiplication of\n-                                        \/\/ xmm0 by xmm1.\n-\n-    \/\/ We shift the result of the multiplication by one bit position\n-    \/\/ to the left to cope for the fact that the bits are reversed.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp6);\n-    __ pslld(xmm_temp3, 1);\n-    __ pslld(xmm_temp6, 1);\n-    __ psrld(xmm_temp7, 31);\n-    __ psrld(xmm_temp8, 31);\n-    __ movdqu(xmm_temp9, xmm_temp7);\n-    __ pslldq(xmm_temp8, 4);\n-    __ pslldq(xmm_temp7, 4);\n-    __ psrldq(xmm_temp9, 12);\n-    __ por(xmm_temp3, xmm_temp7);\n-    __ por(xmm_temp6, xmm_temp8);\n-    __ por(xmm_temp6, xmm_temp9);\n-\n+    \/\/ Lower 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+    \/\/ | C2 | D0 | D1 | D2 |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n@@ -5390,1 +1739,5 @@\n-    \/\/ First phase of the reduction\n+    \/\/ Upper 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+    \/\/ | XX | XX | XX | XX |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n@@ -5392,15 +1745,3 @@\n-    \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n-    \/\/ independently.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp3);\n-    __ movdqu(xmm_temp9, xmm_temp3);\n-    __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n-    __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n-    __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n-    __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp7, xmm_temp9);\n-    __ movdqu(xmm_temp8, xmm_temp7);\n-    __ pslldq(xmm_temp7, 12);\n-    __ psrldq(xmm_temp8, 4);\n-    __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n-\n+    \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+    \/\/ The alphabetical significance denotes the 3 bytes to be\n+    \/\/ consumed and encoded into 4 bytes.\n@@ -5408,1 +1749,5 @@\n-    \/\/ Second phase of the reduction\n+    \/\/ We then shuffle the register so each 32-bit word contains\n+    \/\/ the sequence:\n+    \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+    \/\/ Each of these byte sequences are then manipulated into 4\n+    \/\/ 6-bit values ready for encoding.\n@@ -5410,19 +1755,109 @@\n-    \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n-    \/\/ shift operations.\n-    __ movdqu(xmm_temp2, xmm_temp3);\n-    __ movdqu(xmm_temp4, xmm_temp3);\n-    __ movdqu(xmm_temp5, xmm_temp3);\n-    __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n-    __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n-    __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n-    __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp2, xmm_temp5);\n-    __ pxor(xmm_temp2, xmm_temp8);\n-    __ pxor(xmm_temp3, xmm_temp2);\n-    __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n-\n-    __ decrement(blocks);\n-    __ jcc(Assembler::zero, L_exit);\n-    __ movdqu(xmm_temp0, xmm_temp6);\n-    __ addptr(data, 16);\n-    __ jmp(L_ghash_loop);\n+    \/\/ If we focus on one set of 3-byte chunks, changing the\n+    \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+    \/\/ shuffle such that each 24-bit chunk contains:\n+    \/\/\n+    \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+    \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+    \/\/ Explain this step.\n+    \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+    \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+    \/\/\n+    \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+    \/\/ a5..a0) and shift them using a vector multiplication\n+    \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+    \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+    \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+    \/\/ bits respectively.  This is done using vpmullw.  We end up\n+    \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+    \/\/ ready for encoding:\n+    \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+    \/\/\n+    \/\/ For translation, we recognize that there are 5 distinct\n+    \/\/ ranges of legal Base64 characters as below:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value | ASCII range |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |    A..Z     |     65     |\n+    \/\/   |   26..51    |    a..z     |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |   + or -    | -19 or -17 |\n+    \/\/   |     63      |   \/ or _    | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We note that vpshufb does a parallel lookup in a\n+    \/\/ destination register using the lower 4 bits of bytes from a\n+    \/\/ source register.  If we use a saturated subtraction and\n+    \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+    \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+    \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+    \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+    \/\/ with:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value |   Reduced   |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |     13      |     65     |\n+    \/\/   |   26..51    |      0      |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |     11      | -19 or -17 |\n+    \/\/   |     63      |     12      | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We then use a final vpshufb to add the appropriate offset,\n+    \/\/ translating the bytes.\n+    \/\/\n+    \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+    \/\/ not load into the full register.\n+    __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+    \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+    \/\/ ordering by:\n+    \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+    \/\/   for easy masking\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ addl(start_offset, 24);\n+\n+    \/\/ Load masking register for first and third (and multiples)\n+    \/\/ 6-bit values.\n+    __ movl(rax, 0x003f03f0);\n+    __ movdl(xmm6, rax);\n+    __ vpbroadcastd(xmm6, xmm6, Assembler::AVX_256bit);\n+    \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+    __ movl(rax, 0x01000010);\n+    __ movdl(xmm5, rax);\n+    __ vpbroadcastd(xmm5, xmm5, Assembler::AVX_256bit);\n+\n+    \/\/ Isolate 6-bit chunks of interest\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ Load constants for encoding\n+    __ movl(rax, 0x19191919);\n+    __ movdl(xmm3, rax);\n+    __ vpbroadcastd(xmm3, xmm3, Assembler::AVX_256bit);\n+    __ movl(rax, 0x33333333);\n+    __ movdl(xmm4, rax);\n+    __ vpbroadcastd(xmm4, xmm4, Assembler::AVX_256bit);\n+\n+    \/\/ Shift output bytes 0 and 2 into proper lanes\n+    __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+    \/\/ combine\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Find out which are 0..25.  This indicates which input\n+    \/\/ values fall in the range of 'A'-'Z', which require an\n+    \/\/ additional offset (see comments above)\n+    __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Load the proper lookup table\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 5);\n+    __ vmovdqu(xmm2, Address(r11, r15));\n@@ -5430,7 +1865,5 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n-    __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+    \/\/ Shuffle the offsets based on the range calculation done\n+    \/\/ above. This allows us to add the correct offset to the\n+    \/\/ 6-bit value corresponding to the range documented above.\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n@@ -5438,17 +1871,3 @@\n-  address base64_shuffle_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n-    __ emit_data64(0x1617151613141213, relocInfo::none);\n-    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n-    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n-    __ emit_data64(0x2829272825262425, relocInfo::none);\n-    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n-    return start;\n-  }\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n@@ -5456,11 +1875,2 @@\n-  address base64_avx2_shuffle_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x0809070805060405, relocInfo::none);\n-    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    return start;\n-  }\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n@@ -5468,9 +1878,36 @@\n-  address base64_avx2_input_mask_addr()\n-  {\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x8000000000000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    return start;\n+    __ BIND(L_32byteLoop);\n+\n+    \/\/ Get next 32 bytes\n+    __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+    __ subl(length, 24);\n+    __ addl(start_offset, 24);\n+\n+    \/\/ This logic is identical to the above, with only constant\n+    \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+    \/\/ chunks, shift them into place, then add the offset to\n+    \/\/ encode.\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+    __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+    __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n+\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::above, L_32byteLoop);\n+\n+    __ BIND(L_process3);\n+    __ vzeroupper();\n+  } else {\n+    __ BIND(L_process3);\n@@ -5480,17 +1917,2 @@\n-  address base64_avx2_lut_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-\n-    \/\/ URL LUT\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    return start;\n-  }\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::below, L_exit);\n@@ -5498,26 +1920,5 @@\n-  address base64_encoding_table_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Load the encoding table based on isURL\n+  __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+  __ movl(r15, isURL);\n+  __ shll(r15, 6);\n+  __ addptr(r11, r15);\n@@ -5525,22 +1926,1 @@\n-  \/\/ Code for generating Base64 encoding.\n-  \/\/ Intrinsic function prototype in Base64.java:\n-  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n-  \/\/ boolean isURL) {\n-  address generate_base64_encodeBlock()\n-  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0;       \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2;   \/\/ end offset\n-    const Register dest = c_rarg3;   \/\/ destination array\n+  __ BIND(L_processdata);\n@@ -5548,11 +1928,4 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n-#else\n-    const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10; \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-#endif\n+  \/\/ Load 3 bytes\n+  __ load_unsigned_byte(r15, Address(source, start_offset));\n+  __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+  __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n@@ -5560,3 +1933,4 @@\n-    const Register length = r14;\n-    const Register encode_table = r13;\n-    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+  \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+  __ movl(rax, r10);\n+  __ shll(r10, 24);\n+  __ orl(rax, r10);\n@@ -5564,5 +1938,1 @@\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n+  __ subl(length, 3);\n@@ -5570,6 +1940,3 @@\n-    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n-    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n-    \/\/ sure not to read past the end of the input buffer.\n-    if (VM_Version::supports_avx512_vbmi()) {\n-      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n-      __ jcc(Assembler::below, L_not512);\n+  __ shll(r15, 8);\n+  __ shll(r13, 16);\n+  __ orl(rax, r15);\n@@ -5577,4 +1944,1 @@\n-      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n-      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-      __ addptr(encode_table, isURL);\n-      __ shrl(isURL, 6); \/\/ restore isURL\n+  __ addl(start_offset, 3);\n@@ -5582,4 +1946,6 @@\n-      __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n-      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n-      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n-      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+  __ orl(rax, r13);\n+  \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+  \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+  \/\/ This translated byte is the fourth output byte.\n+  __ shrl(r13, 16);\n+  __ andl(r13, 0x3f);\n@@ -5587,2 +1953,3 @@\n-      __ align32();\n-      __ BIND(L_vbmiLoop);\n+  \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+  \/\/ The translated byte is the first output byte.\n+  __ shrl(r15, 10);\n@@ -5590,2 +1957,2 @@\n-      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ subl(length, 48);\n+  __ load_unsigned_byte(r13, Address(r11, r13));\n+  __ load_unsigned_byte(r15, Address(r11, r15));\n@@ -5593,4 +1960,1 @@\n-      \/\/ Put the input bytes into the proper lanes for writing, then\n-      \/\/ encode them.\n-      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n-      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+  __ movb(Address(dest, dp, Address::times_1, 3), r13);\n@@ -5598,2 +1962,5 @@\n-      \/\/ Write to destination\n-      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+  \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+  \/\/ This translated byte is the second output byte.\n+  __ shrl(rax, 4);\n+  __ movl(r10, rax);\n+  __ andl(rax, 0x3f);\n@@ -5601,4 +1968,1 @@\n-      __ addptr(dest, 64);\n-      __ addptr(source, 48);\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+  __ movb(Address(dest, dp, Address::times_1, 0), r15);\n@@ -5606,2 +1970,1 @@\n-      __ vzeroupper();\n-    }\n+  __ load_unsigned_byte(rax, Address(r11, rax));\n@@ -5609,216 +1972,4 @@\n-    __ BIND(L_not512);\n-    if (VM_Version::supports_avx2()\n-        && VM_Version::supports_avx512vlbw()) {\n-      \/*\n-      ** This AVX2 encoder is based off the paper at:\n-      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n-      **\n-      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n-      ** output bytes.\n-      **\n-      *\/\n-      \/\/ Lengths under 32 bytes are done with scalar routine\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      \/\/ Set up supporting constant table data\n-      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n-      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n-      __ movl(rax, 0x0fc0fc00);\n-      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n-      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n-      \/\/ bits\n-      __ movl(rax, 0x04000040);\n-\n-      __ subl(length, 24);\n-      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n-\n-      \/\/ For the first load, we mask off reading of the first 4\n-      \/\/ bytes into the register. This is so we can get 4 3-byte\n-      \/\/ chunks into each lane of the register, avoiding having to\n-      \/\/ handle end conditions.  We then shuffle these bytes into a\n-      \/\/ specific order so that manipulation is easier.\n-      \/\/\n-      \/\/ The initial read loads the XMM register like this:\n-      \/\/\n-      \/\/ Lower 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n-      \/\/ | C2 | D0 | D1 | D2 |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Upper 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n-      \/\/ | XX | XX | XX | XX |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n-      \/\/ The alphabetical significance denotes the 3 bytes to be\n-      \/\/ consumed and encoded into 4 bytes.\n-      \/\/\n-      \/\/ We then shuffle the register so each 32-bit word contains\n-      \/\/ the sequence:\n-      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n-      \/\/ Each of these byte sequences are then manipulated into 4\n-      \/\/ 6-bit values ready for encoding.\n-      \/\/\n-      \/\/ If we focus on one set of 3-byte chunks, changing the\n-      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n-      \/\/ shuffle such that each 24-bit chunk contains:\n-      \/\/\n-      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n-      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n-      \/\/ Explain this step.\n-      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n-      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n-      \/\/\n-      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n-      \/\/ a5..a0) and shift them using a vector multiplication\n-      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n-      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n-      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n-      \/\/ bits respectively.  This is done using vpmullw.  We end up\n-      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n-      \/\/ ready for encoding:\n-      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n-      \/\/\n-      \/\/ For translation, we recognize that there are 5 distinct\n-      \/\/ ranges of legal Base64 characters as below:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value | ASCII range |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |    A..Z     |     65     |\n-      \/\/   |   26..51    |    a..z     |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |   + or -    | -19 or -17 |\n-      \/\/   |     63      |   \/ or _    | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We note that vpshufb does a parallel lookup in a\n-      \/\/ destination register using the lower 4 bits of bytes from a\n-      \/\/ source register.  If we use a saturated subtraction and\n-      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n-      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n-      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n-      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n-      \/\/ with:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value |   Reduced   |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |     13      |     65     |\n-      \/\/   |   26..51    |      0      |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |     11      | -19 or -17 |\n-      \/\/   |     63      |     12      | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We then use a final vpshufb to add the appropriate offset,\n-      \/\/ translating the bytes.\n-      \/\/\n-      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n-      \/\/ not load into the full register.\n-      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n-\n-      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n-      \/\/ ordering by:\n-      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n-      \/\/   for easy masking\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ addl(start_offset, 24);\n-\n-      \/\/ Load masking register for first and third (and multiples)\n-      \/\/ 6-bit values.\n-      __ movl(rax, 0x003f03f0);\n-      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n-      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n-      __ movl(rax, 0x01000010);\n-      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Isolate 6-bit chunks of interest\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-\n-      \/\/ Load constants for encoding\n-      __ movl(rax, 0x19191919);\n-      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n-      __ movl(rax, 0x33333333);\n-      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Shift output bytes 0 and 2 into proper lanes\n-      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n-\n-      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n-      \/\/ combine\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Find out which are 0..25.  This indicates which input\n-      \/\/ values fall in the range of 'A'-'Z', which require an\n-      \/\/ additional offset (see comments above)\n-      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Load the proper lookup table\n-      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n-      __ movl(r15, isURL);\n-      __ shll(r15, 5);\n-      __ vmovdqu(xmm2, Address(r11, r15));\n-\n-      \/\/ Shuffle the offsets based on the range calculation done\n-      \/\/ above. This allows us to add the correct offset to the\n-      \/\/ 6-bit value corresponding to the range documented above.\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      __ align32();\n-      __ BIND(L_32byteLoop);\n-\n-      \/\/ Get next 32 bytes\n-      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n-\n-      __ subl(length, 24);\n-      __ addl(start_offset, 24);\n-\n-      \/\/ This logic is identical to the above, with only constant\n-      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n-      \/\/ chunks, shift them into place, then add the offset to\n-      \/\/ encode.\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n-      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::above, L_32byteLoop);\n-\n-      __ BIND(L_process3);\n-      __ vzeroupper();\n-    } else {\n-      __ BIND(L_process3);\n-    }\n+  \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+  \/\/ This translated byte is the third output byte.\n+  __ shrl(r10, 18);\n+  __ andl(r10, 0x3f);\n@@ -5826,2 +1977,1 @@\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::below, L_exit);\n+  __ load_unsigned_byte(r10, Address(r11, r10));\n@@ -5829,5 +1979,2 @@\n-    \/\/ Load the encoding table based on isURL\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-    __ movl(r15, isURL);\n-    __ shll(r15, 6);\n-    __ addptr(r11, r15);\n+  __ movb(Address(dest, dp, Address::times_1, 1), rax);\n+  __ movb(Address(dest, dp, Address::times_1, 2), r10);\n@@ -5835,1 +1982,14 @@\n-    __ BIND(L_processdata);\n+  __ addl(dp, 4);\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::aboveEqual, L_processdata);\n+\n+  __ BIND(L_exit);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n@@ -5837,4 +1997,16 @@\n-    \/\/ Load 3 bytes\n-    __ load_unsigned_byte(r15, Address(source, start_offset));\n-    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n-    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+\/\/ base64 AVX512vbmi tables\n+address StubGenerator::base64_vbmi_lookup_lo_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x3f8080803e808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -5842,4 +2014,2 @@\n-    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n-    __ movl(rax, r10);\n-    __ shll(r10, 24);\n-    __ orl(rax, r10);\n+  return start;\n+}\n@@ -5847,1 +2017,15 @@\n-    __ subl(length, 3);\n+address StubGenerator::base64_vbmi_lookup_hi_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x8080808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -5849,3 +2033,17 @@\n-    __ shll(r15, 8);\n-    __ shll(r13, 16);\n-    __ orl(rax, r15);\n+  return start;\n+}\n+address StubGenerator::base64_vbmi_lookup_lo_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x80803e8080808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -5853,1 +2051,2 @@\n-    __ addl(start_offset, 3);\n+  return start;\n+}\n@@ -5855,6 +2054,15 @@\n-    __ orl(rax, r13);\n-    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n-    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n-    \/\/ This translated byte is the fourth output byte.\n-    __ shrl(r13, 16);\n-    __ andl(r13, 0x3f);\n+address StubGenerator::base64_vbmi_lookup_hi_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3f80808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -5862,3 +2070,2 @@\n-    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n-    \/\/ The translated byte is the first output byte.\n-    __ shrl(r15, 10);\n+  return start;\n+}\n@@ -5866,2 +2073,15 @@\n-    __ load_unsigned_byte(r13, Address(r11, r13));\n-    __ load_unsigned_byte(r15, Address(r11, r15));\n+address StubGenerator::base64_vbmi_pack_vec_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -5869,1 +2089,2 @@\n-    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+  return start;\n+}\n@@ -5871,5 +2092,15 @@\n-    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n-    \/\/ This translated byte is the second output byte.\n-    __ shrl(rax, 4);\n-    __ movl(r10, rax);\n-    __ andl(rax, 0x3f);\n+address StubGenerator::base64_vbmi_join_0_1_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n@@ -5877,1 +2108,2 @@\n-    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+  return start;\n+}\n@@ -5879,1 +2111,15 @@\n-    __ load_unsigned_byte(rax, Address(r11, rax));\n+address StubGenerator::base64_vbmi_join_1_2_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n@@ -5881,4 +2127,2 @@\n-    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n-    \/\/ This translated byte is the third output byte.\n-    __ shrl(r10, 18);\n-    __ andl(r10, 0x3f);\n+  return start;\n+}\n@@ -5886,1 +2130,15 @@\n-    __ load_unsigned_byte(r10, Address(r11, r10));\n+address StubGenerator::base64_vbmi_join_2_3_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n+  __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n+  __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n@@ -5888,2 +2146,2 @@\n-    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n-    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+  return start;\n+}\n@@ -5891,3 +2149,4 @@\n-    __ addl(dp, 4);\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::aboveEqual, L_processdata);\n+address StubGenerator::base64_AVX2_decode_tables_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"AVX2_tables_base64\");\n+  address start = __ pc();\n@@ -5895,9 +2154,4 @@\n-    __ BIND(L_exit);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data(0x2f2f2f2f, relocInfo::none, 0);\n+  __ emit_data(0x5f5f5f5f, relocInfo::none, 0);  \/\/ for URL\n@@ -5905,17 +2159,2 @@\n-  \/\/ base64 AVX512vbmi tables\n-  address base64_vbmi_lookup_lo_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x3f8080803e808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data(0xffffffff, relocInfo::none, 0);\n+  __ emit_data(0xfcfcfcfc, relocInfo::none, 0);  \/\/ for URL\n@@ -5923,32 +2162,5 @@\n-  address base64_vbmi_lookup_hi_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x8080808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n-  address base64_vbmi_lookup_lo_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x80803e8080808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Permute table\n+  __ emit_data64(0x0000000100000000, relocInfo::none);\n+  __ emit_data64(0x0000000400000002, relocInfo::none);\n+  __ emit_data64(0x0000000600000005, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n@@ -5956,16 +2168,5 @@\n-  address base64_vbmi_lookup_hi_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3f80808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Shuffle table\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0xffffffff0c0d0e08, relocInfo::none);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0xffffffff0c0d0e08, relocInfo::none);\n@@ -5973,16 +2174,2 @@\n-  address base64_vbmi_pack_vec_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ merge table\n+  __ emit_data(0x01400140, relocInfo::none, 0);\n@@ -5990,16 +2177,2 @@\n-  address base64_vbmi_join_0_1_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ merge multiplier\n+  __ emit_data(0x00011000, relocInfo::none, 0);\n@@ -6007,16 +2180,2 @@\n-  address base64_vbmi_join_1_2_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -6024,16 +2183,36 @@\n-  address base64_vbmi_join_2_3_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n-    __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::base64_AVX2_decode_LUT_tables_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"AVX2_tables_URL_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  \/\/ lut_lo\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1a1b1b1b1a131111, relocInfo::none);\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1a1b1b1b1a131111, relocInfo::none);\n+\n+  \/\/ lut_roll\n+  __ emit_data64(0xb9b9bfbf04131000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xb9b9bfbf04131000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  \/\/ lut_lo URL\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1b1b1a1b1b131111, relocInfo::none);\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1b1b1a1b1b131111, relocInfo::none);\n+\n+  \/\/ lut_roll URL\n+  __ emit_data64(0xb9b9bfbf0411e000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xb9b9bfbf0411e000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  \/\/ lut_hi\n+  __ emit_data64(0x0804080402011010, relocInfo::none);\n+  __ emit_data64(0x1010101010101010, relocInfo::none);\n+  __ emit_data64(0x0804080402011010, relocInfo::none);\n+  __ emit_data64(0x1010101010101010, relocInfo::none);\n@@ -6041,71 +2220,76 @@\n-  address base64_decoding_table_addr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0xffffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffff3effffffffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3fffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n+\n+address StubGenerator::base64_decoding_table_addr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0xffffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffff3effffffffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3fffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  return start;\n+}\n@@ -6120,19 +2304,20 @@\n-  address generate_base64_decodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0; \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2; \/\/ end offset\n-    const Register dest = c_rarg3; \/\/ destination array\n-    const Register isMIME = rbx;\n+address StubGenerator::generate_base64_decodeBlock() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0; \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2; \/\/ end offset\n+  const Register dest = c_rarg3; \/\/ destination array\n+  const Register isMIME = rbx;\n@@ -6141,3 +2326,3 @@\n-    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n-    __ movl(isMIME, Address(rbp, 2 * wordSize));\n+  const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n+  __ movl(isMIME, Address(rbp, 2 * wordSize));\n@@ -6145,7 +2330,7 @@\n-    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10;      \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-    __ movl(isMIME, Address(rbp, 8 * wordSize));\n+  const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10;      \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+  __ movl(isMIME, Address(rbp, 8 * wordSize));\n@@ -6154,338 +2339,156 @@\n-    const XMMRegister lookup_lo = xmm5;\n-    const XMMRegister lookup_hi = xmm6;\n-    const XMMRegister errorvec = xmm7;\n-    const XMMRegister pack16_op = xmm9;\n-    const XMMRegister pack32_op = xmm8;\n-    const XMMRegister input0 = xmm3;\n-    const XMMRegister input1 = xmm20;\n-    const XMMRegister input2 = xmm21;\n-    const XMMRegister input3 = xmm19;\n-    const XMMRegister join01 = xmm12;\n-    const XMMRegister join12 = xmm11;\n-    const XMMRegister join23 = xmm10;\n-    const XMMRegister translated0 = xmm2;\n-    const XMMRegister translated1 = xmm1;\n-    const XMMRegister translated2 = xmm0;\n-    const XMMRegister translated3 = xmm4;\n-\n-    const XMMRegister merged0 = xmm2;\n-    const XMMRegister merged1 = xmm1;\n-    const XMMRegister merged2 = xmm0;\n-    const XMMRegister merged3 = xmm4;\n-    const XMMRegister merge_ab_bc0 = xmm2;\n-    const XMMRegister merge_ab_bc1 = xmm1;\n-    const XMMRegister merge_ab_bc2 = xmm0;\n-    const XMMRegister merge_ab_bc3 = xmm4;\n-\n-    const XMMRegister pack24bits = xmm4;\n-\n-    const Register length = r14;\n-    const Register output_size = r13;\n-    const Register output_mask = r15;\n-    const KRegister input_mask = k1;\n-\n-    const XMMRegister input_initial_valid_b64 = xmm0;\n-    const XMMRegister tmp = xmm10;\n-    const XMMRegister mask = xmm0;\n-    const XMMRegister invalid_b64 = xmm1;\n-\n-    Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n-    Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n-    Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n-\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ push(dest);          \/\/ Save for return value calc\n-\n-    \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n-    if(VM_Version::supports_avx512_vbmi() &&\n-       VM_Version::supports_avx512bw()) {\n-      __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n-      __ jcc(Assembler::lessEqual, L_bruteForce);\n-\n-      __ cmpl(isMIME, 0);\n-      __ jcc(Assembler::notEqual, L_bruteForce);\n-\n-      \/\/ Load lookup tables based on isURL\n-      __ cmpl(isURL, 0);\n-      __ jcc(Assembler::notZero, L_loadURL);\n-\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ BIND(L_continue);\n-\n-      __ movl(r15, 0x01400140);\n-      __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n-\n-      __ movl(r15, 0x00011000);\n-      __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n-\n-      __ cmpl(length, 0xff);\n-      __ jcc(Assembler::lessEqual, L_process64);\n-\n-      \/\/ load masks required for decoding data\n-      __ BIND(L_processdata);\n-      __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n-      __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ align32();\n-      __ BIND(L_process256);\n-      \/\/ Grab input data\n-      __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n-      __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n-      __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n-      __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n-\n-      \/\/ Copy the low part of the lookup table into the destination of the permutation\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n-\n-      \/\/ Translate the base64 input into \"decoded\" bytes\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n-\n-      \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n-      __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n-\n-      __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n-      __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n-      __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check if there was an error - if so, try 64-byte chunks\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_process64);\n-\n-      \/\/ The merging and shuffling happens here\n-      \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n-      \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n-      \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n-\n-      \/\/ Now do the same with packed 16-bit values.\n-      \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n-      \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n-      \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n-\n-      \/\/ The join vectors specify which byte from which vector goes into the outputs\n-      \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n-      \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n-      __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n-      __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n-      __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n-\n-      \/\/ Store result\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n-\n-      __ addptr(source, 0x100);\n-      __ addptr(dest, 0xc0);\n-      __ subl(length, 0x100);\n-      __ cmpl(length, 64 * 4);\n-      __ jcc(Assembler::greaterEqual, L_process256);\n-\n-      \/\/ At this point, we've decoded 64 * 4 * n bytes.\n-      \/\/ The remaining length will be <= 64 * 4 - 1.\n-      \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n-      \/\/ case, the length will be arbitrarily long.\n-      \/\/\n-      \/\/ Note that this will be the path for MIME-encoded strings.\n-\n-      __ BIND(L_process64);\n-\n-      __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ cmpl(length, 63);\n-      __ jcc(Assembler::lessEqual, L_finalBit);\n-\n-      __ mov64(rax, 0x0000ffffffffffff);\n-      __ kmovql(k2, rax);\n-\n-      __ align32();\n-      __ BIND(L_process64Loop);\n-\n-      \/\/ Handle first 64-byte block\n-\n-      __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-\n-      __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error and bomb out before updating dest\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_exit);\n-\n-      \/\/ Pack output register, selecting correct byte ordering\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n-\n-      __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n-\n-      __ subl(length, 64);\n-      __ addptr(source, 64);\n-      __ addptr(dest, 48);\n-\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::greaterEqual, L_process64Loop);\n-\n-      __ cmpl(length, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ BIND(L_finalBit);\n-      \/\/ Now have 1 to 63 bytes left to decode\n-\n-      \/\/ I was going to let Java take care of the final fragment\n-      \/\/ however it will repeatedly call this routine for every 4 bytes\n-      \/\/ of input data, so handle the rest here.\n-      __ movq(rax, -1);\n-      __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n-\n-      __ movl(output_size, length);\n-      __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n-      __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n-      \/\/ output_size in r13\n-\n-      \/\/ Strip pad characters, if any, and adjust length and mask\n-      __ cmpb(Address(source, length, Address::times_1, -1), '=');\n-      __ jcc(Assembler::equal, L_padding);\n-\n-      __ BIND(L_donePadding);\n-\n-      \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n-      __ kmovql(input_mask, rax);\n-      __ movq(output_mask, -1);\n-      __ bzhiq(output_mask, output_mask, output_size);\n-\n-      \/\/ Load initial input with all valid base64 characters.  Will be used\n-      \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n-      __ movl(rax, 0x61616161);\n-      __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ A register containing all invalid base64 decoded values\n-      __ movl(rax, 0x80808080);\n-      __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ input_mask is in k1\n-      \/\/ output_size is in r13\n-      \/\/ output_mask is in r15\n-      \/\/ zmm0 - free\n-      \/\/ zmm1 - 0x00011000\n-      \/\/ zmm2 - 0x01400140\n-      \/\/ zmm3 - errorvec\n-      \/\/ zmm4 - pack vector\n-      \/\/ zmm5 - lookup_lo\n-      \/\/ zmm6 - lookup_hi\n-      \/\/ zmm7 - errorvec\n-      \/\/ zmm8 - 0x61616161\n-      \/\/ zmm9 - 0x80808080\n-\n-      \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n-      __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n-\n-      \/\/ Decode all bytes within our merged input\n-      __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n-      __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n-      \/\/ If any bytes have their high-order bit set, then we have an error.\n-      __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n-      __ kortestql(k2, k2);\n-\n-      \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n-      __ jcc(Assembler::notZero, L_bruteForce);\n-\n-      \/\/ Shuffle output bytes\n-      __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n-\n-      __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n-      __ kmovql(k1, output_mask);\n-      __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n-\n-      __ addptr(dest, output_size);\n-\n-      __ BIND(L_exit);\n-      __ vzeroupper();\n-      __ pop(rax);             \/\/ Get original dest value\n-      __ subptr(dest, rax);      \/\/ Number of bytes converted\n-      __ movptr(rax, dest);\n-      __ pop(rbx);\n-      __ pop(r15);\n-      __ pop(r14);\n-      __ pop(r13);\n-      __ pop(r12);\n-      __ leave();\n-      __ ret(0);\n-\n-      __ BIND(L_loadURL);\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n-      __ jmp(L_continue);\n-\n-      __ BIND(L_padding);\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-\n-      __ cmpb(Address(source, length, Address::times_1, -2), '=');\n-      __ jcc(Assembler::notEqual, L_donePadding);\n-\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-      __ jmp(L_donePadding);\n-\n-      __ align32();\n-      __ BIND(L_bruteForce);\n-    }   \/\/ End of if(avx512_vbmi)\n-\n-    \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n-\n-    \/\/ Register state (Linux):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rdi - src\n-    \/\/ rsi - sp\n-    \/\/ rdx - sl\n-    \/\/ rcx - dst\n-    \/\/ r8 - dp\n-    \/\/ r9 - isURL\n-\n-    \/\/ Register state (Windows):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rcx - src\n-    \/\/ rdx - sp\n-    \/\/ r8 - sl\n-    \/\/ r9 - dst\n-    \/\/ r12 - dp\n-    \/\/ r10 - isURL\n-\n-    \/\/ Registers (common):\n-    \/\/ length (r14) - bytes in src\n-\n-    const Register decode_table = r11;\n-    const Register out_byte_count = rbx;\n-    const Register byte1 = r13;\n-    const Register byte2 = r15;\n-    const Register byte3 = WINDOWS_ONLY(r8) NOT_WINDOWS(rdx);\n-    const Register byte4 = WINDOWS_ONLY(r10) NOT_WINDOWS(r9);\n-\n-    __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+  const XMMRegister lookup_lo = xmm5;\n+  const XMMRegister lookup_hi = xmm6;\n+  const XMMRegister errorvec = xmm7;\n+  const XMMRegister pack16_op = xmm9;\n+  const XMMRegister pack32_op = xmm8;\n+  const XMMRegister input0 = xmm3;\n+  const XMMRegister input1 = xmm20;\n+  const XMMRegister input2 = xmm21;\n+  const XMMRegister input3 = xmm19;\n+  const XMMRegister join01 = xmm12;\n+  const XMMRegister join12 = xmm11;\n+  const XMMRegister join23 = xmm10;\n+  const XMMRegister translated0 = xmm2;\n+  const XMMRegister translated1 = xmm1;\n+  const XMMRegister translated2 = xmm0;\n+  const XMMRegister translated3 = xmm4;\n+\n+  const XMMRegister merged0 = xmm2;\n+  const XMMRegister merged1 = xmm1;\n+  const XMMRegister merged2 = xmm0;\n+  const XMMRegister merged3 = xmm4;\n+  const XMMRegister merge_ab_bc0 = xmm2;\n+  const XMMRegister merge_ab_bc1 = xmm1;\n+  const XMMRegister merge_ab_bc2 = xmm0;\n+  const XMMRegister merge_ab_bc3 = xmm4;\n+\n+  const XMMRegister pack24bits = xmm4;\n+\n+  const Register length = r14;\n+  const Register output_size = r13;\n+  const Register output_mask = r15;\n+  const KRegister input_mask = k1;\n+\n+  const XMMRegister input_initial_valid_b64 = xmm0;\n+  const XMMRegister tmp = xmm10;\n+  const XMMRegister mask = xmm0;\n+  const XMMRegister invalid_b64 = xmm1;\n+\n+  Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n+  Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n+  Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero, L_lastChunk;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ push(dest);          \/\/ Save for return value calc\n+\n+  \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n+  if(VM_Version::supports_avx512_vbmi() &&\n+     VM_Version::supports_avx512bw()) {\n+    __ cmpl(length, 31);     \/\/ 32-bytes is break-even for AVX-512\n+    __ jcc(Assembler::lessEqual, L_lastChunk);\n+\n+    __ cmpl(isMIME, 0);\n+    __ jcc(Assembler::notEqual, L_lastChunk);\n+\n+    \/\/ Load lookup tables based on isURL\n+    __ cmpl(isURL, 0);\n+    __ jcc(Assembler::notZero, L_loadURL);\n+\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n+\n+    __ BIND(L_continue);\n+\n+    __ movl(r15, 0x01400140);\n+    __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n+\n+    __ movl(r15, 0x00011000);\n+    __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n+\n+    __ cmpl(length, 0xff);\n+    __ jcc(Assembler::lessEqual, L_process64);\n+\n+    \/\/ load masks required for decoding data\n+    __ BIND(L_processdata);\n+    __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n+    __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n+\n+    __ align32();\n+    __ BIND(L_process256);\n+    \/\/ Grab input data\n+    __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n+    __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n+    __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n+    __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n+\n+    \/\/ Copy the low part of the lookup table into the destination of the permutation\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n+\n+    \/\/ Translate the base64 input into \"decoded\" bytes\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n+\n+    \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n+    __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n+\n+    __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n+    __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n+    __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n+\n+    \/\/ Check if there was an error - if so, try 64-byte chunks\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_process64);\n+\n+    \/\/ The merging and shuffling happens here\n+    \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n+    \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n+    \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n+\n+    \/\/ Now do the same with packed 16-bit values.\n+    \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n+    \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n+    \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n+\n+    \/\/ The join vectors specify which byte from which vector goes into the outputs\n+    \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n+    \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n+    __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n+    __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n+    __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n+\n+    \/\/ Store result\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n+\n+    __ addptr(source, 0x100);\n+    __ addptr(dest, 0xc0);\n+    __ subl(length, 0x100);\n+    __ cmpl(length, 64 * 4);\n+    __ jcc(Assembler::greaterEqual, L_process256);\n+\n+    \/\/ At this point, we've decoded 64 * 4 * n bytes.\n+    \/\/ The remaining length will be <= 64 * 4 - 1.\n+    \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n+    \/\/ case, the length will be arbitrarily long.\n+    \/\/\n+    \/\/ Note that this will be the path for MIME-encoded strings.\n+\n+    __ BIND(L_process64);\n@@ -6493,3 +2496,1 @@\n-    __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n-    __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n-    __ addptr(decode_table, isURL);\n+    __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n@@ -6497,1 +2498,5 @@\n-    __ jmp(L_bottomLoop);\n+    __ cmpl(length, 63);\n+    __ jcc(Assembler::lessEqual, L_finalBit);\n+\n+    __ mov64(rax, 0x0000ffffffffffff);\n+    __ kmovql(k2, rax);\n@@ -6500,37 +2505,108 @@\n-    __ BIND(L_forceLoop);\n-    __ shll(byte1, 18);\n-    __ shll(byte2, 12);\n-    __ shll(byte3, 6);\n-    __ orl(byte1, byte2);\n-    __ orl(byte1, byte3);\n-    __ orl(byte1, byte4);\n-\n-    __ addptr(source, 4);\n-\n-    __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n-\n-    __ addptr(dest, 3);\n-    __ decrementl(length, 1);\n-    __ jcc(Assembler::zero, L_exit_no_vzero);\n-\n-    __ BIND(L_bottomLoop);\n-    __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n-    __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n-    __ load_signed_byte(byte1, Address(decode_table, byte1));\n-    __ load_signed_byte(byte2, Address(decode_table, byte2));\n-    __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n-    __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n-    __ load_signed_byte(byte3, Address(decode_table, byte3));\n-    __ load_signed_byte(byte4, Address(decode_table, byte4));\n-\n-    __ mov(rax, byte1);\n-    __ orl(rax, byte2);\n-    __ orl(rax, byte3);\n-    __ orl(rax, byte4);\n-    __ jcc(Assembler::positive, L_forceLoop);\n-\n-    __ BIND(L_exit_no_vzero);\n+    __ BIND(L_process64Loop);\n+\n+    \/\/ Handle first 64-byte block\n+\n+    __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+\n+    __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n+\n+    \/\/ Check for error and bomb out before updating dest\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_exit);\n+\n+    \/\/ Pack output register, selecting correct byte ordering\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n+\n+    __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n+\n+    __ subl(length, 64);\n+    __ addptr(source, 64);\n+    __ addptr(dest, 48);\n+\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::greaterEqual, L_process64Loop);\n+\n+    __ cmpl(length, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    __ BIND(L_finalBit);\n+    \/\/ Now have 1 to 63 bytes left to decode\n+\n+    \/\/ I was going to let Java take care of the final fragment\n+    \/\/ however it will repeatedly call this routine for every 4 bytes\n+    \/\/ of input data, so handle the rest here.\n+    __ movq(rax, -1);\n+    __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n+\n+    __ movl(output_size, length);\n+    __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n+    __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n+    \/\/ output_size in r13\n+\n+    \/\/ Strip pad characters, if any, and adjust length and mask\n+    __ cmpb(Address(source, length, Address::times_1, -1), '=');\n+    __ jcc(Assembler::equal, L_padding);\n+\n+    __ BIND(L_donePadding);\n+\n+    \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n+    __ kmovql(input_mask, rax);\n+    __ movq(output_mask, -1);\n+    __ bzhiq(output_mask, output_mask, output_size);\n+\n+    \/\/ Load initial input with all valid base64 characters.  Will be used\n+    \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n+    __ movl(rax, 0x61616161);\n+    __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ A register containing all invalid base64 decoded values\n+    __ movl(rax, 0x80808080);\n+    __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ input_mask is in k1\n+    \/\/ output_size is in r13\n+    \/\/ output_mask is in r15\n+    \/\/ zmm0 - free\n+    \/\/ zmm1 - 0x00011000\n+    \/\/ zmm2 - 0x01400140\n+    \/\/ zmm3 - errorvec\n+    \/\/ zmm4 - pack vector\n+    \/\/ zmm5 - lookup_lo\n+    \/\/ zmm6 - lookup_hi\n+    \/\/ zmm7 - errorvec\n+    \/\/ zmm8 - 0x61616161\n+    \/\/ zmm9 - 0x80808080\n+\n+    \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n+    __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n+\n+    \/\/ Decode all bytes within our merged input\n+    __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n+    __ evporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n+\n+    \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n+    \/\/ If any bytes have their high-order bit set, then we have an error.\n+    __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n+    __ kortestql(k2, k2);\n+\n+    \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n+    __ jcc(Assembler::notZero, L_bruteForce);\n+\n+    \/\/ Shuffle output bytes\n+    __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n+\n+    __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n+    __ kmovql(k1, output_mask);\n+    __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n+\n+    __ addptr(dest, output_size);\n+\n+    __ BIND(L_exit);\n+    __ vzeroupper();\n@@ -6548,2 +2624,294 @@\n-    return start;\n-  }\n+    __ BIND(L_loadURL);\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n+    __ jmp(L_continue);\n+\n+    __ BIND(L_padding);\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n+\n+    __ cmpb(Address(source, length, Address::times_1, -2), '=');\n+    __ jcc(Assembler::notEqual, L_donePadding);\n+\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n+    __ jmp(L_donePadding);\n+\n+    __ align32();\n+    __ BIND(L_bruteForce);\n+  }   \/\/ End of if(avx512_vbmi)\n+\n+  if (VM_Version::supports_avx2()) {\n+    Label L_tailProc, L_topLoop, L_enterLoop;\n+\n+    __ cmpl(isMIME, 0);\n+    __ jcc(Assembler::notEqual, L_lastChunk);\n+\n+    \/\/ Check for buffer too small (for algorithm)\n+    __ subl(length, 0x2c);\n+    __ jcc(Assembler::less, L_tailProc);\n+\n+    __ shll(isURL, 2);\n+\n+    \/\/ Algorithm adapted from https:\/\/arxiv.org\/abs\/1704.00605, \"Faster Base64\n+    \/\/ Encoding and Decoding using AVX2 Instructions\".  URL modifications added.\n+\n+    \/\/ Set up constants\n+    __ lea(r13, ExternalAddress(StubRoutines::x86::base64_AVX2_decode_tables_addr()));\n+    __ vpbroadcastd(xmm4, Address(r13, isURL, Address::times_1), Assembler::AVX_256bit);  \/\/ 2F or 5F\n+    __ vpbroadcastd(xmm10, Address(r13, isURL, Address::times_1, 0x08), Assembler::AVX_256bit);  \/\/ -1 or -4\n+    __ vmovdqu(xmm12, Address(r13, 0x10));  \/\/ permute\n+    __ vmovdqu(xmm13, Address(r13, 0x30)); \/\/ shuffle\n+    __ vpbroadcastd(xmm7, Address(r13, 0x50), Assembler::AVX_256bit);  \/\/ merge\n+    __ vpbroadcastd(xmm6, Address(r13, 0x54), Assembler::AVX_256bit);  \/\/ merge mult\n+\n+    __ lea(r13, ExternalAddress(StubRoutines::x86::base64_AVX2_decode_LUT_tables_addr()));\n+    __ shll(isURL, 4);\n+    __ vmovdqu(xmm11, Address(r13, isURL, Address::times_1, 0x00));  \/\/ lut_lo\n+    __ vmovdqu(xmm8, Address(r13, isURL, Address::times_1, 0x20)); \/\/ lut_roll\n+    __ shrl(isURL, 6);  \/\/ restore isURL\n+    __ vmovdqu(xmm9, Address(r13, 0x80));  \/\/ lut_hi\n+    __ jmp(L_enterLoop);\n+\n+    __ align32();\n+    __ bind(L_topLoop);\n+    \/\/ Add in the offset value (roll) to get 6-bit out values\n+    __ vpaddb(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+    \/\/ Merge and permute the output bits into appropriate output byte lanes\n+    __ vpmaddubsw(xmm0, xmm0, xmm7, Assembler::AVX_256bit);\n+    __ vpmaddwd(xmm0, xmm0, xmm6, Assembler::AVX_256bit);\n+    __ vpshufb(xmm0, xmm0, xmm13, Assembler::AVX_256bit);\n+    __ vpermd(xmm0, xmm12, xmm0, Assembler::AVX_256bit);\n+    \/\/ Store the output bytes\n+    __ vmovdqu(Address(dest, dp, Address::times_1, 0), xmm0);\n+    __ addptr(source, 0x20);\n+    __ addptr(dest, 0x18);\n+    __ subl(length, 0x20);\n+    __ jcc(Assembler::less, L_tailProc);\n+\n+    __ bind(L_enterLoop);\n+\n+    \/\/ Load in encoded string (32 bytes)\n+    __ vmovdqu(xmm2, Address(source, start_offset, Address::times_1, 0x0));\n+    \/\/ Extract the high nibble for indexing into the lut tables.  High 4 bits are don't care.\n+    __ vpsrld(xmm1, xmm2, 0x4, Assembler::AVX_256bit);\n+    __ vpand(xmm1, xmm4, xmm1, Assembler::AVX_256bit);\n+    \/\/ Extract the low nibble. 5F\/2F will isolate the low-order 4 bits.  High 4 bits are don't care.\n+    __ vpand(xmm3, xmm2, xmm4, Assembler::AVX_256bit);\n+    \/\/ Check for special-case (0x2F or 0x5F (URL))\n+    __ vpcmpeqb(xmm0, xmm4, xmm2, Assembler::AVX_256bit);\n+    \/\/ Get the bitset based on the low nibble.  vpshufb uses low-order 4 bits only.\n+    __ vpshufb(xmm3, xmm11, xmm3, Assembler::AVX_256bit);\n+    \/\/ Get the bit value of the high nibble\n+    __ vpshufb(xmm5, xmm9, xmm1, Assembler::AVX_256bit);\n+    \/\/ Make sure 2F \/ 5F shows as valid\n+    __ vpandn(xmm3, xmm0, xmm3, Assembler::AVX_256bit);\n+    \/\/ Make adjustment for roll index.  For non-URL, this is a no-op,\n+    \/\/ for URL, this adjusts by -4.  This is to properly index the\n+    \/\/ roll value for 2F \/ 5F.\n+    __ vpand(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+    \/\/ If the and of the two is non-zero, we have an invalid input character\n+    __ vptest(xmm3, xmm5);\n+    \/\/ Extract the \"roll\" value - value to add to the input to get 6-bit out value\n+    __ vpaddb(xmm0, xmm0, xmm1, Assembler::AVX_256bit); \/\/ Handle 2F \/ 5F\n+    __ vpshufb(xmm0, xmm8, xmm0, Assembler::AVX_256bit);\n+    __ jcc(Assembler::equal, L_topLoop);  \/\/ Fall through on error\n+\n+    __ bind(L_tailProc);\n+\n+    __ addl(length, 0x2c);\n+\n+    __ vzeroupper();\n+  }\n+\n+  \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n+\n+  \/\/ Register state (Linux):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rdi - src\n+  \/\/ rsi - sp\n+  \/\/ rdx - sl\n+  \/\/ rcx - dst\n+  \/\/ r8 - dp\n+  \/\/ r9 - isURL\n+\n+  \/\/ Register state (Windows):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rcx - src\n+  \/\/ rdx - sp\n+  \/\/ r8 - sl\n+  \/\/ r9 - dst\n+  \/\/ r12 - dp\n+  \/\/ r10 - isURL\n+\n+  \/\/ Registers (common):\n+  \/\/ length (r14) - bytes in src\n+\n+  const Register decode_table = r11;\n+  const Register out_byte_count = rbx;\n+  const Register byte1 = r13;\n+  const Register byte2 = r15;\n+  const Register byte3 = WIN64_ONLY(r8) NOT_WIN64(rdx);\n+  const Register byte4 = WIN64_ONLY(r10) NOT_WIN64(r9);\n+\n+  __ bind(L_lastChunk);\n+\n+  __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+\n+  __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n+  __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n+  __ addptr(decode_table, isURL);\n+\n+  __ jmp(L_bottomLoop);\n+\n+  __ align32();\n+  __ BIND(L_forceLoop);\n+  __ shll(byte1, 18);\n+  __ shll(byte2, 12);\n+  __ shll(byte3, 6);\n+  __ orl(byte1, byte2);\n+  __ orl(byte1, byte3);\n+  __ orl(byte1, byte4);\n+\n+  __ addptr(source, 4);\n+\n+  __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n+\n+  __ addptr(dest, 3);\n+  __ decrementl(length, 1);\n+  __ jcc(Assembler::zero, L_exit_no_vzero);\n+\n+  __ BIND(L_bottomLoop);\n+  __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n+  __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n+  __ load_signed_byte(byte1, Address(decode_table, byte1));\n+  __ load_signed_byte(byte2, Address(decode_table, byte2));\n+  __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n+  __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n+  __ load_signed_byte(byte3, Address(decode_table, byte3));\n+  __ load_signed_byte(byte4, Address(decode_table, byte4));\n+\n+  __ mov(rax, byte1);\n+  __ orl(rax, byte2);\n+  __ orl(rax, byte3);\n+  __ orl(rax, byte4);\n+  __ jcc(Assembler::positive, L_forceLoop);\n+\n+  __ BIND(L_exit_no_vzero);\n+  __ pop(rax);             \/\/ Get original dest value\n+  __ subptr(dest, rax);      \/\/ Number of bytes converted\n+  __ movptr(rax, dest);\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/**\n+ *  Arguments:\n+ *\n+ * Inputs:\n+ *   c_rarg0   - int crc\n+ *   c_rarg1   - byte* buf\n+ *   c_rarg2   - int length\n+ *\n+ * Output:\n+ *       rax   - int crc result\n+ *\/\n+address StubGenerator::generate_updateBytesCRC32() {\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n+\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ rscratch1: r10\n+  const Register crc   = c_rarg0;  \/\/ crc\n+  const Register buf   = c_rarg1;  \/\/ source java byte array address\n+  const Register len   = c_rarg2;  \/\/ length\n+  const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n+  const Register tmp1   = r11;\n+  const Register tmp2   = r10;\n+  assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+      \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n+      \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n+      \/\/ difference before calling and after returning.\n+    __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n+    __ notl(crc);\n+    __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n+    __ notl(crc);\n+  } else {\n+    __ kernel_crc32(crc, buf, len, table, tmp1);\n+  }\n+\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/**\n+*  Arguments:\n+*\n+* Inputs:\n+*   c_rarg0   - int crc\n+*   c_rarg1   - byte* buf\n+*   c_rarg2   - long length\n+*   c_rarg3   - table_start - optional (present only when doing a library_call,\n+*              not used by x86 algorithm)\n+*\n+* Output:\n+*       rax   - int crc result\n+*\/\n+address StubGenerator::generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n+  assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n+  address start = __ pc();\n+\n+  \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n+  \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n+  \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n+  const Register crc = c_rarg0;  \/\/ crc\n+  const Register buf = c_rarg1;  \/\/ source java byte array address\n+  const Register len = c_rarg2;  \/\/ length\n+  const Register a = rax;\n+  const Register j = r9;\n+  const Register k = r10;\n+  const Register l = r11;\n+#ifdef _WIN64\n+  const Register y = rdi;\n+  const Register z = rsi;\n+#else\n+  const Register y = rcx;\n+  const Register z = r8;\n+#endif\n+  assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  Label L_continue;\n@@ -6551,0 +2919,4 @@\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+    Label L_doSmall;\n@@ -6552,45 +2924,2 @@\n-  \/**\n-   *  Arguments:\n-   *\n-   * Inputs:\n-   *   c_rarg0   - int crc\n-   *   c_rarg1   - byte* buf\n-   *   c_rarg2   - int length\n-   *\n-   * Output:\n-   *       rax   - int crc result\n-   *\/\n-  address generate_updateBytesCRC32() {\n-    assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ rscratch1: r10\n-    const Register crc   = c_rarg0;  \/\/ crc\n-    const Register buf   = c_rarg1;  \/\/ source java byte array address\n-    const Register len   = c_rarg2;  \/\/ length\n-    const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n-    const Register tmp1   = r11;\n-    const Register tmp2   = r10;\n-    assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-        VM_Version::supports_avx512bw() &&\n-        VM_Version::supports_avx512vl()) {\n-        \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n-        \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n-        \/\/ difference before calling and after returning.\n-      __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n-      __ notl(crc);\n-      __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n-      __ notl(crc);\n-    } else {\n-      __ kernel_crc32(crc, buf, len, table, tmp1);\n-    }\n+    __ cmpl(len, 384);\n+    __ jcc(Assembler::lessEqual, L_doSmall);\n@@ -6598,4 +2927,2 @@\n-    __ movl(rax, crc);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+    __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n+    __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n@@ -6603,2 +2930,1 @@\n-    return start;\n-  }\n+    __ jmp(L_continue);\n@@ -6606,45 +2932,2 @@\n-  \/**\n-  *  Arguments:\n-  *\n-  * Inputs:\n-  *   c_rarg0   - int crc\n-  *   c_rarg1   - byte* buf\n-  *   c_rarg2   - long length\n-  *   c_rarg3   - table_start - optional (present only when doing a library_call,\n-  *              not used by x86 algorithm)\n-  *\n-  * Output:\n-  *       rax   - int crc result\n-  *\/\n-  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n-      assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n-      address start = __ pc();\n-      \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n-      \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n-      \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n-      const Register crc = c_rarg0;  \/\/ crc\n-      const Register buf = c_rarg1;  \/\/ source java byte array address\n-      const Register len = c_rarg2;  \/\/ length\n-      const Register a = rax;\n-      const Register j = r9;\n-      const Register k = r10;\n-      const Register l = r11;\n-#ifdef _WIN64\n-      const Register y = rdi;\n-      const Register z = rsi;\n-#else\n-      const Register y = rcx;\n-      const Register z = r8;\n-#endif\n-      assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-          VM_Version::supports_avx512bw() &&\n-          VM_Version::supports_avx512vl()) {\n-        __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n-        __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n-      } else {\n+    __ bind(L_doSmall);\n+  }\n@@ -6652,2 +2935,2 @@\n-        __ push(y);\n-        __ push(z);\n+  __ push(y);\n+  __ push(z);\n@@ -6655,5 +2938,5 @@\n-        __ crc32c_ipl_alg2_alt2(crc, buf, len,\n-                                a, j, k,\n-                                l, y, z,\n-                                c_farg0, c_farg1, c_farg2,\n-                                is_pclmulqdq_supported);\n+  __ crc32c_ipl_alg2_alt2(crc, buf, len,\n+                          a, j, k,\n+                          l, y, z,\n+                          c_farg0, c_farg1, c_farg2,\n+                          is_pclmulqdq_supported);\n@@ -6661,2 +2944,2 @@\n-        __ pop(z);\n-        __ pop(y);\n+  __ pop(z);\n+  __ pop(y);\n@@ -6664,7 +2947,5 @@\n-      }\n-      __ movl(rax, crc);\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-      return start;\n-  }\n+  __ bind(L_continue);\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6673,0 +2954,2 @@\n+  return start;\n+}\n@@ -6674,77 +2957,38 @@\n-  \/***\n-   *  Arguments:\n-   *\n-   *  Inputs:\n-   *   c_rarg0   - int   adler\n-   *   c_rarg1   - byte* buff\n-   *   c_rarg2   - int   len\n-   *\n-   * Output:\n-   *   rax   - int adler result\n-   *\/\n-\n-  address generate_updateBytesAdler32() {\n-      assert(UseAdler32Intrinsics, \"need AVX2\");\n-\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n-\n-      address start = __ pc();\n-\n-      const Register data = r9;\n-      const Register size = r10;\n-\n-      const XMMRegister yshuf0 = xmm6;\n-      const XMMRegister yshuf1 = xmm7;\n-      assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-      __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table), r9);\n-      __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table), r9);\n-      __ movptr(data, c_rarg1); \/\/data\n-      __ movl(size, c_rarg2); \/\/length\n-      __ updateBytesAdler32(c_rarg0, data, size, yshuf0, yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table));\n-      __ leave();\n-      __ ret(0);\n-      return start;\n-  }\n-  \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - x address\n-   *    c_rarg1   - x length\n-   *    c_rarg2   - y address\n-   *    c_rarg3   - y length\n-   * not Win64\n-   *    c_rarg4   - z address\n-   *    c_rarg5   - z length\n-   * Win64\n-   *    rsp+40    - z address\n-   *    rsp+48    - z length\n-   *\/\n-  address generate_multiplyToLen() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register x     = rdi;\n-    const Register xlen  = rax;\n-    const Register y     = rsi;\n-    const Register ylen  = rcx;\n-    const Register z     = r8;\n-    const Register zlen  = r11;\n-\n-    \/\/ Next registers will be saved on stack in multiply_to_len().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - x address\n+ *    c_rarg1   - x length\n+ *    c_rarg2   - y address\n+ *    c_rarg3   - y length\n+ * not Win64\n+ *    c_rarg4   - z address\n+ *    c_rarg5   - z length\n+ * Win64\n+ *    rsp+40    - z address\n+ *    rsp+48    - z length\n+ *\/\n+address StubGenerator::generate_multiplyToLen() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register x     = rdi;\n+  const Register xlen  = rax;\n+  const Register y     = rsi;\n+  const Register ylen  = rcx;\n+  const Register z     = r8;\n+  const Register zlen  = r11;\n+\n+  \/\/ Next registers will be saved on stack in multiply_to_len().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -6754,1 +2998,1 @@\n-    __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n+  __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n@@ -6756,3 +3000,3 @@\n-    setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n-                       \/\/ ylen => rcx, z => r8, zlen => r11\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n+  setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n+                     \/\/ ylen => rcx, z => r8, zlen => r11\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -6760,3 +3004,3 @@\n-    \/\/ last 2 arguments (#4, #5) are on stack on Win64\n-    __ movptr(z, Address(rsp, 6 * wordSize));\n-    __ movptr(zlen, Address(rsp, 7 * wordSize));\n+  \/\/ last 2 arguments (#4, #5) are on stack on Win64\n+  __ movptr(z, Address(rsp, 6 * wordSize));\n+  __ movptr(zlen, Address(rsp, 7 * wordSize));\n@@ -6765,3 +3009,3 @@\n-    __ movptr(xlen, rsi);\n-    __ movptr(y,    rdx);\n-    __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n+  __ movptr(xlen, rsi);\n+  __ movptr(y,    rdx);\n+  __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n@@ -6769,1 +3013,1 @@\n-    restore_arg_regs();\n+  restore_arg_regs();\n@@ -6771,2 +3015,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6774,2 +3018,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -6777,19 +3021,19 @@\n-  \/**\n-  *  Arguments:\n-  *\n-  *  Input:\n-  *    c_rarg0   - obja     address\n-  *    c_rarg1   - objb     address\n-  *    c_rarg3   - length   length\n-  *    c_rarg4   - scale    log2_array_indxscale\n-  *\n-  *  Output:\n-  *        rax   - int >= mismatched index, < 0 bitwise complement of tail\n-  *\/\n-  address generate_vectorizedMismatch() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter();\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*    c_rarg0   - obja     address\n+*    c_rarg1   - objb     address\n+*    c_rarg3   - length   length\n+*    c_rarg4   - scale    log2_array_indxscale\n+*\n+*  Output:\n+*        rax   - int >= mismatched index, < 0 bitwise complement of tail\n+*\/\n+address StubGenerator::generate_vectorizedMismatch() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter();\n@@ -6798,8 +3042,8 @@\n-    const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n-    const Register objb = c_rarg1;   \/\/rdx\n-    const Register length = c_rarg2; \/\/r8\n-    const Register obja = c_rarg3;   \/\/r9\n-    __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n-\n-    const Register tmp1 = r10;\n-    const Register tmp2 = r11;\n+  const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n+  const Register objb = c_rarg1;   \/\/rdx\n+  const Register length = c_rarg2; \/\/r8\n+  const Register obja = c_rarg3;   \/\/r9\n+  __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n+\n+  const Register tmp1 = r10;\n+  const Register tmp2 = r11;\n@@ -6808,6 +3052,6 @@\n-    const Register obja = c_rarg0;   \/\/U:rdi\n-    const Register objb = c_rarg1;   \/\/U:rsi\n-    const Register length = c_rarg2; \/\/U:rdx\n-    const Register scale = c_rarg3;  \/\/U:rcx\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n+  const Register obja = c_rarg0;   \/\/U:rdi\n+  const Register objb = c_rarg1;   \/\/U:rsi\n+  const Register length = c_rarg2; \/\/U:rdx\n+  const Register scale = c_rarg3;  \/\/U:rcx\n+  const Register tmp1 = r8;\n+  const Register tmp2 = r9;\n@@ -6815,4 +3059,4 @@\n-    const Register result = rax; \/\/return value\n-    const XMMRegister vec0 = xmm0;\n-    const XMMRegister vec1 = xmm1;\n-    const XMMRegister vec2 = xmm2;\n+  const Register result = rax; \/\/return value\n+  const XMMRegister vec0 = xmm0;\n+  const XMMRegister vec1 = xmm1;\n+  const XMMRegister vec2 = xmm2;\n@@ -6820,1 +3064,1 @@\n-    __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n+  __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n@@ -6822,3 +3066,3 @@\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -6826,2 +3070,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -6830,36 +3074,10 @@\n-   *  Arguments:\n-   *\n-  \/\/  Input:\n-  \/\/    c_rarg0   - x address\n-  \/\/    c_rarg1   - x length\n-  \/\/    c_rarg2   - z address\n-  \/\/    c_rarg3   - z length\n-   *\n-   *\/\n-  address generate_squareToLen() {\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n-    const Register x      = rdi;\n-    const Register len    = rsi;\n-    const Register z      = r8;\n-    const Register zlen   = rcx;\n-\n-   const Register tmp1      = r12;\n-   const Register tmp2      = r13;\n-   const Register tmp3      = r14;\n-   const Register tmp4      = r15;\n-   const Register tmp5      = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n-                       \/\/ zlen => rcx\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-    __ movptr(r8, rdx);\n-    __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+ *  Arguments:\n+ *\n+\/\/  Input:\n+\/\/    c_rarg0   - x address\n+\/\/    c_rarg1   - x length\n+\/\/    c_rarg2   - z address\n+\/\/    c_rarg3   - z length\n+ *\n+ *\/\n+address StubGenerator::generate_squareToLen() {\n@@ -6867,1 +3085,3 @@\n-    restore_arg_regs();\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n+  address start = __ pc();\n@@ -6869,2 +3089,6 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n+  const Register x      = rdi;\n+  const Register len    = rsi;\n+  const Register z      = r8;\n+  const Register zlen   = rcx;\n@@ -6872,2 +3096,5 @@\n-    return start;\n-  }\n+ const Register tmp1      = r12;\n+ const Register tmp2      = r13;\n+ const Register tmp3      = r14;\n+ const Register tmp4      = r15;\n+ const Register tmp5      = rbx;\n@@ -6875,3 +3102,2 @@\n-  address generate_method_entry_barrier() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -6879,1 +3105,5 @@\n-    Label deoptimize_label;\n+  setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n+                     \/\/ zlen => rcx\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+  __ movptr(r8, rdx);\n+  __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -6881,1 +3111,1 @@\n-    address start = __ pc();\n+  restore_arg_regs();\n@@ -6883,1 +3113,2 @@\n-    __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6885,2 +3116,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ save rbp\n+  return start;\n+}\n@@ -6888,3 +3119,4 @@\n-    \/\/ save c_rarg0, because we want to use that value.\n-    \/\/ We could do without it but then we depend on the number of slots used by pusha\n-    __ push(c_rarg0);\n+address StubGenerator::generate_method_entry_barrier() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+  address start = __ pc();\n@@ -6892,1 +3124,1 @@\n-    __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n+  Label deoptimize_label;\n@@ -6894,1 +3126,1 @@\n-    __ pusha();\n+  __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n@@ -6896,14 +3128,2 @@\n-    \/\/ The method may have floats as arguments, and we must spill them before calling\n-    \/\/ the VM runtime.\n-    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-    const int xmm_size = wordSize * 2;\n-    const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-    __ subptr(rsp, xmm_spill_size);\n-    __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-    __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-    __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-    __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-    __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-    __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-    __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-    __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n@@ -6911,1 +3131,3 @@\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n+  \/\/ save c_rarg0, because we want to use that value.\n+  \/\/ We could do without it but then we depend on the number of slots used by pusha\n+  __ push(c_rarg0);\n@@ -6913,9 +3135,1 @@\n-    __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-    __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-    __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-    __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-    __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-    __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-    __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-    __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-    __ addptr(rsp, xmm_spill_size);\n+  __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n@@ -6923,2 +3137,1 @@\n-    __ cmpl(rax, 1); \/\/ 1 means deoptimize\n-    __ jcc(Assembler::equal, deoptimize_label);\n+  __ pusha();\n@@ -6926,2 +3139,14 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n@@ -6929,1 +3154,1 @@\n-    __ leave();\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n@@ -6931,2 +3156,9 @@\n-    __ addptr(rsp, 1 * wordSize); \/\/ cookie\n-    __ ret(0);\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n@@ -6934,0 +3166,2 @@\n+  __ cmpl(rax, 1); \/\/ 1 means deoptimize\n+  __ jcc(Assembler::equal, deoptimize_label);\n@@ -6935,1 +3169,2 @@\n-    __ BIND(deoptimize_label);\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -6937,2 +3172,1 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+  __ leave();\n@@ -6940,1 +3174,2 @@\n-    __ leave();\n+  __ addptr(rsp, 1 * wordSize); \/\/ cookie\n+  __ ret(0);\n@@ -6942,5 +3177,1 @@\n-    \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n-    \/\/ here while still having a correct stack is valuable\n-    __ testptr(rsp, Address(rsp, 0));\n-    __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n-    __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n+  __ BIND(deoptimize_label);\n@@ -6949,2 +3180,11 @@\n-    return start;\n-  }\n+  __ popa();\n+  __ pop(c_rarg0);\n+\n+  __ leave();\n+\n+  \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n+  \/\/ here while still having a correct stack is valuable\n+  __ testptr(rsp, Address(rsp, 0));\n+\n+  __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n+  __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n@@ -6952,39 +3192,42 @@\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - out address\n-   *    c_rarg1   - in address\n-   *    c_rarg2   - offset\n-   *    c_rarg3   - len\n-   * not Win64\n-   *    c_rarg4   - k\n-   * Win64\n-   *    rsp+40    - k\n-   *\/\n-  address generate_mulAdd() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register out     = rdi;\n-    const Register in      = rsi;\n-    const Register offset  = r11;\n-    const Register len     = rcx;\n-    const Register k       = r8;\n-\n-    \/\/ Next registers will be saved on stack in mul_add().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n-                       \/\/ len => rcx, k => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n+  return start;\n+}\n+\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - out address\n+ *    c_rarg1   - in address\n+ *    c_rarg2   - offset\n+ *    c_rarg3   - len\n+ * not Win64\n+ *    c_rarg4   - k\n+ * Win64\n+ *    rsp+40    - k\n+ *\/\n+address StubGenerator::generate_mulAdd() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register out     = rdi;\n+  const Register in      = rsi;\n+  const Register offset  = r11;\n+  const Register len     = rcx;\n+  const Register k       = r8;\n+\n+  \/\/ Next registers will be saved on stack in mul_add().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n+                     \/\/ len => rcx, k => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -6992,2 +3235,2 @@\n-    \/\/ last argument is on stack on Win64\n-    __ movl(k, Address(rsp, 6 * wordSize));\n+  \/\/ last argument is on stack on Win64\n+  __ movl(k, Address(rsp, 6 * wordSize));\n@@ -6995,2 +3238,2 @@\n-    __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n-    __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+  __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n+  __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -6998,1 +3241,1 @@\n-    restore_arg_regs();\n+  restore_arg_regs();\n@@ -7000,2 +3243,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7003,2 +3246,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -7006,36 +3249,36 @@\n-  address generate_bigIntegerRightShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n-\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-    const Register tmp6 = r15;\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n+address StubGenerator::generate_bigIntegerRightShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+  const Register tmp6 = r15;\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n@@ -7043,21 +3286,25 @@\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code.\n-    const Register idx = tmp1;\n-    const Register nIdx = tmp2;\n-\n-    __ xorl(idx, idx);\n-\n-    \/\/ Start right shift from end of the array.\n-    \/\/ For example, if #iteration = 4 and newIdx = 1\n-    \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    \/\/ if #iteration = 4 and newIdx = 0\n-    \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    __ movl(idx, totalNumIter);\n-    __ movl(nIdx, idx);\n-    __ addl(nIdx, newIdx);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShifTwo processing 2 iterations\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code.\n+  const Register idx = tmp1;\n+  const Register nIdx = tmp2;\n+\n+  __ xorl(idx, idx);\n+\n+  \/\/ Start right shift from end of the array.\n+  \/\/ For example, if #iteration = 4 and newIdx = 1\n+  \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  \/\/ if #iteration = 4 and newIdx = 0\n+  \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  __ movl(idx, totalNumIter);\n+  __ movl(nIdx, idx);\n+  __ addl(nIdx, newIdx);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShifTwo processing 2 iterations\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n@@ -7065,59 +3312,13 @@\n-\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(idx, 16);\n-      __ subl(nIdx, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n-      __ subl(nIdx, 16);\n-      __ subl(idx, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(idx, 16);\n-      __ addl(nIdx, 16);\n-    __ BIND(ShiftTwo);\n-    __ cmpl(idx, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-    __ subl(idx, 2);\n-    __ subl(nIdx, 2);\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp5, tmp4);\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ subl(nIdx, 2);\n-    __ subl(idx, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-    __ addl(idx, 2);\n-    __ addl(nIdx, 2);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ cmpl(idx, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ subl(idx, 1);\n-    __ subl(nIdx, 1);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(idx, 16);\n+    __ subl(nIdx, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n+    __ subl(nIdx, 16);\n+    __ subl(idx, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(idx, 16);\n+    __ addl(nIdx, 16);\n@@ -7126,0 +3327,40 @@\n+  __ BIND(ShiftTwo);\n+  __ cmpl(idx, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+  __ subl(idx, 2);\n+  __ subl(nIdx, 2);\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp5, tmp4);\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ subl(nIdx, 2);\n+  __ subl(idx, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+  __ addl(idx, 2);\n+  __ addl(nIdx, 2);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ cmpl(idx, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ subl(idx, 1);\n+  __ subl(nIdx, 1);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n+#ifdef _WIN64\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7127,45 +3368,49 @@\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - newArr address\n-   *    c_rarg1   - oldArr address\n-   *    c_rarg2   - newIdx\n-   *    c_rarg3   - shiftCount\n-   * not Win64\n-   *    c_rarg4   - numIter\n-   * Win64\n-   *    rsp40    - numIter\n-   *\/\n-  address generate_bigIntegerLeftShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n+  return start;\n+}\n+\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - newArr address\n+ *    c_rarg1   - oldArr address\n+ *    c_rarg2   - newIdx\n+ *    c_rarg3   - shiftCount\n+ * not Win64\n+ *    c_rarg4   - numIter\n+ * Win64\n+ *    rsp40    - numIter\n+ *\/\n+address StubGenerator::generate_bigIntegerLeftShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n@@ -7173,16 +3418,20 @@\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code\n-    const Register idx = tmp1;\n-    const Register numIterTmp = tmp2;\n-\n-    \/\/ Start idx from zero.\n-    __ xorl(idx, idx);\n-    \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n-    __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n-    __ movl(numIterTmp, totalNumIter);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpl(totalNumIter, (AVX3Threshold\/64));\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code\n+  const Register idx = tmp1;\n+  const Register numIterTmp = tmp2;\n+\n+  \/\/ Start idx from zero.\n+  __ xorl(idx, idx);\n+  \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n+  __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n+  __ movl(numIterTmp, totalNumIter);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpl(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n@@ -7190,56 +3439,11 @@\n-\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(numIterTmp, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n-      __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n-      __ addl(idx, 16);\n-      __ subl(numIterTmp, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(numIterTmp, 16);\n-    __ BIND(ShiftTwo);\n-    __ cmpl(totalNumIter, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n-    __ shldl(tmp3, tmp4);\n-    __ shldl(tmp4, tmp5);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-    __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n-    __ movl(tmp3, tmp5);\n-    __ addl(idx, 2);\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ addl(numIterTmp, 2);\n-    __ cmpl(numIterTmp, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ shldl(tmp3, tmp4);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(numIterTmp, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n+    __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n+    __ addl(idx, 16);\n+    __ subl(numIterTmp, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(numIterTmp, 16);\n@@ -7248,0 +3452,39 @@\n+  __ BIND(ShiftTwo);\n+  __ cmpl(totalNumIter, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n+  __ shldl(tmp3, tmp4);\n+  __ shldl(tmp4, tmp5);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+  __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n+  __ movl(tmp3, tmp5);\n+  __ addl(idx, 2);\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ addl(numIterTmp, 2);\n+  __ cmpl(numIterTmp, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ shldl(tmp3, tmp4);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n+#ifdef _WIN64\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7249,4 +3492,2 @@\n-  address generate_libmExp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmExp\");\n-\n-    address start = __ pc();\n+  return start;\n+}\n@@ -7254,4 +3495,25 @@\n-    const XMMRegister x0  = xmm0;\n-    const XMMRegister x1  = xmm1;\n-    const XMMRegister x2  = xmm2;\n-    const XMMRegister x3  = xmm3;\n+void StubGenerator::generate_libm_stubs() {\n+  if (UseLibmIntrinsic && InlineIntrinsics) {\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n+      StubRoutines::_dsin = generate_libmSin(); \/\/ from stubGenerator_x86_64_sin.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n+      StubRoutines::_dcos = generate_libmCos(); \/\/ from stubGenerator_x86_64_cos.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n+      StubRoutines::_dtan = generate_libmTan(); \/\/ from stubGenerator_x86_64_tan.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n+      StubRoutines::_dexp = generate_libmExp(); \/\/ from stubGenerator_x86_64_exp.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n+      StubRoutines::_dpow = generate_libmPow(); \/\/ from stubGenerator_x86_64_pow.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n+      StubRoutines::_dlog = generate_libmLog(); \/\/ from stubGenerator_x86_64_log.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n+      StubRoutines::_dlog10 = generate_libmLog10(); \/\/ from stubGenerator_x86_64_log.cpp\n+    }\n+  }\n+}\n@@ -7259,4 +3521,11 @@\n-    const XMMRegister x4  = xmm4;\n-    const XMMRegister x5  = xmm5;\n-    const XMMRegister x6  = xmm6;\n-    const XMMRegister x7  = xmm7;\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*    c_rarg0   - float16  jshort\n+*\n+*  Output:\n+*       xmm0   - float\n+*\/\n+address StubGenerator::generate_float16ToFloat() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"float16ToFloat\");\n@@ -7264,1 +3533,1 @@\n-    const Register tmp   = r11;\n+  address start = __ pc();\n@@ -7266,2 +3535,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  BLOCK_COMMENT(\"Entry:\");\n+  \/\/ No need for RuntimeStub frame since it is called only during JIT compilation\n@@ -7269,1 +3538,2 @@\n-    __ fast_exp(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+  \/\/ Load value into xmm0 and convert\n+  __ flt16_to_flt(xmm0, c_rarg0);\n@@ -7271,2 +3541,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ ret(0);\n@@ -7274,1 +3543,2 @@\n-    return start;\n+  return start;\n+}\n@@ -7276,1 +3546,11 @@\n-  }\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*       xmm0   - float\n+*\n+*  Output:\n+*        rax   - float16  jshort\n+*\/\n+address StubGenerator::generate_floatToFloat16() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"floatToFloat16\");\n@@ -7278,2 +3558,1 @@\n-  address generate_libmLog() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog\");\n+  address start = __ pc();\n@@ -7281,1 +3560,2 @@\n-    address start = __ pc();\n+  BLOCK_COMMENT(\"Entry:\");\n+  \/\/ No need for RuntimeStub frame since it is called only during JIT compilation\n@@ -7283,4 +3563,2 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  \/\/ Convert and put result into rax\n+  __ flt_to_flt16(rax, xmm0, xmm1);\n@@ -7288,4 +3566,1 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  __ ret(0);\n@@ -7293,2 +3568,2 @@\n-    const Register tmp1 = r11;\n-    const Register tmp2 = r8;\n+  return start;\n+}\n@@ -7296,2 +3571,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+address StubGenerator::generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n+  if (!Continuations::enabled()) return nullptr;\n@@ -7299,1 +3574,2 @@\n-    __ fast_log(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2);\n+  bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+  bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n@@ -7301,2 +3577,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  StubCodeMark mark(this, \"StubRoutines\", label);\n+  address start = __ pc();\n@@ -7304,1 +3580,1 @@\n-    return start;\n+  \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n@@ -7306,0 +3582,6 @@\n+  if (!return_barrier) {\n+    \/\/ Pop return address. If we don't do this, we get a drift,\n+    \/\/ where the bottom-most frozen frame continuously grows.\n+    __ pop(c_rarg3);\n+  } else {\n+    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n@@ -7308,9 +3590,9 @@\n-  address generate_libmLog10() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog10\");\n-\n-    address start = __ pc();\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp at thaw entry\");\n+    __ BIND(L_good_sp);\n+  }\n+#endif \/\/ ASSERT\n@@ -7318,4 +3600,5 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier.\n+    __ push(rax);\n+    __ push_d(xmm0);\n+  }\n@@ -7323,1 +3606,4 @@\n-    const Register tmp = r11;\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n+  __ movptr(rbx, rax);\n@@ -7325,2 +3611,6 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier.\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  }\n@@ -7328,1 +3618,9 @@\n-    __ fast_log10(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp after prepare thaw\");\n+    __ BIND(L_good_sp);\n+  }\n+#endif \/\/ ASSERT\n@@ -7330,2 +3628,6 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n+  Label L_thaw_success;\n+  __ testptr(rbx, rbx);\n+  __ jccb(Assembler::notZero, L_thaw_success);\n+  __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+  __ bind(L_thaw_success);\n@@ -7333,1 +3635,3 @@\n-    return start;\n+  \/\/ Make room for the thawed frames and align the stack.\n+  __ subptr(rsp, rbx);\n+  __ andptr(rsp, -StackAlignmentInBytes);\n@@ -7335,0 +3639,4 @@\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n+    __ push(rax);\n+    __ push_d(xmm0);\n@@ -7337,4 +3645,5 @@\n-  address generate_libmPow() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmPow\");\n-\n-    address start = __ pc();\n+  \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, kind);\n+  __ call_VM_leaf(Continuation::thaw_entry(), 2);\n+  __ movptr(rbx, rax);\n@@ -7342,4 +3651,9 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier. (Again.)\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  } else {\n+    \/\/ Return 0 (success) from doYield.\n+    __ xorptr(rax, rax);\n+  }\n@@ -7347,4 +3661,4 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  \/\/ After thawing, rbx is the SP of the yielding frame.\n+  \/\/ Move there, and then to saved RBP slot.\n+  __ movptr(rsp, rbx);\n+  __ subptr(rsp, 2*wordSize);\n@@ -7352,4 +3666,3 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  if (return_barrier_exception) {\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n@@ -7357,2 +3670,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    \/\/ rax still holds the original exception oop, save it before the call\n+    __ push(rax);\n@@ -7360,1 +3673,2 @@\n-    __ fast_pow(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+    __ movptr(rbx, rax);\n@@ -7362,1 +3676,12 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    \/\/ Continue at exception handler:\n+    \/\/   rax: exception oop\n+    \/\/   rbx: exception handler\n+    \/\/   rdx: exception pc\n+    __ pop(rax);\n+    __ verify_oop(rax);\n+    __ pop(rbp); \/\/ pop out RBP here too\n+    __ pop(rdx);\n+    __ jmp(rbx);\n+  } else {\n+    \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+    __ pop(rbp);\n@@ -7364,0 +3689,1 @@\n+  }\n@@ -7365,1 +3691,2 @@\n-    return start;\n+  return start;\n+}\n@@ -7367,1 +3694,5 @@\n-  }\n+address StubGenerator::generate_cont_thaw() {\n+  return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n+}\n+\n+\/\/ TODO: will probably need multiple return barriers depending on return type\n@@ -7369,2 +3700,3 @@\n-  address generate_libmSin() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmSin\");\n+address StubGenerator::generate_cont_returnBarrier() {\n+  return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n+}\n@@ -7372,1 +3704,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_cont_returnBarrier_exception() {\n+  return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n+}\n@@ -7374,4 +3708,13 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+#if INCLUDE_JFR\n+\n+\/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+\/\/ It returns a jobject handle to the event writer.\n+\/\/ The handle is dereferenced and the return value is the event writer oop.\n+RuntimeStub* StubGenerator::generate_jfr_write_checkpoint() {\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7379,4 +3722,3 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  CodeBuffer code(\"jfr_write_checkpoint\", 1024, 64);\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n+  address start = __ pc();\n@@ -7384,4 +3726,2 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  __ enter();\n+  address the_pc = __ pc();\n@@ -7389,2 +3729,1 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  int frame_complete = the_pc - start;\n@@ -7392,5 +3731,4 @@\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_sin(x0, x1, x2, x3, x4, x5, x6, x7, rax, rbx, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+  __ reset_last_Java_frame(true);\n@@ -7398,4 +3736,2 @@\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n-#endif\n+  \/\/ rax is jobject handle result, unpack and process it through a barrier.\n+  __ resolve_global_jobject(rax, r15_thread, c_rarg0);\n@@ -7403,2 +3739,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ leave();\n+  __ ret(0);\n@@ -7406,1 +3742,3 @@\n-    return start;\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n@@ -7408,1 +3746,9 @@\n-  }\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(code.name(),\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps,\n+                                  false);\n+  return stub;\n+}\n@@ -7410,2 +3756,32 @@\n-  address generate_libmCos() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmCos\");\n+#endif \/\/ INCLUDE_JFR\n+\n+\/\/ Continuation point for throwing of implicit exceptions that are\n+\/\/ not handled in the current activation. Fabricates an exception\n+\/\/ oop and initiates normal exception dispatching in this\n+\/\/ frame. Since we need to preserve callee-saved values (currently\n+\/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+\/\/ map and therefore have to make these stubs into RuntimeStubs\n+\/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+\/\/ be preserved between the fault point and the exception handler\n+\/\/ then it must assume responsibility for that in\n+\/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+\/\/ continuation_for_implicit_division_by_zero_exception. All other\n+\/\/ implicit exceptions (e.g., NullPointerException or\n+\/\/ AbstractMethodError on entry) are either at call sites or\n+\/\/ otherwise assume that stack unwinding will be initiated, so\n+\/\/ caller saved registers were assumed volatile in the compiler.\n+address StubGenerator::generate_throw_exception(const char* name,\n+                                                address runtime_entry,\n+                                                Register arg1,\n+                                                Register arg2) {\n+  \/\/ Information about frame layout at time of blocking runtime call.\n+  \/\/ Note that we only have to preserve callee-saved registers since\n+  \/\/ the compilers are responsible for supplying a continuation point\n+  \/\/ if they expect all registers to be preserved.\n+  enum layout {\n+    rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7413,1 +3789,2 @@\n-    address start = __ pc();\n+  int insts_size = 512;\n+  int locs_size  = 64;\n@@ -7415,4 +3792,3 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  CodeBuffer code(name, insts_size, locs_size);\n+  OopMapSet* oop_maps  = new OopMapSet();\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n@@ -7420,4 +3796,1 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  address start = __ pc();\n@@ -7425,4 +3798,4 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  \/\/ This is an inlined and slightly modified version of call_VM\n+  \/\/ which has the ability to fetch the return PC out of\n+  \/\/ thread-local storage and also sets up last_Java_sp slightly\n+  \/\/ differently than the real call_VM\n@@ -7430,2 +3803,1 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7433,5 +3805,1 @@\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_cos(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+  assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n@@ -7439,4 +3807,2 @@\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n-#endif\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n@@ -7444,2 +3810,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  int frame_complete = __ pc() - start;\n@@ -7447,1 +3812,4 @@\n-    return start;\n+  \/\/ Set up last_Java_sp and last_Java_fp\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n@@ -7449,0 +3817,4 @@\n+  \/\/ Call runtime\n+  if (arg1 != noreg) {\n+    assert(arg2 != c_rarg1, \"clobbered\");\n+    __ movptr(c_rarg1, arg1);\n@@ -7450,0 +3822,6 @@\n+  if (arg2 != noreg) {\n+    __ movptr(c_rarg2, arg2);\n+  }\n+  __ movptr(c_rarg0, r15_thread);\n+  BLOCK_COMMENT(\"call runtime_entry\");\n+  __ call(RuntimeAddress(runtime_entry));\n@@ -7451,2 +3829,2 @@\n-  address generate_libmTan() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmTan\");\n+  \/\/ Generate oop map\n+  OopMap* map = new OopMap(framesize, 0);\n@@ -7454,1 +3832,1 @@\n-    address start = __ pc();\n+  oop_maps->add_gc_map(the_pc - start, map);\n@@ -7456,4 +3834,1 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  __ reset_last_Java_frame(true);\n@@ -7461,4 +3836,1 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7466,4 +3838,9 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  \/\/ check for pending exceptions\n+#ifdef ASSERT\n+  Label L;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+  __ jcc(Assembler::notEqual, L);\n+  __ should_not_reach_here();\n+  __ bind(L);\n+#endif \/\/ ASSERT\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n@@ -7471,7 +3848,9 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_tan(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+  \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(name,\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps, false);\n+  return stub->entry_point();\n+}\n@@ -7480,4 +3859,4 @@\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n-#endif\n+void StubGenerator::create_control_words() {\n+  \/\/ Round to nearest, 64-bit mode, exceptions masked\n+  StubRoutines::x86::_mxcsr_std = 0x1F80;\n+}\n@@ -7485,2 +3864,3 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+\/\/ Initialization\n+void StubGenerator::generate_initial_stubs() {\n+  \/\/ Generates all stubs and initializes the entry points\n@@ -7488,1 +3868,2 @@\n-    return start;\n+  \/\/ This platform-specific settings are needed by generate_call_stub()\n+  create_control_words();\n@@ -7490,0 +3871,3 @@\n+  \/\/ Initialize table for unsafe copy memeory check.\n+  if (UnsafeCopyMemory::_table == nullptr) {\n+    UnsafeCopyMemory::create_table(16);\n@@ -7492,36 +3876,48 @@\n-#undef __\n-#define __ masm->\n-\n-  \/\/ Continuation point for throwing of implicit exceptions that are\n-  \/\/ not handled in the current activation. Fabricates an exception\n-  \/\/ oop and initiates normal exception dispatching in this\n-  \/\/ frame. Since we need to preserve callee-saved values (currently\n-  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n-  \/\/ map and therefore have to make these stubs into RuntimeStubs\n-  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n-  \/\/ be preserved between the fault point and the exception handler\n-  \/\/ then it must assume responsibility for that in\n-  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n-  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n-  \/\/ implicit exceptions (e.g., NullPointerException or\n-  \/\/ AbstractMethodError on entry) are either at call sites or\n-  \/\/ otherwise assume that stack unwinding will be initiated, so\n-  \/\/ caller saved registers were assumed volatile in the compiler.\n-  address generate_throw_exception(const char* name,\n-                                   address runtime_entry,\n-                                   Register arg1 = noreg,\n-                                   Register arg2 = noreg) {\n-    \/\/ Information about frame layout at time of blocking runtime call.\n-    \/\/ Note that we only have to preserve callee-saved registers since\n-    \/\/ the compilers are responsible for supplying a continuation point\n-    \/\/ if they expect all registers to be preserved.\n-    enum layout {\n-      rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n-      rbp_off2,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-\n-    int insts_size = 512;\n-    int locs_size  = 64;\n+  \/\/ entry points that exist in all platforms Note: This is code\n+  \/\/ that could be shared among different platforms - however the\n+  \/\/ benefit seems to be smaller than the disadvantage of having a\n+  \/\/ much more complicated generator structure. See also comment in\n+  \/\/ stubRoutines.hpp.\n+\n+  StubRoutines::_forward_exception_entry = generate_forward_exception();\n+\n+  StubRoutines::_call_stub_entry =\n+    generate_call_stub(StubRoutines::_call_stub_return_address);\n+\n+  \/\/ is referenced by megamorphic call\n+  StubRoutines::_catch_exception_entry = generate_catch_exception();\n+\n+  \/\/ atomic calls\n+  StubRoutines::_fence_entry                = generate_orderaccess_fence();\n+\n+  \/\/ platform dependent\n+  StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n+\n+  StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n+\n+  StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n+  StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n+  StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n+  StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n+\n+  StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n+  StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n+\n+  \/\/ Build this early so it's available for the interpreter.\n+  StubRoutines::_throw_StackOverflowError_entry =\n+    generate_throw_exception(\"StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_StackOverflowError));\n+  StubRoutines::_throw_delayed_StackOverflowError_entry =\n+    generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_delayed_StackOverflowError));\n+  if (UseCRC32Intrinsics) {\n+    \/\/ set table address before stub generation which use it\n+    StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n+    StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n+  }\n@@ -7529,3 +3925,6 @@\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n+  if (UseCRC32CIntrinsics) {\n+    bool supports_clmul = VM_Version::supports_clmul();\n+    StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n+    StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n+    StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n+  }\n@@ -7533,1 +3932,9 @@\n-    address start = __ pc();\n+  if (VM_Version::supports_float16()) {\n+    \/\/ For results consistency both intrinsics should be enabled.\n+    \/\/ vmIntrinsics checks InlineIntrinsics flag, no need to check it here.\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_float16ToFloat) &&\n+        vmIntrinsics::is_intrinsic_available(vmIntrinsics::_floatToFloat16)) {\n+      StubRoutines::_hf2f = generate_float16ToFloat();\n+      StubRoutines::_f2hf = generate_floatToFloat16();\n+    }\n+  }\n@@ -7535,4 +3942,2 @@\n-    \/\/ This is an inlined and slightly modified version of call_VM\n-    \/\/ which has the ability to fetch the return PC out of\n-    \/\/ thread-local storage and also sets up last_Java_sp slightly\n-    \/\/ differently than the real call_VM\n+  generate_libm_stubs();\n+}\n@@ -7540,1 +3945,5 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+void StubGenerator::generate_continuation_stubs() {\n+  \/\/ Continuation stubs:\n+  StubRoutines::_cont_thaw          = generate_cont_thaw();\n+  StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+  StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n@@ -7542,1 +3951,3 @@\n-    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+}\n@@ -7544,2 +3955,28 @@\n-    \/\/ return address and rbp are already in place\n-    __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n+void StubGenerator::generate_final_stubs() {\n+  \/\/ Generates the rest of stubs and initializes the entry points\n+\n+  \/\/ These entry points require SharedInfo::stack0 to be set up in\n+  \/\/ non-core builds and need to be relocatable, so they each\n+  \/\/ fabricate a RuntimeStub internally.\n+  StubRoutines::_throw_AbstractMethodError_entry =\n+    generate_throw_exception(\"AbstractMethodError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_AbstractMethodError));\n+\n+  StubRoutines::_throw_IncompatibleClassChangeError_entry =\n+    generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_IncompatibleClassChangeError));\n+\n+  StubRoutines::_throw_NullPointerException_at_call_entry =\n+    generate_throw_exception(\"NullPointerException at call throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_NullPointerException_at_call));\n+\n+  \/\/ support for verify_oop (must happen after universe_init)\n+  if (VerifyOops) {\n+    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  }\n@@ -7547,1 +3984,3 @@\n-    int frame_complete = __ pc() - start;\n+  \/\/ data cache line writeback\n+  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n+  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n@@ -7549,4 +3988,2 @@\n-    \/\/ Set up last_Java_sp and last_Java_fp\n-    address the_pc = __ pc();\n-    __ set_last_Java_frame(rsp, rbp, the_pc);\n-    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n+  \/\/ arraycopy stubs used by compilers\n+  generate_arraycopy_stubs();\n@@ -7554,11 +3991,4 @@\n-    \/\/ Call runtime\n-    if (arg1 != noreg) {\n-      assert(arg2 != c_rarg1, \"clobbered\");\n-      __ movptr(c_rarg1, arg1);\n-    }\n-    if (arg2 != noreg) {\n-      __ movptr(c_rarg2, arg2);\n-    }\n-    __ movptr(c_rarg0, r15_thread);\n-    BLOCK_COMMENT(\"call runtime_entry\");\n-    __ call(RuntimeAddress(runtime_entry));\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n+    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+  }\n@@ -7566,2 +3996,4 @@\n-    \/\/ Generate oop map\n-    OopMap* map = new OopMap(framesize, 0);\n+  if (UseVectorizedMismatchIntrinsic) {\n+    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+  }\n+}\n@@ -7569,1 +4001,2 @@\n-    oop_maps->add_gc_map(the_pc - start, map);\n+void StubGenerator::generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n@@ -7571,1 +4004,35 @@\n-    __ reset_last_Java_frame(true);\n+  \/\/ Entry points that are C2 compiler specific.\n+\n+  StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n+  StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n+  StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n+  StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n+  StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n+  StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n+  StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0, 0, 0);\n+  StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n+  StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n+  StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n+  StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n+  StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n+  StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n+  StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+  StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n+\n+  StubRoutines::x86::_vector_halffloat_sign_mask = generate_vector_fp_mask(\"vector_halffloat_sign_mask\", 0x7FFF7FFF7FFF7FFF);\n+  StubRoutines::x86::_vector_halffloat_sign_flip = generate_vector_fp_mask(\"vector_halffloat_sign_flip\", 0x8000800080008000);\n+\n+  if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n+    StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n+  }\n@@ -7573,1 +4040,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  generate_aes_stubs();\n@@ -7575,10 +4042,1 @@\n-    \/\/ check for pending exceptions\n-#ifdef ASSERT\n-    Label L;\n-    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()),\n-            (int32_t) NULL_WORD);\n-    __ jcc(Assembler::notEqual, L);\n-    __ should_not_reach_here();\n-    __ bind(L);\n-#endif \/\/ ASSERT\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+  generate_ghash_stubs();\n@@ -7586,0 +4044,1 @@\n+  generate_chacha_stubs();\n@@ -7587,8 +4046,2 @@\n-    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(name,\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps, false);\n-    return stub->entry_point();\n+  if (UseAdler32Intrinsics) {\n+     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n@@ -7597,3 +4050,2 @@\n-  void create_control_words() {\n-    \/\/ Round to nearest, 64-bit mode, exceptions masked\n-    StubRoutines::x86::_mxcsr_std = 0x1F80;\n+  if (UsePoly1305Intrinsics) {\n+    StubRoutines::_poly1305_processBlocks = generate_poly1305_processBlocks();\n@@ -7602,108 +4054,3 @@\n-  \/\/ Initialization\n-  void generate_initial() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ This platform-specific settings are needed by generate_call_stub()\n-    create_control_words();\n-\n-    \/\/ entry points that exist in all platforms Note: This is code\n-    \/\/ that could be shared among different platforms - however the\n-    \/\/ benefit seems to be smaller than the disadvantage of having a\n-    \/\/ much more complicated generator structure. See also comment in\n-    \/\/ stubRoutines.hpp.\n-\n-    StubRoutines::_forward_exception_entry = generate_forward_exception();\n-\n-    StubRoutines::_call_stub_entry =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n-\n-    \/\/ is referenced by megamorphic call\n-    StubRoutines::_catch_exception_entry = generate_catch_exception();\n-\n-    \/\/ atomic calls\n-    StubRoutines::_fence_entry                = generate_orderaccess_fence();\n-\n-    \/\/ platform dependent\n-    StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n-\n-    StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n-\n-    StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n-    StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n-    StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n-    StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n-\n-    StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n-    StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n-\n-    \/\/ Build this early so it's available for the interpreter.\n-    StubRoutines::_throw_StackOverflowError_entry =\n-      generate_throw_exception(\"StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_StackOverflowError));\n-    StubRoutines::_throw_delayed_StackOverflowError_entry =\n-      generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_delayed_StackOverflowError));\n-    if (UseCRC32Intrinsics) {\n-      \/\/ set table address before stub generation which use it\n-      StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n-      StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n-    }\n-\n-    if (UseCRC32CIntrinsics) {\n-      bool supports_clmul = VM_Version::supports_clmul();\n-      StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n-      StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n-      StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n-    }\n-\n-    if (UseAdler32Intrinsics) {\n-       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n-    }\n-\n-    if (UseLibmIntrinsic && InlineIntrinsics) {\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||\n-          vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos) ||\n-          vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::x86::_ONEHALF_adr = (address)StubRoutines::x86::_ONEHALF;\n-        StubRoutines::x86::_P_2_adr = (address)StubRoutines::x86::_P_2;\n-        StubRoutines::x86::_SC_4_adr = (address)StubRoutines::x86::_SC_4;\n-        StubRoutines::x86::_Ctable_adr = (address)StubRoutines::x86::_Ctable;\n-        StubRoutines::x86::_SC_2_adr = (address)StubRoutines::x86::_SC_2;\n-        StubRoutines::x86::_SC_3_adr = (address)StubRoutines::x86::_SC_3;\n-        StubRoutines::x86::_SC_1_adr = (address)StubRoutines::x86::_SC_1;\n-        StubRoutines::x86::_PI_INV_TABLE_adr = (address)StubRoutines::x86::_PI_INV_TABLE;\n-        StubRoutines::x86::_PI_4_adr = (address)StubRoutines::x86::_PI_4;\n-        StubRoutines::x86::_PI32INV_adr = (address)StubRoutines::x86::_PI32INV;\n-        StubRoutines::x86::_SIGN_MASK_adr = (address)StubRoutines::x86::_SIGN_MASK;\n-        StubRoutines::x86::_P_1_adr = (address)StubRoutines::x86::_P_1;\n-        StubRoutines::x86::_P_3_adr = (address)StubRoutines::x86::_P_3;\n-        StubRoutines::x86::_NEG_ZERO_adr = (address)StubRoutines::x86::_NEG_ZERO;\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n-        StubRoutines::_dexp = generate_libmExp();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n-        StubRoutines::_dlog = generate_libmLog();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n-        StubRoutines::_dlog10 = generate_libmLog10();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n-        StubRoutines::_dpow = generate_libmPow();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n-        StubRoutines::_dsin = generate_libmSin();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n-        StubRoutines::_dcos = generate_libmCos();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::_dtan = generate_libmTan();\n-      }\n-    }\n+  if (UseMD5Intrinsics) {\n+    StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n+    StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n@@ -7712,89 +4059,6 @@\n-  void generate_all() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ These entry points require SharedInfo::stack0 to be set up in\n-    \/\/ non-core builds and need to be relocatable, so they each\n-    \/\/ fabricate a RuntimeStub internally.\n-    StubRoutines::_throw_AbstractMethodError_entry =\n-      generate_throw_exception(\"AbstractMethodError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_AbstractMethodError));\n-\n-    StubRoutines::_throw_IncompatibleClassChangeError_entry =\n-      generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_IncompatibleClassChangeError));\n-\n-    StubRoutines::_throw_NullPointerException_at_call_entry =\n-      generate_throw_exception(\"NullPointerException at call throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_NullPointerException_at_call));\n-\n-    \/\/ entry points that are platform specific\n-    StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n-    StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n-    StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n-    StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n-    StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n-    StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n-    StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0, 0, 0);\n-    StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n-    StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n-    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n-    StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n-    StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n-    StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n-    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n-\n-    StubRoutines::x86::_vector_halffloat_sign_mask = generate_vector_fp_mask(\"vector_halffloat_sign_mask\", 0x7FFF7FFF7FFF7FFF);\n-    StubRoutines::x86::_vector_halffloat_sign_flip = generate_vector_fp_mask(\"vector_halffloat_sign_flip\", 0x8000800080008000);\n-\n-    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n-      \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n-      StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n-    }\n-\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    if (VerifyOops) {\n-      StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n-    }\n-\n-    \/\/ data cache line writeback\n-    StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-    StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n-    \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n-    if (UseAESIntrinsics) {\n-      StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n-      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-      StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n-      if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n-        StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n-        StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n-        StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-        StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n-      } else {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n-      }\n-    }\n+  if (UseSHA1Intrinsics) {\n+    StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n+    StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n+    StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n+    StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n+  }\n@@ -7802,10 +4066,7 @@\n-    if (UseAESCTRIntrinsics) {\n-      if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n-        if (StubRoutines::x86::_counter_mask_addr == NULL) {\n-          StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        }\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n-      } else {\n-        StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n-      }\n+  if (UseSHA256Intrinsics) {\n+    StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n+    char* dst = (char*)StubRoutines::x86::_k256_W;\n+    char* src = (char*)StubRoutines::x86::_k256;\n+    for (int ii = 0; ii < 16; ++ii) {\n+      memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n+      memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n@@ -7813,0 +4074,5 @@\n+    StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n+    StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n+    StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n+  }\n@@ -7814,29 +4080,6 @@\n-    if (UseMD5Intrinsics) {\n-      StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n-      StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n-    }\n-    if (UseSHA1Intrinsics) {\n-      StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n-      StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n-      StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n-      StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n-    }\n-    if (UseSHA256Intrinsics) {\n-      StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n-      char* dst = (char*)StubRoutines::x86::_k256_W;\n-      char* src = (char*)StubRoutines::x86::_k256;\n-      for (int ii = 0; ii < 16; ++ii) {\n-        memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n-        memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n-      }\n-      StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n-      StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n-    }\n-    if (UseSHA512Intrinsics) {\n-      StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n-      StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n-      StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n-    }\n+  if (UseSHA512Intrinsics) {\n+    StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n+    StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n+    StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n+  }\n@@ -7844,13 +4087,7 @@\n-    \/\/ Generate GHASH intrinsics code\n-    if (UseGHASHIntrinsics) {\n-      if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-      }\n-    StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n-      if (VM_Version::supports_avx()) {\n-        StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n-        StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n-        StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n-      } else {\n-        StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n-      }\n+  if (UseBASE64Intrinsics) {\n+    if(VM_Version::supports_avx2()) {\n+      StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+      StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+      StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n+      StubRoutines::x86::_avx2_decode_tables_base64 = base64_AVX2_decode_tables_addr();\n+      StubRoutines::x86::_avx2_decode_lut_tables_base64 = base64_AVX2_decode_LUT_tables_addr();\n@@ -7858,25 +4095,11 @@\n-\n-\n-    if (UseBASE64Intrinsics) {\n-      if(VM_Version::supports_avx2() &&\n-         VM_Version::supports_avx512bw() &&\n-         VM_Version::supports_avx512vl()) {\n-        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n-        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n-        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n-      }\n-      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n-      if (VM_Version::supports_avx512_vbmi()) {\n-        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n-        StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n-        StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n-        StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n-        StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n-        StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n-        StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n-        StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n-        StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n-      }\n-      StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n-      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n-      StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+    StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n+      StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n+      StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n+      StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n+      StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n+      StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n+      StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n+      StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n+      StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n@@ -7884,0 +4107,4 @@\n+    StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n+    StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n+    StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+  }\n@@ -7885,25 +4112,21 @@\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n-    if (UseMultiplyToLenIntrinsic) {\n-      StubRoutines::_multiplyToLen = generate_multiplyToLen();\n-    }\n-    if (UseSquareToLenIntrinsic) {\n-      StubRoutines::_squareToLen = generate_squareToLen();\n-    }\n-    if (UseMulAddIntrinsic) {\n-      StubRoutines::_mulAdd = generate_mulAdd();\n-    }\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n-      StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n-    }\n-    if (UseMontgomeryMultiplyIntrinsic) {\n-      StubRoutines::_montgomeryMultiply\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n-    }\n-    if (UseMontgomerySquareIntrinsic) {\n-      StubRoutines::_montgomerySquare\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n-    }\n+  if (UseMultiplyToLenIntrinsic) {\n+    StubRoutines::_multiplyToLen = generate_multiplyToLen();\n+  }\n+  if (UseSquareToLenIntrinsic) {\n+    StubRoutines::_squareToLen = generate_squareToLen();\n+  }\n+  if (UseMulAddIntrinsic) {\n+    StubRoutines::_mulAdd = generate_mulAdd();\n+  }\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n+    StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n+  }\n+  if (UseMontgomeryMultiplyIntrinsic) {\n+    StubRoutines::_montgomeryMultiply\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n+  }\n+  if (UseMontgomerySquareIntrinsic) {\n+    StubRoutines::_montgomerySquare\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n+  }\n@@ -7912,39 +4135,25 @@\n-    \/\/ Get svml stub routine addresses\n-    void *libjsvml = NULL;\n-    char ebuf[1024];\n-    char dll_name[JVM_MAXPATHLEN];\n-    if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n-      libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n-    }\n-    if (libjsvml != NULL) {\n-      \/\/ SVML method naming convention\n-      \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n-      \/\/   Where:\n-      \/\/      ha stands for high accuracy\n-      \/\/      <T> is optional to indicate float\/double\n-      \/\/              Set to f for vector float operation\n-      \/\/              Omitted for vector double operation\n-      \/\/      <N> is the number of elements in the vector\n-      \/\/              1, 2, 4, 8, 16\n-      \/\/              e.g. 128 bit float vector has 4 float elements\n-      \/\/      <VV> indicates the avx\/sse level:\n-      \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n-      \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n-      \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n-\n-      log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n-      if (UseAVX > 2) {\n-        for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n-          int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n-          if ((!VM_Version::supports_avx512dq()) &&\n-              (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n-            continue;\n-          }\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-        }\n-      }\n-      const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+  \/\/ Get svml stub routine addresses\n+  void *libjsvml = nullptr;\n+  char ebuf[1024];\n+  char dll_name[JVM_MAXPATHLEN];\n+  if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n+    libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n+  }\n+  if (libjsvml != nullptr) {\n+    \/\/ SVML method naming convention\n+    \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n+    \/\/   Where:\n+    \/\/      ha stands for high accuracy\n+    \/\/      <T> is optional to indicate float\/double\n+    \/\/              Set to f for vector float operation\n+    \/\/              Omitted for vector double operation\n+    \/\/      <N> is the number of elements in the vector\n+    \/\/              1, 2, 4, 8, 16\n+    \/\/              e.g. 128 bit float vector has 4 float elements\n+    \/\/      <VV> indicates the avx\/sse level:\n+    \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n+    \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n+    \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n+\n+    log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n+    if (UseAVX > 2) {\n@@ -7953,1 +4162,2 @@\n-        if (vop == VectorSupport::VECTOR_OP_POW) {\n+        if ((!VM_Version::supports_avx512dq()) &&\n+            (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n@@ -7956,2 +4166,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7959,2 +4169,12 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      }\n+    }\n+    const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+    for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n+      int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n+      if (vop == VectorSupport::VECTOR_OP_POW) {\n+        continue;\n+      }\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7962,2 +4182,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7965,2 +4185,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7968,2 +4188,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7971,5 +4191,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-      }\n-    }\n-#endif \/\/ COMPILER2\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7977,2 +4194,2 @@\n-    if (UseVectorizedMismatchIntrinsic) {\n-      StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -7981,0 +4198,3 @@\n+#endif \/\/ COMPILER2\n+#endif \/\/ COMPILER2_OR_JVMCI\n+}\n@@ -7982,9 +4202,20 @@\n- public:\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n-      generate_initial();\n-    }\n-  }\n-}; \/\/ end class declaration\n+StubGenerator::StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    DEBUG_ONLY( _regs_in_thread = false; )\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n+}\n@@ -7992,6 +4223,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 16\n-void StubGenerator_generate(CodeBuffer* code, bool all) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, all);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n@@ -7999,0 +4226,2 @@\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":3659,"deletions":7430,"binary":false,"changes":11089,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -36,36 +36,30 @@\n-address StubRoutines::x86::_verify_mxcsr_entry = NULL;\n-address StubRoutines::x86::_key_shuffle_mask_addr = NULL;\n-address StubRoutines::x86::_counter_shuffle_mask_addr = NULL;\n-address StubRoutines::x86::_ghash_long_swap_mask_addr = NULL;\n-address StubRoutines::x86::_ghash_byte_swap_mask_addr = NULL;\n-address StubRoutines::x86::_ghash_poly_addr = NULL;\n-address StubRoutines::x86::_ghash_shuffmask_addr = NULL;\n-address StubRoutines::x86::_upper_word_mask_addr = NULL;\n-address StubRoutines::x86::_shuffle_byte_flip_mask_addr = NULL;\n-address StubRoutines::x86::_k256_adr = NULL;\n-address StubRoutines::x86::_vector_short_to_byte_mask = NULL;\n-address StubRoutines::x86::_vector_int_to_byte_mask = NULL;\n-address StubRoutines::x86::_vector_int_to_short_mask = NULL;\n-address StubRoutines::x86::_vector_all_bits_set = NULL;\n-address StubRoutines::x86::_vector_byte_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_int_mask_cmp_bits = NULL;\n-address StubRoutines::x86::_vector_short_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_int_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_long_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_float_sign_mask = NULL;\n-address StubRoutines::x86::_vector_float_sign_flip = NULL;\n-address StubRoutines::x86::_vector_double_sign_mask = NULL;\n-address StubRoutines::x86::_vector_double_sign_flip = NULL;\n-address StubRoutines::x86::_vector_byte_perm_mask = NULL;\n-address StubRoutines::x86::_vector_long_sign_mask = NULL;\n-address StubRoutines::x86::_vector_iota_indices = NULL;\n-address StubRoutines::x86::_vector_reverse_bit_lut = NULL;\n-address StubRoutines::x86::_vector_reverse_byte_perm_mask_long = NULL;\n-address StubRoutines::x86::_vector_reverse_byte_perm_mask_int = NULL;\n-address StubRoutines::x86::_vector_reverse_byte_perm_mask_short = NULL;\n-address StubRoutines::x86::_vector_popcount_lut = NULL;\n-address StubRoutines::x86::_vector_count_leading_zeros_lut = NULL;\n-address StubRoutines::x86::_vector_32_bit_mask = NULL;\n-address StubRoutines::x86::_vector_64_bit_mask = NULL;\n-address StubRoutines::x86::_vector_halffloat_sign_mask = NULL;\n-address StubRoutines::x86::_vector_halffloat_sign_flip = NULL;\n+address StubRoutines::x86::_verify_mxcsr_entry = nullptr;\n+address StubRoutines::x86::_upper_word_mask_addr = nullptr;\n+address StubRoutines::x86::_shuffle_byte_flip_mask_addr = nullptr;\n+address StubRoutines::x86::_k256_adr = nullptr;\n+address StubRoutines::x86::_vector_short_to_byte_mask = nullptr;\n+address StubRoutines::x86::_vector_int_to_byte_mask = nullptr;\n+address StubRoutines::x86::_vector_int_to_short_mask = nullptr;\n+address StubRoutines::x86::_vector_all_bits_set = nullptr;\n+address StubRoutines::x86::_vector_byte_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_int_mask_cmp_bits = nullptr;\n+address StubRoutines::x86::_vector_short_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_int_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_long_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_float_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_float_sign_flip = nullptr;\n+address StubRoutines::x86::_vector_double_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_double_sign_flip = nullptr;\n+address StubRoutines::x86::_vector_byte_perm_mask = nullptr;\n+address StubRoutines::x86::_vector_long_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_iota_indices = nullptr;\n+address StubRoutines::x86::_vector_reverse_bit_lut = nullptr;\n+address StubRoutines::x86::_vector_reverse_byte_perm_mask_long = nullptr;\n+address StubRoutines::x86::_vector_reverse_byte_perm_mask_int = nullptr;\n+address StubRoutines::x86::_vector_reverse_byte_perm_mask_short = nullptr;\n+address StubRoutines::x86::_vector_popcount_lut = nullptr;\n+address StubRoutines::x86::_vector_count_leading_zeros_lut = nullptr;\n+address StubRoutines::x86::_vector_32_bit_mask = nullptr;\n+address StubRoutines::x86::_vector_64_bit_mask = nullptr;\n+address StubRoutines::x86::_vector_halffloat_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_halffloat_sign_flip = nullptr;\n@@ -73,3 +67,3 @@\n-address StubRoutines::x86::_k256_W_adr = NULL;\n-address StubRoutines::x86::_k512_W_addr = NULL;\n-address StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = NULL;\n+address StubRoutines::x86::_k256_W_adr = nullptr;\n+address StubRoutines::x86::_k512_W_addr = nullptr;\n+address StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = nullptr;\n@@ -77,16 +71,16 @@\n-address StubRoutines::x86::_encoding_table_base64 = NULL;\n-address StubRoutines::x86::_shuffle_base64 = NULL;\n-address StubRoutines::x86::_avx2_shuffle_base64 = NULL;\n-address StubRoutines::x86::_avx2_input_mask_base64 = NULL;\n-address StubRoutines::x86::_avx2_lut_base64 = NULL;\n-address StubRoutines::x86::_counter_mask_addr = NULL;\n-address StubRoutines::x86::_lookup_lo_base64 = NULL;\n-address StubRoutines::x86::_lookup_hi_base64 = NULL;\n-address StubRoutines::x86::_lookup_lo_base64url = NULL;\n-address StubRoutines::x86::_lookup_hi_base64url = NULL;\n-address StubRoutines::x86::_pack_vec_base64 = NULL;\n-address StubRoutines::x86::_join_0_1_base64 = NULL;\n-address StubRoutines::x86::_join_1_2_base64 = NULL;\n-address StubRoutines::x86::_join_2_3_base64 = NULL;\n-address StubRoutines::x86::_decoding_table_base64 = NULL;\n-address StubRoutines::x86::_ghash_poly512_addr = NULL;\n+address StubRoutines::x86::_encoding_table_base64 = nullptr;\n+address StubRoutines::x86::_shuffle_base64 = nullptr;\n+address StubRoutines::x86::_avx2_shuffle_base64 = nullptr;\n+address StubRoutines::x86::_avx2_input_mask_base64 = nullptr;\n+address StubRoutines::x86::_avx2_lut_base64 = nullptr;\n+address StubRoutines::x86::_avx2_decode_tables_base64 = nullptr;\n+address StubRoutines::x86::_avx2_decode_lut_tables_base64 = nullptr;\n+address StubRoutines::x86::_lookup_lo_base64 = nullptr;\n+address StubRoutines::x86::_lookup_hi_base64 = nullptr;\n+address StubRoutines::x86::_lookup_lo_base64url = nullptr;\n+address StubRoutines::x86::_lookup_hi_base64url = nullptr;\n+address StubRoutines::x86::_pack_vec_base64 = nullptr;\n+address StubRoutines::x86::_join_0_1_base64 = nullptr;\n+address StubRoutines::x86::_join_1_2_base64 = nullptr;\n+address StubRoutines::x86::_join_2_3_base64 = nullptr;\n+address StubRoutines::x86::_decoding_table_base64 = nullptr;\n@@ -94,26 +88,3 @@\n-address StubRoutines::x86::_pshuffle_byte_flip_mask_addr = NULL;\n-\n-\/\/tables common for sin and cos\n-address StubRoutines::x86::_ONEHALF_adr = NULL;\n-address StubRoutines::x86::_P_2_adr = NULL;\n-address StubRoutines::x86::_SC_4_adr = NULL;\n-address StubRoutines::x86::_Ctable_adr = NULL;\n-address StubRoutines::x86::_SC_2_adr = NULL;\n-address StubRoutines::x86::_SC_3_adr = NULL;\n-address StubRoutines::x86::_SC_1_adr = NULL;\n-address StubRoutines::x86::_PI_INV_TABLE_adr = NULL;\n-address StubRoutines::x86::_PI_4_adr = NULL;\n-address StubRoutines::x86::_PI32INV_adr = NULL;\n-address StubRoutines::x86::_SIGN_MASK_adr = NULL;\n-address StubRoutines::x86::_P_1_adr = NULL;\n-address StubRoutines::x86::_P_3_adr = NULL;\n-address StubRoutines::x86::_NEG_ZERO_adr = NULL;\n-\n-\/\/tables common for sincos and tancot\n-address StubRoutines::x86::_L_2il0floatpacket_0_adr = NULL;\n-address StubRoutines::x86::_Pi4Inv_adr = NULL;\n-address StubRoutines::x86::_Pi4x3_adr = NULL;\n-address StubRoutines::x86::_Pi4x4_adr = NULL;\n-address StubRoutines::x86::_ones_adr = NULL;\n-\n-uint64_t StubRoutines::x86::_crc_by128_masks[] =\n+address StubRoutines::x86::_pshuffle_byte_flip_mask_addr = nullptr;\n+\n+const uint64_t StubRoutines::x86::_crc_by128_masks[] =\n@@ -158,1 +129,1 @@\n-juint StubRoutines::x86::_crc_table[] =\n+const juint StubRoutines::x86::_crc_table[] =\n@@ -215,1 +186,1 @@\n-juint StubRoutines::x86::_crc_table_avx512[] =\n+const juint StubRoutines::x86::_crc_table_avx512[] =\n@@ -232,1 +203,1 @@\n-juint StubRoutines::x86::_crc32c_table_avx512[] =\n+const juint StubRoutines::x86::_crc32c_table_avx512[] =\n@@ -249,1 +220,1 @@\n-juint StubRoutines::x86::_crc_by128_masks_avx512[] =\n+const juint StubRoutines::x86::_crc_by128_masks_avx512[] =\n@@ -256,1 +227,1 @@\n-juint StubRoutines::x86::_shuf_table_crc32_avx512[] =\n+const juint StubRoutines::x86::_shuf_table_crc32_avx512[] =\n@@ -261,0 +232,1 @@\n+#endif \/\/ _LP64\n@@ -262,13 +234,1 @@\n-juint StubRoutines::x86::_adler32_ascale_table[] =\n-{\n-    0x00000000UL, 0x00000001UL, 0x00000002UL, 0x00000003UL,\n-    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL\n-};\n-\n-juint StubRoutines::x86::_adler32_shuf0_table[] =\n-{\n-    0xFFFFFF00UL, 0xFFFFFF01UL, 0xFFFFFF02UL, 0xFFFFFF03UL,\n-    0xFFFFFF04UL, 0xFFFFFF05UL, 0xFFFFFF06UL, 0xFFFFFF07UL\n-};\n-\n-juint StubRoutines::x86::_adler32_shuf1_table[] =\n+const jint StubRoutines::x86::_arrays_hashcode_powers_of_31[] =\n@@ -276,2 +236,33 @@\n-    0xFFFFFF08UL, 0xFFFFFF09, 0xFFFFFF0AUL, 0xFFFFFF0BUL,\n-    0xFFFFFF0CUL, 0xFFFFFF0D, 0xFFFFFF0EUL, 0xFFFFFF0FUL\n+     2111290369,\n+    -2010103841,\n+      350799937,\n+       11316127,\n+      693101697,\n+     -254736545,\n+      961614017,\n+       31019807,\n+    -2077209343,\n+      -67006753,\n+     1244764481,\n+    -2038056289,\n+      211350913,\n+     -408824225,\n+     -844471871,\n+     -997072353,\n+     1353309697,\n+     -510534177,\n+     1507551809,\n+     -505558625,\n+     -293403007,\n+      129082719,\n+    -1796951359,\n+     -196513505,\n+    -1807454463,\n+     1742810335,\n+      887503681,\n+       28629151,\n+         923521,\n+          29791,\n+            961,\n+             31,\n+              1,\n@@ -280,2 +271,0 @@\n-#endif \/\/ _LP64\n-\n@@ -386,1 +375,1 @@\n-ATTRIBUTE_ALIGNED(64) juint StubRoutines::x86::_k256[] =\n+ATTRIBUTE_ALIGNED(64) const juint StubRoutines::x86::_k256[] =\n@@ -412,1 +401,1 @@\n-ATTRIBUTE_ALIGNED(64) julong StubRoutines::x86::_k512_W[] =\n+ATTRIBUTE_ALIGNED(64) const julong StubRoutines::x86::_k512_W[] =\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":96,"deletions":107,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,2 +35,7 @@\n-  code_size1 = 20000 LP64_ONLY(+10000),         \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 35300 LP64_ONLY(+35000)          \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 20000 WINDOWS_ONLY(+1000),\n+  _continuation_stubs_code_size =  1000 LP64_ONLY(+1000),\n+  \/\/ AVX512 intrinsics add more code in 64-bit VM,\n+  \/\/ Windows have more code to save\/restore registers\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+30000) WINDOWS_ONLY(+2000),\n+  _final_stubs_code_size        = 10000 LP64_ONLY(+20000) WINDOWS_ONLY(+2000)\n@@ -126,5 +131,0 @@\n-  \/\/ shuffle mask for fixing up 128-bit words consisting of big-endian 32-bit integers\n-  static address _key_shuffle_mask_addr;\n-\n-  \/\/shuffle mask for big-endian 128-bit integers\n-  static address _counter_shuffle_mask_addr;\n@@ -135,2 +135,2 @@\n-  static uint64_t _crc_by128_masks[];\n-  static juint    _crc_table[];\n+  static const uint64_t _crc_by128_masks[];\n+  static const juint    _crc_table[];\n@@ -138,7 +138,4 @@\n-  static juint    _crc_by128_masks_avx512[];\n-  static juint    _crc_table_avx512[];\n-  static juint    _crc32c_table_avx512[];\n-  static juint    _shuf_table_crc32_avx512[];\n-  static juint    _adler32_shuf0_table[];\n-  static juint    _adler32_shuf1_table[];\n-  static juint    _adler32_ascale_table[];\n+  static const juint    _crc_by128_masks_avx512[];\n+  static const juint    _crc_table_avx512[];\n+  static const juint    _crc32c_table_avx512[];\n+  static const juint    _shuf_table_crc32_avx512[];\n@@ -148,5 +145,2 @@\n-  \/\/ swap mask for ghash\n-  static address _ghash_long_swap_mask_addr;\n-  static address _ghash_byte_swap_mask_addr;\n-  static address _ghash_poly_addr;\n-  static address _ghash_shuffmask_addr;\n+  \/\/ table for arrays_hashcode\n+  static const jint _arrays_hashcode_powers_of_31[];\n@@ -160,1 +154,1 @@\n-  static juint _k256[];\n+  static const juint _k256[];\n@@ -191,1 +185,1 @@\n-  static julong _k512_W[];\n+  static const julong _k512_W[];\n@@ -195,1 +189,0 @@\n-  static address _counter_mask_addr;\n@@ -202,0 +195,2 @@\n+  static address _avx2_decode_tables_base64;\n+  static address _avx2_decode_lut_tables_base64;\n@@ -211,1 +206,0 @@\n-  static address _ghash_poly512_addr;\n@@ -216,42 +210,0 @@\n-  \/\/tables common for LIBM sin and cos\n-  static juint _ONEHALF[];\n-  static address _ONEHALF_adr;\n-  static juint _P_2[];\n-  static address _P_2_adr;\n-  static juint _SC_4[];\n-  static address _SC_4_adr;\n-  static juint _Ctable[];\n-  static address _Ctable_adr;\n-  static juint _SC_2[];\n-  static address _SC_2_adr;\n-  static juint _SC_3[];\n-  static address _SC_3_adr;\n-  static juint _SC_1[];\n-  static address _SC_1_adr;\n-  static juint _PI_INV_TABLE[];\n-  static address _PI_INV_TABLE_adr;\n-  static juint _PI_4[];\n-  static address _PI_4_adr;\n-  static juint _PI32INV[];\n-  static address _PI32INV_adr;\n-  static juint _SIGN_MASK[];\n-  static address _SIGN_MASK_adr;\n-  static juint _P_1[];\n-  static address _P_1_adr;\n-  static juint _P_3[];\n-  static address _P_3_adr;\n-  static juint _NEG_ZERO[];\n-  static address _NEG_ZERO_adr;\n-\n-  \/\/tables common for LIBM sincos and tancot\n-  static juint _L_2il0floatpacket_0[];\n-  static address _L_2il0floatpacket_0_adr;\n-  static juint _Pi4Inv[];\n-  static address _Pi4Inv_adr;\n-  static juint _Pi4x3[];\n-  static address _Pi4x3_adr;\n-  static juint _Pi4x4[];\n-  static address _Pi4x4_adr;\n-  static juint _ones[];\n-  static address _ones_adr;\n-\n@@ -261,2 +213,0 @@\n-  static address key_shuffle_mask_addr() { return _key_shuffle_mask_addr; }\n-  static address counter_shuffle_mask_addr() { return _counter_shuffle_mask_addr; }\n@@ -269,5 +219,0 @@\n-  static address ghash_polynomial512_addr() { return _ghash_poly512_addr; }\n-  static address ghash_long_swap_mask_addr() { return _ghash_long_swap_mask_addr; }\n-  static address ghash_byte_swap_mask_addr() { return _ghash_byte_swap_mask_addr; }\n-  static address ghash_shufflemask_addr() { return _ghash_shuffmask_addr; }\n-  static address ghash_polynomial_addr() { return _ghash_poly_addr; }\n@@ -391,1 +336,0 @@\n-  static address counter_mask_addr() { return _counter_mask_addr; }\n@@ -401,0 +345,2 @@\n+  static address base64_AVX2_decode_tables_addr() { return _avx2_decode_tables_base64; }\n+  static address base64_AVX2_decode_LUT_tables_addr() { return _avx2_decode_lut_tables_base64; }\n@@ -403,0 +349,1 @@\n+  static address arrays_hashcode_powers_of_31() { return (address)_arrays_hashcode_powers_of_31; }\n@@ -404,20 +351,0 @@\n-  static address _ONEHALF_addr()      { return _ONEHALF_adr; }\n-  static address _P_2_addr()      { return _P_2_adr; }\n-  static address _SC_4_addr()      { return _SC_4_adr; }\n-  static address _Ctable_addr()      { return _Ctable_adr; }\n-  static address _SC_2_addr()      { return _SC_2_adr; }\n-  static address _SC_3_addr()      { return _SC_3_adr; }\n-  static address _SC_1_addr()      { return _SC_1_adr; }\n-  static address _PI_INV_TABLE_addr()      { return _PI_INV_TABLE_adr; }\n-  static address _PI_4_addr()      { return _PI_4_adr; }\n-  static address _PI32INV_addr()      { return _PI32INV_adr; }\n-  static address _SIGN_MASK_addr()      { return _SIGN_MASK_adr; }\n-  static address _P_1_addr()      { return _P_1_adr; }\n-  static address _P_3_addr()      { return _P_3_adr; }\n-  static address _NEG_ZERO_addr()      { return _NEG_ZERO_adr; }\n-  static address _L_2il0floatpacket_0_addr()      { return _L_2il0floatpacket_0_adr; }\n-  static address _Pi4Inv_addr()      { return _Pi4Inv_adr; }\n-  static address _Pi4x3_addr()      { return _Pi4x3_adr; }\n-  static address _Pi4x4_addr()      { return _Pi4x4_adr; }\n-  static address _ones_addr()      { return _ones_adr; }\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":23,"deletions":96,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,0 @@\n-#include \"jvm.h\"\n@@ -29,0 +28,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -30,0 +30,2 @@\n+#include \"compiler\/compilerDefinitions.inline.hpp\"\n+#include \"jvm.h\"\n@@ -36,1 +38,1 @@\n-#include \"runtime\/os.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n@@ -42,2 +44,0 @@\n-#include OS_HEADER_INLINE(os)\n-\n@@ -66,2 +66,2 @@\n-static get_cpu_info_stub_t get_cpu_info_stub = NULL;\n-static detect_virt_stub_t detect_virt_stub = NULL;\n+static get_cpu_info_stub_t get_cpu_info_stub = nullptr;\n+static detect_virt_stub_t detect_virt_stub = nullptr;\n@@ -405,1 +405,1 @@\n-    \/\/ Generate SEGV here (reference through NULL)\n+    \/\/ Generate SEGV here (reference through null)\n@@ -408,2 +408,2 @@\n-    intx saved_useavx = UseAVX;\n-    intx saved_usesse = UseSSE;\n+    int saved_useavx = UseAVX;\n+    int saved_usesse = UseSSE;\n@@ -858,3 +858,0 @@\n-  \/\/ If the OS doesn't support SSE, we can't use this feature even if the HW does\n-  if (!os::supports_sse())\n-    _features &= ~(CPU_SSE|CPU_SSE2|CPU_SSE3|CPU_SSSE3|CPU_SSE4A|CPU_SSE4_1|CPU_SSE4_2);\n@@ -884,0 +881,24 @@\n+  \/\/ UseSSE is set to the smaller of what hardware supports and what\n+  \/\/ the command line requires.  I.e., you cannot set UseSSE to 2 on\n+  \/\/ older Pentiums which do not support it.\n+  int use_sse_limit = 0;\n+  if (UseSSE > 0) {\n+    if (UseSSE > 3 && supports_sse4_1()) {\n+      use_sse_limit = 4;\n+    } else if (UseSSE > 2 && supports_sse3()) {\n+      use_sse_limit = 3;\n+    } else if (UseSSE > 1 && supports_sse2()) {\n+      use_sse_limit = 2;\n+    } else if (UseSSE > 0 && supports_sse()) {\n+      use_sse_limit = 1;\n+    } else {\n+      use_sse_limit = 0;\n+    }\n+  }\n+  if (FLAG_IS_DEFAULT(UseSSE)) {\n+    FLAG_SET_DEFAULT(UseSSE, use_sse_limit);\n+  } else if (UseSSE > use_sse_limit) {\n+    warning(\"UseSSE=%d is not supported on this CPU, setting it to UseSSE=%d\", UseSSE, use_sse_limit);\n+    FLAG_SET_DEFAULT(UseSSE, use_sse_limit);\n+  }\n+\n@@ -887,1 +908,4 @@\n-    if (UseAVX > 2 && supports_evex()) {\n+    if (UseSSE < 4) {\n+      \/\/ Don't use AVX if SSE is unavailable or has been disabled.\n+      use_avx_limit = 0;\n+    } else if (UseAVX > 2 && supports_evex()) {\n@@ -906,1 +930,5 @@\n-    warning(\"UseAVX=%d is not supported on this CPU, setting it to UseAVX=%d\", (int) UseAVX, use_avx_limit);\n+    if (UseSSE < 4) {\n+      warning(\"UseAVX=%d requires UseSSE=4, setting it to UseAVX=0\", UseAVX);\n+    } else {\n+      warning(\"UseAVX=%d is not supported on this CPU, setting it to UseAVX=%d\", UseAVX, use_avx_limit);\n+    }\n@@ -908,3 +936,0 @@\n-  } else if (UseAVX < 0) {\n-    warning(\"UseAVX=%d is not valid, setting it to UseAVX=0\", (int) UseAVX);\n-    FLAG_SET_DEFAULT(UseAVX, 0);\n@@ -926,0 +951,1 @@\n+    _features &= ~CPU_AVX512_IFMA;\n@@ -935,0 +961,1 @@\n+    _features &= ~CPU_F16C;\n@@ -958,0 +985,1 @@\n+      _features &= ~CPU_AVX512_IFMA;\n@@ -967,1 +995,1 @@\n-  char buf[512];\n+  char buf[1024];\n@@ -978,27 +1006,0 @@\n-  \/\/ UseSSE is set to the smaller of what hardware supports and what\n-  \/\/ the command line requires.  I.e., you cannot set UseSSE to 2 on\n-  \/\/ older Pentiums which do not support it.\n-  int use_sse_limit = 0;\n-  if (UseSSE > 0) {\n-    if (UseSSE > 3 && supports_sse4_1()) {\n-      use_sse_limit = 4;\n-    } else if (UseSSE > 2 && supports_sse3()) {\n-      use_sse_limit = 3;\n-    } else if (UseSSE > 1 && supports_sse2()) {\n-      use_sse_limit = 2;\n-    } else if (UseSSE > 0 && supports_sse()) {\n-      use_sse_limit = 1;\n-    } else {\n-      use_sse_limit = 0;\n-    }\n-  }\n-  if (FLAG_IS_DEFAULT(UseSSE)) {\n-    FLAG_SET_DEFAULT(UseSSE, use_sse_limit);\n-  } else if (UseSSE > use_sse_limit) {\n-    warning(\"UseSSE=%d is not supported on this CPU, setting it to UseSSE=%d\", (int) UseSSE, use_sse_limit);\n-    FLAG_SET_DEFAULT(UseSSE, use_sse_limit);\n-  } else if (UseSSE < 0) {\n-    warning(\"UseSSE=%d is not valid, setting it to UseSSE=0\", (int) UseSSE);\n-    FLAG_SET_DEFAULT(UseSSE, 0);\n-  }\n-\n@@ -1127,0 +1128,16 @@\n+  \/\/ ChaCha20 Intrinsics\n+  \/\/ As long as the system supports AVX as a baseline we can do a\n+  \/\/ SIMD-enabled block function.  StubGenerator makes the determination\n+  \/\/ based on the VM capabilities whether to use an AVX2 or AVX512-enabled\n+  \/\/ version.\n+  if (UseAVX >= 1) {\n+      if (FLAG_IS_DEFAULT(UseChaCha20Intrinsics)) {\n+          UseChaCha20Intrinsics = true;\n+      }\n+  } else if (UseChaCha20Intrinsics) {\n+      if (!FLAG_IS_DEFAULT(UseChaCha20Intrinsics)) {\n+          warning(\"ChaCha20 intrinsic requires AVX instructions\");\n+      }\n+      FLAG_SET_DEFAULT(UseChaCha20Intrinsics, false);\n+  }\n+\n@@ -1128,1 +1145,1 @@\n-  if ((UseAVX > 2) && supports_avx512vl() && supports_avx512bw()) {\n+  if (UseAVX >= 2) {\n@@ -1316,0 +1333,12 @@\n+#ifdef _LP64\n+  if (supports_avx512ifma() && supports_avx512vlbw() && MaxVectorSize >= 64) {\n+    if (FLAG_IS_DEFAULT(UsePoly1305Intrinsics)) {\n+      FLAG_SET_DEFAULT(UsePoly1305Intrinsics, true);\n+    }\n+  } else\n+#endif\n+  if (UsePoly1305Intrinsics) {\n+    warning(\"Intrinsics for Poly1305 crypto hash functions not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UsePoly1305Intrinsics, false);\n+  }\n+\n@@ -1650,0 +1679,7 @@\n+  if (UseAVX >= 2) {\n+    FLAG_SET_DEFAULT(UseVectorizedHashCodeIntrinsic, true);\n+  } else if (UseVectorizedHashCodeIntrinsic) {\n+    if (!FLAG_IS_DEFAULT(UseVectorizedHashCodeIntrinsic))\n+      warning(\"vectorizedHashCode intrinsics are not available on this CPU\");\n+    FLAG_SET_DEFAULT(UseVectorizedHashCodeIntrinsic, false);\n+  }\n@@ -1657,0 +1693,6 @@\n+  if (UseVectorizedHashCodeIntrinsic) {\n+    if (!FLAG_IS_DEFAULT(UseVectorizedHashCodeIntrinsic)) {\n+      warning(\"vectorizedHashCode intrinsic is not available in 32-bit VM\");\n+    }\n+    FLAG_SET_DEFAULT(UseVectorizedHashCodeIntrinsic, false);\n+  }\n@@ -1843,1 +1885,1 @@\n-    log->print(\"UseSSE=%d\", (int) UseSSE);\n+    log->print(\"UseSSE=%d\", UseSSE);\n@@ -1845,1 +1887,1 @@\n-      log->print(\"  UseAVX=%d\", (int) UseAVX);\n+      log->print(\"  UseAVX=%d\", UseAVX);\n@@ -2035,0 +2077,11 @@\n+#ifdef COMPILER2\n+\/\/ Determine if it's running on Cascade Lake using default options.\n+bool VM_Version::is_default_intel_cascade_lake() {\n+  return FLAG_IS_DEFAULT(UseAVX) &&\n+         FLAG_IS_DEFAULT(MaxVectorSize) &&\n+         UseAVX > 2 &&\n+         is_intel_skylake() &&\n+         _stepping >= 5;\n+}\n+#endif\n+\n@@ -2052,1 +2105,1 @@\n-  if (stub_blob == NULL) {\n+  if (stub_blob == nullptr) {\n@@ -2126,1 +2179,1 @@\n-static getCPUIDBrandString_stub_t getCPUIDBrandString_stub = NULL;\n+static getCPUIDBrandString_stub_t getCPUIDBrandString_stub = nullptr;\n@@ -2136,1 +2189,1 @@\n-static char* _cpu_brand_string = NULL;\n+static char* _cpu_brand_string = nullptr;\n@@ -2261,1 +2314,1 @@\n-  NULL\n+  nullptr\n@@ -2276,1 +2329,1 @@\n-  NULL\n+  nullptr\n@@ -2424,1 +2477,1 @@\n-  if (cpuid_brand_string_stub_blob == NULL) {\n+  if (cpuid_brand_string_stub_blob == nullptr) {\n@@ -2436,1 +2489,1 @@\n-  const char* model = NULL;\n+  const char* model = nullptr;\n@@ -2441,1 +2494,1 @@\n-      if (model == NULL) {\n+      if (model == nullptr) {\n@@ -2450,1 +2503,1 @@\n-  if (_cpu_brand_string == NULL) {\n+  if (_cpu_brand_string == nullptr) {\n@@ -2452,2 +2505,2 @@\n-    if (NULL == _cpu_brand_string) {\n-      return NULL;\n+    if (nullptr == _cpu_brand_string) {\n+      return nullptr;\n@@ -2458,1 +2511,1 @@\n-      _cpu_brand_string = NULL;\n+      _cpu_brand_string = nullptr;\n@@ -2465,1 +2518,1 @@\n-  const char*  brand  = NULL;\n+  const char*  brand  = nullptr;\n@@ -2470,1 +2523,1 @@\n-    for (int i = 0; brand != NULL && i <= brand_num; i += 1) {\n+    for (int i = 0; brand != nullptr && i <= brand_num; i += 1) {\n@@ -2560,1 +2613,1 @@\n-  assert(buf != NULL, \"buffer is NULL!\");\n+  assert(buf != nullptr, \"buffer is null!\");\n@@ -2563,2 +2616,2 @@\n-  const char* cpu_type = NULL;\n-  const char* x64 = NULL;\n+  const char* cpu_type = nullptr;\n+  const char* x64 = nullptr;\n@@ -2597,1 +2650,1 @@\n-  assert(buf != NULL, \"buffer is NULL!\");\n+  assert(buf != nullptr, \"buffer is null!\");\n@@ -2599,1 +2652,1 @@\n-  assert(getCPUIDBrandString_stub != NULL, \"not initialized\");\n+  assert(getCPUIDBrandString_stub != nullptr, \"not initialized\");\n@@ -2622,1 +2675,1 @@\n-  guarantee(buf != NULL, \"buffer is NULL!\");\n+  guarantee(buf != nullptr, \"buffer is null!\");\n@@ -2683,1 +2736,1 @@\n-  assert(buf != NULL, \"buffer is NULL!\");\n+  assert(buf != nullptr, \"buffer is null!\");\n@@ -2688,3 +2741,3 @@\n-  const char*        family = NULL;\n-  const char*        model = NULL;\n-  const char*        brand = NULL;\n+  const char*        family = nullptr;\n+  const char*        model = nullptr;\n+  const char*        brand = nullptr;\n@@ -2694,1 +2747,1 @@\n-  if (family == NULL) {\n+  if (family == nullptr) {\n@@ -2699,1 +2752,1 @@\n-  if (model == NULL) {\n+  if (model == nullptr) {\n@@ -2705,1 +2758,1 @@\n-  if (brand == NULL) {\n+  if (brand == nullptr) {\n@@ -2707,1 +2760,1 @@\n-    if (brand == NULL) {\n+    if (brand == nullptr) {\n@@ -2776,1 +2829,1 @@\n-  if (brand_string == NULL) {\n+  if (brand_string == nullptr) {\n@@ -2824,0 +2877,356 @@\n+uint64_t VM_Version::feature_flags() {\n+  uint64_t result = 0;\n+  if (_cpuid_info.std_cpuid1_edx.bits.cmpxchg8 != 0)\n+    result |= CPU_CX8;\n+  if (_cpuid_info.std_cpuid1_edx.bits.cmov != 0)\n+    result |= CPU_CMOV;\n+  if (_cpuid_info.std_cpuid1_edx.bits.clflush != 0)\n+    result |= CPU_FLUSH;\n+#ifdef _LP64\n+  \/\/ clflush should always be available on x86_64\n+  \/\/ if not we are in real trouble because we rely on it\n+  \/\/ to flush the code cache.\n+  assert ((result & CPU_FLUSH) != 0, \"clflush should be available\");\n+#endif\n+  if (_cpuid_info.std_cpuid1_edx.bits.fxsr != 0 || (is_amd_family() &&\n+      _cpuid_info.ext_cpuid1_edx.bits.fxsr != 0))\n+    result |= CPU_FXSR;\n+  \/\/ HT flag is set for multi-core processors also.\n+  if (threads_per_core() > 1)\n+    result |= CPU_HT;\n+  if (_cpuid_info.std_cpuid1_edx.bits.mmx != 0 || (is_amd_family() &&\n+      _cpuid_info.ext_cpuid1_edx.bits.mmx != 0))\n+    result |= CPU_MMX;\n+  if (_cpuid_info.std_cpuid1_edx.bits.sse != 0)\n+    result |= CPU_SSE;\n+  if (_cpuid_info.std_cpuid1_edx.bits.sse2 != 0)\n+    result |= CPU_SSE2;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse3 != 0)\n+    result |= CPU_SSE3;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.ssse3 != 0)\n+    result |= CPU_SSSE3;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse4_1 != 0)\n+    result |= CPU_SSE4_1;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse4_2 != 0)\n+    result |= CPU_SSE4_2;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.popcnt != 0)\n+    result |= CPU_POPCNT;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.avx != 0 &&\n+      _cpuid_info.std_cpuid1_ecx.bits.osxsave != 0 &&\n+      _cpuid_info.xem_xcr0_eax.bits.sse != 0 &&\n+      _cpuid_info.xem_xcr0_eax.bits.ymm != 0) {\n+    result |= CPU_AVX;\n+    result |= CPU_VZEROUPPER;\n+    if (_cpuid_info.std_cpuid1_ecx.bits.f16c != 0)\n+      result |= CPU_F16C;\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.avx2 != 0)\n+      result |= CPU_AVX2;\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.avx512f != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.opmask != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.zmm512 != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.zmm32 != 0) {\n+      result |= CPU_AVX512F;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512cd != 0)\n+        result |= CPU_AVX512CD;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512dq != 0)\n+        result |= CPU_AVX512DQ;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512ifma != 0)\n+        result |= CPU_AVX512_IFMA;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512pf != 0)\n+        result |= CPU_AVX512PF;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512er != 0)\n+        result |= CPU_AVX512ER;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512bw != 0)\n+        result |= CPU_AVX512BW;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512vl != 0)\n+        result |= CPU_AVX512VL;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpopcntdq != 0)\n+        result |= CPU_AVX512_VPOPCNTDQ;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpclmulqdq != 0)\n+        result |= CPU_AVX512_VPCLMULQDQ;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.vaes != 0)\n+        result |= CPU_AVX512_VAES;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.gfni != 0)\n+        result |= CPU_GFNI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vnni != 0)\n+        result |= CPU_AVX512_VNNI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_bitalg != 0)\n+        result |= CPU_AVX512_BITALG;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi != 0)\n+        result |= CPU_AVX512_VBMI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi2 != 0)\n+        result |= CPU_AVX512_VBMI2;\n+      if (_cpuid_info.sef_cpuid7_edx.bits.avx512_fp16 != 0)\n+          result |= CPU_AVX512_FP16;\n+    }\n+  }\n+  if (_cpuid_info.std_cpuid1_ecx.bits.hv != 0)\n+    result |= CPU_HV;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.bmi1 != 0)\n+    result |= CPU_BMI1;\n+  if (_cpuid_info.std_cpuid1_edx.bits.tsc != 0)\n+    result |= CPU_TSC;\n+  if (_cpuid_info.ext_cpuid7_edx.bits.tsc_invariance != 0)\n+    result |= CPU_TSCINV_BIT;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.aes != 0)\n+    result |= CPU_AES;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.erms != 0)\n+    result |= CPU_ERMS;\n+  if (_cpuid_info.sef_cpuid7_edx.bits.fast_short_rep_mov != 0)\n+    result |= CPU_FSRM;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.clmul != 0)\n+    result |= CPU_CLMUL;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.rtm != 0)\n+    result |= CPU_RTM;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.adx != 0)\n+     result |= CPU_ADX;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.bmi2 != 0)\n+    result |= CPU_BMI2;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.sha != 0)\n+    result |= CPU_SHA;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.fma != 0)\n+    result |= CPU_FMA;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0)\n+    result |= CPU_FLUSHOPT;\n+  if (_cpuid_info.ext_cpuid1_edx.bits.rdtscp != 0)\n+    result |= CPU_RDTSCP;\n+  if (_cpuid_info.sef_cpuid7_ecx.bits.rdpid != 0)\n+    result |= CPU_RDPID;\n+\n+  \/\/ AMD|Hygon features.\n+  if (is_amd_family()) {\n+    if ((_cpuid_info.ext_cpuid1_edx.bits.tdnow != 0) ||\n+        (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0))\n+      result |= CPU_3DNOW_PREFETCH;\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0)\n+      result |= CPU_LZCNT;\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.sse4a != 0)\n+      result |= CPU_SSE4A;\n+  }\n+\n+  \/\/ Intel features.\n+  if (is_intel()) {\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n+      result |= CPU_LZCNT;\n+    }\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n+      result |= CPU_3DNOW_PREFETCH;\n+    }\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.clwb != 0) {\n+      result |= CPU_CLWB;\n+    }\n+    if (_cpuid_info.sef_cpuid7_edx.bits.serialize != 0)\n+      result |= CPU_SERIALIZE;\n+  }\n+\n+  \/\/ ZX features.\n+  if (is_zx()) {\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n+      result |= CPU_LZCNT;\n+    }\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n+      result |= CPU_3DNOW_PREFETCH;\n+    }\n+  }\n+\n+  \/\/ Protection key features.\n+  if (_cpuid_info.sef_cpuid7_ecx.bits.pku != 0) {\n+    result |= CPU_PKU;\n+  }\n+  if (_cpuid_info.sef_cpuid7_ecx.bits.ospke != 0) {\n+    result |= CPU_OSPKE;\n+  }\n+\n+  \/\/ Control flow enforcement (CET) features.\n+  if (_cpuid_info.sef_cpuid7_ecx.bits.cet_ss != 0) {\n+    result |= CPU_CET_SS;\n+  }\n+  if (_cpuid_info.sef_cpuid7_edx.bits.cet_ibt != 0) {\n+    result |= CPU_CET_IBT;\n+  }\n+\n+  \/\/ Composite features.\n+  if (supports_tscinv_bit() &&\n+      ((is_amd_family() && !is_amd_Barcelona()) ||\n+       is_intel_tsc_synched_at_init())) {\n+    result |= CPU_TSCINV;\n+  }\n+\n+  return result;\n+}\n+\n+bool VM_Version::os_supports_avx_vectors() {\n+  bool retVal = false;\n+  int nreg = 2 LP64_ONLY(+2);\n+  if (supports_evex()) {\n+    \/\/ Verify that OS save\/restore all bits of EVEX registers\n+    \/\/ during signal processing.\n+    retVal = true;\n+    for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n+      if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n+        retVal = false;\n+        break;\n+      }\n+    }\n+  } else if (supports_avx()) {\n+    \/\/ Verify that OS save\/restore all bits of AVX registers\n+    \/\/ during signal processing.\n+    retVal = true;\n+    for (int i = 0; i < 8 * nreg; i++) { \/\/ 32 bytes per ymm register\n+      if (_cpuid_info.ymm_save[i] != ymm_test_value()) {\n+        retVal = false;\n+        break;\n+      }\n+    }\n+    \/\/ zmm_save will be set on a EVEX enabled machine even if we choose AVX code gen\n+    if (retVal == false) {\n+      \/\/ Verify that OS save\/restore all bits of EVEX registers\n+      \/\/ during signal processing.\n+      retVal = true;\n+      for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n+        if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n+          retVal = false;\n+          break;\n+        }\n+      }\n+    }\n+  }\n+  return retVal;\n+}\n+\n+uint VM_Version::cores_per_cpu() {\n+  uint result = 1;\n+  if (is_intel()) {\n+    bool supports_topology = supports_processor_topology();\n+    if (supports_topology) {\n+      result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n+               _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+    }\n+    if (!supports_topology || result == 0) {\n+      result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n+    }\n+  } else if (is_amd_family()) {\n+    result = (_cpuid_info.ext_cpuid8_ecx.bits.cores_per_cpu + 1);\n+  } else if (is_zx()) {\n+    bool supports_topology = supports_processor_topology();\n+    if (supports_topology) {\n+      result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n+               _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+    }\n+    if (!supports_topology || result == 0) {\n+      result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n+    }\n+  }\n+  return result;\n+}\n+\n+uint VM_Version::threads_per_core() {\n+  uint result = 1;\n+  if (is_intel() && supports_processor_topology()) {\n+    result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+  } else if (is_zx() && supports_processor_topology()) {\n+    result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+  } else if (_cpuid_info.std_cpuid1_edx.bits.ht != 0) {\n+    if (cpu_family() >= 0x17) {\n+      result = _cpuid_info.ext_cpuid1E_ebx.bits.threads_per_core + 1;\n+    } else {\n+      result = _cpuid_info.std_cpuid1_ebx.bits.threads_per_cpu \/\n+                 cores_per_cpu();\n+    }\n+  }\n+  return (result == 0 ? 1 : result);\n+}\n+\n+intx VM_Version::L1_line_size() {\n+  intx result = 0;\n+  if (is_intel()) {\n+    result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n+  } else if (is_amd_family()) {\n+    result = _cpuid_info.ext_cpuid5_ecx.bits.L1_line_size;\n+  } else if (is_zx()) {\n+    result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n+  }\n+  if (result < 32) \/\/ not defined ?\n+    result = 32;   \/\/ 32 bytes by default on x86 and other x64\n+  return result;\n+}\n+\n+bool VM_Version::is_intel_tsc_synched_at_init() {\n+  if (is_intel_family_core()) {\n+    uint32_t ext_model = extended_cpu_model();\n+    if (ext_model == CPU_MODEL_NEHALEM_EP     ||\n+        ext_model == CPU_MODEL_WESTMERE_EP    ||\n+        ext_model == CPU_MODEL_SANDYBRIDGE_EP ||\n+        ext_model == CPU_MODEL_IVYBRIDGE_EP) {\n+      \/\/ <= 2-socket invariant tsc support. EX versions are usually used\n+      \/\/ in > 2-socket systems and likely don't synchronize tscs at\n+      \/\/ initialization.\n+      \/\/ Code that uses tsc values must be prepared for them to arbitrarily\n+      \/\/ jump forward or backward.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+intx VM_Version::allocate_prefetch_distance(bool use_watermark_prefetch) {\n+  \/\/ Hardware prefetching (distance\/size in bytes):\n+  \/\/ Pentium 3 -  64 \/  32\n+  \/\/ Pentium 4 - 256 \/ 128\n+  \/\/ Athlon    -  64 \/  32 ????\n+  \/\/ Opteron   - 128 \/  64 only when 2 sequential cache lines accessed\n+  \/\/ Core      - 128 \/  64\n+  \/\/\n+  \/\/ Software prefetching (distance in bytes \/ instruction with best score):\n+  \/\/ Pentium 3 - 128 \/ prefetchnta\n+  \/\/ Pentium 4 - 512 \/ prefetchnta\n+  \/\/ Athlon    - 128 \/ prefetchnta\n+  \/\/ Opteron   - 256 \/ prefetchnta\n+  \/\/ Core      - 256 \/ prefetchnta\n+  \/\/ It will be used only when AllocatePrefetchStyle > 0\n+\n+  if (is_amd_family()) { \/\/ AMD | Hygon\n+    if (supports_sse2()) {\n+      return 256; \/\/ Opteron\n+    } else {\n+      return 128; \/\/ Athlon\n+    }\n+  } else { \/\/ Intel\n+    if (supports_sse3() && cpu_family() == 6) {\n+      if (supports_sse4_2() && supports_ht()) { \/\/ Nehalem based cpus\n+        return 192;\n+      } else if (use_watermark_prefetch) { \/\/ watermark prefetching on Core\n+#ifdef _LP64\n+        return 384;\n+#else\n+        return 320;\n+#endif\n+      }\n+    }\n+    if (supports_sse2()) {\n+      if (cpu_family() == 6) {\n+        return 256; \/\/ Pentium M, Core, Core2\n+      } else {\n+        return 512; \/\/ Pentium 4\n+      }\n+    } else {\n+      return 128; \/\/ Pentium 3 (and all other old CPUs)\n+    }\n+  }\n+}\n+\n+bool VM_Version::is_intrinsic_supported(vmIntrinsicID id) {\n+  assert(id != vmIntrinsics::_none, \"must be a VM intrinsic\");\n+  switch (id) {\n+  case vmIntrinsics::_floatToFloat16:\n+  case vmIntrinsics::_float16ToFloat:\n+    if (!supports_float16()) {\n+      return false;\n+    }\n+    break;\n+  default:\n+    break;\n+  }\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":487,"deletions":78,"binary":false,"changes":565,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -92,1 +92,2 @@\n-                        : 2,\n+               f16c     : 1,\n+                        : 1,\n@@ -166,1 +167,4 @@\n-                         : 4,\n+               fxsr_opt  : 1,\n+               pdpe1gb   : 1,\n+               rdtscp    : 1,\n+                         : 1,\n@@ -222,1 +226,3 @@\n-                        : 3,\n+                        : 1,\n+             avx512ifma : 1,\n+                        : 1,\n@@ -245,1 +251,1 @@\n-                           : 1,\n+                    cet_ss : 1,\n@@ -253,1 +259,5 @@\n-                           : 17;\n+                           : 1,\n+                           : 1,\n+                     mawau : 5,\n+                     rdpid : 1,\n+                           : 9;\n@@ -263,1 +273,2 @@\n-                           : 10,\n+        fast_short_rep_mov : 1,\n+                           : 9,\n@@ -265,1 +276,1 @@\n-                           : 8,\n+                           : 5,\n@@ -267,1 +278,3 @@\n-                           : 8;\n+                           : 8,\n+                   cet_ibt : 1,\n+                           : 11;\n@@ -305,0 +318,5 @@\n+  \/*\n+   * Update following files when declaring new flags:\n+   * test\/lib-test\/jdk\/test\/whitebox\/CPUInfoTest.java\n+   * src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.amd64\/src\/jdk\/vm\/ci\/amd64\/AMD64.java\n+   *\/\n@@ -367,2 +385,11 @@\n-    decl(GFNI,              \"gfni\",              48) \/* Vector GFNI instructions *\/ \\\n-    decl(AVX512_BITALG,     \"avx512_bitalg\",     49) \/* Vector sub-word popcount and bit gather instructions *\/\\\n+    decl(RDTSCP,            \"rdtscp\",            48) \/* RDTSCP instruction *\/ \\\n+    decl(RDPID,             \"rdpid\",             49) \/* RDPID instruction *\/ \\\n+    decl(FSRM,              \"fsrm\",              50) \/* Fast Short REP MOV *\/ \\\n+    decl(GFNI,              \"gfni\",              51) \/* Vector GFNI instructions *\/ \\\n+    decl(AVX512_BITALG,     \"avx512_bitalg\",     52) \/* Vector sub-word popcount and bit gather instructions *\/\\\n+    decl(F16C,              \"f16c\",              53) \/* Half-precision and single precision FP conversion instructions*\/ \\\n+    decl(PKU,               \"pku\",               54) \/* Protection keys for user-mode pages *\/ \\\n+    decl(OSPKE,             \"ospke\",             55) \/* OS enables protection keys *\/ \\\n+    decl(CET_IBT,           \"cet_ibt\",           56) \/* Control Flow Enforcement - Indirect Branch Tracking *\/ \\\n+    decl(CET_SS,            \"cet_ss\",            57) \/* Control Flow Enforcement - Shadow Stack *\/ \\\n+    decl(AVX512_IFMA,       \"avx512_ifma\",       58) \/* Integer Vector FMA instructions*\/ \\\n@@ -503,0 +530,1 @@\n+private:\n@@ -531,194 +559,2 @@\n-  static uint64_t feature_flags() {\n-    uint64_t result = 0;\n-    if (_cpuid_info.std_cpuid1_edx.bits.cmpxchg8 != 0)\n-      result |= CPU_CX8;\n-    if (_cpuid_info.std_cpuid1_edx.bits.cmov != 0)\n-      result |= CPU_CMOV;\n-    if (_cpuid_info.std_cpuid1_edx.bits.clflush != 0)\n-      result |= CPU_FLUSH;\n-#ifdef _LP64\n-    \/\/ clflush should always be available on x86_64\n-    \/\/ if not we are in real trouble because we rely on it\n-    \/\/ to flush the code cache.\n-    assert ((result & CPU_FLUSH) != 0, \"clflush should be available\");\n-#endif\n-    if (_cpuid_info.std_cpuid1_edx.bits.fxsr != 0 || (is_amd_family() &&\n-        _cpuid_info.ext_cpuid1_edx.bits.fxsr != 0))\n-      result |= CPU_FXSR;\n-    \/\/ HT flag is set for multi-core processors also.\n-    if (threads_per_core() > 1)\n-      result |= CPU_HT;\n-    if (_cpuid_info.std_cpuid1_edx.bits.mmx != 0 || (is_amd_family() &&\n-        _cpuid_info.ext_cpuid1_edx.bits.mmx != 0))\n-      result |= CPU_MMX;\n-    if (_cpuid_info.std_cpuid1_edx.bits.sse != 0)\n-      result |= CPU_SSE;\n-    if (_cpuid_info.std_cpuid1_edx.bits.sse2 != 0)\n-      result |= CPU_SSE2;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.sse3 != 0)\n-      result |= CPU_SSE3;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.ssse3 != 0)\n-      result |= CPU_SSSE3;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.sse4_1 != 0)\n-      result |= CPU_SSE4_1;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.sse4_2 != 0)\n-      result |= CPU_SSE4_2;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.popcnt != 0)\n-      result |= CPU_POPCNT;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.avx != 0 &&\n-        _cpuid_info.std_cpuid1_ecx.bits.osxsave != 0 &&\n-        _cpuid_info.xem_xcr0_eax.bits.sse != 0 &&\n-        _cpuid_info.xem_xcr0_eax.bits.ymm != 0) {\n-      result |= CPU_AVX;\n-      result |= CPU_VZEROUPPER;\n-      if (_cpuid_info.sef_cpuid7_ebx.bits.avx2 != 0)\n-        result |= CPU_AVX2;\n-      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512f != 0 &&\n-          _cpuid_info.xem_xcr0_eax.bits.opmask != 0 &&\n-          _cpuid_info.xem_xcr0_eax.bits.zmm512 != 0 &&\n-          _cpuid_info.xem_xcr0_eax.bits.zmm32 != 0) {\n-        result |= CPU_AVX512F;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512cd != 0)\n-          result |= CPU_AVX512CD;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512dq != 0)\n-          result |= CPU_AVX512DQ;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512pf != 0)\n-          result |= CPU_AVX512PF;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512er != 0)\n-          result |= CPU_AVX512ER;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512bw != 0)\n-          result |= CPU_AVX512BW;\n-        if (_cpuid_info.sef_cpuid7_ebx.bits.avx512vl != 0)\n-          result |= CPU_AVX512VL;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpopcntdq != 0)\n-          result |= CPU_AVX512_VPOPCNTDQ;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpclmulqdq != 0)\n-          result |= CPU_AVX512_VPCLMULQDQ;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.vaes != 0)\n-          result |= CPU_AVX512_VAES;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.gfni != 0)\n-          result |= CPU_GFNI;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vnni != 0)\n-          result |= CPU_AVX512_VNNI;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_bitalg != 0)\n-          result |= CPU_AVX512_BITALG;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi != 0)\n-          result |= CPU_AVX512_VBMI;\n-        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi2 != 0)\n-          result |= CPU_AVX512_VBMI2;\n-        if (_cpuid_info.sef_cpuid7_edx.bits.avx512_fp16 != 0)\n-          result |= CPU_AVX512_FP16;\n-      }\n-    }\n-    if (_cpuid_info.std_cpuid1_ecx.bits.hv != 0)\n-      result |= CPU_HV;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.bmi1 != 0)\n-      result |= CPU_BMI1;\n-    if (_cpuid_info.std_cpuid1_edx.bits.tsc != 0)\n-      result |= CPU_TSC;\n-    if (_cpuid_info.ext_cpuid7_edx.bits.tsc_invariance != 0)\n-      result |= CPU_TSCINV_BIT;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.aes != 0)\n-      result |= CPU_AES;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.erms != 0)\n-      result |= CPU_ERMS;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.clmul != 0)\n-      result |= CPU_CLMUL;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.rtm != 0)\n-      result |= CPU_RTM;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.adx != 0)\n-       result |= CPU_ADX;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.bmi2 != 0)\n-      result |= CPU_BMI2;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.sha != 0)\n-      result |= CPU_SHA;\n-    if (_cpuid_info.std_cpuid1_ecx.bits.fma != 0)\n-      result |= CPU_FMA;\n-    if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0)\n-      result |= CPU_FLUSHOPT;\n-\n-    \/\/ AMD|Hygon features.\n-    if (is_amd_family()) {\n-      if ((_cpuid_info.ext_cpuid1_edx.bits.tdnow != 0) ||\n-          (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0))\n-        result |= CPU_3DNOW_PREFETCH;\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0)\n-        result |= CPU_LZCNT;\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.sse4a != 0)\n-        result |= CPU_SSE4A;\n-    }\n-\n-    \/\/ Intel features.\n-    if (is_intel()) {\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n-        result |= CPU_LZCNT;\n-      }\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n-        result |= CPU_3DNOW_PREFETCH;\n-      }\n-      if (_cpuid_info.sef_cpuid7_ebx.bits.clwb != 0) {\n-        result |= CPU_CLWB;\n-      }\n-      if (_cpuid_info.sef_cpuid7_edx.bits.serialize != 0)\n-        result |= CPU_SERIALIZE;\n-    }\n-\n-    \/\/ ZX features.\n-    if (is_zx()) {\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n-        result |= CPU_LZCNT;\n-      }\n-      if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n-        result |= CPU_3DNOW_PREFETCH;\n-      }\n-    }\n-\n-    \/\/ Composite features.\n-    if (supports_tscinv_bit() &&\n-        ((is_amd_family() && !is_amd_Barcelona()) ||\n-         is_intel_tsc_synched_at_init())) {\n-      result |= CPU_TSCINV;\n-    }\n-\n-    return result;\n-  }\n-\n-  static bool os_supports_avx_vectors() {\n-    bool retVal = false;\n-    int nreg = 2 LP64_ONLY(+2);\n-    if (supports_evex()) {\n-      \/\/ Verify that OS save\/restore all bits of EVEX registers\n-      \/\/ during signal processing.\n-      retVal = true;\n-      for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n-        if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n-          retVal = false;\n-          break;\n-        }\n-      }\n-    } else if (supports_avx()) {\n-      \/\/ Verify that OS save\/restore all bits of AVX registers\n-      \/\/ during signal processing.\n-      retVal = true;\n-      for (int i = 0; i < 8 * nreg; i++) { \/\/ 32 bytes per ymm register\n-        if (_cpuid_info.ymm_save[i] != ymm_test_value()) {\n-          retVal = false;\n-          break;\n-        }\n-      }\n-      \/\/ zmm_save will be set on a EVEX enabled machine even if we choose AVX code gen\n-      if (retVal == false) {\n-        \/\/ Verify that OS save\/restore all bits of EVEX registers\n-        \/\/ during signal processing.\n-        retVal = true;\n-        for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n-          if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n-            retVal = false;\n-            break;\n-          }\n-        }\n-      }\n-    }\n-    return retVal;\n-  }\n-\n+  static uint64_t feature_flags();\n+  static bool os_supports_avx_vectors();\n@@ -758,1 +594,0 @@\n-\n@@ -802,56 +637,3 @@\n-  static uint cores_per_cpu()  {\n-    uint result = 1;\n-    if (is_intel()) {\n-      bool supports_topology = supports_processor_topology();\n-      if (supports_topology) {\n-        result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n-                 _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-      }\n-      if (!supports_topology || result == 0) {\n-        result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n-      }\n-    } else if (is_amd_family()) {\n-      result = (_cpuid_info.ext_cpuid8_ecx.bits.cores_per_cpu + 1);\n-    } else if (is_zx()) {\n-      bool supports_topology = supports_processor_topology();\n-      if (supports_topology) {\n-        result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n-                 _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-      }\n-      if (!supports_topology || result == 0) {\n-        result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n-      }\n-    }\n-    return result;\n-  }\n-\n-  static uint threads_per_core()  {\n-    uint result = 1;\n-    if (is_intel() && supports_processor_topology()) {\n-      result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-    } else if (is_zx() && supports_processor_topology()) {\n-      result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n-    } else if (_cpuid_info.std_cpuid1_edx.bits.ht != 0) {\n-      if (cpu_family() >= 0x17) {\n-        result = _cpuid_info.ext_cpuid1E_ebx.bits.threads_per_core + 1;\n-      } else {\n-        result = _cpuid_info.std_cpuid1_ebx.bits.threads_per_cpu \/\n-                 cores_per_cpu();\n-      }\n-    }\n-    return (result == 0 ? 1 : result);\n-  }\n-\n-  static intx L1_line_size()  {\n-    intx result = 0;\n-    if (is_intel()) {\n-      result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n-    } else if (is_amd_family()) {\n-      result = _cpuid_info.ext_cpuid5_ecx.bits.L1_line_size;\n-    } else if (is_zx()) {\n-      result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n-    }\n-    if (result < 32) \/\/ not defined ?\n-      result = 32;   \/\/ 32 bytes by default on x86 and other x64\n-    return result;\n-  }\n+  static uint cores_per_cpu();\n+  static uint threads_per_core();\n+  static intx L1_line_size();\n@@ -882,0 +664,2 @@\n+  static bool supports_rdtscp()       { return (_features & CPU_RDTSCP) != 0; }\n+  static bool supports_rdpid()        { return (_features & CPU_RDPID) != 0; }\n@@ -884,0 +668,1 @@\n+  static bool supports_fsrm()         { return (_features & CPU_FSRM) != 0; }\n@@ -891,0 +676,1 @@\n+  static bool supports_avx512ifma()   { return (_features & CPU_AVX512_IFMA) != 0; }\n@@ -918,0 +704,5 @@\n+  static bool supports_f16c()         { return (_features & CPU_F16C) != 0; }\n+  static bool supports_pku()          { return (_features & CPU_PKU) != 0; }\n+  static bool supports_ospke()        { return (_features & CPU_OSPKE) != 0; }\n+  static bool supports_cet_ss()       { return (_features & CPU_CET_SS) != 0; }\n+  static bool supports_cet_ibt()      { return (_features & CPU_CET_IBT) != 0; }\n@@ -927,0 +718,5 @@\n+#ifdef COMPILER2\n+  \/\/ Determine if it's running on Cascade Lake using default options.\n+  static bool is_default_intel_cascade_lake();\n+#endif\n+\n@@ -929,17 +725,1 @@\n-  static bool is_intel_tsc_synched_at_init()  {\n-    if (is_intel_family_core()) {\n-      uint32_t ext_model = extended_cpu_model();\n-      if (ext_model == CPU_MODEL_NEHALEM_EP     ||\n-          ext_model == CPU_MODEL_WESTMERE_EP    ||\n-          ext_model == CPU_MODEL_SANDYBRIDGE_EP ||\n-          ext_model == CPU_MODEL_IVYBRIDGE_EP) {\n-        \/\/ <= 2-socket invariant tsc support. EX versions are usually used\n-        \/\/ in > 2-socket systems and likely don't synchronize tscs at\n-        \/\/ initialization.\n-        \/\/ Code that uses tsc values must be prepared for them to arbitrarily\n-        \/\/ jump forward or backward.\n-        return true;\n-      }\n-    }\n-    return false;\n-  }\n+  static bool is_intel_tsc_synched_at_init();\n@@ -975,45 +755,1 @@\n-  static intx allocate_prefetch_distance(bool use_watermark_prefetch) {\n-    \/\/ Hardware prefetching (distance\/size in bytes):\n-    \/\/ Pentium 3 -  64 \/  32\n-    \/\/ Pentium 4 - 256 \/ 128\n-    \/\/ Athlon    -  64 \/  32 ????\n-    \/\/ Opteron   - 128 \/  64 only when 2 sequential cache lines accessed\n-    \/\/ Core      - 128 \/  64\n-    \/\/\n-    \/\/ Software prefetching (distance in bytes \/ instruction with best score):\n-    \/\/ Pentium 3 - 128 \/ prefetchnta\n-    \/\/ Pentium 4 - 512 \/ prefetchnta\n-    \/\/ Athlon    - 128 \/ prefetchnta\n-    \/\/ Opteron   - 256 \/ prefetchnta\n-    \/\/ Core      - 256 \/ prefetchnta\n-    \/\/ It will be used only when AllocatePrefetchStyle > 0\n-\n-    if (is_amd_family()) { \/\/ AMD | Hygon\n-      if (supports_sse2()) {\n-        return 256; \/\/ Opteron\n-      } else {\n-        return 128; \/\/ Athlon\n-      }\n-    } else { \/\/ Intel\n-      if (supports_sse3() && cpu_family() == 6) {\n-        if (supports_sse4_2() && supports_ht()) { \/\/ Nehalem based cpus\n-          return 192;\n-        } else if (use_watermark_prefetch) { \/\/ watermark prefetching on Core\n-#ifdef _LP64\n-          return 384;\n-#else\n-          return 320;\n-#endif\n-        }\n-      }\n-      if (supports_sse2()) {\n-        if (cpu_family() == 6) {\n-          return 256; \/\/ Pentium M, Core, Core2\n-        } else {\n-          return 512; \/\/ Pentium 4\n-        }\n-      } else {\n-        return 128; \/\/ Pentium 3 (and all other old CPUs)\n-      }\n-    }\n-  }\n+  static intx allocate_prefetch_distance(bool use_watermark_prefetch);\n@@ -1035,0 +771,8 @@\n+  \/\/ For AVX CPUs only. f16c support is disabled if UseAVX == 0.\n+  static bool supports_float16() {\n+    return supports_f16c() || supports_avx512vl();\n+  }\n+\n+  \/\/ Check intrinsic support\n+  static bool is_intrinsic_supported(vmIntrinsicID id);\n+\n@@ -1052,1 +796,0 @@\n-\n@@ -1057,0 +800,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":69,"deletions":325,"binary":false,"changes":394,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1346,1 +1346,1 @@\n-  __ pushptr(here.addr());\n+  __ pushptr(here.addr(), noreg);\n@@ -1467,0 +1467,6 @@\n+    case Op_IsInfiniteF:\n+    case Op_IsInfiniteD:\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n@@ -1479,0 +1485,1 @@\n+    case Op_VectorMaskCast:\n@@ -1483,0 +1490,5 @@\n+    case Op_PopulateIndex:\n+      if (!is_LP64 || (UseAVX < 2)) {\n+        return false;\n+      }\n+      break;\n@@ -1583,2 +1595,0 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n@@ -1630,0 +1640,10 @@\n+    case Op_CompressBits:\n+      if (!VM_Version::supports_bmi2() || (!is_LP64 && UseSSE < 2)) {\n+        return false;\n+      }\n+      break;\n+    case Op_ExpandBits:\n+      if (!VM_Version::supports_bmi2() || (!is_LP64 && (UseSSE < 2 || !VM_Version::supports_bmi1()))) {\n+        return false;\n+      }\n+      break;\n@@ -1666,0 +1686,12 @@\n+    case Op_ConvF2HF:\n+    case Op_ConvHF2F:\n+      if (!VM_Version::supports_float16()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      break;\n@@ -1677,0 +1709,4 @@\n+const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n@@ -1709,1 +1745,0 @@\n-    case Op_MulVL:\n@@ -1733,2 +1768,0 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n@@ -1742,0 +1775,6 @@\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n+        return false;\n+      }\n+      break;\n@@ -1816,2 +1855,0 @@\n-      } else if (size_in_bits == 512 && (VM_Version::supports_avx512bw() == false)) {\n-        return false; \/\/ Implementation limitation\n@@ -1826,4 +1863,0 @@\n-      } else if (bt == T_BYTE && size_in_bits > 256 && !VM_Version::supports_avx512_vbmi())  {\n-        return false; \/\/ Implementation limitation\n-      } else if (bt == T_SHORT && size_in_bits > 256 && !VM_Version::supports_avx512bw())  {\n-        return false; \/\/ Implementation limitation\n@@ -1833,0 +1866,1 @@\n+    case Op_VectorMaskCast:\n@@ -1842,0 +1876,5 @@\n+    case Op_PopulateIndex:\n+      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n@@ -1856,3 +1895,8 @@\n-    case Op_VectorCastD2X:\n-      if (is_subword_type(bt) || bt == T_INT) {\n-        return false;\n+    case Op_VectorCastF2X: {\n+        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n+        \/\/ happen after intermediate conversion to integer and special handling\n+        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n+        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n+        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n+          return false;\n+        }\n@@ -1860,0 +1904,2 @@\n+      \/\/ fallthrough\n+    case Op_VectorCastD2X:\n@@ -1869,5 +1915,0 @@\n-    case Op_VectorCastF2X:\n-      if (is_subword_type(bt) || bt == T_LONG) {\n-        return false;\n-      }\n-      break;\n@@ -1934,0 +1975,6 @@\n+    case Op_SignumVD:\n+    case Op_SignumVF:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      break;\n@@ -2127,1 +2174,1 @@\n-      if ((bt == T_INT || bt == T_LONG) && VM_Version::supports_avx512cd()) {\n+      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n@@ -2136,0 +2183,17 @@\n+const bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  return false;\n+}\n+\n+\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n+const bool Matcher::vector_needs_load_shuffle(BasicType elem_bt, int vlen) {\n+  switch (elem_bt) {\n+    case T_BYTE:  return false;\n+    case T_SHORT: return !VM_Version::supports_avx512bw();\n+    case T_INT:   return !VM_Version::supports_avx();\n+    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+\n@@ -2261,0 +2325,9 @@\n+const int Matcher::superword_max_vector_size(const BasicType bt) {\n+  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n+  \/\/ by default on Cascade Lake\n+  if (VM_Version::is_default_intel_cascade_lake()) {\n+    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n+  }\n+  return Matcher::max_vector_size(bt);\n+}\n+\n@@ -2744,0 +2817,1 @@\n+    C2_MacroAssembler _masm(&cbuf);\n@@ -2747,1 +2821,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -3624,0 +3697,73 @@\n+instruct convF2HF_reg_reg(rRegI dst, regF src, regF tmp) %{\n+  effect(TEMP tmp);\n+  match(Set dst (ConvF2HF src));\n+  ins_cost(125);\n+  format %{ \"vcvtps2ph $dst,$src \\t using $tmp as TEMP\"%}\n+  ins_encode %{\n+    __ flt_to_flt16($dst$$Register, $src$$XMMRegister, $tmp$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct convF2HF_mem_reg(memory mem, regF src, kReg ktmp, rRegI rtmp) %{\n+  predicate((UseAVX > 2) && VM_Version::supports_avx512vl());\n+  effect(TEMP ktmp, TEMP rtmp);\n+  match(Set mem (StoreC mem (ConvF2HF src)));\n+  format %{ \"evcvtps2ph $mem,$src \\t using $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    __ movl($rtmp$$Register, 0x1);\n+    __ kmovwl($ktmp$$KRegister, $rtmp$$Register);\n+    __ evcvtps2ph($mem$$Address, $ktmp$$KRegister, $src$$XMMRegister, 0x04, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vconvF2HF(vec dst, vec src) %{\n+  match(Set dst (VectorCastF2HF src));\n+  format %{ \"vector_conv_F2HF $dst $src\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    __ vcvtps2ph($dst$$XMMRegister, $src$$XMMRegister, 0x04, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vconvF2HF_mem_reg(memory mem, vec src) %{\n+  match(Set mem (StoreVector mem (VectorCastF2HF src)));\n+  format %{ \"vcvtps2ph $mem,$src\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    __ vcvtps2ph($mem$$Address, $src$$XMMRegister, 0x04, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct convHF2F_reg_reg(regF dst, rRegI src) %{\n+  match(Set dst (ConvHF2F src));\n+  format %{ \"vcvtph2ps $dst,$src\" %}\n+  ins_encode %{\n+    __ flt16_to_flt($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vconvHF2F_reg_mem(vec dst, memory mem) %{\n+  match(Set dst (VectorCastHF2F (LoadVector mem)));\n+  format %{ \"vcvtph2ps $dst,$mem\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vcvtph2ps($dst$$XMMRegister, $mem$$Address, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vconvHF2F(vec dst, vec src) %{\n+  match(Set dst (VectorCastHF2F src));\n+  ins_cost(125);\n+  format %{ \"vector_conv_HF2F $dst,$src\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vcvtph2ps($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3712,1 +3858,1 @@\n-instruct reinterpret_expand(vec dst, vec src, rRegP scratch) %{\n+instruct reinterpret_expand(vec dst, vec src) %{\n@@ -3717,2 +3863,2 @@\n-  effect(TEMP dst, TEMP scratch);\n-  format %{ \"vector_reinterpret_expand $dst,$src\\t! using $scratch as TEMP\" %}\n+  effect(TEMP dst);\n+  format %{ \"vector_reinterpret_expand $dst,$src\" %}\n@@ -3725,1 +3871,1 @@\n-      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), $scratch$$Register);\n+      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), noreg);\n@@ -3728,1 +3874,1 @@\n-      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), $scratch$$Register);\n+      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), noreg);\n@@ -3735,1 +3881,1 @@\n-instruct vreinterpret_expand4(legVec dst, vec src, rRegP scratch) %{\n+instruct vreinterpret_expand4(legVec dst, vec src) %{\n@@ -3742,2 +3888,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_reinterpret_expand $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_reinterpret_expand $dst,$src\" %}\n@@ -3745,1 +3890,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, noreg);\n@@ -3813,1 +3958,1 @@\n-instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{\n+instruct roundD_imm(legRegD dst, immD con, immU8 rmode) %{\n@@ -3815,1 +3960,0 @@\n-  effect(TEMP scratch_reg);\n@@ -3820,1 +3964,1 @@\n-    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);\n+    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, noreg);\n@@ -3981,1 +4125,1 @@\n-      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()));\n+      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -3983,1 +4127,1 @@\n-      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()));\n+      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4004,1 +4148,1 @@\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4045,1 +4189,1 @@\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4073,1 +4217,2 @@\n-instruct ReplB_reg(vec dst, rRegI src) %{\n+instruct vReplB_reg(vec dst, rRegI src) %{\n+  predicate(UseAVX >= 2);\n@@ -4078,0 +4223,1 @@\n+    int vlen_enc = vector_length_encoding(this);\n@@ -4080,5 +4226,0 @@\n-      int vlen_enc = vector_length_encoding(this);\n-    } else if (VM_Version::supports_avx2()) {\n-      int vlen_enc = vector_length_encoding(this);\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -4088,9 +4229,1 @@\n-      __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n-      if (vlen >= 16) {\n-        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-        if (vlen >= 32) {\n-          assert(vlen == 32, \"sanity\");\n-          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-        }\n-      }\n+      __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -4102,4 +4235,4 @@\n-instruct ReplB_mem(vec dst, memory mem) %{\n-  predicate(VM_Version::supports_avx2());\n-  match(Set dst (ReplicateB (LoadB mem)));\n-  format %{ \"replicateB $dst,$mem\" %}\n+instruct ReplB_reg(vec dst, rRegI src) %{\n+  predicate(UseAVX < 2);\n+  match(Set dst (ReplicateB src));\n+  format %{ \"replicateB $dst,$src\" %}\n@@ -4107,2 +4240,8 @@\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vlen_enc);\n+    uint vlen = Matcher::vector_length(this);\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);\n+    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n+    if (vlen >= 16) {\n+      assert(vlen == 16, \"\");\n+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n@@ -4113,3 +4252,4 @@\n-instruct ReplB_imm(vec dst, immI con) %{\n-  match(Set dst (ReplicateB con));\n-  format %{ \"replicateB $dst,$con\" %}\n+instruct ReplB_mem(vec dst, memory mem) %{\n+  predicate(UseAVX >= 2);\n+  match(Set dst (ReplicateB (LoadB mem)));\n+  format %{ \"replicateB $dst,$mem\" %}\n@@ -4117,2 +4257,2 @@\n-    InternalAddress addr = $constantaddress(T_BYTE, vreplicate_imm(T_BYTE, $con$$constant, Matcher::vector_length(this)));\n-    __ load_vector($dst$$XMMRegister, addr, Matcher::vector_length_in_bytes(this));\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -4125,1 +4265,2 @@\n-instruct ReplS_reg(vec dst, rRegI src) %{\n+instruct vReplS_reg(vec dst, rRegI src) %{\n+  predicate(UseAVX >= 2);\n@@ -4130,0 +4271,1 @@\n+    int vlen_enc = vector_length_encoding(this);\n@@ -4132,5 +4274,0 @@\n-      int vlen_enc = vector_length_encoding(this);\n-    } else if (VM_Version::supports_avx2()) {\n-      int vlen_enc = vector_length_encoding(this);\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -4140,8 +4277,1 @@\n-      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n-      if (vlen >= 8) {\n-        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-        if (vlen >= 16) {\n-          assert(vlen == 16, \"sanity\");\n-          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-        }\n-      }\n+      __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -4153,4 +4283,4 @@\n-instruct ReplS_mem(vec dst, memory mem) %{\n-  predicate(VM_Version::supports_avx2());\n-  match(Set dst (ReplicateS (LoadS mem)));\n-  format %{ \"replicateS $dst,$mem\" %}\n+instruct ReplS_reg(vec dst, rRegI src) %{\n+  predicate(UseAVX < 2);\n+  match(Set dst (ReplicateS src));\n+  format %{ \"replicateS $dst,$src\" %}\n@@ -4158,0 +4288,1 @@\n+    uint vlen = Matcher::vector_length(this);\n@@ -4159,1 +4290,6 @@\n-    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n+    if (vlen >= 8) {\n+      assert(vlen == 8, \"\");\n+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n@@ -4164,3 +4300,4 @@\n-instruct ReplS_imm(vec dst, immI con) %{\n-  match(Set dst (ReplicateS con));\n-  format %{ \"replicateS $dst,$con\" %}\n+instruct ReplS_mem(vec dst, memory mem) %{\n+  predicate(UseAVX >= 2);\n+  match(Set dst (ReplicateS (LoadS mem)));\n+  format %{ \"replicateS $dst,$mem\" %}\n@@ -4168,2 +4305,2 @@\n-    InternalAddress addr = $constantaddress(T_SHORT, vreplicate_imm(T_SHORT, $con$$constant, Matcher::vector_length(this)));\n-    __ load_vector($dst$$XMMRegister, addr, Matcher::vector_length_in_bytes(this));\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -4181,0 +4318,1 @@\n+    int vlen_enc = vector_length_encoding(this);\n@@ -4182,1 +4320,0 @@\n-      int vlen_enc = vector_length_encoding(this);\n@@ -4185,1 +4322,0 @@\n-      int vlen_enc = vector_length_encoding(this);\n@@ -4191,4 +4327,0 @@\n-      if (vlen >= 8) {\n-        assert(vlen == 8, \"sanity\");\n-        __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-      }\n@@ -4204,2 +4336,6 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen <= 4) {\n+    int vlen_enc = vector_length_encoding(this);\n+    if (VM_Version::supports_avx2()) {\n+      __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vlen_enc);\n+    } else if (VM_Version::supports_avx()) {\n+      __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vlen_enc);\n+    } else {\n@@ -4208,4 +4344,0 @@\n-    } else {\n-      assert(VM_Version::supports_avx2(), \"sanity\");\n-      int vlen_enc = vector_length_encoding(this);\n-      __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -4218,0 +4350,2 @@\n+  match(Set dst (ReplicateB con));\n+  match(Set dst (ReplicateS con));\n@@ -4221,2 +4355,7 @@\n-    InternalAddress addr = $constantaddress(T_INT, vreplicate_imm(T_INT, $con$$constant, Matcher::vector_length(this)));\n-    __ load_vector($dst$$XMMRegister, addr, Matcher::vector_length_in_bytes(this));\n+    InternalAddress addr = $constantaddress(Matcher::vector_element_basic_type(this),\n+        vreplicate_imm(Matcher::vector_element_basic_type(this), $con$$constant,\n+            (VM_Version::supports_sse3() ? (VM_Version::supports_avx() ? 4 : 8) : 8) \/\n+                type2aelembytes(Matcher::vector_element_basic_type(this))));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    __ load_constant_vector(bt, $dst$$XMMRegister, addr, vlen);\n@@ -4234,5 +4373,2 @@\n-    uint vsize = Matcher::vector_length_in_bytes(this);\n-    if (vsize <= 16) {\n-      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n-    } else {\n-      int vlen_enc = vector_length_encoding(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vl()) {\n@@ -4240,0 +4376,2 @@\n+    } else {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n@@ -4246,1 +4384,1 @@\n-  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) >= 16);\n+  predicate(UseSSE >= 2);\n@@ -4250,1 +4388,0 @@\n-  effect(TEMP dst);\n@@ -4267,6 +4404,3 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ movdq($dst$$XMMRegister, $src$$Register);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { \/\/ AVX512VL for <512bit operands\n-      int vlen_enc = vector_length_encoding(this);\n+    int vlen = Matcher::vector_length(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    if (vlen == 8 || VM_Version::supports_avx512vl()) { \/\/ AVX512VL for <512bit operands\n@@ -4275,2 +4409,0 @@\n-      assert(vlen == 4, \"sanity\");\n-      int vlen_enc = vector_length_encoding(this);\n@@ -4280,1 +4412,0 @@\n-      assert(vlen == 4, \"sanity\");\n@@ -4283,1 +4414,0 @@\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n@@ -4348,2 +4478,6 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n+    int vlen_enc = vector_length_encoding(this);\n+    if (VM_Version::supports_avx2()) {\n+      __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);\n+    } else if (VM_Version::supports_sse3()) {\n+      __ movddup($dst$$XMMRegister, $mem$$Address);\n+    } else {\n@@ -4352,4 +4486,0 @@\n-    } else {\n-      assert(VM_Version::supports_avx2(), \"sanity\");\n-      int vlen_enc = vector_length_encoding(this);\n-      __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -4366,2 +4496,3 @@\n-    InternalAddress addr = $constantaddress(T_LONG, vreplicate_imm(T_LONG, $con$$constant, Matcher::vector_length(this)));\n-    __ load_vector($dst$$XMMRegister, addr, Matcher::vector_length_in_bytes(this));\n+    InternalAddress addr = $constantaddress(T_LONG, vreplicate_imm(T_LONG, $con$$constant, 1));\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    __ load_constant_vector(T_LONG, $dst$$XMMRegister, addr, vlen);\n@@ -4376,5 +4507,2 @@\n-    int vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n-    } else {\n-      int vlen_enc = vector_length_encoding(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vl()) {\n@@ -4382,0 +4510,2 @@\n+    } else {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n@@ -4388,1 +4518,1 @@\n-  predicate(UseAVX > 0);\n+  predicate(UseSSE >= 2);\n@@ -4390,1 +4520,0 @@\n-  effect(TEMP dst);\n@@ -4401,1 +4530,2 @@\n-instruct ReplF_reg(vec dst, vlRegF src) %{\n+instruct vReplF_reg(vec dst, vlRegF src) %{\n+  predicate(UseAVX > 0);\n@@ -4406,0 +4536,1 @@\n+    int vlen_enc = vector_length_encoding(this);\n@@ -4407,3 +4538,2 @@\n-      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);\n-   } else if (VM_Version::supports_avx2()) {\n-      int vlen_enc = vector_length_encoding(this);\n+      __ vpermilps($dst$$XMMRegister, $src$$XMMRegister, 0x00, Assembler::AVX_128bit);\n+    } else if (VM_Version::supports_avx2()) {\n@@ -4413,1 +4543,1 @@\n-      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);\n+      __ vpermilps($dst$$XMMRegister, $src$$XMMRegister, 0x00, Assembler::AVX_128bit);\n@@ -4420,0 +4550,10 @@\n+instruct ReplF_reg(vec dst, vlRegF src) %{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ReplicateF src));\n+  format %{ \"replicateF $dst,$src\" %}\n+  ins_encode %{\n+    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -4421,0 +4561,1 @@\n+  predicate(UseAVX > 0);\n@@ -4424,9 +4565,2 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen <= 4) {\n-      __ movdl($dst$$XMMRegister, $mem$$Address);\n-      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n-    } else {\n-      assert(VM_Version::supports_avx(), \"sanity\");\n-      int vlen_enc = vector_length_encoding(this);\n-      __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vlen_enc);\n-    }\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -4442,2 +4576,4 @@\n-    InternalAddress addr = $constantaddress(T_FLOAT, vreplicate_imm(T_FLOAT, $con$$constant, Matcher::vector_length(this)));\n-    __ load_vector($dst$$XMMRegister, addr, Matcher::vector_length_in_bytes(this));\n+    InternalAddress addr = $constantaddress(T_FLOAT, vreplicate_imm(T_FLOAT, $con$$constant,\n+        VM_Version::supports_sse3() ? (VM_Version::supports_avx() ? 1 : 2) : 2));\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    __ load_constant_vector(T_FLOAT, $dst$$XMMRegister, addr, vlen);\n@@ -4452,3 +4588,3 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen <= 4) {\n-      __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n+    int vlen_enc = vector_length_encoding(this);\n+    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vldq()) {\n+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -4456,2 +4592,1 @@\n-      int vlen_enc = vector_length_encoding(this);\n-      __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); \/\/ 512bit vxorps requires AVX512DQ\n+      __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n@@ -4466,1 +4601,2 @@\n-instruct ReplD_reg(vec dst, vlRegD src) %{\n+instruct vReplD_reg(vec dst, vlRegD src) %{\n+  predicate(UseSSE >= 3);\n@@ -4471,2 +4607,3 @@\n-    if (vlen == 2) {\n-      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);\n+    int vlen_enc = vector_length_encoding(this);\n+    if (vlen <= 2) {\n+      __ movddup($dst$$XMMRegister, $src$$XMMRegister);\n@@ -4474,1 +4611,0 @@\n-      int vlen_enc = vector_length_encoding(this);\n@@ -4478,1 +4614,1 @@\n-      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);\n+      __ movddup($dst$$XMMRegister, $src$$XMMRegister);\n@@ -4485,3 +4621,4 @@\n-instruct ReplD_mem(vec dst, memory mem) %{\n-  match(Set dst (ReplicateD (LoadD mem)));\n-  format %{ \"replicateD $dst,$mem\" %}\n+instruct ReplD_reg(vec dst, vlRegD src) %{\n+  predicate(UseSSE < 3);\n+  match(Set dst (ReplicateD src));\n+  format %{ \"replicateD $dst,$src\" %}\n@@ -4489,6 +4626,11 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ movq($dst$$XMMRegister, $mem$$Address);\n-      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);\n-    } else {\n-      assert(VM_Version::supports_avx(), \"sanity\");\n+    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct ReplD_mem(vec dst, memory mem) %{\n+  predicate(UseSSE >= 3);\n+  match(Set dst (ReplicateD (LoadD mem)));\n+  format %{ \"replicateD $dst,$mem\" %}\n+  ins_encode %{\n+    if (Matcher::vector_length(this) >= 4) {\n@@ -4497,0 +4639,2 @@\n+    } else {\n+      __ movddup($dst$$XMMRegister, $mem$$Address);\n@@ -4507,2 +4651,3 @@\n-    InternalAddress addr = $constantaddress(T_DOUBLE, vreplicate_imm(T_DOUBLE, $con$$constant, Matcher::vector_length(this)));\n-    __ load_vector($dst$$XMMRegister, addr, Matcher::vector_length_in_bytes(this));\n+    InternalAddress addr = $constantaddress(T_DOUBLE, vreplicate_imm(T_DOUBLE, $con$$constant, 1));\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    __ load_constant_vector(T_DOUBLE, $dst$$XMMRegister, addr, vlen);\n@@ -4517,3 +4662,3 @@\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);\n+    int vlen_enc = vector_length_encoding(this);\n+    if (VM_Version::supports_evex() && !VM_Version::supports_avx512vldq()) {\n+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -4521,2 +4666,1 @@\n-      int vlen_enc = vector_length_encoding(this);\n-      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); \/\/ 512bit vxorps requires AVX512DQ\n+      __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n@@ -5616,3 +5760,2 @@\n-instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{\n-  predicate(Matcher::vector_length(n) == 4 ||\n-            Matcher::vector_length(n) == 8);\n+instruct vmul8B(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 8);\n@@ -5620,2 +5763,2 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVB   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5624,5 +5767,5 @@\n-    __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);\n-    __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n-    __ pand($dst$$XMMRegister, $tmp$$XMMRegister);\n+    __ pmovsxbw($dst$$XMMRegister, $src1$$XMMRegister);\n+    __ pmovsxbw($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllw($dst$$XMMRegister, 8);\n+    __ psrlw($dst$$XMMRegister, 8);\n@@ -5634,2 +5777,2 @@\n-instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n-  predicate(Matcher::vector_length(n) == 16 && UseAVX <= 1);\n+instruct vmulB(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(UseAVX == 0 && Matcher::vector_length_in_bytes(n) > 8);\n@@ -5637,2 +5780,2 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVB   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5641,56 +5784,14 @@\n-    __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);\n-    __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);\n-    __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);\n-    __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);\n-    __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);\n-    __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n-    __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);\n-    __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);\n-    __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{\n-  predicate(Matcher::vector_length(n) == 16 && UseAVX > 1);\n-  match(Set dst (MulVB src1 src2));\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n-  ins_encode %{\n-  int vlen_enc = Assembler::AVX_256bit;\n-    __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n-  predicate(Matcher::vector_length(n) == 32);\n-  match(Set dst (MulVB src1 src2));\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n-  ins_encode %{\n-    assert(UseAVX > 1, \"required\");\n-    int vlen_enc = Assembler::AVX_256bit;\n-    __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);\n-    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vlen_enc);\n+    \/\/ Odd-index elements\n+    __ movdqu($dst$$XMMRegister, $src1$$XMMRegister);\n+    __ psrlw($dst$$XMMRegister, 8);\n+    __ movdqu($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ psrlw($xtmp$$XMMRegister, 8);\n+    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllw($dst$$XMMRegister, 8);\n+    \/\/ Even-index elements\n+    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pmullw($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ psllw($xtmp$$XMMRegister, 8);\n+    __ psrlw($xtmp$$XMMRegister, 8);\n+    \/\/ Combine\n+    __ por($dst$$XMMRegister, $xtmp$$XMMRegister);\n@@ -5701,2 +5802,2 @@\n-instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n-  predicate(Matcher::vector_length(n) == 64);\n+instruct vmulB_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{\n+  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) > 8);\n@@ -5704,2 +5805,2 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n-  format %{\"vector_mulB $dst,$src1,$src2\\n\\t\" %}\n+  effect(TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vmulVB  $dst, $src1, $src2\\t! using $xtmp1, $xtmp2 as TEMP\" %}\n@@ -5707,17 +5808,12 @@\n-    assert(UseAVX > 2, \"required\");\n-    int vlen_enc = Assembler::AVX_512bit;\n-    __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, $scratch$$Register);\n-    __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this);\n+    \/\/ Odd-index elements\n+    __ vpsrlw($xtmp2$$XMMRegister, $src1$$XMMRegister, 8, vlen_enc);\n+    __ vpsrlw($xtmp1$$XMMRegister, $src2$$XMMRegister, 8, vlen_enc);\n+    __ vpmullw($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n+    __ vpsllw($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 8, vlen_enc);\n+    \/\/ Even-index elements\n+    __ vpmullw($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ vpsllw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);\n+    __ vpsrlw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);\n+    \/\/ Combine\n+    __ vpor($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n@@ -5732,1 +5828,1 @@\n-  format %{ \"pmullw $dst,$src\\t! mul packedS\" %}\n+  format %{ \"pmullw  $dst,$src\\t! mul packedS\" %}\n@@ -5798,2 +5894,4 @@\n-instruct vmulL_reg(vec dst, vec src1, vec src2) %{\n-  predicate(VM_Version::supports_avx512dq());\n+instruct evmulL_reg(vec dst, vec src1, vec src2) %{\n+  predicate((Matcher::vector_length_in_bytes(n) == 64 &&\n+             VM_Version::supports_avx512dq()) ||\n+            VM_Version::supports_avx512vldq());\n@@ -5801,1 +5899,1 @@\n-  format %{ \"vpmullq $dst,$src1,$src2\\t! mul packedL\" %}\n+  format %{ \"evpmullq $dst,$src1,$src2\\t! mul packedL\" %}\n@@ -5805,1 +5903,1 @@\n-    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ evpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n@@ -5810,3 +5908,5 @@\n-instruct vmulL_mem(vec dst, vec src, memory mem) %{\n-  predicate(VM_Version::supports_avx512dq() &&\n-              (Matcher::vector_length_in_bytes(n->in(1)) > 8));\n+instruct evmulL_mem(vec dst, vec src, memory mem) %{\n+  predicate((Matcher::vector_length_in_bytes(n) == 64 &&\n+             VM_Version::supports_avx512dq()) ||\n+            (Matcher::vector_length_in_bytes(n) > 8 &&\n+             VM_Version::supports_avx512vldq()));\n@@ -5814,1 +5914,1 @@\n-  format %{ \"vpmullq $dst,$src,$mem\\t! mul packedL\" %}\n+  format %{ \"evpmullq $dst,$src,$mem\\t! mul packedL\" %}\n@@ -5818,1 +5918,1 @@\n-    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);\n+    __ evpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -5823,12 +5923,5 @@\n-instruct mul2L_reg(vec dst, vec src2, legVec tmp) %{\n-  predicate(Matcher::vector_length(n) == 2 && !VM_Version::supports_avx512dq());\n-  match(Set dst (MulVL dst src2));\n-  effect(TEMP dst, TEMP tmp);\n-  format %{ \"pshufd $tmp,$src2, 177\\n\\t\"\n-            \"pmulld $tmp,$dst\\n\\t\"\n-            \"phaddd $tmp,$tmp\\n\\t\"\n-            \"pmovzxdq $tmp,$tmp\\n\\t\"\n-            \"psllq $tmp, 32\\n\\t\"\n-            \"pmuludq $dst,$src2\\n\\t\"\n-            \"paddq $dst,$tmp\\n\\t! mul packed2L\" %}\n-\n+instruct vmulL(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(UseAVX == 0);\n+  match(Set dst (MulVL src1 src2));\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVL   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5837,8 +5930,10 @@\n-    int vlen_enc = Assembler::AVX_128bit;\n-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 177);\n-    __ pmulld($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);\n-    __ pmovzxdq($tmp$$XMMRegister, $tmp$$XMMRegister);\n-    __ psllq($tmp$$XMMRegister, 32);\n-    __ pmuludq($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ paddq($dst$$XMMRegister, $tmp$$XMMRegister);\n+    \/\/ Get the lo-hi products, only the lower 32 bits is in concerns\n+    __ pshufd($xtmp$$XMMRegister, $src2$$XMMRegister, 0xB1);\n+    __ pmulld($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pshufd($dst$$XMMRegister, $xtmp$$XMMRegister, 0xB1);\n+    __ paddd($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllq($dst$$XMMRegister, 32);\n+    \/\/ Get the lo-lo products\n+    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pmuludq($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ paddq($dst$$XMMRegister, $xtmp$$XMMRegister);\n@@ -5849,2 +5944,6 @@\n-instruct vmul4L_reg_avx(vec dst, vec src1, vec src2, legVec tmp, legVec tmp1) %{\n-  predicate(Matcher::vector_length(n) == 4 && !VM_Version::supports_avx512dq());\n+instruct vmulL_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{\n+  predicate(UseAVX > 0 &&\n+            ((Matcher::vector_length_in_bytes(n) == 64 &&\n+              !VM_Version::supports_avx512dq()) ||\n+             (Matcher::vector_length_in_bytes(n) < 64 &&\n+              !VM_Version::supports_avx512vldq())));\n@@ -5852,8 +5951,2 @@\n-  effect(TEMP tmp1, TEMP tmp);\n-  format %{ \"vpshufd $tmp,$src2\\n\\t\"\n-            \"vpmulld $tmp,$src1,$tmp\\n\\t\"\n-            \"vphaddd $tmp,$tmp,$tmp\\n\\t\"\n-            \"vpmovzxdq $tmp,$tmp\\n\\t\"\n-            \"vpsllq $tmp,$tmp\\n\\t\"\n-            \"vpmuludq $tmp1,$src1,$src2\\n\\t\"\n-            \"vpaddq $dst,$tmp,$tmp1\\t! mul packed4L\" %}\n+  effect(TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vmulVL  $dst, $src1, $src2\\t! using $xtmp1, $xtmp2 as TEMP\" %}\n@@ -5861,9 +5954,10 @@\n-    int vlen_enc = Assembler::AVX_256bit;\n-    __ vpshufd($tmp$$XMMRegister, $src2$$XMMRegister, 177, vlen_enc);\n-    __ vpmulld($tmp$$XMMRegister, $src1$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vextracti128_high($tmp1$$XMMRegister, $tmp$$XMMRegister);\n-    __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovzxdq($tmp$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vpsllq($tmp$$XMMRegister, $tmp$$XMMRegister, 32, vlen_enc);\n-    __ vpmuludq($tmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpaddq($dst$$XMMRegister, $tmp$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this);\n+    \/\/ Get the lo-hi products, only the lower 32 bits is in concerns\n+    __ vpshufd($xtmp1$$XMMRegister, $src2$$XMMRegister, 0xB1, vlen_enc);\n+    __ vpmulld($xtmp1$$XMMRegister, $src1$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);\n+    __ vpshufd($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, 0xB1, vlen_enc);\n+    __ vpaddd($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);\n+    __ vpsllq($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 32, vlen_enc);\n+    \/\/ Get the lo-lo products\n+    __ vpmuludq($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ vpaddq($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n@@ -6208,1 +6302,1 @@\n-instruct signumF_reg(regF dst, regF zero, regF one, rRegP scratch, rFlagsReg cr) %{\n+instruct signumF_reg(regF dst, regF zero, regF one, rFlagsReg cr) %{\n@@ -6210,2 +6304,2 @@\n-  effect(TEMP scratch, KILL cr);\n-  format %{ \"signumF $dst, $dst\\t! using $scratch as TEMP\" %}\n+  effect(KILL cr);\n+  format %{ \"signumF $dst, $dst\" %}\n@@ -6214,1 +6308,1 @@\n-    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);\n@@ -6219,1 +6313,1 @@\n-instruct signumD_reg(regD dst, regD zero, regD one, rRegP scratch, rFlagsReg cr) %{\n+instruct signumD_reg(regD dst, regD zero, regD one, rFlagsReg cr) %{\n@@ -6221,2 +6315,2 @@\n-  effect(TEMP scratch, KILL cr);\n-  format %{ \"signumD $dst, $dst\\t! using $scratch as TEMP\" %}\n+  effect(KILL cr);\n+  format %{ \"signumD $dst, $dst\" %}\n@@ -6225,1 +6319,31 @@\n-    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct signumV_reg_avx(vec dst, vec src, vec zero, vec one, vec xtmp1) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  match(Set dst (SignumVF src (Binary zero one)));\n+  match(Set dst (SignumVD src (Binary zero one)));\n+  effect(TEMP dst, TEMP xtmp1);\n+  format %{ \"vector_signum_avx $dst, $src\\t! using $xtmp1 as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vec_enc = vector_length_encoding(this);\n+    __ vector_signum_avx(opcode, $dst$$XMMRegister, $src$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister,\n+                         $xtmp1$$XMMRegister, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct signumV_reg_evex(vec dst, vec src, vec zero, vec one, kReg ktmp1) %{\n+  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  match(Set dst (SignumVF src (Binary zero one)));\n+  match(Set dst (SignumVD src (Binary zero one)));\n+  effect(TEMP dst, TEMP ktmp1);\n+  format %{ \"vector_signum_evex $dst, $src\\t! using $ktmp1 as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vec_enc = vector_length_encoding(this);\n+    __ vector_signum_evex(opcode, $dst$$XMMRegister, $src$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister,\n+                          $ktmp1$$KRegister, vec_enc);\n@@ -6275,0 +6399,1 @@\n+\n@@ -6277,0 +6402,42 @@\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsI_reg(rRegI dst, rRegI src, rRegI mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextl  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextl($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsI_reg(rRegI dst, rRegI src, rRegI mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepl  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepl($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsI_mem(rRegI dst, rRegI src, memory mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (CompressBits src (LoadI mask)));\n+  format %{ \"pextl  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextl($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsI_mem(rRegI dst, rRegI src, memory mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (ExpandBits src (LoadI mask)));\n+  format %{ \"pdepl  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepl($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6281,1 +6448,0 @@\n-  ins_cost(400);\n@@ -6294,1 +6460,0 @@\n-  ins_cost(400);\n@@ -6307,1 +6472,0 @@\n-  ins_cost(400);\n@@ -6320,1 +6484,0 @@\n-  ins_cost(400);\n@@ -6345,1 +6508,1 @@\n-instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshiftB(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6350,1 +6513,1 @@\n-  effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, USE src, USE shift, TEMP tmp);\n@@ -6358,1 +6521,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6365,1 +6528,1 @@\n-instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{\n@@ -6371,1 +6534,1 @@\n-  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2);\n@@ -6382,1 +6545,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6390,1 +6553,1 @@\n-instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6396,1 +6559,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6404,1 +6567,1 @@\n-    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -6411,1 +6574,1 @@\n-instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6416,1 +6579,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6428,2 +6591,2 @@\n-    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -6436,1 +6599,1 @@\n-instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{\n@@ -6441,1 +6604,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -6453,1 +6616,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6458,1 +6621,1 @@\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, $scratch$$Register);\n+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n@@ -6595,1 +6758,1 @@\n-instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6598,1 +6761,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6606,1 +6769,1 @@\n-      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);\n+      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);\n@@ -6615,1 +6778,1 @@\n-      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);\n+      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);\n@@ -6637,1 +6800,1 @@\n-instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6644,2 +6807,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp as TEMP\" %}\n@@ -6651,1 +6814,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);\n@@ -6657,1 +6820,1 @@\n-instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6664,2 +6827,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -6672,1 +6835,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6677,1 +6840,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6685,1 +6848,1 @@\n-instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4, rRegP scratch) %{\n+instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4) %{\n@@ -6692,2 +6855,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 as TEMP\" %}\n@@ -6700,1 +6863,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6703,1 +6866,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6709,1 +6872,1 @@\n-    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister);\n@@ -6712,1 +6875,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6721,1 +6884,1 @@\n-instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6728,2 +6891,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp as TEMP\" %}\n@@ -6735,1 +6898,1 @@\n-    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);\n@@ -6740,1 +6903,1 @@\n-instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6747,2 +6910,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -6754,1 +6917,1 @@\n-    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6757,1 +6920,1 @@\n-    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6764,1 +6927,1 @@\n-instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6771,1 +6934,1 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp);\n@@ -6782,1 +6945,1 @@\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -6789,1 +6952,1 @@\n-instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6796,1 +6959,1 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n@@ -6808,1 +6971,1 @@\n-    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -6816,1 +6979,1 @@\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7023,22 +7186,1 @@\n-    switch (to_elem_bt) {\n-      case T_SHORT:\n-        __ vpmovsxbw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        break;\n-      case T_INT:\n-        __ vpmovsxbd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        break;\n-      case T_FLOAT:\n-        __ vpmovsxbd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        __ vcvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-        break;\n-      case T_LONG:\n-        __ vpmovsxbq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        break;\n-      case T_DOUBLE: {\n-        int mid_vlen_enc = (vlen_enc == Assembler::AVX_512bit) ? Assembler::AVX_256bit : Assembler::AVX_128bit;\n-        __ vpmovsxbd($dst$$XMMRegister, $src$$XMMRegister, mid_vlen_enc);\n-        __ vcvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-        break;\n-      }\n-      default: assert(false, \"%s\", type2name(to_elem_bt));\n-    }\n+    __ vconvert_b2x(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -7049,1 +7191,1 @@\n-instruct castStoX(vec dst, vec src, rRegP scratch) %{\n+instruct castStoX(vec dst, vec src) %{\n@@ -7053,2 +7195,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_cast_s2x $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_cast_s2x $dst,$src\" %}\n@@ -7059,1 +7200,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, noreg);\n@@ -7065,1 +7206,1 @@\n-instruct vcastStoX(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct vcastStoX(vec dst, vec src, vec vtmp) %{\n@@ -7069,1 +7210,1 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp);\n@@ -7071,1 +7212,1 @@\n-  format %{ \"vector_cast_s2x $dst,$src\\t! using $vtmp, $scratch as TEMP\" %}\n+  format %{ \"vector_cast_s2x $dst,$src\\t! using $vtmp as TEMP\" %}\n@@ -7076,1 +7217,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -7122,1 +7263,1 @@\n-instruct castItoX(vec dst, vec src, rRegP scratch) %{\n+instruct castItoX(vec dst, vec src) %{\n@@ -7127,2 +7268,1 @@\n-  format %{ \"vector_cast_i2x $dst,$src\\t! using $scratch as TEMP\" %}\n-  effect(TEMP scratch);\n+  format %{ \"vector_cast_i2x $dst,$src\" %}\n@@ -7136,1 +7276,1 @@\n-      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);\n@@ -7141,1 +7281,1 @@\n-      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7148,1 +7288,1 @@\n-instruct vcastItoX(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct vcastItoX(vec dst, vec src, vec vtmp) %{\n@@ -7153,2 +7293,2 @@\n-  format %{ \"vector_cast_i2x $dst,$src\\t! using $vtmp and $scratch as TEMP\" %}\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  format %{ \"vector_cast_i2x $dst,$src\\t! using $vtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n@@ -7162,1 +7302,1 @@\n-      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);\n@@ -7168,1 +7308,1 @@\n-      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7216,1 +7356,1 @@\n-instruct vcastLtoBS(vec dst, vec src, rRegP scratch) %{\n+instruct vcastLtoBS(vec dst, vec src) %{\n@@ -7220,2 +7360,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_cast_l2x  $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_cast_l2x  $dst,$src\" %}\n@@ -7231,1 +7370,1 @@\n-      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);\n@@ -7237,1 +7376,1 @@\n-      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);\n@@ -7319,4 +7458,3 @@\n-instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vl() &&\n-            Matcher::vector_length_in_bytes(n) < 64 &&\n-            Matcher::vector_element_basic_type(n) == T_INT);\n+instruct castFtoX_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64 &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) <= 4);\n@@ -7324,2 +7462,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, KILL cr);\n+  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3 and $xtmp4 as TEMP\" %}\n@@ -7327,2 +7465,10 @@\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_castF2I_avx($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    \/\/ JDK-8292878 removed the need for an explicit scratch register needed to load greater than\n+    \/\/ 32 bit addresses for register indirect addressing mode since stub constants\n+    \/\/ are part of code cache and there is a cap of 2G on ReservedCodeCacheSize currently.\n+    \/\/ However, targets are free to increase this limit, but having a large code cache size\n+    \/\/ greater than 2G looks unreasonable in practical scenario, on the hind side with given\n+    \/\/ cap we save a temporary register allocation which in limiting case can prevent\n+    \/\/ spilling in high register pressure blocks.\n+    __ vector_castF2X_avx(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n@@ -7330,1 +7476,1 @@\n-                          ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                          ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7335,4 +7481,3 @@\n-instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n-  predicate((VM_Version::supports_avx512vl() ||\n-             Matcher::vector_length_in_bytes(n) == 64) &&\n-             Matcher::vector_element_basic_type(n) == T_INT);\n+instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n@@ -7340,2 +7485,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n@@ -7343,4 +7488,12 @@\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_castF2I_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n-                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n-                           ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    if (to_elem_bt == T_LONG) {\n+      int vlen_enc = vector_length_encoding(this);\n+      __ vector_castF2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                             $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                             ExternalAddress(vector_double_signflip()), noreg, vlen_enc);\n+    } else {\n+      int vlen_enc = vector_length_encoding(this, $src);\n+      __ vector_castF2X_evex(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                             $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                             ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n+    }\n@@ -7362,2 +7515,3 @@\n-instruct castDtoL_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n-  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+instruct castDtoX_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, vec xtmp5, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64 &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n@@ -7365,2 +7519,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_d2l $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP xtmp5, KILL cr);\n+  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $xtmp5 as TEMP\" %}\n@@ -7368,4 +7522,22 @@\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_castD2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n-                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n-                           ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    __ vector_castD2X_avx(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                          $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister, $xtmp5$$XMMRegister,\n+                          ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    AddressLiteral signflip = VM_Version::supports_avx512dq() ? ExternalAddress(vector_double_signflip()) :\n+                              ExternalAddress(vector_float_signflip());\n+    __ vector_castD2X_evex(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister, signflip, noreg, vlen_enc);\n@@ -7440,1 +7612,1 @@\n-instruct vround_float_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+instruct vround_float_avx(vec dst, vec src, rRegP tmp, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rFlagsReg cr) %{\n@@ -7445,2 +7617,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n-  format %{ \"vector_round_float $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP tmp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, KILL cr);\n+  format %{ \"vector_round_float $dst,$src\\t! using $tmp, $xtmp1, $xtmp2, $xtmp3, $xtmp4 as TEMP\" %}\n@@ -7450,3 +7622,3 @@\n-    __ vector_round_float_avx($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n-                              $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,\n-                              ExternalAddress(vector_float_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+    __ vector_round_float_avx($dst$$XMMRegister, $src$$XMMRegister,\n+                              ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), new_mxcsr, vlen_enc,\n+                              $tmp$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister);\n@@ -7457,1 +7629,1 @@\n-instruct vround_float_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct vround_float_evex(vec dst, vec src, rRegP tmp, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7462,2 +7634,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_round_float $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP tmp, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_round_float $dst,$src\\t! using $tmp, $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7467,3 +7639,3 @@\n-    __ vector_round_float_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n-                               $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n-                               ExternalAddress(vector_float_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+    __ vector_round_float_evex($dst$$XMMRegister, $src$$XMMRegister,\n+                               ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), new_mxcsr, vlen_enc,\n+                               $tmp$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n@@ -7474,1 +7646,1 @@\n-instruct vround_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct vround_reg_evex(vec dst, vec src, rRegP tmp, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7477,2 +7649,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_round_long $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP tmp, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2,  KILL cr);\n+  format %{ \"vector_round_long $dst,$src\\t! using $tmp, $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7482,3 +7654,3 @@\n-    __ vector_round_double_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n-                                $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n-                                ExternalAddress(vector_double_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+    __ vector_round_double_evex($dst$$XMMRegister, $src$$XMMRegister,\n+                                ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), new_mxcsr, vlen_enc,\n+                                $tmp$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n@@ -7488,1 +7660,3 @@\n-#endif\n+\n+#endif \/\/ _LP64\n+\n@@ -7510,1 +7684,1 @@\n-instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{\n@@ -7515,2 +7689,2 @@\n-  effect(TEMP scratch, TEMP ktmp);\n-  format %{ \"vector_compare $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  effect(TEMP ktmp);\n+  format %{ \"vector_compare $dst,$src1,$src2,$cond\" %}\n@@ -7523,1 +7697,1 @@\n-      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);\n@@ -7526,1 +7700,1 @@\n-      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);\n@@ -7619,1 +7793,1 @@\n-instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{\n@@ -7624,2 +7798,2 @@\n-  effect(TEMP scratch, TEMP ktmp);\n-  format %{ \"vector_compare $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  effect(TEMP ktmp);\n+  format %{ \"vector_compare $dst,$src1,$src2,$cond\" %}\n@@ -7639,1 +7813,1 @@\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);\n@@ -7644,1 +7818,1 @@\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);\n@@ -7761,1 +7935,1 @@\n-instruct extractF(legRegF dst, legVec src, immU8 idx, rRegI tmp, legVec vtmp) %{\n+instruct extractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{\n@@ -7764,2 +7938,2 @@\n-  effect(TEMP dst, TEMP tmp, TEMP vtmp);\n-  format %{ \"extractF $dst,$src,$idx\\t! using $tmp, $vtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"extractF $dst,$src,$idx\\t! using $vtmp as TEMP\" %}\n@@ -7769,1 +7943,1 @@\n-    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $tmp$$Register, $vtmp$$XMMRegister);\n+    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $vtmp$$XMMRegister);\n@@ -7774,1 +7948,1 @@\n-instruct vextractF(legRegF dst, legVec src, immU8 idx, rRegI tmp, legVec vtmp) %{\n+instruct vextractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{\n@@ -7778,2 +7952,2 @@\n-  effect(TEMP tmp, TEMP vtmp);\n-  format %{ \"vextractF $dst,$src,$idx\\t! using $tmp, $vtmp as TEMP\" %}\n+  effect(TEMP vtmp);\n+  format %{ \"vextractF $dst,$src,$idx\\t! using $vtmp as TEMP\" %}\n@@ -7784,1 +7958,1 @@\n-    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant, $tmp$$Register);\n+    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant);\n@@ -7862,1 +8036,1 @@\n-instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, rRegP scratch, kReg ktmp) %{\n+instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, kReg ktmp) %{\n@@ -7866,2 +8040,2 @@\n-  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n-  effect(TEMP scratch, TEMP ktmp);\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using k2 as TEMP\" %}\n+  effect(TEMP ktmp);\n@@ -7871,1 +8045,1 @@\n-    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, $scratch$$Register);\n+    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, noreg);\n@@ -7878,1 +8052,1 @@\n-instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask) %{\n@@ -7883,2 +8057,1 @@\n-  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n-  effect(TEMP scratch);\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using k2 as TEMP\" %}\n@@ -7897,1 +8070,0 @@\n-  ins_cost(450);\n@@ -7913,1 +8085,0 @@\n-  ins_cost(450);\n@@ -7930,1 +8101,0 @@\n-  ins_cost(250);\n@@ -7945,1 +8115,0 @@\n-  ins_cost(450);\n@@ -7982,1 +8151,1 @@\n-instruct vabsnegF(vec dst, vec src, rRegI scratch) %{\n+instruct vabsnegF(vec dst, vec src) %{\n@@ -7986,1 +8155,0 @@\n-  effect(TEMP scratch);\n@@ -7993,1 +8161,1 @@\n-      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);\n+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister);\n@@ -7997,1 +8165,1 @@\n-      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);\n+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -8003,1 +8171,1 @@\n-instruct vabsneg4F(vec dst, rRegI scratch) %{\n+instruct vabsneg4F(vec dst) %{\n@@ -8007,1 +8175,0 @@\n-  effect(TEMP scratch);\n@@ -8012,1 +8179,1 @@\n-    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);\n+    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister);\n@@ -8017,1 +8184,1 @@\n-instruct vabsnegD(vec dst, vec src, rRegI scratch) %{\n+instruct vabsnegD(vec dst, vec src) %{\n@@ -8020,1 +8187,0 @@\n-  effect(TEMP scratch);\n@@ -8027,1 +8193,1 @@\n-      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);\n+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister);\n@@ -8030,1 +8196,1 @@\n-      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);\n+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -8039,8 +8205,5 @@\n-instruct vptest_alltrue_lt16(rRegI dst, legVec src1, legVec src2, legVec vtmp1, legVec vtmp2, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512bwdq() &&\n-            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n-            Matcher::vector_length_in_bytes(n->in(1)) < 16 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(TEMP vtmp1, TEMP vtmp2, KILL cr);\n-  format %{ \"vptest_alltrue_lt16 $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n+instruct vptest_lt16(rFlagsRegU cr, legVec src1, legVec src2, legVec vtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(1)) < 16);\n+  match(Set cr (VectorTest src1 src2));\n+  effect(TEMP vtmp);\n+  format %{ \"vptest_lt16  $src1, $src2\\t! using $vtmp as TEMP\" %}\n@@ -8048,0 +8211,1 @@\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n@@ -8049,3 +8213,1 @@\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);\n-    __ setb(Assembler::carrySet, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    __ vectortest(bt, $src1$$XMMRegister, $src2$$XMMRegister, $vtmp$$XMMRegister, vlen);\n@@ -8056,8 +8218,4 @@\n-instruct vptest_alltrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512bwdq() &&\n-            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n-            Matcher::vector_length_in_bytes(n->in(1)) <  64 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr);\n-  format %{ \"vptest_alltrue_ge16  $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+instruct vptest_ge16(rFlagsRegU cr, legVec src1, legVec src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16);\n+  match(Set cr (VectorTest src1 src2));\n+  format %{ \"vptest_ge16  $src1, $src2\\n\\t\" %}\n@@ -8065,0 +8223,1 @@\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n@@ -8066,3 +8225,1 @@\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, knoreg);\n-    __ setb(Assembler::carrySet, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    __ vectortest(bt, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, vlen);\n@@ -8073,27 +8230,7 @@\n-instruct vptest_alltrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bwdq() &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n-            n->in(1)->bottom_type()->isa_vectmask() &&\n-            Matcher::vector_length(n->in(1)) < 8);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(KILL cr, TEMP kscratch);\n-  format %{ \"vptest_alltrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n-  ins_encode %{\n-    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n-    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n-    uint masklen = Matcher::vector_length(this, $src1);\n-    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, $kscratch$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-instruct vptest_alltrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bwdq() &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n-            n->in(1)->bottom_type()->isa_vectmask() &&\n-            Matcher::vector_length(n->in(1)) >= 8);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(KILL cr);\n-  format %{ \"vptest_alltrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+instruct ktest_alltrue_le8(rFlagsRegU cr, kReg src1, kReg src2, rRegI tmp) %{\n+  predicate((Matcher::vector_length(n->in(1)) < 8 ||\n+             (Matcher::vector_length(n->in(1)) == 8 && !VM_Version::supports_avx512dq())) &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set cr (VectorTest src1 src2));\n+  effect(TEMP tmp);\n+  format %{ \"ktest_alltrue_le8  $src1, $src2\\t! using $tmp as TEMP\" %}\n@@ -8101,4 +8238,3 @@\n-    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n-    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n-    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, knoreg);\n+    __ kmovwl($tmp$$Register, $src1$$KRegister);\n+    __ andl($tmp$$Register, (1 << masklen) - 1);\n+    __ cmpl($tmp$$Register, (1 << masklen) - 1);\n@@ -8110,5 +8246,3 @@\n-\n-instruct vptest_anytrue_lt16(rRegI dst, legVec src1, legVec src2, legVec vtmp, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512bwdq() &&\n-            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n-            Matcher::vector_length_in_bytes(n->in(1)) < 16 &&\n+instruct ktest_anytrue_le8(rFlagsRegU cr, kReg src1, kReg src2, rRegI tmp) %{\n+  predicate((Matcher::vector_length(n->in(1)) < 8 ||\n+             (Matcher::vector_length(n->in(1)) == 8 && !VM_Version::supports_avx512dq())) &&\n@@ -8116,67 +8250,3 @@\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(TEMP vtmp, KILL cr);\n-  format %{ \"vptest_anytrue_lt16 $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n-  ins_encode %{\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, $vtmp$$XMMRegister);\n-    __ setb(Assembler::notZero, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vptest_anytrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512bwdq() &&\n-            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n-            Matcher::vector_length_in_bytes(n->in(1)) < 64  &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr);\n-  format %{ \"vptest_anytrue_ge16 $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n-  ins_encode %{\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, knoreg);\n-    __ setb(Assembler::notZero, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vptest_anytrue_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bwdq() &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(KILL cr);\n-  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n-  ins_encode %{\n-    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n-    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n-    uint  masklen = Matcher::vector_length(this, $src1);\n-    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpvptest_anytrue_lt16(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero, legVec vtmp) %{\n-  predicate(!VM_Version::supports_avx512bwdq() &&\n-            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n-            Matcher::vector_length_in_bytes(n->in(1)->in(1)) < 16 &&\n-            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne);\n-  match(Set cr (CmpI (VectorTest src1 src2) zero));\n-  effect(TEMP vtmp);\n-  format %{ \"cmpvptest_anytrue_lt16 $src1,$src2\\t! using $vtmp as TEMP\" %}\n-  ins_encode %{\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, $vtmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpvptest_anytrue_ge16(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n-  predicate(!VM_Version::supports_avx512bwdq() &&\n-            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n-            Matcher::vector_length_in_bytes(n->in(1)->in(1)) <  64 &&\n-            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne);\n-  match(Set cr (CmpI (VectorTest src1 src2) zero));\n-  format %{ \"cmpvptest_anytrue_ge16 $src1,$src2\\t!\" %}\n+  match(Set cr (VectorTest src1 src2));\n+  effect(TEMP tmp);\n+  format %{ \"ktest_anytrue_le8  $src1, $src2\\t! using $tmp as TEMP\" %}\n@@ -8184,2 +8254,3 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, knoreg);\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ kmovwl($tmp$$Register, $src1$$KRegister);\n+    __ andl($tmp$$Register, (1 << masklen) - 1);\n@@ -8190,5 +8261,5 @@\n-instruct cmpvptest_anytrue_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n-  predicate(VM_Version::supports_avx512bwdq() &&\n-            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne);\n-  match(Set cr (CmpI (VectorTest src1 src2) zero));\n-  format %{ \"cmpvptest_anytrue_evex $src1,$src2\\t!\" %}\n+instruct ktest_ge8(rFlagsRegU cr, kReg src1, kReg src2) %{\n+  predicate(Matcher::vector_length(n->in(1)) >= 16 ||\n+            (Matcher::vector_length(n->in(1)) == 8 && VM_Version::supports_avx512dq()));\n+  match(Set cr (VectorTest src1 src2));\n+  format %{ \"ktest_ge8  $src1, $src2\\n\\t\" %}\n@@ -8197,5 +8268,1 @@\n-    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n-    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n-    masklen = masklen < 8 ? 8 : masklen;\n-    __ ktest(masklen, $src1$$KRegister, $src2$$KRegister);\n+    __ kortest(masklen, $src1$$KRegister, $src1$$KRegister);\n@@ -8222,1 +8289,1 @@\n-instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI tmp) %{\n+instruct loadMask64(kReg dst, vec src, vec xtmp) %{\n@@ -8225,2 +8292,2 @@\n-  effect(TEMP xtmp, TEMP tmp);\n-  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp and $tmp as TEMP\" %}\n+  effect(TEMP xtmp);\n+  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp as TEMP\" %}\n@@ -8229,1 +8296,1 @@\n-                        $tmp$$Register, true, Assembler::AVX_512bit);\n+                        true, Assembler::AVX_512bit);\n@@ -8242,1 +8309,1 @@\n-                        noreg, false, vlen_enc);\n+                        false, vlen_enc);\n@@ -8339,1 +8406,1 @@\n-    __ vpshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);\n+    __ vshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);\n@@ -8382,1 +8449,1 @@\n-instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size, rRegI tmp) %{\n+instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size) %{\n@@ -8385,1 +8452,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n+  effect(TEMP_DEF dst);\n@@ -8390,1 +8457,1 @@\n-                 false, Assembler::AVX_512bit, $tmp$$Register);\n+                 false, Assembler::AVX_512bit, noreg);\n@@ -8410,1 +8477,0 @@\n-  predicate(Matcher::vector_length(n) == Matcher::vector_length(n->in(1)));\n@@ -8421,2 +8487,1 @@\n-  predicate((Matcher::vector_length(n) == Matcher::vector_length(n->in(1))) &&\n-            (Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))));\n+  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));\n@@ -8432,0 +8497,13 @@\n+instruct vmaskcast_avx(vec dst, vec src) %{\n+  predicate(Matcher::vector_length_in_bytes(n) != Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst (VectorMaskCast src));\n+  format %{ \"vector_mask_cast $dst, $src\" %}\n+  ins_encode %{\n+    int vlen = Matcher::vector_length(this);\n+    BasicType src_bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType dst_bt = Matcher::vector_element_basic_type(this);\n+    __ vector_mask_cast($dst$$XMMRegister, $src$$XMMRegister, dst_bt, src_bt, vlen);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -8434,2 +8512,1 @@\n-instruct loadIotaIndices(vec dst, immI_0 src, rRegP scratch) %{\n-  predicate(Matcher::vector_element_basic_type(n) == T_BYTE);\n+instruct loadIotaIndices(vec dst, immI_0 src) %{\n@@ -8437,1 +8514,0 @@\n-  effect(TEMP scratch);\n@@ -8441,1 +8517,2 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen_in_bytes);\n+     BasicType bt = Matcher::vector_element_basic_type(this);\n+     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes, bt);\n@@ -8446,3 +8523,16 @@\n-\/\/-------------------------------- Rearrange ----------------------------------\n-\n-\/\/ LoadShuffle\/Rearrange for Byte\n+#ifdef _LP64\n+instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp) %{\n+  match(Set dst (PopulateIndex src1 src2));\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp as TEMP\" %}\n+  ins_encode %{\n+     assert($src2$$constant == 1, \"required\");\n+     int vlen_in_bytes = Matcher::vector_length_in_bytes(this);\n+     int vlen_enc = vector_length_encoding(this);\n+     BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+     __ vpbroadcast(elem_bt, $vtmp$$XMMRegister, $src1$$Register, vlen_enc);\n+     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes, elem_bt);\n+     __ vpadd(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -8450,4 +8540,4 @@\n-instruct loadShuffleB(vec dst) %{\n-  predicate(Matcher::vector_element_basic_type(n) == T_BYTE);\n-  match(Set dst (VectorLoadShuffle dst));\n-  format %{ \"vector_load_shuffle $dst, $dst\" %}\n+instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp) %{\n+  match(Set dst (PopulateIndex src1 src2));\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp as TEMP\" %}\n@@ -8455,1 +8545,7 @@\n-    \/\/ empty\n+     assert($src2$$constant == 1, \"required\");\n+     int vlen_in_bytes = Matcher::vector_length_in_bytes(this);\n+     int vlen_enc = vector_length_encoding(this);\n+     BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+     __ vpbroadcast(elem_bt, $vtmp$$XMMRegister, $src1$$Register, vlen_enc);\n+     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes, elem_bt);\n+     __ vpadd(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n@@ -8459,0 +8555,2 @@\n+#endif\n+\/\/-------------------------------- Rearrange ----------------------------------\n@@ -8460,0 +8558,1 @@\n+\/\/ LoadShuffle\/Rearrange for Byte\n@@ -8472,1 +8571,1 @@\n-instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{\n@@ -8476,2 +8575,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -8487,1 +8586,1 @@\n-    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);\n@@ -8494,1 +8593,17 @@\n-instruct rearrangeB_evex(vec dst, vec src, vec shuffle) %{\n+\n+instruct rearrangeB_evex(vec dst, vec src, vec shuffle, vec xtmp1, vec xtmp2, vec xtmp3, kReg ktmp, rRegI rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_BYTE &&\n+            Matcher::vector_length(n) > 32 && !VM_Version::supports_avx512_vbmi());\n+  match(Set dst (VectorRearrange src shuffle));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP ktmp, TEMP rtmp);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src!\\t using $xtmp1, $xtmp2, $xtmp3, $rtmp and $ktmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ rearrange_bytes($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister,\n+                       $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister,\n+                       $rtmp$$Register, $ktmp$$KRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct rearrangeB_evex_vbmi(vec dst, vec src, vec shuffle) %{\n@@ -8508,1 +8623,1 @@\n-instruct loadShuffleS(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleS(vec dst, vec src, vec vtmp) %{\n@@ -8510,1 +8625,1 @@\n-            Matcher::vector_length(n) <= 16 && !VM_Version::supports_avx512bw()); \/\/ NB! aligned with rearrangeS\n+            !VM_Version::supports_avx512bw());\n@@ -8512,2 +8627,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8521,1 +8636,1 @@\n-      __ pmovzxbw($vtmp$$XMMRegister, $src$$XMMRegister);\n+      __ movdqu($vtmp$$XMMRegister, $src$$XMMRegister);\n@@ -8530,1 +8645,1 @@\n-      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), $scratch$$Register);\n+      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), noreg);\n@@ -8536,2 +8651,1 @@\n-      __ vpmovzxbw($vtmp$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-      __ vpsllw($vtmp$$XMMRegister, $vtmp$$XMMRegister, 1, vlen_enc);\n+      __ vpsllw($vtmp$$XMMRegister, $src$$XMMRegister, 1, vlen_enc);\n@@ -8544,1 +8658,1 @@\n-      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, $scratch$$Register);\n+      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, noreg);\n@@ -8562,1 +8676,1 @@\n-instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{\n@@ -8566,2 +8680,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -8577,1 +8691,1 @@\n-    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);\n@@ -8584,15 +8698,0 @@\n-instruct loadShuffleS_evex(vec dst, vec src) %{\n-  predicate(Matcher::vector_element_basic_type(n) == T_SHORT &&\n-            VM_Version::supports_avx512bw());\n-  match(Set dst (VectorLoadShuffle src));\n-  format %{ \"vector_load_shuffle $dst, $src\" %}\n-  ins_encode %{\n-    int vlen_enc = vector_length_encoding(this);\n-    if (!VM_Version::supports_avx512vl()) {\n-      vlen_enc = Assembler::AVX_512bit;\n-    }\n-    __ vpmovzxbw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -8616,1 +8715,1 @@\n-instruct loadShuffleI(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleI(vec dst, vec src, vec vtmp) %{\n@@ -8618,1 +8717,1 @@\n-            Matcher::vector_length(n) == 4 && UseAVX < 2);\n+            Matcher::vector_length(n) == 4 && UseAVX == 0);\n@@ -8620,2 +8719,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8629,1 +8728,1 @@\n-    __ pmovzxbd($vtmp$$XMMRegister, $src$$XMMRegister);\n+    __ movdqu($vtmp$$XMMRegister, $src$$XMMRegister);\n@@ -8640,1 +8739,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), noreg);\n@@ -8647,2 +8746,2 @@\n- predicate((Matcher::vector_element_basic_type(n) == T_INT || Matcher::vector_element_basic_type(n) == T_FLOAT) &&\n-           Matcher::vector_length(n) == 4 && UseAVX < 2);\n+  predicate((Matcher::vector_element_basic_type(n) == T_INT || Matcher::vector_element_basic_type(n) == T_FLOAT) &&\n+            UseAVX == 0);\n@@ -8658,12 +8757,0 @@\n-instruct loadShuffleI_avx(vec dst, vec src) %{\n-  predicate((Matcher::vector_element_basic_type(n) == T_INT || Matcher::vector_element_basic_type(n) == T_FLOAT) &&\n-            UseAVX >= 2);\n-  match(Set dst (VectorLoadShuffle src));\n-  format %{ \"vector_load_shuffle $dst, $src\" %}\n-  ins_encode %{\n-  int vlen_enc = vector_length_encoding(this);\n-    __ vpmovzxbd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -8672,1 +8759,1 @@\n-            UseAVX >= 2);\n+            UseAVX > 0);\n@@ -8677,4 +8764,2 @@\n-    if (vlen_enc == Assembler::AVX_128bit) {\n-      vlen_enc = Assembler::AVX_256bit;\n-    }\n-    __ vpermd($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_rearrange_int_float(bt, $dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -8687,1 +8772,1 @@\n-instruct loadShuffleL(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleL(vec dst, vec src, vec vtmp) %{\n@@ -8691,2 +8776,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8701,2 +8786,1 @@\n-    __ vpmovzxbq($vtmp$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-    __ vpsllq($vtmp$$XMMRegister, $vtmp$$XMMRegister, 1, vlen_enc);\n+    __ vpsllq($vtmp$$XMMRegister, $src$$XMMRegister, 1, vlen_enc);\n@@ -8709,1 +8793,1 @@\n-    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, $scratch$$Register);\n+    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, noreg);\n@@ -8728,14 +8812,0 @@\n-instruct loadShuffleL_evex(vec dst, vec src) %{\n-  predicate(is_double_word_type(Matcher::vector_element_basic_type(n)) && \/\/ T_LONG, T_DOUBLE\n-            (Matcher::vector_length(n) == 8 || VM_Version::supports_avx512vl()));\n-  match(Set dst (VectorLoadShuffle src));\n-  format %{ \"vector_load_shuffle $dst, $src\" %}\n-  ins_encode %{\n-    assert(UseAVX > 2, \"required\");\n-\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vpmovzxbq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -8883,1 +8953,0 @@\n-  ins_cost(400);\n@@ -8890,6 +8959,0 @@\n-    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n-    \/\/ should be succeeded by its corresponding vector IR and following\n-    \/\/ special handling should be removed.\n-    if (opcode == Op_PopCountVL && Matcher::vector_element_basic_type(this) == T_INT) {\n-      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n@@ -8926,12 +8989,0 @@\n-    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n-    \/\/ should be succeeded by its corresponding vector IR and following\n-    \/\/ special handling should be removed.\n-    if (opcode == Op_PopCountVL && Matcher::vector_element_basic_type(this) == T_INT) {\n-      if (VM_Version::supports_avx512vl()) {\n-        __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-      } else {\n-        assert(VM_Version::supports_avx2(), \"\");\n-        __ vpshufd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n-        __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n-      }\n-    }\n@@ -8954,1 +9005,0 @@\n-    BasicType rbt = Matcher::vector_element_basic_type(this);\n@@ -8957,6 +9007,0 @@\n-    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountTrailingZerosV\n-    \/\/ should be succeeded by its corresponding vector IR and following\n-    \/\/ special handling should be removed.\n-    if (bt == T_LONG && rbt == T_INT) {\n-      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n@@ -9008,1 +9052,0 @@\n-    BasicType rbt = Matcher::vector_element_basic_type(this);\n@@ -9011,12 +9054,0 @@\n-    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n-    \/\/ should be succeeded by its corresponding vector IR and following\n-    \/\/ special handling should be removed.\n-    if (bt == T_LONG && rbt == T_INT) {\n-      if (VM_Version::supports_avx512vl()) {\n-        __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-      } else {\n-        assert(VM_Version::supports_avx2(), \"\");\n-        __ vpshufd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n-        __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n-      }\n-    }\n@@ -9080,1 +9111,37 @@\n-#ifdef _LP64\n+instruct vmasked_load_avx_non_subword(vec dst, memory mem, vec mask) %{\n+  predicate(!n->in(3)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorMasked mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType = this->bottom_type()->is_vect()->element_basic_type();\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vmovmask(elmType, $dst$$XMMRegister, $mem$$Address, $mask$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vmasked_load_evex(vec dst, memory mem, kReg mask) %{\n+  predicate(n->in(3)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorMasked mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(this);\n+    __ evmovdqu(elmType, $mask$$KRegister, $dst$$XMMRegister, $mem$$Address, false, vector_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmasked_store_avx_non_subword(memory mem, vec src, vec mask) %{\n+  predicate(!n->in(3)->in(2)->bottom_type()->isa_vectmask());\n+  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    int vlen_enc = vector_length_encoding(src_node);\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    __ vmovmask(elmType, $mem$$Address, $src$$XMMRegister, $mask$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -9083,0 +9150,14 @@\n+instruct vmasked_store_evex(memory mem, vec src, kReg mask) %{\n+  predicate(n->in(3)->in(2)->bottom_type()->isa_vectmask());\n+  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    int vlen_enc = vector_length_encoding(src_node);\n+    __ evmovdqu(elmType, $mask$$KRegister, $mem$$Address, $src$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+#ifdef _LP64\n@@ -9109,11 +9190,0 @@\n-instruct vmasked_load64(vec dst, memory mem, kReg mask) %{\n-  match(Set dst (LoadVectorMasked mem mask));\n-  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n-  ins_encode %{\n-    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n-    int vector_len = vector_length_encoding(this);\n-    __ evmovdqu(elmType, $mask$$KRegister, $dst$$XMMRegister, $mem$$Address, vector_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -9141,12 +9211,0 @@\n-instruct vmasked_store64(memory mem, vec src, kReg mask) %{\n-  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n-  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n-  ins_encode %{\n-    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n-    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n-    int vector_len = vector_length_encoding(src_node);\n-    __ evmovdqu(elmType, $mask$$KRegister, $mem$$Address, $src$$XMMRegister, vector_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -9348,1 +9406,1 @@\n-instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp, rRegI rtmp) %{\n+instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp) %{\n@@ -9351,2 +9409,2 @@\n-  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n-  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $rtmp and $xtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $xtmp as TEMP\" %}\n@@ -9357,2 +9415,2 @@\n-    __ vector_reverse_bit_gfni(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n-                               addr, $rtmp$$Register, vec_enc);\n+    __ vector_reverse_bit_gfni(bt, $dst$$XMMRegister, $src$$XMMRegister, addr, vec_enc,\n+                               $xtmp$$XMMRegister);\n@@ -9363,1 +9421,1 @@\n-instruct vreverse_byte_reg(vec dst, vec src, rRegI rtmp) %{\n+instruct vreverse_byte_reg(vec dst, vec src) %{\n@@ -9366,2 +9424,2 @@\n-  effect(TEMP dst, TEMP rtmp);\n-  format %{ \"vector_reverse_byte $dst, $src!\\t using $rtmp as TEMP\" %}\n+  effect(TEMP dst);\n+  format %{ \"vector_reverse_byte $dst, $src\" %}\n@@ -9371,1 +9429,1 @@\n-    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, $rtmp$$Register, vec_enc);\n+    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, vec_enc);\n@@ -9400,1 +9458,0 @@\n-     BasicType rbt = Matcher::vector_element_basic_type(this);\n@@ -9403,6 +9460,0 @@\n-     \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n-     \/\/ should be succeeded by its corresponding vector IR and following\n-     \/\/ special handling should be removed.\n-     if (rbt == T_INT && bt == T_LONG) {\n-       __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-     }\n@@ -9483,1 +9534,0 @@\n-    BasicType rbt = Matcher::vector_element_basic_type(this);\n@@ -9486,6 +9536,0 @@\n-    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n-    \/\/ should be succeeded by its corresponding vector IR and following\n-    \/\/ special handling should be removed.\n-    if (rbt == T_INT && bt == T_LONG) {\n-      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n@@ -9685,1 +9729,0 @@\n-  ins_cost(100);\n@@ -10011,1 +10054,0 @@\n-  ins_cost(100);\n@@ -10050,1 +10092,1 @@\n-instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask) %{\n@@ -10052,2 +10094,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\\t! using $scratch as TEMP\" %}\n+  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\" %}\n@@ -10252,0 +10293,26 @@\n+\n+instruct FloatClassCheck_reg_reg_vfpclass(rRegI dst, regF src, kReg ktmp, rFlagsReg cr)\n+%{\n+  match(Set dst (IsInfiniteF src));\n+  effect(TEMP ktmp, KILL cr);\n+  format %{ \"float_class_check $dst, $src\" %}\n+  ins_encode %{\n+    __ vfpclassss($ktmp$$KRegister, $src$$XMMRegister, 0x18);\n+    __ kmovbl($dst$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct DoubleClassCheck_reg_reg_vfpclass(rRegI dst, regD src, kReg ktmp, rFlagsReg cr)\n+%{\n+  match(Set dst (IsInfiniteD src));\n+  effect(TEMP ktmp, KILL cr);\n+  format %{ \"double_class_check $dst, $src\" %}\n+  ins_encode %{\n+    __ vfpclasssd($ktmp$$KRegister, $src$$XMMRegister, 0x18);\n+    __ kmovbl($dst$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":910,"deletions":843,"binary":false,"changes":1753,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#define remaining_buflen(buffer, position) (sizeof(buffer) - ((position) - (buffer)))\n+\n@@ -68,1 +70,1 @@\n-    _is_mach_constant(false),\n+    _is_mach_constant(instr->_is_mach_constant),\n@@ -424,2 +426,0 @@\n-  if(_matrule->find_type(\"CallNative\",idx))       return Form::JAVA_NATIVE;\n-  idx = 0;\n@@ -608,2 +608,2 @@\n-  \/\/ String.(compareTo\/equals\/indexOf) and Arrays.equals use many memorys edges,\n-  \/\/ but writes none\n+  \/\/ String.(compareTo\/equals\/indexOf\/hashCode) and Arrays.(equals\/hashCode)\n+  \/\/ use many memorys edges, but writes none\n@@ -616,1 +616,2 @@\n-        strcmp(_matrule->_rChild->_opType,\"AryEq\"      )==0 ))\n+        strcmp(_matrule->_rChild->_opType,\"AryEq\"      )==0 ||\n+        strcmp(_matrule->_rChild->_opType,\"VectorizedHashCode\")==0 ))\n@@ -899,0 +900,1 @@\n+        strcmp(_matrule->_rChild->_opType,\"VectorizedHashCode\")==0 ||\n@@ -907,1 +909,1 @@\n-        \/\/ String.(compareTo\/equals\/indexOf) and Arrays.equals\n+        \/\/ String.(compareTo\/equals\/indexOf\/hashCode) and Arrays.equals\n@@ -1146,3 +1148,0 @@\n-  else if( is_ideal_call() == Form::JAVA_NATIVE ) {\n-    return \"MachCallNativeNode\";\n-  }\n@@ -1306,1 +1305,1 @@\n-    fprintf(fp, \"char reg[128];  ra->dump_register(in(mach_constant_base_node_input()), reg);\\n\");\n+    fprintf(fp, \"char reg[128];  ra->dump_register(in(mach_constant_base_node_input()), reg, sizeof(reg));\\n\");\n@@ -1541,1 +1540,1 @@\n-      sprintf(s,\"\/*%s*\/(\",(char*)i._key);\n+      snprintf_checked(s, remaining_buflen(buf, s), \"\/*%s*\/(\",(char*)i._key);\n@@ -2505,1 +2504,1 @@\n-    fprintf(fp,\"    ra->dump_register(node,reg_str);\\n\");\n+    fprintf(fp,\"    ra->dump_register(node,reg_str, sizeof(reg_str));\\n\");\n@@ -2513,1 +2512,1 @@\n-    fprintf(fp,\"    ra->dump_register(node,reg_str);\\n\");\n+    fprintf(fp,\"    ra->dump_register(node,reg_str, sizeof(reg_str));\\n\");\n@@ -2534,1 +2533,1 @@\n-    fprintf(fp,                                      \"),reg_str);\\n\");\n+    fprintf(fp,                                      \"),reg_str,sizeof(reg_str));\\n\");\n@@ -2544,1 +2543,1 @@\n-    fprintf(fp,                                       \"),reg_str);\\n\");\n+    fprintf(fp,                                       \"),reg_str,sizeof(reg_str));\\n\");\n@@ -3480,1 +3479,1 @@\n-  sprintf(subtree,\"_%s_%s_%s\", _opType, lstr, rstr);\n+  snprintf_checked(subtree, len, \"_%s_%s_%s\", _opType, lstr, rstr);\n@@ -3521,2 +3520,0 @@\n-    \"LoadPLocked\",\n-    \"StorePConditional\", \"StoreIConditional\", \"StoreLConditional\",\n@@ -3929,2 +3926,3 @@\n-  char* buf = (char*) AdlAllocateHeap(strlen(instr_ident) + 4);\n-  sprintf(buf, \"%s_%d\", instr_ident, match_rules_cnt++);\n+  const size_t buf_size = strlen(instr_ident) + 4;\n+  char* buf = (char*) AdlAllocateHeap(buf_size);\n+  snprintf_checked(buf, buf_size, \"%s_%d\", instr_ident, match_rules_cnt++);\n@@ -4100,6 +4098,1 @@\n-        strcmp(opType,\"ReplicateB\")==0 ||\n-        strcmp(opType,\"ReplicateS\")==0 ||\n-        strcmp(opType,\"ReplicateI\")==0 ||\n-        strcmp(opType,\"ReplicateL\")==0 ||\n-        strcmp(opType,\"ReplicateF\")==0 ||\n-        strcmp(opType,\"ReplicateD\")==0 ||\n+        strcmp(opType,\"PopulateIndex\")==0 ||\n@@ -4121,1 +4114,1 @@\n-        0 \/* 0 to line up columns nicely *\/ )\n+        0 \/* 0 to line up columns nicely *\/ ) {\n@@ -4123,0 +4116,1 @@\n+    }\n@@ -4220,1 +4214,1 @@\n-    \"CompressV\", \"ExpandV\", \"CompressM\",\n+    \"CompressV\", \"ExpandV\", \"CompressM\", \"CompressBitsV\", \"ExpandBitsV\",\n@@ -4232,1 +4226,1 @@\n-    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\", \"ReverseV\", \"ReverseBytesV\",\n+    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\",\"ReverseV\",\"ReverseBytesV\",\n@@ -4238,1 +4232,1 @@\n-    \"VectorCastL2X\", \"VectorCastF2X\", \"VectorCastD2X\",\n+    \"VectorCastL2X\", \"VectorCastF2X\", \"VectorCastD2X\", \"VectorCastF2HF\", \"VectorCastHF2F\",\n@@ -4242,2 +4236,2 @@\n-    \"FmaVD\",\"FmaVF\",\"PopCountVI\", \"PopCountVL\", \"VectorLongToMask\",\n-    \"CountLeadingZerosV\", \"CountTrailingZerosV\",\n+    \"FmaVD\",\"FmaVF\",\"PopCountVI\",\"PopCountVL\",\"PopulateIndex\",\"VectorLongToMask\",\n+    \"CountLeadingZerosV\", \"CountTrailingZerosV\", \"SignumVF\", \"SignumVD\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":28,"deletions":34,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,1 +65,0 @@\n-macro(CallNative)\n@@ -81,0 +80,2 @@\n+macro(CompressBitsV)\n+macro(ExpandBitsV)\n@@ -101,0 +102,1 @@\n+macro(CmpU3)\n@@ -102,0 +104,1 @@\n+macro(CmpUL3)\n@@ -151,1 +154,1 @@\n-macro(ConvHF2F)\n+macro(ConvHF2F)\n@@ -174,2 +177,0 @@\n-macro(NoOvfDivI)\n-macro(NoOvfDivL)\n@@ -181,2 +182,0 @@\n-macro(NoOvfDivModI)\n-macro(NoOvfDivModL)\n@@ -217,1 +216,0 @@\n-macro(LoadPLocked)\n@@ -252,2 +250,0 @@\n-macro(NoOvfModI)\n-macro(NoOvfModL)\n@@ -260,0 +256,4 @@\n+macro(IsInfiniteF)\n+macro(IsFiniteF)\n+macro(IsInfiniteD)\n+macro(IsFiniteD)\n@@ -276,1 +276,1 @@\n-macro(Opaque2)\n+macro(OpaqueZeroTripGuard)\n@@ -297,0 +297,1 @@\n+macro(PopulateIndex)\n@@ -336,0 +337,2 @@\n+macro(SignumVF)\n+macro(SignumVD)\n@@ -345,3 +348,0 @@\n-macro(StorePConditional)\n-macro(StoreIConditional)\n-macro(StoreLConditional)\n@@ -521,3 +521,0 @@\n-macro(VectorUCastB2X)\n-macro(VectorUCastS2X)\n-macro(VectorUCastI2X)\n@@ -528,0 +525,4 @@\n+macro(VectorUCastB2X)\n+macro(VectorUCastS2X)\n+macro(VectorUCastI2X)\n+macro(VectorizedHashCode)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":18,"deletions":17,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -120,0 +120,1 @@\n+  virtual const Type* Value(PhaseGVN* phase) const;\n@@ -167,0 +168,1 @@\n+  virtual const Type* Value(PhaseGVN* phase) const;\n","filename":"src\/hotspot\/share\/opto\/convertnode.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,1 +44,1 @@\n-#include \"runtime\/os.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n@@ -86,46 +86,46 @@\n-  idealreg2spillmask  [Op_RegI] = NULL;\n-  idealreg2spillmask  [Op_RegN] = NULL;\n-  idealreg2spillmask  [Op_RegL] = NULL;\n-  idealreg2spillmask  [Op_RegF] = NULL;\n-  idealreg2spillmask  [Op_RegD] = NULL;\n-  idealreg2spillmask  [Op_RegP] = NULL;\n-  idealreg2spillmask  [Op_VecA] = NULL;\n-  idealreg2spillmask  [Op_VecS] = NULL;\n-  idealreg2spillmask  [Op_VecD] = NULL;\n-  idealreg2spillmask  [Op_VecX] = NULL;\n-  idealreg2spillmask  [Op_VecY] = NULL;\n-  idealreg2spillmask  [Op_VecZ] = NULL;\n-  idealreg2spillmask  [Op_RegFlags] = NULL;\n-  idealreg2spillmask  [Op_RegVectMask] = NULL;\n-\n-  idealreg2debugmask  [Op_RegI] = NULL;\n-  idealreg2debugmask  [Op_RegN] = NULL;\n-  idealreg2debugmask  [Op_RegL] = NULL;\n-  idealreg2debugmask  [Op_RegF] = NULL;\n-  idealreg2debugmask  [Op_RegD] = NULL;\n-  idealreg2debugmask  [Op_RegP] = NULL;\n-  idealreg2debugmask  [Op_VecA] = NULL;\n-  idealreg2debugmask  [Op_VecS] = NULL;\n-  idealreg2debugmask  [Op_VecD] = NULL;\n-  idealreg2debugmask  [Op_VecX] = NULL;\n-  idealreg2debugmask  [Op_VecY] = NULL;\n-  idealreg2debugmask  [Op_VecZ] = NULL;\n-  idealreg2debugmask  [Op_RegFlags] = NULL;\n-  idealreg2debugmask  [Op_RegVectMask] = NULL;\n-\n-  idealreg2mhdebugmask[Op_RegI] = NULL;\n-  idealreg2mhdebugmask[Op_RegN] = NULL;\n-  idealreg2mhdebugmask[Op_RegL] = NULL;\n-  idealreg2mhdebugmask[Op_RegF] = NULL;\n-  idealreg2mhdebugmask[Op_RegD] = NULL;\n-  idealreg2mhdebugmask[Op_RegP] = NULL;\n-  idealreg2mhdebugmask[Op_VecA] = NULL;\n-  idealreg2mhdebugmask[Op_VecS] = NULL;\n-  idealreg2mhdebugmask[Op_VecD] = NULL;\n-  idealreg2mhdebugmask[Op_VecX] = NULL;\n-  idealreg2mhdebugmask[Op_VecY] = NULL;\n-  idealreg2mhdebugmask[Op_VecZ] = NULL;\n-  idealreg2mhdebugmask[Op_RegFlags] = NULL;\n-  idealreg2mhdebugmask[Op_RegVectMask] = NULL;\n-\n-  debug_only(_mem_node = NULL;)   \/\/ Ideal memory node consumed by mach node\n+  idealreg2spillmask  [Op_RegI] = nullptr;\n+  idealreg2spillmask  [Op_RegN] = nullptr;\n+  idealreg2spillmask  [Op_RegL] = nullptr;\n+  idealreg2spillmask  [Op_RegF] = nullptr;\n+  idealreg2spillmask  [Op_RegD] = nullptr;\n+  idealreg2spillmask  [Op_RegP] = nullptr;\n+  idealreg2spillmask  [Op_VecA] = nullptr;\n+  idealreg2spillmask  [Op_VecS] = nullptr;\n+  idealreg2spillmask  [Op_VecD] = nullptr;\n+  idealreg2spillmask  [Op_VecX] = nullptr;\n+  idealreg2spillmask  [Op_VecY] = nullptr;\n+  idealreg2spillmask  [Op_VecZ] = nullptr;\n+  idealreg2spillmask  [Op_RegFlags] = nullptr;\n+  idealreg2spillmask  [Op_RegVectMask] = nullptr;\n+\n+  idealreg2debugmask  [Op_RegI] = nullptr;\n+  idealreg2debugmask  [Op_RegN] = nullptr;\n+  idealreg2debugmask  [Op_RegL] = nullptr;\n+  idealreg2debugmask  [Op_RegF] = nullptr;\n+  idealreg2debugmask  [Op_RegD] = nullptr;\n+  idealreg2debugmask  [Op_RegP] = nullptr;\n+  idealreg2debugmask  [Op_VecA] = nullptr;\n+  idealreg2debugmask  [Op_VecS] = nullptr;\n+  idealreg2debugmask  [Op_VecD] = nullptr;\n+  idealreg2debugmask  [Op_VecX] = nullptr;\n+  idealreg2debugmask  [Op_VecY] = nullptr;\n+  idealreg2debugmask  [Op_VecZ] = nullptr;\n+  idealreg2debugmask  [Op_RegFlags] = nullptr;\n+  idealreg2debugmask  [Op_RegVectMask] = nullptr;\n+\n+  idealreg2mhdebugmask[Op_RegI] = nullptr;\n+  idealreg2mhdebugmask[Op_RegN] = nullptr;\n+  idealreg2mhdebugmask[Op_RegL] = nullptr;\n+  idealreg2mhdebugmask[Op_RegF] = nullptr;\n+  idealreg2mhdebugmask[Op_RegD] = nullptr;\n+  idealreg2mhdebugmask[Op_RegP] = nullptr;\n+  idealreg2mhdebugmask[Op_VecA] = nullptr;\n+  idealreg2mhdebugmask[Op_VecS] = nullptr;\n+  idealreg2mhdebugmask[Op_VecD] = nullptr;\n+  idealreg2mhdebugmask[Op_VecX] = nullptr;\n+  idealreg2mhdebugmask[Op_VecY] = nullptr;\n+  idealreg2mhdebugmask[Op_VecZ] = nullptr;\n+  idealreg2mhdebugmask[Op_RegFlags] = nullptr;\n+  idealreg2mhdebugmask[Op_RegVectMask] = nullptr;\n+\n+  debug_only(_mem_node = nullptr;)   \/\/ Ideal memory node consumed by mach node\n@@ -145,0 +145,1 @@\n+      \/\/ Bailout. We do not have space to represent all arguments.\n@@ -175,1 +176,1 @@\n-      if (in != NULL) {\n+      if (in != nullptr) {\n@@ -279,0 +280,1 @@\n+      _parm_regs[i].set_bad();\n@@ -313,0 +315,1 @@\n+    \/\/ Bailout. We do not have space to represent all arguments.\n@@ -327,1 +330,1 @@\n-  \/\/ Create new ideal node ConP #NULL even if it does exist in old space\n+  \/\/ Create new ideal node ConP #null even if it does exist in old space\n@@ -338,1 +341,1 @@\n-  if (_old_node_note_array != NULL) {\n+  if (_old_node_note_array != nullptr) {\n@@ -341,1 +344,1 @@\n-                            0, NULL));\n+                            0, nullptr));\n@@ -358,1 +361,1 @@\n-    if (xroot == NULL) {\n+    if (xroot == nullptr) {\n@@ -360,0 +363,1 @@\n+      assert(false, \"instruction match failed\");\n@@ -375,2 +379,2 @@\n-      \/\/ Generate new mach node for ConP #NULL\n-      assert(new_ideal_null != NULL, \"sanity\");\n+      \/\/ Generate new mach node for ConP #null\n+      assert(new_ideal_null != nullptr, \"sanity\");\n@@ -381,1 +385,1 @@\n-      assert(_mach_null != NULL, \"\");\n+      assert(_mach_null != nullptr, \"\");\n@@ -383,1 +387,1 @@\n-      C->set_root(xroot->is_Root() ? xroot->as_Root() : NULL);\n+      C->set_root(xroot->is_Root() ? xroot->as_Root() : nullptr);\n@@ -390,2 +394,10 @@\n-  if (C->top() == NULL || C->root() == NULL) {\n-    C->record_method_not_compilable(\"graph lost\"); \/\/ %%% cannot happen?\n+  if (C->top() == nullptr || C->root() == nullptr) {\n+    \/\/ New graph lost. This is due to a compilation failure we encountered earlier.\n+    stringStream ss;\n+    if (C->failure_reason() != nullptr) {\n+      ss.print(\"graph lost: %s\", C->failure_reason());\n+    } else {\n+      assert(C->failure_reason() != nullptr, \"graph lost: reason unknown\");\n+      ss.print(\"graph lost: reason unknown\");\n+    }\n+    C->record_method_not_compilable(ss.as_string());\n@@ -1027,1 +1039,1 @@\n-      if (n1at != NULL) {\n+      if (n1at != nullptr) {\n@@ -1069,0 +1081,1 @@\n+    case Op_VectorizedHashCode:\n@@ -1077,1 +1090,1 @@\n-      nat = NULL;\n+      nat = nullptr;\n@@ -1100,1 +1113,1 @@\n-  mstack.push(n, Visit, NULL, -1);  \/\/ set NULL as parent to indicate root\n+  mstack.push(n, Visit, nullptr, -1);  \/\/ set null as parent to indicate root\n@@ -1103,1 +1116,1 @@\n-    if (C->failing()) return NULL;\n+    if (C->failing()) return nullptr;\n@@ -1120,2 +1133,2 @@\n-            if (C->failing())  return NULL;\n-            if (m == NULL) { Matcher::soft_match_failure(); return NULL; }\n+            if (C->failing())  return nullptr;\n+            if (m == nullptr) { Matcher::soft_match_failure(); return nullptr; }\n@@ -1126,1 +1139,1 @@\n-            if (n->is_Proj() && n->in(0) != NULL && n->in(0)->is_Multi()) {       \/\/ Projections?\n+            if (n->is_Proj() && n->in(0) != nullptr && n->in(0)->is_Multi()) {       \/\/ Projections?\n@@ -1130,1 +1143,1 @@\n-              if (m->in(0) != NULL) \/\/ m might be top\n+              if (m->in(0) != nullptr) \/\/ m might be top\n@@ -1142,1 +1155,1 @@\n-          if (_old_node_note_array != NULL) {\n+          if (_old_node_note_array != nullptr) {\n@@ -1160,1 +1173,1 @@\n-        if (m == NULL) break;\n+        if (m == nullptr) break;\n@@ -1168,1 +1181,1 @@\n-        if (m == NULL || C->node_arena()->contains(m)) continue;\n+        if (m == nullptr || C->node_arena()->contains(m)) continue;\n@@ -1203,1 +1216,1 @@\n-        if(m != NULL)\n+        if(m != nullptr)\n@@ -1211,1 +1224,1 @@\n-      if (p != NULL) { \/\/ root doesn't have parent\n+      if (p != nullptr) { \/\/ root doesn't have parent\n@@ -1243,0 +1256,1 @@\n+      \/\/ Bailout. For example not enough space on stack for all arguments. Happens for methods with too many arguments.\n@@ -1257,2 +1271,2 @@\n-  MachSafePointNode *msfpt = NULL;\n-  MachCallNode      *mcall = NULL;\n+  MachSafePointNode *msfpt = nullptr;\n+  MachCallNode      *mcall = nullptr;\n@@ -1263,1 +1277,1 @@\n-  ciMethod*        method = NULL;\n+  ciMethod*        method = nullptr;\n@@ -1272,2 +1286,2 @@\n-    if (C->failing())  return NULL;\n-    if( m == NULL ) { Matcher::soft_match_failure(); return NULL; }\n+    if (C->failing())  return nullptr;\n+    if( m == nullptr ) { Matcher::soft_match_failure(); return nullptr; }\n@@ -1309,7 +1323,0 @@\n-    else if( mcall->is_MachCallNative() ) {\n-      MachCallNativeNode* mach_call_native = mcall->as_MachCallNative();\n-      CallNativeNode* call_native = call->as_CallNative();\n-      mach_call_native->_name = call_native->_name;\n-      mach_call_native->_arg_regs = call_native->_arg_regs;\n-      mach_call_native->_ret_regs = call_native->_ret_regs;\n-    }\n@@ -1320,2 +1327,2 @@\n-    call = NULL;\n-    domain = NULL;\n+    call = nullptr;\n+    domain = nullptr;\n@@ -1323,1 +1330,1 @@\n-    if (C->failing())  return NULL;\n+    if (C->failing())  return nullptr;\n@@ -1348,1 +1355,1 @@\n-  if( call != NULL && call->is_CallRuntime() )\n+  if( call != nullptr && call->is_CallRuntime() )\n@@ -1350,2 +1357,0 @@\n-  if( call != NULL && call->is_CallNative() )\n-    out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call, call->as_CallNative()->_shadow_space_bytes);\n@@ -1444,0 +1449,1 @@\n+      \/\/ Bailout. We do not have space to represent all arguments.\n@@ -1461,1 +1467,1 @@\n-  assert((mcall == NULL) || (mcall->jvms() == NULL) ||\n+  assert((mcall == nullptr) || (mcall->jvms() == nullptr) ||\n@@ -1500,1 +1506,1 @@\n-  _mem_node = n->is_Store() ? (Node*)n : NULL;\n+  _mem_node = n->is_Store() ? (Node*)n : nullptr;\n@@ -1505,2 +1511,2 @@\n-  s->_kids[0] = NULL;\n-  s->_kids[1] = NULL;\n+  s->_kids[0] = nullptr;\n+  s->_kids[1] = nullptr;\n@@ -1511,1 +1517,1 @@\n-  if (C->failing())  return NULL;\n+  if (C->failing())  return nullptr;\n@@ -1531,1 +1537,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1578,1 +1584,1 @@\n-    Node* mem_control = (m->is_Load()) ? m->in(MemNode::Memory)->in(0) : NULL;\n+    Node* mem_control = (m->is_Load()) ? m->in(MemNode::Memory)->in(0) : nullptr;\n@@ -1631,0 +1637,1 @@\n+    \/\/ Bailout. Can for example be hit with a deep chain of operations.\n@@ -1632,1 +1639,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1642,1 +1649,1 @@\n-  Node *input_mem = NULL;\n+  Node *input_mem = nullptr;\n@@ -1648,1 +1655,1 @@\n-      if( input_mem == NULL ) {\n+      if( input_mem == nullptr ) {\n@@ -1670,2 +1677,2 @@\n-    s->_kids[0] = NULL;\n-    s->_kids[1] = NULL;\n+    s->_kids[0] = nullptr;\n+    s->_kids[1] = nullptr;\n@@ -1696,1 +1703,1 @@\n-      if( control == NULL && m->in(0) != NULL && m->req() > 1 )\n+      if( control == nullptr && m->in(0) != nullptr && m->req() > 1 )\n@@ -1700,1 +1707,1 @@\n-      if (C->failing()) return NULL;\n+      if (C->failing()) return nullptr;\n@@ -1728,1 +1735,1 @@\n-  if (!leaf->is_Con() && !leaf->is_DecodeNarrowPtr()) return NULL;\n+  if (!leaf->is_Con() && !leaf->is_DecodeNarrowPtr()) return nullptr;\n@@ -1731,1 +1738,1 @@\n-  if (_shared_nodes.Size() <= leaf->_idx) return NULL;\n+  if (_shared_nodes.Size() <= leaf->_idx) return nullptr;\n@@ -1733,1 +1740,1 @@\n-  if (last != NULL && rule == last->rule()) {\n+  if (last != nullptr && rule == last->rule()) {\n@@ -1739,1 +1746,1 @@\n-    if (xroot == NULL) {\n+    if (xroot == nullptr) {\n@@ -1741,1 +1748,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -1748,1 +1755,1 @@\n-      if (control == NULL || control == C->root()) {\n+      if (control == nullptr || control == C->root()) {\n@@ -1752,1 +1759,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -1757,1 +1764,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1783,1 +1790,1 @@\n-  if (shared_node != NULL) {\n+  if (shared_node != nullptr) {\n@@ -1789,1 +1796,1 @@\n-  guarantee(mach != NULL, \"Missing MachNode\");\n+  guarantee(mach != nullptr, \"Missing MachNode\");\n@@ -1791,1 +1798,1 @@\n-  assert( mach->_opnds[0] != NULL, \"Missing result operand\" );\n+  assert( mach->_opnds[0] != nullptr, \"Missing result operand\" );\n@@ -1814,1 +1821,1 @@\n-    if (oper != NULL && oper != (MachOper*)-1) {\n+    if (oper != nullptr && oper != (MachOper*)-1) {\n@@ -1816,1 +1823,1 @@\n-      Node* m = NULL;\n+      Node* m = nullptr;\n@@ -1821,1 +1828,1 @@\n-        assert(m != NULL && m->is_Mem(), \"expecting memory node\");\n+        assert(m != nullptr && m->is_Mem(), \"expecting memory node\");\n@@ -1860,1 +1867,1 @@\n-      mach->set_req(i,NULL);\n+      mach->set_req(i,nullptr);\n@@ -1893,1 +1900,1 @@\n-    if (n->in(i) != NULL) {\n+    if (n->in(i) != nullptr) {\n@@ -1940,2 +1947,2 @@\n-  if( s->_leaf->in(0) != NULL && s->_leaf->req() > 1) {\n-    if( mach->in(0) == NULL )\n+  if( s->_leaf->in(0) != nullptr && s->_leaf->req() > 1) {\n+    if( mach->in(0) == nullptr )\n@@ -1948,1 +1955,1 @@\n-    if( newstate == NULL ) break;      \/\/ Might only have 1 child\n+    if( newstate == nullptr ) break;      \/\/ Might only have 1 child\n@@ -2003,1 +2010,1 @@\n-  assert( kid == NULL || s->_leaf->in(0) == NULL, \"internal operands have no control\" );\n+  assert( kid == nullptr || s->_leaf->in(0) == nullptr, \"internal operands have no control\" );\n@@ -2006,1 +2013,1 @@\n-  if( kid == NULL && !_swallowed[rule] ) {\n+  if( kid == nullptr && !_swallowed[rule] ) {\n@@ -2027,1 +2034,1 @@\n-  for (uint i = 0; kid != NULL && i < 2; kid = s->_kids[1], i++) {   \/\/ binary tree\n+  for (uint i = 0; kid != nullptr && i < 2; kid = s->_kids[1], i++) {   \/\/ binary tree\n@@ -2067,1 +2074,1 @@\n-  if (n != NULL && m != NULL) {\n+  if (n != nullptr && m != nullptr) {\n@@ -2147,2 +2154,2 @@\n-        if (m == NULL) {\n-          continue;  \/\/ Ignore NULLs\n+        if (m == nullptr) {\n+          continue;  \/\/ Ignore nulls\n@@ -2255,0 +2262,1 @@\n+    case Op_VectorizedHashCode:\n@@ -2265,1 +2273,0 @@\n-    case Op_LoadVectorMasked:\n@@ -2340,3 +2347,0 @@\n-    case Op_StorePConditional:\n-    case Op_StoreIConditional:\n-    case Op_StoreLConditional:\n@@ -2373,3 +2377,1 @@\n-    case Op_CMoveP:\n-    case Op_CMoveVF:\n-    case Op_CMoveVD:  {\n+    case Op_CMoveP: {\n@@ -2387,3 +2389,11 @@\n-    case Op_VectorCmpMasked: {\n-      Node* pair1 = new BinaryNode(n->in(2), n->in(3));\n-      n->set_req(2, pair1);\n+    case Op_CMoveVF:\n+    case Op_CMoveVD: {\n+      \/\/ Restructure into a binary tree for Matching:\n+      \/\/ CMoveVF (Binary bool mask) (Binary src1 src2)\n+      Node* in_cc = n->in(1);\n+      assert(in_cc->is_Con(), \"The condition input of cmove vector node must be a constant.\");\n+      Node* bol = new BoolNode(in_cc, (BoolTest::mask)in_cc->get_int());\n+      Node* pair1 = new BinaryNode(bol, in_cc);\n+      n->set_req(1, pair1);\n+      Node* pair2 = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair2);\n@@ -2424,1 +2434,2 @@\n-    case Op_StrIndexOf: {\n+    case Op_StrIndexOf:\n+    case Op_VectorizedHashCode: {\n@@ -2433,0 +2444,1 @@\n+    case Op_EncodeISOArray:\n@@ -2434,2 +2446,1 @@\n-    case Op_StrInflatedCopy:\n-    case Op_EncodeISOArray: {\n+    case Op_StrInflatedCopy: {\n@@ -2462,0 +2473,1 @@\n+    case Op_VectorCmpMasked:\n@@ -2463,0 +2475,2 @@\n+    case Op_SignumVF:\n+    case Op_SignumVD:\n@@ -2565,1 +2579,1 @@\n-          \/\/ use it to do implicit NULL check in address.\n+          \/\/ use it to do implicit null check in address.\n@@ -2575,1 +2589,1 @@\n-              val->set_req(0, NULL); \/\/ Unpin now.\n+              val->set_req(0, nullptr); \/\/ Unpin now.\n@@ -2577,1 +2591,1 @@\n-              \/\/ a regular case: CmpP(DecodeN, NULL).\n+              \/\/ a regular case: CmpP(DecodeN, null).\n@@ -2591,1 +2605,1 @@\n-\/\/ Its possible that the value being NULL checked is not the root of a match\n+\/\/ Its possible that the value being null checked is not the root of a match\n@@ -2603,1 +2617,1 @@\n-        assert(val->is_DecodeNarrowPtr() && val->in(0) == NULL, \"sanity\");\n+        assert(val->is_DecodeNarrowPtr() && val->in(0) == nullptr, \"sanity\");\n@@ -2608,1 +2622,1 @@\n-        new_val->set_req(0, NULL);\n+        new_val->set_req(0, nullptr);\n@@ -2634,1 +2648,1 @@\n-          CompressedOops::base() != NULL);\n+          CompressedOops::base() != nullptr);\n@@ -2640,1 +2654,1 @@\n-  if (t == NULL) {\n+  if (t == nullptr) {\n@@ -2642,1 +2656,1 @@\n-    return NULL; \/\/ not supported\n+    return nullptr; \/\/ not supported\n@@ -2651,6 +2665,6 @@\n-    case Op_RegN: spill = new LoadNNode(NULL, mem, fp, atp, t->is_narrowoop(), mo); break;\n-    case Op_RegI: spill = new LoadINode(NULL, mem, fp, atp, t->is_int(),       mo); break;\n-    case Op_RegP: spill = new LoadPNode(NULL, mem, fp, atp, t->is_ptr(),       mo); break;\n-    case Op_RegF: spill = new LoadFNode(NULL, mem, fp, atp, t,                 mo); break;\n-    case Op_RegD: spill = new LoadDNode(NULL, mem, fp, atp, t,                 mo); break;\n-    case Op_RegL: spill = new LoadLNode(NULL, mem, fp, atp, t->is_long(),      mo); break;\n+    case Op_RegN: spill = new LoadNNode(nullptr, mem, fp, atp, t->is_narrowoop(), mo); break;\n+    case Op_RegI: spill = new LoadINode(nullptr, mem, fp, atp, t->is_int(),       mo); break;\n+    case Op_RegP: spill = new LoadPNode(nullptr, mem, fp, atp, t->is_ptr(),       mo); break;\n+    case Op_RegF: spill = new LoadFNode(nullptr, mem, fp, atp, t,                 mo); break;\n+    case Op_RegD: spill = new LoadDNode(nullptr, mem, fp, atp, t,                 mo); break;\n+    case Op_RegL: spill = new LoadLNode(nullptr, mem, fp, atp, t->is_long(),      mo); break;\n@@ -2663,1 +2677,1 @@\n-    case Op_VecZ: spill = new LoadVectorNode(NULL, mem, fp, atp, t->is_vect()); break;\n+    case Op_VecZ: spill = new LoadVectorNode(nullptr, mem, fp, atp, t->is_vect()); break;\n@@ -2669,1 +2683,1 @@\n-  assert(mspill != NULL, \"matching failed: %d\", ideal_reg);\n+  assert(mspill != nullptr, \"matching failed: %d\", ideal_reg);\n@@ -2705,1 +2719,1 @@\n-  Node* def = NULL;\n+  Node* def = nullptr;\n@@ -2747,1 +2761,1 @@\n-    if (m != NULL) {\n+    if (m != nullptr) {\n@@ -2803,1 +2817,1 @@\n-      if (m != NULL) {\n+      if (m != nullptr) {\n@@ -2826,1 +2840,1 @@\n-  Node* ctrl = NULL;\n+  Node* ctrl = nullptr;\n@@ -2836,1 +2850,1 @@\n-  assert((ctrl != NULL), \"missing control projection\");\n+  assert((ctrl != nullptr), \"missing control projection\");\n@@ -2909,1 +2923,1 @@\n-  Node *ifFalse = NULL;\n+  Node *ifFalse = nullptr;\n@@ -2921,3 +2935,3 @@\n-  while (reg != NULL && cnt > 0) {\n-    CallNode *call = NULL;\n-    RegionNode *nxt_reg = NULL;\n+  while (reg != nullptr && cnt > 0) {\n+    CallNode *call = nullptr;\n+    RegionNode *nxt_reg = nullptr;\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":182,"deletions":168,"binary":false,"changes":350,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-  ciInstanceKlass* ik = vbox_type->klass()->as_instance_klass();\n+  ciInstanceKlass* ik = vbox_type->instance_klass();\n@@ -46,1 +46,1 @@\n-  assert(fd1 != NULL, \"element type info is missing\");\n+  assert(fd1 != nullptr, \"element type info is missing\");\n@@ -53,1 +53,1 @@\n-  assert(fd2 != NULL, \"vector length info is missing\");\n+  assert(fd2 != nullptr, \"vector length info is missing\");\n@@ -66,4 +66,0 @@\n-static bool is_vector_shuffle(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n-}\n-\n@@ -83,1 +79,3 @@\n-    \/\/ Check whether mask unboxing is supported.\n+    \/\/ Check if mask unboxing is supported, this is a two step process which first loads the contents\n+    \/\/ of boolean array into vector followed by either lane expansion to match the lane size of masked\n+    \/\/ vector operation or populate the predicate register.\n@@ -129,2 +127,1 @@\n-    default:\n-      assert(false, \"Unexpected type\");\n+    default: fatal(\"Unexpected type: %s\", type2name(elem_bt));\n@@ -157,1 +154,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->instance_klass()));\n@@ -162,1 +159,1 @@\n-Node* GraphKit::unbox_vector(Node* v, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool shuffle_to_vector) {\n+Node* GraphKit::unbox_vector(Node* v, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem) {\n@@ -165,2 +162,2 @@\n-  if (vbox_type->klass() != vbox_type_v->klass()) {\n-    return NULL; \/\/ arguments don't agree on vector shapes\n+  if (vbox_type->instance_klass() != vbox_type_v->instance_klass()) {\n+    return nullptr; \/\/ arguments don't agree on vector shapes\n@@ -169,1 +166,1 @@\n-    return NULL; \/\/ no nulls are allowed\n+    return nullptr; \/\/ no nulls are allowed\n@@ -172,2 +169,2 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n-  Node* unbox = gvn().transform(new VectorUnboxNode(C, vt, v, merged_memory(), shuffle_to_vector));\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->instance_klass()));\n+  Node* unbox = gvn().transform(new VectorUnboxNode(C, vt, v, merged_memory()));\n@@ -264,1 +261,3 @@\n-  \/\/ Check whether mask unboxing is supported.\n+  \/\/ Check if mask unboxing is supported, this is a two step process which first loads the contents\n+  \/\/ of boolean array into vector followed by either lane expansion to match the lane size of masked\n+  \/\/ vector operation or populate the predicate register.\n@@ -278,1 +277,3 @@\n-  \/\/ Check whether mask boxing is supported.\n+  \/\/ Check if mask boxing is supported, this is a two step process which first stores the contents\n+  \/\/ of mask vector \/ predicate register into a boolean vector followed by vector store operation to\n+  \/\/ transfer the contents to underlined storage of mask boxes which is a boolean array.\n@@ -317,1 +318,1 @@\n-  if (vec_klass->const_oop() == NULL) {\n+  if (vec_klass->const_oop() == nullptr) {\n@@ -320,1 +321,1 @@\n-  assert(vec_klass->const_oop()->as_instance()->java_lang_Class_klass() != NULL, \"klass instance expected\");\n+  assert(vec_klass->const_oop()->as_instance()->java_lang_Class_klass() != nullptr, \"klass instance expected\");\n@@ -356,2 +357,2 @@\n-  if (opr == NULL || vector_klass == NULL || elem_klass == NULL || vlen == NULL ||\n-      !opr->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (opr == nullptr || vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      !opr->is_con() || vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -387,1 +388,1 @@\n-    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+    if (mask_klass == nullptr || mask_klass->const_oop() == nullptr) {\n@@ -479,1 +480,1 @@\n-  Node* opd1 = NULL; Node* opd2 = NULL; Node* opd3 = NULL;\n+  Node* opd1 = nullptr; Node* opd2 = nullptr; Node* opd3 = nullptr;\n@@ -483,1 +484,1 @@\n-      if (opd3 == NULL) {\n+      if (opd3 == nullptr) {\n@@ -494,1 +495,1 @@\n-      if (opd2 == NULL) {\n+      if (opd2 == nullptr) {\n@@ -505,1 +506,1 @@\n-      if (opd1 == NULL) {\n+      if (opd1 == nullptr) {\n@@ -517,1 +518,1 @@\n-  Node* mask = NULL;\n+  Node* mask = nullptr;\n@@ -523,1 +524,1 @@\n-    if (mask == NULL) {\n+    if (mask == nullptr) {\n@@ -532,1 +533,1 @@\n-  Node* operation = NULL;\n+  Node* operation = nullptr;\n@@ -536,1 +537,1 @@\n-    if (operation == NULL) {\n+    if (operation == nullptr) {\n@@ -561,1 +562,1 @@\n-  if (is_masked_op && mask != NULL) {\n+  if (is_masked_op && mask != nullptr) {\n@@ -566,1 +567,1 @@\n-      operation = gvn().transform(operation);\n+      operation = gvn().transform(operation);\n@@ -580,105 +581,0 @@\n-\/\/ <Sh extends VectorShuffle<E>,  E>\n-\/\/  Sh ShuffleIota(Class<?> E, Class<?> shuffleClass, Vector.Species<E> s, int length,\n-\/\/                  int start, int step, int wrap, ShuffleIotaOperation<Sh, E> defaultImpl)\n-bool LibraryCallKit::inline_vector_shuffle_iota() {\n-  const TypeInstPtr* elem_klass    = gvn().type(argument(0))->isa_instptr();\n-  const TypeInstPtr* shuffle_klass = gvn().type(argument(1))->isa_instptr();\n-  const TypeInt*     vlen          = gvn().type(argument(3))->isa_int();\n-  const TypeInt*     start_val     = gvn().type(argument(4))->isa_int();\n-  const TypeInt*     step_val      = gvn().type(argument(5))->isa_int();\n-  const TypeInt*     wrap          = gvn().type(argument(6))->isa_int();\n-\n-  Node* start = argument(4);\n-  Node* step  = argument(5);\n-\n-  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n-  if (!elem_type->is_primitive_type()) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n-    }\n-    return false; \/\/ should be primitive type\n-  }\n-  if (shuffle_klass == NULL || vlen == NULL || start_val == NULL || step_val == NULL || wrap == NULL) {\n-    return false; \/\/ dead code\n-  }\n-  if (!vlen->is_con() || !is_power_of_2(vlen->get_con()) ||\n-      shuffle_klass->const_oop() == NULL || !wrap->is_con()) {\n-    return false; \/\/ not enough info for intrinsification\n-  }\n-  if (!is_klass_initialized(shuffle_klass)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** klass argument not initialized\");\n-    }\n-    return false;\n-  }\n-\n-  int do_wrap = wrap->get_con();\n-  int num_elem = vlen->get_con();\n-  BasicType elem_bt = T_BYTE;\n-\n-  if (!arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed)) {\n-    return false;\n-  }\n-  if (!arch_supports_vector(Op_AddVB, num_elem, elem_bt, VecMaskNotUsed)) {\n-    return false;\n-  }\n-  if (!arch_supports_vector(Op_AndV, num_elem, elem_bt, VecMaskNotUsed)) {\n-    return false;\n-  }\n-  if (!arch_supports_vector(Op_VectorLoadConst, num_elem, elem_bt, VecMaskNotUsed)) {\n-    return false;\n-  }\n-  if (!arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n-    return false;\n-  }\n-  if (!arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUseStore)) {\n-    return false;\n-  }\n-\n-  const Type * type_bt = Type::get_const_basic_type(elem_bt);\n-  const TypeVect * vt  = TypeVect::make(type_bt, num_elem);\n-\n-  Node* res =  gvn().transform(new VectorLoadConstNode(gvn().makecon(TypeInt::ZERO), vt));\n-\n-  if(!step_val->is_con() || !is_power_of_2(step_val->get_con())) {\n-    Node* bcast_step     = gvn().transform(VectorNode::scalar2vector(step, num_elem, type_bt));\n-    res = gvn().transform(VectorNode::make(Op_MulI, res, bcast_step, num_elem, elem_bt));\n-  } else if (step_val->get_con() > 1) {\n-    Node* cnt = gvn().makecon(TypeInt::make(log2i_exact(step_val->get_con())));\n-    Node* shift_cnt = vector_shift_count(cnt, Op_LShiftI, elem_bt, num_elem);\n-    res = gvn().transform(VectorNode::make(Op_LShiftVB, res, shift_cnt, vt));\n-  }\n-\n-  if (!start_val->is_con() || start_val->get_con() != 0) {\n-    Node* bcast_start    = gvn().transform(VectorNode::scalar2vector(start, num_elem, type_bt));\n-    res = gvn().transform(VectorNode::make(Op_AddI, res, bcast_start, num_elem, elem_bt));\n-  }\n-\n-  Node * mod_val = gvn().makecon(TypeInt::make(num_elem-1));\n-  Node * bcast_mod  = gvn().transform(VectorNode::scalar2vector(mod_val, num_elem, type_bt));\n-  if(do_wrap)  {\n-    \/\/ Wrap the indices greater than lane count.\n-    res = gvn().transform(VectorNode::make(Op_AndI, res, bcast_mod, num_elem, elem_bt));\n-  } else {\n-    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::ge));\n-    Node * lane_cnt  = gvn().makecon(TypeInt::make(num_elem));\n-    Node * bcast_lane_cnt = gvn().transform(VectorNode::scalar2vector(lane_cnt, num_elem, type_bt));\n-    const TypeVect* vmask_type = TypeVect::makemask(elem_bt, num_elem);\n-    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vmask_type));\n-\n-    \/\/ Make the indices greater than lane count as -ve values. This matches the java side implementation.\n-    res = gvn().transform(VectorNode::make(Op_AndI, res, bcast_mod, num_elem, elem_bt));\n-    Node * biased_val = gvn().transform(VectorNode::make(Op_SubI, res, bcast_lane_cnt, num_elem, elem_bt));\n-    res = gvn().transform(new VectorBlendNode(biased_val, res, mask));\n-  }\n-\n-  ciKlass* sbox_klass = shuffle_klass->const_oop()->as_instance()->java_lang_Class_klass();\n-  const TypeInstPtr* shuffle_box_type = TypeInstPtr::make_exact(TypePtr::NotNull, sbox_klass);\n-\n-  \/\/ Wrap it up in VectorBox to keep object type information.\n-  res = box_vector(res, shuffle_box_type, elem_bt, num_elem);\n-  set_result(res);\n-  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n-  return true;\n-}\n-\n@@ -695,1 +591,1 @@\n-  if (mask_klass == NULL || elem_klass == NULL || mask->is_top() || vlen == NULL) {\n+  if (mask_klass == nullptr || elem_klass == nullptr || mask->is_top() || vlen == nullptr) {\n@@ -728,2 +624,2 @@\n-  Node* mask_vec = unbox_vector(mask, mask_box_type, elem_bt, num_elem, true);\n-  if (mask_vec == NULL) {\n+  Node* mask_vec = unbox_vector(mask, mask_box_type, elem_bt, num_elem);\n+  if (mask_vec == nullptr) {\n@@ -737,1 +633,1 @@\n-  if (mask_vec->bottom_type()->isa_vectmask() == NULL) {\n+  if (mask_vec->bottom_type()->isa_vectmask() == nullptr) {\n@@ -751,71 +647,0 @@\n-\/\/ public static\n-\/\/ <V,\n-\/\/  Sh extends VectorShuffle<E>,\n-\/\/  E>\n-\/\/ V shuffleToVector(Class<? extends Vector<E>> vclass, Class<E> elementType,\n-\/\/                   Class<? extends Sh> shuffleClass, Sh s, int length,\n-\/\/                   ShuffleToVectorOperation<V, Sh, E> defaultImpl)\n-bool LibraryCallKit::inline_vector_shuffle_to_vector() {\n-  const TypeInstPtr* vector_klass  = gvn().type(argument(0))->isa_instptr();\n-  const TypeInstPtr* elem_klass    = gvn().type(argument(1))->isa_instptr();\n-  const TypeInstPtr* shuffle_klass = gvn().type(argument(2))->isa_instptr();\n-  Node*              shuffle       = argument(3);\n-  const TypeInt*     vlen          = gvn().type(argument(4))->isa_int();\n-\n-  if (vector_klass == NULL || elem_klass == NULL || shuffle_klass == NULL || shuffle->is_top() || vlen == NULL) {\n-    return false; \/\/ dead code\n-  }\n-  if (!vlen->is_con() || vector_klass->const_oop() == NULL || shuffle_klass->const_oop() == NULL) {\n-    return false; \/\/ not enough info for intrinsification\n-  }\n-  if (!is_klass_initialized(shuffle_klass) || !is_klass_initialized(vector_klass) ) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** klass argument not initialized\");\n-    }\n-    return false;\n-  }\n-\n-  int num_elem = vlen->get_con();\n-  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n-  if (!elem_type->is_primitive_type()) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n-    }\n-    return false; \/\/ should be primitive type\n-  }\n-  BasicType elem_bt = elem_type->basic_type();\n-\n-  if (num_elem < 4) {\n-    return false;\n-  }\n-\n-  int cast_vopc = VectorCastNode::opcode(-1, T_BYTE); \/\/ from shuffle of type T_BYTE\n-  \/\/ Make sure that cast is implemented to particular type\/size combination.\n-  if (!arch_supports_vector(cast_vopc, num_elem, elem_bt, VecMaskNotUsed)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=1 op=cast#%d\/3 vlen2=%d etype2=%s\",\n-        cast_vopc, num_elem, type2name(elem_bt));\n-    }\n-    return false;\n-  }\n-\n-  ciKlass* sbox_klass = shuffle_klass->const_oop()->as_instance()->java_lang_Class_klass();\n-  const TypeInstPtr* shuffle_box_type = TypeInstPtr::make_exact(TypePtr::NotNull, sbox_klass);\n-\n-  \/\/ Unbox shuffle with true flag to indicate its load shuffle to vector\n-  \/\/ shuffle is a byte array\n-  Node* shuffle_vec = unbox_vector(shuffle, shuffle_box_type, T_BYTE, num_elem, true);\n-\n-  \/\/ cast byte to target element type\n-  shuffle_vec = gvn().transform(VectorCastNode::make(cast_vopc, shuffle_vec, elem_bt, num_elem));\n-\n-  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n-  const TypeInstPtr* vec_box_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n-\n-  \/\/ Box vector\n-  Node* res = box_vector(shuffle_vec, vec_box_type, elem_bt, num_elem);\n-  set_result(res);\n-  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n-  return true;\n-}\n-\n@@ -839,2 +664,2 @@\n-  if (vector_klass == NULL || elem_klass == NULL || vlen == NULL || mode == NULL ||\n-      bits_type == NULL || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL ||\n+  if (vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr || mode == nullptr ||\n+      bits_type == nullptr || vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr ||\n@@ -885,1 +710,1 @@\n-  Node* broadcast = NULL;\n+  Node* broadcast = nullptr;\n@@ -934,1 +759,1 @@\n-  assert(arr_type != NULL, \"unexpected\");\n+  assert(arr_type != nullptr, \"unexpected\");\n@@ -973,2 +798,2 @@\n-  if (vector_klass == NULL || elem_klass == NULL || vlen == NULL ||\n-      vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1034,1 +859,1 @@\n-  const bool is_mismatched_access = in_heap && (addr_type->isa_aryptr() == NULL);\n+  const bool is_mismatched_access = in_heap && (addr_type->isa_aryptr() == nullptr);\n@@ -1039,1 +864,1 @@\n-  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  bool using_byte_array = arr_type != nullptr && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n@@ -1042,1 +867,1 @@\n-  if (arr_type != NULL && !using_byte_array && !is_mask && !elem_consistent_with_arr(elem_bt, arr_type)) {\n+  if (arr_type != nullptr && !using_byte_array && !is_mask && !elem_consistent_with_arr(elem_bt, arr_type)) {\n@@ -1091,1 +916,1 @@\n-    if (val == NULL) {\n+    if (val == nullptr) {\n@@ -1112,1 +937,1 @@\n-    Node* vload = NULL;\n+    Node* vload = nullptr;\n@@ -1131,1 +956,1 @@\n-  old_map->destruct(&_gvn);\n+  destruct_map_clone(old_map);\n@@ -1148,1 +973,1 @@\n-\/\/              int length, Object base, long offset, M m,\n+\/\/              int length, Object base, long offset, M m, int offsetInRange,\n@@ -1169,3 +994,3 @@\n-  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n-      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n-      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (vector_klass == nullptr || mask_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      vector_klass->const_oop() == nullptr || mask_klass->const_oop() == nullptr ||\n+      elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1218,1 +1043,1 @@\n-  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  bool using_byte_array = arr_type != nullptr && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n@@ -1220,1 +1045,1 @@\n-  if (arr_type != NULL && !using_byte_array && !elem_consistent_with_arr(elem_bt, arr_type)) {\n+  if (arr_type != nullptr && !using_byte_array && !elem_consistent_with_arr(elem_bt, arr_type)) {\n@@ -1233,9 +1058,34 @@\n-  bool use_predicate = arch_supports_vector(is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked,\n-                                            mem_num_elem, mem_elem_bt,\n-                                            (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred));\n-  \/\/ Masked vector store operation needs the architecture predicate feature. We need to check\n-  \/\/ whether the predicated vector operation is supported by backend.\n-  if (is_store && !use_predicate) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: op=storeMasked vlen=%d etype=%s using_byte_array=%d\",\n-                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+  bool supports_predicate = arch_supports_vector(is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked,\n+                                                mem_num_elem, mem_elem_bt, VecMaskUseLoad);\n+\n+  \/\/ If current arch does not support the predicated operations, we have to bail\n+  \/\/ out when current case uses the predicate feature.\n+  if (!supports_predicate) {\n+    bool needs_predicate = false;\n+    if (is_store) {\n+      \/\/ Masked vector store always uses the predicated store.\n+      needs_predicate = true;\n+    } else {\n+      \/\/ Masked vector load with IOOBE always uses the predicated load.\n+      const TypeInt* offset_in_range = gvn().type(argument(8))->isa_int();\n+      if (!offset_in_range->is_con()) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** missing constant: offsetInRange=%s\",\n+                        NodeClassNames[argument(8)->Opcode()]);\n+        }\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+      needs_predicate = (offset_in_range->get_con() == 0);\n+    }\n+\n+    if (needs_predicate) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: op=%s vlen=%d etype=%s using_byte_array=%d\",\n+                      is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n@@ -1243,3 +1093,0 @@\n-    set_map(old_map);\n-    set_sp(old_sp);\n-    return false;\n@@ -1250,1 +1097,1 @@\n-  if (!use_predicate && (!arch_supports_vector(Op_LoadVector, mem_num_elem, mem_elem_bt, VecMaskNotUsed) ||\n+  if (!supports_predicate && (!arch_supports_vector(Op_LoadVector, mem_num_elem, mem_elem_bt, VecMaskNotUsed) ||\n@@ -1289,1 +1136,1 @@\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  \/\/ Can base be null? Otherwise, always on-heap access.\n@@ -1302,1 +1149,1 @@\n-  if (mask == NULL) {\n+  if (mask == nullptr) {\n@@ -1315,1 +1162,1 @@\n-    if (val == NULL) {\n+    if (val == nullptr) {\n@@ -1338,1 +1185,1 @@\n-    Node* vload = NULL;\n+    Node* vload = nullptr;\n@@ -1347,1 +1194,1 @@\n-    if (use_predicate) {\n+    if (supports_predicate) {\n@@ -1368,1 +1215,1 @@\n-  old_map->destruct(&_gvn);\n+  destruct_map_clone(old_map);\n@@ -1409,2 +1256,2 @@\n-  if (vector_klass == NULL || elem_klass == NULL || vector_idx_klass == NULL || vlen == NULL ||\n-      vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || vector_idx_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (vector_klass == nullptr || elem_klass == nullptr || vector_idx_klass == nullptr || vlen == nullptr ||\n+      vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || vector_idx_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1442,1 +1289,1 @@\n-    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+    if (mask_klass == nullptr || mask_klass->const_oop() == nullptr) {\n@@ -1508,1 +1355,1 @@\n-  if (arr_type == NULL || (arr_type != NULL && !elem_consistent_with_arr(elem_bt, arr_type))) {\n+  if (arr_type == nullptr || (arr_type != nullptr && !elem_consistent_with_arr(elem_bt, arr_type))) {\n@@ -1522,1 +1369,1 @@\n-  if (vbox_idx_klass == NULL) {\n+  if (vbox_idx_klass == nullptr) {\n@@ -1530,1 +1377,1 @@\n-  if (index_vect == NULL) {\n+  if (index_vect == nullptr) {\n@@ -1536,1 +1383,1 @@\n-  Node* mask = NULL;\n+  Node* mask = nullptr;\n@@ -1541,1 +1388,1 @@\n-    if (mask == NULL) {\n+    if (mask == nullptr) {\n@@ -1556,1 +1403,1 @@\n-    if (val == NULL) {\n+    if (val == nullptr) {\n@@ -1563,2 +1410,2 @@\n-    Node* vstore = NULL;\n-    if (mask != NULL) {\n+    Node* vstore = nullptr;\n+    if (mask != nullptr) {\n@@ -1571,2 +1418,2 @@\n-    Node* vload = NULL;\n-    if (mask != NULL) {\n+    Node* vload = nullptr;\n+    if (mask != nullptr) {\n@@ -1581,1 +1428,1 @@\n-  old_map->destruct(&_gvn);\n+  destruct_map_clone(old_map);\n@@ -1601,2 +1448,2 @@\n-  if (opr == NULL || vector_klass == NULL || elem_klass == NULL || vlen == NULL ||\n-      !opr->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (opr == nullptr || vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      !opr->is_con() || vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1629,1 +1476,1 @@\n-    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+    if (mask_klass == nullptr || mask_klass->const_oop() == nullptr) {\n@@ -1679,1 +1526,1 @@\n-  if (opd == NULL) {\n+  if (opd == nullptr) {\n@@ -1683,1 +1530,1 @@\n-  Node* mask = NULL;\n+  Node* mask = nullptr;\n@@ -1689,1 +1536,1 @@\n-    if (mask == NULL) {\n+    if (mask == nullptr) {\n@@ -1699,2 +1546,2 @@\n-  Node* value = NULL;\n-  if (mask == NULL) {\n+  Node* value = nullptr;\n+  if (mask == nullptr) {\n@@ -1702,1 +1549,1 @@\n-    value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+    value = ReductionNode::make(opc, nullptr, init, opd, elem_bt);\n@@ -1705,1 +1552,1 @@\n-      value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+      value = ReductionNode::make(opc, nullptr, init, opd, elem_bt);\n@@ -1711,1 +1558,1 @@\n-      value = ReductionNode::make(opc, NULL, init, value, elem_bt);\n+      value = ReductionNode::make(opc, nullptr, init, value, elem_bt);\n@@ -1716,1 +1563,1 @@\n-  Node* bits = NULL;\n+  Node* bits = nullptr;\n@@ -1754,2 +1601,2 @@\n-  if (cond == NULL || vector_klass == NULL || elem_klass == NULL || vlen == NULL ||\n-      !cond->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (cond == nullptr || vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      !cond->is_con() || vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1794,2 +1641,8 @@\n-  Node* opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n-  if (opd1 == NULL || opd2 == NULL) {\n+  Node* opd2;\n+  if (Matcher::vectortest_needs_second_argument(booltest == BoolTest::overflow,\n+                                                opd1->bottom_type()->isa_vectmask())) {\n+    opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  } else {\n+    opd2 = opd1;\n+  }\n+  if (opd1 == nullptr || opd2 == nullptr) {\n@@ -1798,3 +1651,7 @@\n-  Node* test = new VectorTestNode(opd1, opd2, booltest);\n-  test = gvn().transform(test);\n-  set_result(test);\n+  Node* cmp = gvn().transform(new VectorTestNode(opd1, opd2, booltest));\n+  BoolTest::mask test = Matcher::vectortest_mask(booltest == BoolTest::overflow,\n+                                                 opd1->bottom_type()->isa_vectmask(), num_elem);\n+  Node* bol = gvn().transform(new BoolNode(cmp, test));\n+  Node* res = gvn().transform(new CMoveINode(bol, gvn().intcon(0), gvn().intcon(1), TypeInt::BOOL));\n+\n+  set_result(res);\n@@ -1819,1 +1676,1 @@\n-  if (mask_klass == NULL || vector_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (mask_klass == nullptr || vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr) {\n@@ -1822,2 +1679,2 @@\n-  if (mask_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n-      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (mask_klass->const_oop() == nullptr || vector_klass->const_oop() == nullptr ||\n+      elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1867,1 +1724,1 @@\n-  if (v1 == NULL || v2 == NULL || mask == NULL) {\n+  if (v1 == nullptr || v2 == nullptr || mask == nullptr) {\n@@ -1893,1 +1750,1 @@\n-  if (cond == NULL || vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (cond == nullptr || vector_klass == nullptr || mask_klass == nullptr || elem_klass == nullptr || vlen == nullptr) {\n@@ -1896,2 +1753,2 @@\n-  if (!cond->is_con() || vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n-      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (!cond->is_con() || vector_klass->const_oop() == nullptr || mask_klass->const_oop() == nullptr ||\n+      elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -1954,2 +1811,2 @@\n-  Node* mask = is_masked_op ? unbox_vector(argument(7), mbox_type, elem_bt, num_elem) : NULL;\n-  if (is_masked_op && mask == NULL) {\n+  Node* mask = is_masked_op ? unbox_vector(argument(7), mbox_type, elem_bt, num_elem) : nullptr;\n+  if (is_masked_op && mask == nullptr) {\n@@ -1972,1 +1829,1 @@\n-  if (v1 == NULL || v2 == NULL) {\n+  if (v1 == nullptr || v2 == nullptr) {\n@@ -2014,1 +1871,1 @@\n-  if (vector_klass == NULL  || shuffle_klass == NULL ||  elem_klass == NULL || vlen == NULL) {\n+  if (vector_klass == nullptr  || shuffle_klass == nullptr ||  elem_klass == nullptr || vlen == nullptr) {\n@@ -2017,3 +1874,3 @@\n-  if (shuffle_klass->const_oop() == NULL ||\n-      vector_klass->const_oop()  == NULL ||\n-      elem_klass->const_oop()    == NULL ||\n+  if (shuffle_klass->const_oop() == nullptr ||\n+      vector_klass->const_oop()  == nullptr ||\n+      elem_klass->const_oop()    == nullptr ||\n@@ -2044,0 +1901,1 @@\n+\n@@ -2046,0 +1904,6 @@\n+  if (shuffle_bt == T_FLOAT) {\n+    shuffle_bt = T_INT;\n+  } else if (shuffle_bt == T_DOUBLE) {\n+    shuffle_bt = T_LONG;\n+  }\n+\n@@ -2047,0 +1911,1 @@\n+  bool need_load_shuffle = Matcher::vector_needs_load_shuffle(shuffle_bt, num_elem);\n@@ -2048,1 +1913,1 @@\n-  if (!arch_supports_vector(Op_VectorLoadShuffle, num_elem, elem_bt, VecMaskNotUsed)) {\n+  if (need_load_shuffle && !arch_supports_vector(Op_VectorLoadShuffle, num_elem, shuffle_bt, VecMaskNotUsed)) {\n@@ -2051,1 +1916,1 @@\n-                    num_elem, type2name(elem_bt));\n+                    num_elem, type2name(shuffle_bt));\n@@ -2059,2 +1924,2 @@\n-      (mask_klass == NULL ||\n-       mask_klass->const_oop() == NULL ||\n+      (mask_klass == nullptr ||\n+       mask_klass->const_oop() == nullptr ||\n@@ -2088,0 +1953,2 @@\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* st = TypeVect::make(shuffle_bt, num_elem);\n@@ -2089,1 +1956,1 @@\n-  if (v1 == NULL || shuffle == NULL) {\n+  if (v1 == nullptr || shuffle == nullptr) {\n@@ -2093,1 +1960,1 @@\n-  Node* mask = NULL;\n+  Node* mask = nullptr;\n@@ -2098,1 +1965,1 @@\n-    if (mask == NULL) {\n+    if (mask == nullptr) {\n@@ -2107,0 +1974,4 @@\n+  if (need_load_shuffle) {\n+    shuffle = gvn().transform(new VectorLoadShuffleNode(shuffle, st));\n+  }\n+\n@@ -2113,1 +1984,0 @@\n-      const TypeVect* vt = v1->bottom_type()->is_vect();\n@@ -2129,1 +1999,1 @@\n-  address addr = NULL;\n+  address addr = nullptr;\n@@ -2131,1 +2001,1 @@\n-  assert(name_ptr != NULL, \"unexpected\");\n+  assert(name_ptr != nullptr, \"unexpected\");\n@@ -2151,1 +2021,1 @@\n-      addr = NULL;\n+      addr = nullptr;\n@@ -2162,1 +2032,1 @@\n-  assert(opd1 != NULL, \"must not be null\");\n+  assert(opd1 != nullptr, \"must not be null\");\n@@ -2164,1 +2034,1 @@\n-  const TypeFunc* call_type = OptoRuntime::Math_Vector_Vector_Type(opd2 != NULL ? 2 : 1, vt, vt);\n+  const TypeFunc* call_type = OptoRuntime::Math_Vector_Vector_Type(opd2 != nullptr ? 2 : 1, vt, vt);\n@@ -2170,2 +2040,2 @@\n-  if (addr == NULL) {\n-    return NULL;\n+  if (addr == nullptr) {\n+    return nullptr;\n@@ -2174,1 +2044,1 @@\n-  assert(name != NULL, \"name must not be null\");\n+  assert(name[0] != '\\0', \"name must not be null\");\n@@ -2200,1 +2070,1 @@\n-  if (opr == NULL || vector_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (opr == nullptr || vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr) {\n@@ -2203,1 +2073,1 @@\n-  if (!opr->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (!opr->is_con() || vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n@@ -2223,1 +2093,1 @@\n-    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+    if (mask_klass == nullptr || mask_klass->const_oop() == nullptr) {\n@@ -2303,1 +2173,1 @@\n-  Node* opd2 = NULL;\n+  Node* opd2 = nullptr;\n@@ -2318,1 +2188,1 @@\n-  if (opd1 == NULL || opd2 == NULL) {\n+  if (opd1 == nullptr || opd2 == nullptr) {\n@@ -2322,1 +2192,1 @@\n-  Node* mask = NULL;\n+  Node* mask = nullptr;\n@@ -2327,1 +2197,1 @@\n-    if (mask == NULL) {\n+    if (mask == nullptr) {\n@@ -2336,1 +2206,1 @@\n-  if (is_masked_op && mask != NULL) {\n+  if (is_masked_op && mask != nullptr) {\n@@ -2372,3 +2242,3 @@\n-  if (opr == NULL ||\n-      vector_klass_from == NULL || elem_klass_from == NULL || vlen_from == NULL ||\n-      vector_klass_to   == NULL || elem_klass_to   == NULL || vlen_to   == NULL) {\n+  if (opr == nullptr ||\n+      vector_klass_from == nullptr || elem_klass_from == nullptr || vlen_from == nullptr ||\n+      vector_klass_to   == nullptr || elem_klass_to   == nullptr || vlen_to   == nullptr) {\n@@ -2378,2 +2248,2 @@\n-      vector_klass_from->const_oop() == NULL || elem_klass_from->const_oop() == NULL || !vlen_from->is_con() ||\n-      vector_klass_to->const_oop() == NULL || elem_klass_to->const_oop() == NULL || !vlen_to->is_con()) {\n+      vector_klass_from->const_oop() == nullptr || elem_klass_from->const_oop() == nullptr || !vlen_from->is_con() ||\n+      vector_klass_to->const_oop() == nullptr || elem_klass_to->const_oop() == nullptr || !vlen_to->is_con()) {\n@@ -2407,3 +2277,1 @@\n-  if (is_vector_shuffle(vbox_klass_from)) {\n-    return false; \/\/ vector shuffles aren't supported\n-  }\n+\n@@ -2463,1 +2331,1 @@\n-  if (opd1 == NULL) {\n+  if (opd1 == nullptr) {\n@@ -2476,2 +2344,2 @@\n-      ((src_type->isa_vectmask() == NULL && dst_type->isa_vectmask()) ||\n-       (dst_type->isa_vectmask() == NULL && src_type->isa_vectmask()))) {\n+      ((src_type->isa_vectmask() == nullptr && dst_type->isa_vectmask()) ||\n+       (dst_type->isa_vectmask() == nullptr && src_type->isa_vectmask()))) {\n@@ -2483,8 +2351,6 @@\n-    BasicType new_elem_bt_to = elem_bt_to;\n-    BasicType new_elem_bt_from = elem_bt_from;\n-    if (is_mask && is_floating_point_type(elem_bt_from)) {\n-      new_elem_bt_from = elem_bt_from == T_FLOAT ? T_INT : T_LONG;\n-    }\n-    int cast_vopc = VectorCastNode::opcode(-1, new_elem_bt_from, !is_ucast);\n-    \/\/ Make sure that cast is implemented to particular type\/size combination.\n-    if (!arch_supports_vector(cast_vopc, num_elem_to, elem_bt_to, VecMaskNotUsed)) {\n+    assert(!is_mask || num_elem_from == num_elem_to, \"vector mask cast needs the same elem num\");\n+    int cast_vopc = VectorCastNode::opcode(-1, elem_bt_from, !is_ucast);\n+\n+    \/\/ Make sure that vector cast is implemented to particular type\/size combination if it is\n+    \/\/ not a mask casting.\n+    if (!is_mask && !arch_supports_vector(cast_vopc, num_elem_to, elem_bt_to, VecMaskNotUsed)) {\n@@ -2493,2 +2359,1 @@\n-                      cast_vopc,\n-                      num_elem_to, type2name(elem_bt_to), is_mask);\n+                      cast_vopc, num_elem_to, type2name(elem_bt_to), is_mask);\n@@ -2519,2 +2384,2 @@\n-      \/\/ Since number elements from input is larger than output, simply reduce size of input (we are supposed to\n-      \/\/ drop top elements anyway).\n+      \/\/ Since number of elements from input is larger than output, simply reduce size of input\n+      \/\/ (we are supposed to drop top elements anyway).\n@@ -2536,4 +2401,2 @@\n-      op = gvn().transform(new VectorReinterpretNode(op,\n-                                                     src_type,\n-                                                     TypeVect::make(elem_bt_from,\n-                                                                    num_elem_for_resize)));\n+      const TypeVect* resize_type = TypeVect::make(elem_bt_from, num_elem_for_resize);\n+      op = gvn().transform(new VectorReinterpretNode(op, src_type, resize_type));\n@@ -2541,1 +2404,1 @@\n-    } else {\n+    } else { \/\/ num_elem_from == num_elem_to\n@@ -2543,5 +2406,7 @@\n-        if ((dst_type->isa_vectmask() && src_type->isa_vectmask()) ||\n-            (type2aelembytes(elem_bt_from) == type2aelembytes(elem_bt_to))) {\n-          op = gvn().transform(new VectorMaskCastNode(op, dst_type));\n-        } else {\n-          op = VectorMaskCastNode::makeCastNode(&gvn(), op, dst_type);\n+        \/\/ Make sure that cast for vector mask is implemented to particular type\/size combination.\n+        if (!arch_supports_vector(Op_VectorMaskCast, num_elem_to, elem_bt_to, VecMaskNotUsed)) {\n+          if (C->print_intrinsics()) {\n+            tty->print_cr(\"  ** not supported: arity=1 op=maskcast vlen2=%d etype2=%s ismask=%d\",\n+                          num_elem_to, type2name(elem_bt_to), is_mask);\n+          }\n+          return false;\n@@ -2549,0 +2414,1 @@\n+        op = gvn().transform(new VectorMaskCastNode(op, dst_type));\n@@ -2579,1 +2445,1 @@\n-  if (vector_klass == NULL || elem_klass == NULL || vlen == NULL || idx == NULL) {\n+  if (vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr || idx == nullptr) {\n@@ -2582,1 +2448,1 @@\n-  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con() || !idx->is_con()) {\n+  if (vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con() || !idx->is_con()) {\n@@ -2619,1 +2485,1 @@\n-  if (opd == NULL) {\n+  if (opd == nullptr) {\n@@ -2624,1 +2490,1 @@\n-  assert(gvn().type(insert_val)->isa_long() != NULL, \"expected to be long\");\n+  assert(gvn().type(insert_val)->isa_long() != nullptr, \"expected to be long\");\n@@ -2672,1 +2538,1 @@\n-  if (vector_klass == NULL || elem_klass == NULL || vlen == NULL || idx == NULL) {\n+  if (vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr || idx == nullptr) {\n@@ -2675,1 +2541,1 @@\n-  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con() || !idx->is_con()) {\n+  if (vector_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con() || !idx->is_con()) {\n@@ -2713,1 +2579,1 @@\n-  if (opd == NULL) {\n+  if (opd == nullptr) {\n@@ -2717,1 +2583,2 @@\n-  Node* operation = gvn().transform(ExtractNode::make(opd, idx->get_con(), elem_bt));\n+  ConINode* idx_con = gvn().intcon(idx->get_con())->as_ConI();\n+  Node* operation = gvn().transform(ExtractNode::make(opd, idx_con, elem_bt));\n@@ -2719,1 +2586,1 @@\n-  Node* bits = NULL;\n+  Node* bits = nullptr;\n@@ -2751,4 +2618,4 @@\n-\/\/  V comExpOp(int opr,\n-\/\/             Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n-\/\/             int length, V v, M m,\n-\/\/             CmpExpOperation<V, M> defaultImpl)\n+\/\/  V compressExpandOp(int opr,\n+\/\/                    Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+\/\/                    int length, V v, M m,\n+\/\/                    CompressExpandOperation<V, M> defaultImpl)\n@@ -2762,3 +2629,3 @@\n-  if (vector_klass == NULL || elem_klass == NULL || mask_klass == NULL || vlen == NULL ||\n-      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n-      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (vector_klass == nullptr || elem_klass == nullptr || mask_klass == nullptr || vlen == nullptr ||\n+      vector_klass->const_oop() == nullptr || mask_klass->const_oop() == nullptr ||\n+      elem_klass->const_oop() == nullptr || !vlen->is_con() || !opr->is_con()) {\n@@ -2803,2 +2670,2 @@\n-  Node* opd1 = NULL;\n-  const TypeInstPtr* vbox_type = NULL;\n+  Node* opd1 = nullptr;\n+  const TypeInstPtr* vbox_type = nullptr;\n@@ -2809,1 +2676,1 @@\n-    if (opd1 == NULL) {\n+    if (opd1 == nullptr) {\n@@ -2823,1 +2690,1 @@\n-  if (mask == NULL) {\n+  if (mask == nullptr) {\n@@ -2841,0 +2708,280 @@\n+\n+\/\/ public static\n+\/\/ <V extends Vector<E>,\n+\/\/  E,\n+\/\/  S extends VectorSpecies<E>>\n+\/\/  V indexVector(Class<? extends V> vClass, Class<E> eClass,\n+\/\/                int length,\n+\/\/                V v, int step, S s,\n+\/\/                IndexOperation<V, S> defaultImpl)\n+bool LibraryCallKit::inline_index_vector() {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(2))->isa_int();\n+\n+  if (vector_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      vector_klass->const_oop() == nullptr || !vlen->is_con() ||\n+      elem_klass->const_oop() == nullptr) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  int num_elem = vlen->get_con();\n+  BasicType elem_bt = elem_type->basic_type();\n+\n+  \/\/ Check whether the iota index generation op is supported by the current hardware\n+  if (!arch_supports_vector(Op_VectorLoadConst, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: vlen=%d etype=%s\", num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  int mul_op = VectorSupport::vop2ideal(VectorSupport::VECTOR_OP_MUL, elem_bt);\n+  int vmul_op = VectorNode::opcode(mul_op, elem_bt);\n+  bool needs_mul = true;\n+  Node* scale = argument(4);\n+  const TypeInt* scale_type = gvn().type(scale)->isa_int();\n+  \/\/ Multiply is not needed if the scale is a constant \"1\".\n+  if (scale_type && scale_type->is_con() && scale_type->get_con() == 1) {\n+    needs_mul = false;\n+  } else {\n+    \/\/ Check whether the vector multiply op is supported by the current hardware\n+    if (!arch_supports_vector(vmul_op, num_elem, elem_bt, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: vlen=%d etype=%s\", num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+\n+    \/\/ Check whether the scalar cast op is supported by the current hardware\n+    if (is_floating_point_type(elem_bt) || elem_bt == T_LONG) {\n+      int cast_op = elem_bt == T_LONG ? Op_ConvI2L :\n+                    elem_bt == T_FLOAT? Op_ConvI2F : Op_ConvI2D;\n+      if (!Matcher::match_rule_supported(cast_op)) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** Rejected op (%s) because architecture does not support it\",\n+                        NodeClassNames[cast_op]);\n+        }\n+        return false; \/\/ not supported\n+      }\n+    }\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  Node* opd = unbox_vector(argument(3), vbox_type, elem_bt, num_elem);\n+  if (opd == nullptr) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** unbox failed vector=%s\",\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false;\n+  }\n+\n+  int add_op = VectorSupport::vop2ideal(VectorSupport::VECTOR_OP_ADD, elem_bt);\n+  int vadd_op = VectorNode::opcode(add_op, elem_bt);\n+  bool needs_add = true;\n+  \/\/ The addition is not needed if all the element values of \"opd\" are zero\n+  if (VectorNode::is_all_zeros_vector(opd)) {\n+    needs_add = false;\n+  } else {\n+    \/\/ Check whether the vector addition op is supported by the current hardware\n+    if (!arch_supports_vector(vadd_op, num_elem, elem_bt, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: vlen=%d etype=%s\", num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+  }\n+\n+  \/\/ Compute the iota indice vector\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  Node* index = gvn().transform(new VectorLoadConstNode(gvn().makecon(TypeInt::ZERO), vt));\n+\n+  \/\/ Broadcast the \"scale\" to a vector, and multiply the \"scale\" with iota indice vector.\n+  if (needs_mul) {\n+    switch (elem_bt) {\n+      case T_BOOLEAN: \/\/ fall-through\n+      case T_BYTE:    \/\/ fall-through\n+      case T_SHORT:   \/\/ fall-through\n+      case T_CHAR:    \/\/ fall-through\n+      case T_INT: {\n+        \/\/ no conversion needed\n+        break;\n+      }\n+      case T_LONG: {\n+        scale = gvn().transform(new ConvI2LNode(scale));\n+        break;\n+      }\n+      case T_FLOAT: {\n+        scale = gvn().transform(new ConvI2FNode(scale));\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        scale = gvn().transform(new ConvI2DNode(scale));\n+        break;\n+      }\n+      default: fatal(\"%s\", type2name(elem_bt));\n+    }\n+    scale = gvn().transform(VectorNode::scalar2vector(scale, num_elem, Type::get_const_basic_type(elem_bt)));\n+    index = gvn().transform(VectorNode::make(vmul_op, index, scale, vt));\n+  }\n+\n+  \/\/ Add \"opd\" if addition is needed.\n+  if (needs_add) {\n+    index = gvn().transform(VectorNode::make(vadd_op, opd, index, vt));\n+  }\n+  Node* vbox = box_vector(index, vbox_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static\n+\/\/ <E,\n+\/\/  M extends VectorMask<E>>\n+\/\/ M indexPartiallyInUpperRange(Class<? extends M> mClass, Class<E> eClass, int length,\n+\/\/                              long offset, long limit,\n+\/\/                              IndexPartiallyInUpperRangeOperation<E, M> defaultImpl)\n+bool LibraryCallKit::inline_index_partially_in_upper_range() {\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(2))->isa_int();\n+\n+  if (mask_klass == nullptr || elem_klass == nullptr || vlen == nullptr ||\n+      mask_klass->const_oop() == nullptr || elem_klass->const_oop() == nullptr || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  int num_elem = vlen->get_con();\n+  BasicType elem_bt = elem_type->basic_type();\n+\n+  \/\/ Check whether the necessary ops are supported by current hardware.\n+  bool supports_mask_gen = arch_supports_vector(Op_VectorMaskGen, num_elem, elem_bt, VecMaskUseStore);\n+  if (!supports_mask_gen) {\n+    if (!arch_supports_vector(Op_VectorLoadConst, num_elem, elem_bt, VecMaskNotUsed) ||\n+        !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed) ||\n+        !arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUseStore)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: vlen=%d etype=%s\", num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+\n+    \/\/ Check whether the scalar cast operation is supported by current hardware.\n+    if (elem_bt != T_LONG) {\n+      int cast_op = is_integral_type(elem_bt) ? Op_ConvL2I\n+                                              : (elem_bt == T_FLOAT ? Op_ConvL2F : Op_ConvL2D);\n+      if (!Matcher::match_rule_supported(cast_op)) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** Rejected op (%s) because architecture does not support it\",\n+                        NodeClassNames[cast_op]);\n+        }\n+        return false; \/\/ not supported\n+      }\n+    }\n+  }\n+\n+  Node* offset = argument(3);\n+  Node* limit = argument(5);\n+  if (offset == nullptr || limit == nullptr) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** offset or limit argument is null\");\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  ciKlass* box_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(is_vector_mask(box_klass), \"argument(0) should be a mask class\");\n+  const TypeInstPtr* box_type = TypeInstPtr::make_exact(TypePtr::NotNull, box_klass);\n+\n+  \/\/ We assume \"offset > 0 && limit >= offset && limit - offset < num_elem\".\n+  \/\/ So directly get indexLimit with \"indexLimit = limit - offset\".\n+  Node* indexLimit = gvn().transform(new SubLNode(limit, offset));\n+  Node* mask = nullptr;\n+  if (supports_mask_gen) {\n+    mask = gvn().transform(VectorMaskGenNode::make(indexLimit, elem_bt, num_elem));\n+  } else {\n+    \/\/ Generate the vector mask based on \"mask = iota < indexLimit\".\n+    \/\/ Broadcast \"indexLimit\" to a vector.\n+    switch (elem_bt) {\n+      case T_BOOLEAN: \/\/ fall-through\n+      case T_BYTE:    \/\/ fall-through\n+      case T_SHORT:   \/\/ fall-through\n+      case T_CHAR:    \/\/ fall-through\n+      case T_INT: {\n+        indexLimit = gvn().transform(new ConvL2INode(indexLimit));\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        indexLimit = gvn().transform(new ConvL2DNode(indexLimit));\n+        break;\n+      }\n+      case T_FLOAT: {\n+        indexLimit = gvn().transform(new ConvL2FNode(indexLimit));\n+        break;\n+      }\n+      case T_LONG: {\n+        \/\/ no conversion needed\n+        break;\n+      }\n+      default: fatal(\"%s\", type2name(elem_bt));\n+    }\n+    indexLimit = gvn().transform(VectorNode::scalar2vector(indexLimit, num_elem, Type::get_const_basic_type(elem_bt)));\n+\n+    \/\/ Load the \"iota\" vector.\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    Node* iota = gvn().transform(new VectorLoadConstNode(gvn().makecon(TypeInt::ZERO), vt));\n+\n+    \/\/ Compute the vector mask with \"mask = iota < indexLimit\".\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::lt));\n+    const TypeVect* vmask_type = TypeVect::makemask(elem_bt, num_elem);\n+    mask = gvn().transform(new VectorMaskCmpNode(BoolTest::lt, iota, indexLimit, pred_node, vmask_type));\n+  }\n+  Node* vbox = box_vector(mask, box_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":523,"deletions":376,"binary":false,"changes":899,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -172,0 +172,5 @@\n+  case Op_ReverseBytesUS:\n+    \/\/ Subword operations in superword usually don't have precise info\n+    \/\/ about signedness. But the behavior of reverseBytes for short and\n+    \/\/ char are exactly the same.\n+    return ((bt == T_SHORT || bt == T_CHAR) ? Op_ReverseBytesV : 0);\n@@ -173,0 +178,4 @@\n+    \/\/ There is no reverseBytes() in Byte class but T_BYTE may appear\n+    \/\/ in VectorAPI calls. We still use ReverseBytesI for T_BYTE to\n+    \/\/ ensure vector intrinsification succeeds.\n+    return ((bt == T_INT || bt == T_BYTE) ? Op_ReverseBytesV : 0);\n@@ -174,1 +183,1 @@\n-    return (is_integral_type(bt) ? Op_ReverseBytesV : 0);\n+    return (bt == T_LONG ? Op_ReverseBytesV : 0);\n@@ -176,2 +185,1 @@\n-    \/\/ Not implemented. Returning 0 temporarily\n-    return 0;\n+    return (bt == T_INT || bt == T_LONG ? Op_CompressBitsV : 0);\n@@ -179,2 +187,1 @@\n-    \/\/ Not implemented. Returning 0 temporarily\n-    return 0;\n+    return (bt == T_INT || bt == T_LONG ? Op_ExpandBitsV : 0);\n@@ -251,8 +258,0 @@\n-  case Op_ConvI2F:\n-    return Op_VectorCastI2X;\n-  case Op_ConvL2D:\n-    return Op_VectorCastL2X;\n-  case Op_ConvF2I:\n-    return Op_VectorCastF2X;\n-  case Op_ConvD2L:\n-    return Op_VectorCastD2X;\n@@ -265,0 +264,4 @@\n+  case Op_SignumF:\n+    return Op_SignumVF;\n+  case Op_SignumD:\n+    return Op_SignumVD;\n@@ -267,0 +270,3 @@\n+    assert(!VectorNode::is_convert_opcode(sopc),\n+           \"Convert node %s should be processed by VectorCastNode::opcode()\",\n+           NodeClassNames[sopc]);\n@@ -334,0 +340,6 @@\n+\/\/ Limits on vector size (number of elements) for auto-vectorization.\n+bool VectorNode::vector_size_supported_superword(const BasicType bt, int size) {\n+  return Matcher::superword_max_vector_size(bt) >= size &&\n+         Matcher::min_vector_size(bt) <= size;\n+}\n+\n@@ -339,1 +351,1 @@\n-      Matcher::vector_size_supported(bt, vlen)) {\n+      vector_size_supported_superword(bt, vlen)) {\n@@ -350,1 +362,1 @@\n-    return vopc > 0 && Matcher::match_rule_supported_vector(vopc, vlen, bt);\n+    return vopc > 0 && Matcher::match_rule_supported_superword(vopc, vlen, bt);\n@@ -374,11 +386,0 @@\n-bool VectorNode::is_type_transition_long_to_int(Node* n) {\n-  switch(n->Opcode()) {\n-    case Op_PopCountL:\n-    case Op_CountLeadingZerosL:\n-    case Op_CountTrailingZerosL:\n-       return true;\n-    default:\n-       return false;\n-  }\n-}\n-\n@@ -420,1 +421,0 @@\n-      assert(false, \"not supported: %s\", type2name(bt));\n@@ -457,0 +457,5 @@\n+bool VectorNode::is_populate_index_supported(BasicType bt) {\n+  int vlen = Matcher::max_vector_size(bt);\n+  return Matcher::match_rule_supported_vector(Op_PopulateIndex, vlen, bt);\n+}\n+\n@@ -473,0 +478,44 @@\n+bool VectorNode::can_transform_shift_op(Node* n, BasicType bt) {\n+  if (n->Opcode() != Op_URShiftI) {\n+    return false;\n+  }\n+  Node* in2 = n->in(2);\n+  if (!in2->is_Con()) {\n+    return false;\n+  }\n+  jint cnt = in2->get_int();\n+  \/\/ Only when shift amount is not greater than number of sign extended\n+  \/\/ bits (16 for short and 24 for byte), unsigned shift right on signed\n+  \/\/ subword types can be vectorized as vector signed shift.\n+  if ((bt == T_BYTE && cnt <= 24) || (bt == T_SHORT && cnt <= 16)) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool VectorNode::is_convert_opcode(int opc) {\n+  switch (opc) {\n+    case Op_ConvI2F:\n+    case Op_ConvL2D:\n+    case Op_ConvF2I:\n+    case Op_ConvD2L:\n+    case Op_ConvI2D:\n+    case Op_ConvL2F:\n+    case Op_ConvL2I:\n+    case Op_ConvI2L:\n+    case Op_ConvF2L:\n+    case Op_ConvD2F:\n+    case Op_ConvF2D:\n+    case Op_ConvD2I:\n+    case Op_ConvF2HF:\n+    case Op_ConvHF2F:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool VectorNode::is_minmax_opcode(int opc) {\n+  return opc == Op_MinI || opc == Op_MaxI;\n+}\n+\n@@ -545,0 +594,1 @@\n+  case Op_RoundDoubleMode:\n@@ -548,0 +598,7 @@\n+  case Op_RotateLeft:\n+  case Op_RotateRight:\n+    \/\/ Rotate shift could have 1 or 2 vector operand(s), depending on\n+    \/\/ whether shift distance is a supported constant or not.\n+    *start = 1;\n+    *end   = (n->is_Con() && Matcher::supports_vector_constant_rotates(n->get_int())) ? 2 : 3;\n+    break;\n@@ -595,1 +652,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -687,1 +744,3 @@\n-  case Op_CompressM: assert(n1 == NULL, \"\"); return new CompressMNode(n2, vt);\n+  case Op_CompressM: assert(n1 == nullptr, \"\"); return new CompressMNode(n2, vt);\n+  case Op_CompressBitsV: return new CompressBitsVNode(n1, n2, vt);\n+  case Op_ExpandBitsV: return new ExpandBitsVNode(n1, n2, vt);\n@@ -692,1 +751,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -712,0 +771,2 @@\n+  case Op_SignumVD: return new SignumVDNode(n1, n2, n3, vt);\n+  case Op_SignumVF: return new SignumVFNode(n1, n2, n3, vt);\n@@ -714,1 +775,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -767,1 +828,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -787,1 +848,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -837,1 +898,1 @@\n-static bool is_con_M1(Node* n) {\n+static bool is_con(Node* n, long con) {\n@@ -840,1 +901,1 @@\n-    if (t->isa_int() && t->is_int()->get_con() == -1) {\n+    if (t->isa_int() && t->is_int()->get_con() == (int)con) {\n@@ -843,1 +904,1 @@\n-    if (t->isa_long() && t->is_long()->get_con() == -1) {\n+    if (t->isa_long() && t->is_long()->get_con() == con) {\n@@ -850,0 +911,1 @@\n+\/\/ Return true if every bit in this vector is 1.\n@@ -856,1 +918,16 @@\n-    return is_con_M1(n->in(1));\n+  case Op_MaskAll:\n+    return is_con(n->in(1), -1);\n+  default:\n+    return false;\n+  }\n+}\n+\n+\/\/ Return true if every bit in this vector is 0.\n+bool VectorNode::is_all_zeros_vector(Node* n) {\n+  switch (n->Opcode()) {\n+  case Op_ReplicateB:\n+  case Op_ReplicateS:\n+  case Op_ReplicateI:\n+  case Op_ReplicateL:\n+  case Op_MaskAll:\n+    return is_con(n->in(1), 0);\n@@ -870,0 +947,56 @@\n+Node* VectorNode::try_to_gen_masked_vector(PhaseGVN* gvn, Node* node, const TypeVect* vt) {\n+  int vopc = node->Opcode();\n+  uint vlen = vt->length();\n+  BasicType bt = vt->element_basic_type();\n+\n+  \/\/ Predicated vectors do not need to add another mask input\n+  if (node->is_predicated_vector() || !Matcher::has_predicated_vectors() ||\n+      !Matcher::match_rule_supported_vector_masked(vopc, vlen, bt) ||\n+      !Matcher::match_rule_supported_vector(Op_VectorMaskGen, vlen, bt)) {\n+    return nullptr;\n+  }\n+\n+  Node* mask = nullptr;\n+  \/\/ Generate a vector mask for vector operation whose vector length is lower than the\n+  \/\/ hardware supported max vector length.\n+  if (vt->length_in_bytes() < (uint)MaxVectorSize) {\n+    Node* length = gvn->transform(new ConvI2LNode(gvn->makecon(TypeInt::make(vlen))));\n+    mask = gvn->transform(VectorMaskGenNode::make(length, bt, vlen));\n+  } else {\n+    return nullptr;\n+  }\n+\n+  \/\/ Generate the related masked op for vector load\/store\/load_gather\/store_scatter.\n+  \/\/ Or append the mask to the vector op's input list by default.\n+  switch(vopc) {\n+  case Op_LoadVector:\n+    return new LoadVectorMaskedNode(node->in(0), node->in(1), node->in(2),\n+                                    node->as_LoadVector()->adr_type(), vt, mask,\n+                                    node->as_LoadVector()->control_dependency());\n+  case Op_LoadVectorGather:\n+    return new LoadVectorGatherMaskedNode(node->in(0), node->in(1), node->in(2),\n+                                          node->as_LoadVector()->adr_type(), vt,\n+                                          node->in(3), mask);\n+  case Op_StoreVector:\n+    return new StoreVectorMaskedNode(node->in(0), node->in(1), node->in(2), node->in(3),\n+                                     node->as_StoreVector()->adr_type(), mask);\n+  case Op_StoreVectorScatter:\n+    return new StoreVectorScatterMaskedNode(node->in(0), node->in(1), node->in(2),\n+                                            node->as_StoreVector()->adr_type(),\n+                                            node->in(3), node->in(4), mask);\n+  default:\n+    \/\/ Add the mask as an additional input to the original vector node by default.\n+    \/\/ This is used for almost all the vector nodes.\n+    node->add_req(mask);\n+    node->add_flag(Node::Flag_is_predicated_vector);\n+    return node;\n+  }\n+}\n+\n+Node* VectorNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  if (Matcher::vector_needs_partial_operations(this, vect_type())) {\n+    return try_to_gen_masked_vector(phase, this, vect_type());\n+  }\n+  return nullptr;\n+}\n+\n@@ -890,1 +1023,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -926,1 +1059,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -940,0 +1073,8 @@\n+Node* LoadVectorNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  const TypeVect* vt = vect_type();\n+  if (Matcher::vector_needs_partial_operations(this, vt)) {\n+    return VectorNode::try_to_gen_masked_vector(phase, this, vt);\n+  }\n+  return LoadNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -941,3 +1082,2 @@\n-StoreVectorNode* StoreVectorNode::make(int opc, Node* ctl, Node* mem,\n-                                       Node* adr, const TypePtr* atyp, Node* val,\n-                                       uint vlen) {\n+StoreVectorNode* StoreVectorNode::make(int opc, Node* ctl, Node* mem, Node* adr,\n+                                       const TypePtr* atyp, Node* val, uint vlen) {\n@@ -947,0 +1087,8 @@\n+Node* StoreVectorNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  const TypeVect* vt = vect_type();\n+  if (Matcher::vector_needs_partial_operations(this, vt)) {\n+    return VectorNode::try_to_gen_masked_vector(phase, this, vt);\n+  }\n+  return StoreNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -963,1 +1111,1 @@\n-  return NULL;\n+  return LoadVectorNode::Ideal(phase, can_reshape);\n@@ -983,1 +1131,1 @@\n-  return NULL;\n+  return StoreVectorNode::Ideal(phase, can_reshape);\n@@ -1003,3 +1151,2 @@\n-Node* ExtractNode::make(Node* v, uint position, BasicType bt) {\n-  assert((int)position < Matcher::max_vector_size(bt), \"pos in range\");\n-  ConINode* pos = ConINode::make((int)position);\n+Node* ExtractNode::make(Node* v, ConINode* pos, BasicType bt) {\n+  assert(pos->get_int() < Matcher::max_vector_size(bt), \"pos in range\");\n@@ -1017,1 +1164,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1214,1 +1361,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1218,0 +1365,8 @@\n+Node* ReductionNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  const TypeVect* vt = vect_type();\n+  if (Matcher::vector_needs_partial_operations(this, vt)) {\n+    return VectorNode::try_to_gen_masked_vector(phase, this, vt);\n+  }\n+  return nullptr;\n+}\n+\n@@ -1273,1 +1428,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -1280,1 +1435,1 @@\n-  \/\/handle special case for\/to Half float conversions\n+  \/\/ Handle special case for to\/from Half Float conversions\n@@ -1299,1 +1454,1 @@\n-  \/\/ handle normal conversions\n+  \/\/ Handle normal conversions\n@@ -1308,1 +1463,1 @@\n-      assert(false, \"unknown type: %s\", type2name(bt));\n+      assert(bt == T_CHAR || bt == T_BOOLEAN, \"unknown type: %s\", type2name(bt));\n@@ -1313,0 +1468,11 @@\n+bool VectorCastNode::implemented(int opc, uint vlen, BasicType src_type, BasicType dst_type) {\n+  if (is_java_primitive(dst_type) &&\n+      is_java_primitive(src_type) &&\n+      (vlen > 1) && is_power_of_2(vlen) &&\n+      VectorNode::vector_size_supported_superword(dst_type, vlen)) {\n+    int vopc = VectorCastNode::opcode(opc, src_type);\n+    return vopc > 0 && Matcher::match_rule_supported_superword(vopc, vlen, dst_type);\n+  }\n+  return false;\n+}\n+\n@@ -1339,1 +1505,1 @@\n-          return NULL;\n+          return nullptr;\n@@ -1372,1 +1538,1 @@\n-          default: Unimplemented(); return NULL;\n+          default: Unimplemented(); return nullptr;\n@@ -1389,1 +1555,1 @@\n-          default: Unimplemented(); return NULL;\n+          default: Unimplemented(); return nullptr;\n@@ -1394,1 +1560,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -1401,1 +1567,1 @@\n-      Matcher::vector_size_supported(bt, vlen)) {\n+      VectorNode::vector_size_supported_superword(bt, vlen)) {\n@@ -1403,1 +1569,1 @@\n-    return vopc != opc && Matcher::match_rule_supported_vector(vopc, vlen, bt);\n+    return vopc != opc && Matcher::match_rule_supported_superword(vopc, vlen, bt);\n@@ -1439,2 +1605,2 @@\n-  Node* shiftRCnt = NULL;\n-  Node* shiftLCnt = NULL;\n+  Node* shiftRCnt = nullptr;\n+  Node* shiftLCnt = nullptr;\n@@ -1469,2 +1635,2 @@\n-    Node* shift_mask_node = NULL;\n-    Node* const_one_node = NULL;\n+    Node* shift_mask_node = nullptr;\n+    Node* const_one_node = nullptr;\n@@ -1518,1 +1684,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1528,1 +1694,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1564,1 +1730,1 @@\n-      ciKlass* vbox_klass = vbox->box_type()->klass();\n+      ciKlass* vbox_klass = vbox->box_type()->instance_klass();\n@@ -1571,2 +1737,1 @@\n-        bool is_vector_mask    = vbox_klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n-        bool is_vector_shuffle = vbox_klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+        bool is_vector_mask = vbox_klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n@@ -1574,0 +1739,1 @@\n+          \/\/ VectorUnbox (VectorBox vmask) ==> VectorMaskCast vmask\n@@ -1575,15 +1741,1 @@\n-          if (in_vt->length_in_bytes() == out_vt->length_in_bytes() &&\n-              Matcher::match_rule_supported_vector(Op_VectorMaskCast, out_vt->length(), out_vt->element_basic_type())) {\n-            \/\/ Apply \"VectorUnbox (VectorBox vmask) ==> VectorMaskCast (vmask)\"\n-            \/\/ directly. This could avoid the transformation ordering issue from\n-            \/\/ \"VectorStoreMask (VectorLoadMask vmask) => vmask\".\n-            return new VectorMaskCastNode(value, vmask_type);\n-          }\n-          \/\/ VectorUnbox (VectorBox vmask) ==> VectorLoadMask (VectorStoreMask vmask)\n-          value = phase->transform(VectorStoreMaskNode::make(*phase, value, in_vt->element_basic_type(), in_vt->length()));\n-          return new VectorLoadMaskNode(value, vmask_type);\n-        } else if (is_vector_shuffle) {\n-          if (!is_shuffle_to_vector()) {\n-            \/\/ VectorUnbox (VectorBox vshuffle) ==> VectorLoadShuffle vshuffle\n-            return new VectorLoadShuffleNode(value, out_vt);\n-          }\n+          return new VectorMaskCastNode(value, vmask_type);\n@@ -1591,1 +1743,1 @@\n-          \/\/ Vector type mismatch is only supported for masks and shuffles, but sometimes it happens in pathological cases.\n+          \/\/ Vector type mismatch is only supported for masks, but sometimes it happens in pathological cases.\n@@ -1599,1 +1751,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1636,1 +1788,5 @@\n-  const TypeVectMask* t_vmask = TypeVectMask::make(mask_bt, max_vector);\n+  return make(length, mask_bt, max_vector);\n+}\n+\n+Node* VectorMaskGenNode::make(Node* length, BasicType mask_bt, int mask_len) {\n+  const TypeVectMask* t_vmask = TypeVectMask::make(mask_bt, mask_len);\n@@ -1653,1 +1809,9 @@\n-  return NULL;\n+  return nullptr;\n+}\n+\n+Node* VectorMaskOpNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  const TypeVect* vt = vect_type();\n+  if (Matcher::vector_needs_partial_operations(this, vt)) {\n+    return VectorNode::try_to_gen_masked_vector(phase, this, vt);\n+  }\n+  return nullptr;\n@@ -1663,43 +1827,0 @@\n-\n-Node* VectorMaskCastNode::makeCastNode(PhaseGVN* phase, Node* src, const TypeVect* dst_type) {\n-  const TypeVect* src_type = src->bottom_type()->is_vect();\n-  assert(src_type->length() == dst_type->length(), \"\");\n-\n-  int num_elem = src_type->length();\n-  BasicType elem_bt_from = src_type->element_basic_type();\n-  BasicType elem_bt_to = dst_type->element_basic_type();\n-\n-  if (dst_type->isa_vectmask() == NULL && src_type->isa_vectmask() == NULL &&\n-      type2aelembytes(elem_bt_from) != type2aelembytes(elem_bt_to)) {\n-\n-    Node* op = src;\n-    BasicType new_elem_bt_from = elem_bt_from;\n-    BasicType new_elem_bt_to = elem_bt_to;\n-    if (is_floating_point_type(elem_bt_from)) {\n-      new_elem_bt_from =  elem_bt_from == T_FLOAT ? T_INT : T_LONG;\n-    }\n-    if (is_floating_point_type(elem_bt_to)) {\n-      new_elem_bt_to = elem_bt_to == T_FLOAT ? T_INT : T_LONG;\n-    }\n-\n-    \/\/ Special handling for casting operation involving floating point types.\n-    \/\/ Case A) F -> X :=  F -> VectorMaskCast (F->I\/L [NOP]) -> VectorCast[I\/L]2X\n-    \/\/ Case B) X -> F :=  X -> VectorCastX2[I\/L] -> VectorMaskCast ([I\/L]->F [NOP])\n-    \/\/ Case C) F -> F :=  VectorMaskCast (F->I\/L [NOP]) -> VectorCast[I\/L]2[L\/I] -> VectotMaskCast (L\/I->F [NOP])\n-\n-    if (new_elem_bt_from != elem_bt_from) {\n-      const TypeVect* new_src_type = TypeVect::makemask(new_elem_bt_from, num_elem);\n-      op = phase->transform(new VectorMaskCastNode(op, new_src_type));\n-    }\n-\n-    op = phase->transform(VectorCastNode::make(VectorCastNode::opcode(-1, new_elem_bt_from), op, new_elem_bt_to, num_elem));\n-\n-    if (new_elem_bt_to != elem_bt_to) {\n-      op = phase->transform(new VectorMaskCastNode(op, dst_type));\n-    }\n-    return op;\n-  } else {\n-    return new VectorMaskCastNode(src, dst_type);\n-  }\n-}\n-\n@@ -1716,1 +1837,1 @@\n-     if (dst_type->isa_vectmask() == NULL) {\n+     if (dst_type->isa_vectmask() == nullptr) {\n@@ -1718,1 +1839,1 @@\n-         return NULL;\n+         return nullptr;\n@@ -1724,1 +1845,1 @@\n-         ((src_type->isa_vectmask() == NULL && dst_type->isa_vectmask() == NULL) ||\n+         ((src_type->isa_vectmask() == nullptr && dst_type->isa_vectmask() == nullptr) ||\n@@ -1726,1 +1847,1 @@\n-       return VectorMaskCastNode::makeCastNode(phase, src, dst_type);\n+       return new VectorMaskCastNode(src, dst_type);\n@@ -1729,1 +1850,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1741,2 +1862,2 @@\n-      Node* const_minus_one = NULL;\n-      Node* const_one = NULL;\n+      Node* const_minus_one = nullptr;\n+      Node* const_one = nullptr;\n@@ -1766,1 +1887,1 @@\n-  Node* const_zero = NULL;\n+  Node* const_zero = nullptr;\n@@ -1792,1 +1913,160 @@\n-  return NULL;\n+  return nullptr;\n+}\n+\n+static Node* reverse_operations_identity(Node* n, Node* in1) {\n+  if (n->is_predicated_using_blend()) {\n+    return n;\n+  }\n+  if (n->Opcode() == in1->Opcode()) {\n+    \/\/ OperationV (OperationV X MASK) MASK =>  X\n+    if (n->is_predicated_vector() && in1->is_predicated_vector() && n->in(2) == in1->in(2)) {\n+      return in1->in(1);\n+    \/\/ OperationV (OperationV X) =>  X\n+    } else if (!n->is_predicated_vector() && !in1->is_predicated_vector())  {\n+      return in1->in(1);\n+    }\n+  }\n+  return n;\n+}\n+\n+Node* ReverseBytesVNode::Identity(PhaseGVN* phase) {\n+  \/\/ \"(ReverseBytesV X) => X\" if the element type is T_BYTE.\n+  if (vect_type()->element_basic_type() == T_BYTE) {\n+    return in(1);\n+  }\n+  return reverse_operations_identity(this, in(1));\n+}\n+\n+Node* ReverseVNode::Identity(PhaseGVN* phase) {\n+  return reverse_operations_identity(this, in(1));\n+}\n+\n+\/\/ Optimize away redundant AndV\/OrV nodes when the operation\n+\/\/ is applied on the same input node multiple times\n+static Node* redundant_logical_identity(Node* n) {\n+  Node* n1 = n->in(1);\n+  \/\/ (OperationV (OperationV src1 src2) src1) => (OperationV src1 src2)\n+  \/\/ (OperationV (OperationV src1 src2) src2) => (OperationV src1 src2)\n+  \/\/ (OperationV (OperationV src1 src2 m1) src1 m1) => (OperationV src1 src2 m1)\n+  \/\/ (OperationV (OperationV src1 src2 m1) src2 m1) => (OperationV src1 src2 m1)\n+  if (n->Opcode() == n1->Opcode()) {\n+    if (((!n->is_predicated_vector() && !n1->is_predicated_vector()) ||\n+         ( n->is_predicated_vector() &&  n1->is_predicated_vector() && n->in(3) == n1->in(3))) &&\n+         ( n->in(2) == n1->in(1) || n->in(2) == n1->in(2))) {\n+      return n1;\n+    }\n+  }\n+\n+  Node* n2 = n->in(2);\n+  if (n->Opcode() == n2->Opcode()) {\n+    \/\/ (OperationV src1 (OperationV src1 src2)) => OperationV(src1, src2)\n+    \/\/ (OperationV src2 (OperationV src1 src2)) => OperationV(src1, src2)\n+    \/\/ (OperationV src1 (OperationV src1 src2 m1) m1) => OperationV(src1 src2 m1)\n+    \/\/ It is not possible to optimize - (OperationV src2 (OperationV src1 src2 m1) m1) as the\n+    \/\/ results of both \"OperationV\" nodes are different for unmasked lanes\n+    if ((!n->is_predicated_vector() && !n2->is_predicated_vector() &&\n+         (n->in(1) == n2->in(1) || n->in(1) == n2->in(2))) ||\n+         (n->is_predicated_vector() && n2->is_predicated_vector() && n->in(3) == n2->in(3) &&\n+         n->in(1) == n2->in(1))) {\n+      return n2;\n+    }\n+  }\n+\n+  return n;\n+}\n+\n+Node* AndVNode::Identity(PhaseGVN* phase) {\n+  \/\/ (AndV src (Replicate m1))   => src\n+  \/\/ (AndVMask src (MaskAll m1)) => src\n+  if (VectorNode::is_all_ones_vector(in(2))) {\n+    return in(1);\n+  }\n+  \/\/ (AndV (Replicate zero) src)   => (Replicate zero)\n+  \/\/ (AndVMask (MaskAll zero) src) => (MaskAll zero)\n+  if (VectorNode::is_all_zeros_vector(in(1))) {\n+    return in(1);\n+  }\n+  \/\/ The following transformations are only applied to\n+  \/\/ the un-predicated operation, since the VectorAPI\n+  \/\/ masked operation requires the unmasked lanes to\n+  \/\/ save the same values in the first operand.\n+  if (!is_predicated_vector()) {\n+    \/\/ (AndV (Replicate m1) src)   => src\n+    \/\/ (AndVMask (MaskAll m1) src) => src\n+    if (VectorNode::is_all_ones_vector(in(1))) {\n+      return in(2);\n+    }\n+    \/\/ (AndV src (Replicate zero))   => (Replicate zero)\n+    \/\/ (AndVMask src (MaskAll zero)) => (MaskAll zero)\n+    if (VectorNode::is_all_zeros_vector(in(2))) {\n+      return in(2);\n+    }\n+  }\n+\n+  \/\/ (AndV src src)     => src\n+  \/\/ (AndVMask src src) => src\n+  if (in(1) == in(2)) {\n+    return in(1);\n+  }\n+  return redundant_logical_identity(this);\n+}\n+\n+Node* OrVNode::Identity(PhaseGVN* phase) {\n+  \/\/ (OrV (Replicate m1) src)   => (Replicate m1)\n+  \/\/ (OrVMask (MaskAll m1) src) => (MaskAll m1)\n+  if (VectorNode::is_all_ones_vector(in(1))) {\n+    return in(1);\n+  }\n+  \/\/ (OrV src (Replicate zero))   => src\n+  \/\/ (OrVMask src (MaskAll zero)) => src\n+  if (VectorNode::is_all_zeros_vector(in(2))) {\n+    return in(1);\n+  }\n+  \/\/ The following transformations are only applied to\n+  \/\/ the un-predicated operation, since the VectorAPI\n+  \/\/ masked operation requires the unmasked lanes to\n+  \/\/ save the same values in the first operand.\n+  if (!is_predicated_vector()) {\n+    \/\/ (OrV src (Replicate m1))   => (Replicate m1)\n+    \/\/ (OrVMask src (MaskAll m1)) => (MaskAll m1)\n+    if (VectorNode::is_all_ones_vector(in(2))) {\n+      return in(2);\n+    }\n+    \/\/ (OrV (Replicate zero) src)   => src\n+    \/\/ (OrVMask (MaskAll zero) src) => src\n+    if (VectorNode::is_all_zeros_vector(in(1))) {\n+      return in(2);\n+    }\n+  }\n+\n+  \/\/ (OrV src src)     => src\n+  \/\/ (OrVMask src src) => src\n+  if (in(1) == in(2)) {\n+    return in(1);\n+  }\n+  return redundant_logical_identity(this);\n+}\n+\n+Node* XorVNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ (XorV src src)      => (Replicate zero)\n+  \/\/ (XorVMask src src)  => (MaskAll zero)\n+  \/\/\n+  \/\/ The transformation is only applied to the un-predicated\n+  \/\/ operation, since the VectorAPI masked operation requires\n+  \/\/ the unmasked lanes to save the same values in the first\n+  \/\/ operand.\n+  if (!is_predicated_vector() && (in(1) == in(2))) {\n+    BasicType bt = vect_type()->element_basic_type();\n+    Node* zero = phase->transform(phase->zerocon(bt));\n+    return VectorNode::scalar2vector(zero, length(), Type::get_const_basic_type(bt),\n+                                     bottom_type()->isa_vectmask() != nullptr);\n+  }\n+  return nullptr;\n+}\n+\n+Node* VectorBlendNode::Identity(PhaseGVN* phase) {\n+  \/\/ (VectorBlend X X MASK) => X\n+  if (in(1) == in(2)) {\n+    return in(1);\n+  }\n+  return this;\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":420,"deletions":140,"binary":false,"changes":560,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -74,0 +74,2 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n@@ -85,0 +87,3 @@\n+  static bool can_transform_shift_op(Node* n, BasicType bt);\n+  static bool is_convert_opcode(int opc);\n+  static bool is_minmax_opcode(int opc);\n@@ -93,0 +98,3 @@\n+\n+  \/\/ Limits on vector size (number of elements) for auto-vectorization.\n+  static bool vector_size_supported_superword(const BasicType bt, int size);\n@@ -99,1 +107,0 @@\n-  static bool is_type_transition_long_to_int(Node* n);\n@@ -104,0 +111,1 @@\n+  static bool is_populate_index_supported(BasicType bt);\n@@ -105,0 +113,1 @@\n+  \/\/ Return true if every bit in this vector is 1.\n@@ -106,0 +115,2 @@\n+  \/\/ Return true if every bit in this vector is 0.\n+  static bool is_all_zeros_vector(Node* n);\n@@ -109,0 +120,1 @@\n+  static Node* try_to_gen_masked_vector(PhaseGVN* gvn, Node* node, const TypeVect* vt);\n@@ -189,0 +201,1 @@\n+  const TypeVect* _vect_type;\n@@ -191,1 +204,2 @@\n-               _bottom_type(Type::get_const_basic_type(in1->bottom_type()->basic_type())) {}\n+               _bottom_type(Type::get_const_basic_type(in1->bottom_type()->basic_type())),\n+               _vect_type(in2->bottom_type()->is_vect()) {}\n@@ -204,0 +218,4 @@\n+  virtual const TypeVect* vect_type() const {\n+    return _vect_type;\n+  }\n+\n@@ -208,0 +226,2 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n@@ -608,1 +628,3 @@\n-  PopCountVLNode(Node* in, const TypeVect* vt) : VectorNode(in,vt) {}\n+  PopCountVLNode(Node* in, const TypeVect* vt) : VectorNode(in,vt) {\n+    assert(vt->element_basic_type() == T_LONG, \"must be long\");\n+  }\n@@ -786,0 +808,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -802,0 +825,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -826,0 +850,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -895,0 +920,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -941,0 +967,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -942,3 +969,2 @@\n-  static StoreVectorNode* make(int opc, Node* ctl, Node* mem,\n-                               Node* adr, const TypePtr* atyp, Node* val,\n-                               uint vlen);\n+  static StoreVectorNode* make(int opc, Node* ctl, Node* mem, Node* adr,\n+                               const TypePtr* atyp, Node* val, uint vlen);\n@@ -976,2 +1002,1 @@\n-    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n-    init_class_id(Class_StoreVector);\n+    init_class_id(Class_StoreVectorMasked);\n@@ -987,1 +1012,1 @@\n-  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -994,4 +1019,4 @@\n-  LoadVectorMaskedNode(Node* c, Node* mem, Node* src, const TypePtr* at, const TypeVect* vt, Node* mask)\n-   : LoadVectorNode(c, mem, src, at, vt) {\n-    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n-    init_class_id(Class_LoadVector);\n+  LoadVectorMaskedNode(Node* c, Node* mem, Node* src, const TypePtr* at, const TypeVect* vt, Node* mask,\n+                       ControlDependency control_dependency = LoadNode::DependsOnlyOnTest)\n+   : LoadVectorNode(c, mem, src, at, vt, control_dependency) {\n+    init_class_id(Class_LoadVectorMasked);\n@@ -1007,1 +1032,1 @@\n-  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -1016,1 +1041,1 @@\n-    init_class_id(Class_LoadVector);\n+    init_class_id(Class_LoadVectorGatherMasked);\n@@ -1036,1 +1061,1 @@\n-     init_class_id(Class_StoreVector);\n+     init_class_id(Class_StoreVectorScatterMasked);\n@@ -1073,0 +1098,1 @@\n+  static Node* make(Node* length, BasicType vmask_bt, int vmask_len);\n@@ -1077,0 +1103,3 @@\n+ private:\n+  int _mopc;\n+  const TypeVect* _vect_type;\n@@ -1079,2 +1108,2 @@\n-    TypeNode(ty, 2), _mopc(mopc) {\n-    assert(Matcher::has_predicated_vectors() || mask->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN, \"\");\n+    TypeNode(ty, 2), _mopc(mopc), _vect_type(mask->bottom_type()->is_vect()) {\n+    assert(Matcher::has_predicated_vectors() || _vect_type->element_basic_type() == T_BOOLEAN, \"\");\n@@ -1084,0 +1113,1 @@\n+  virtual const TypeVect* vect_type() { return _vect_type; }\n@@ -1087,0 +1117,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -1089,3 +1120,0 @@\n-\n-  private:\n-    int _mopc;\n@@ -1130,1 +1158,1 @@\n-  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -1141,1 +1169,1 @@\n-class AndVMaskNode : public VectorNode {\n+class AndVMaskNode : public AndVNode {\n@@ -1143,1 +1171,1 @@\n-  AndVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  AndVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : AndVNode(in1, in2, vt) {}\n@@ -1148,1 +1176,1 @@\n-class OrVMaskNode : public VectorNode {\n+class OrVMaskNode : public OrVNode {\n@@ -1150,1 +1178,1 @@\n-  OrVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  OrVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : OrVNode(in1, in2, vt) {}\n@@ -1155,1 +1183,1 @@\n-class XorVMaskNode : public VectorNode {\n+class XorVMaskNode : public XorVNode {\n@@ -1157,1 +1185,1 @@\n-  XorVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  XorVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : XorVNode(in1, in2, vt) {}\n@@ -1211,0 +1239,7 @@\n+\/\/======================Populate_Indices_into_a_Vector=========================\n+class PopulateIndexNode : public VectorNode {\n+ public:\n+  PopulateIndexNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -1313,1 +1348,1 @@\n-  ExtractNode(Node* src, ConINode* pos) : Node(NULL, src, (Node*)pos) {\n+  ExtractNode(Node* src, ConINode* pos) : Node(nullptr, src, (Node*)pos) {\n@@ -1319,1 +1354,1 @@\n-  static Node* make(Node* v, uint position, BasicType bt);\n+  static Node* make(Node* v, ConINode* pos, BasicType bt);\n@@ -1329,1 +1364,1 @@\n-  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual const Type* bottom_type() const { return TypeInt::BYTE; }\n@@ -1339,1 +1374,1 @@\n-  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual const Type* bottom_type() const { return TypeInt::UBYTE; }\n@@ -1465,1 +1500,1 @@\n-class VectorTestNode : public Node {\n+class VectorTestNode : public CmpNode {\n@@ -1473,1 +1508,1 @@\n-  VectorTestNode(Node* in1, Node* in2, BoolTest::mask predicate) : Node(NULL, in1, in2), _predicate(predicate) {\n+  VectorTestNode(Node* in1, Node* in2, BoolTest::mask predicate) : CmpNode(in1, in2), _predicate(predicate) {\n@@ -1478,0 +1513,4 @@\n+  virtual const Type* Value(PhaseGVN* phase) const { return TypeInt::CC; }\n+  virtual const Type* sub(const Type*, const Type*) const { return TypeInt::CC; }\n+  BoolTest::mask get_predicate() const { return _predicate; }\n+\n@@ -1481,4 +1520,0 @@\n-  virtual const Type *bottom_type() const { return TypeInt::BOOL; }\n-  virtual uint ideal_reg() const { return Op_RegI; }  \/\/ TODO Should be RegFlags but due to missing comparison flags for BoolTest\n-                                                      \/\/ in middle-end, we make it boolean result directly.\n-  BoolTest::mask get_predicate() const { return _predicate; }\n@@ -1494,0 +1529,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1514,3 +1550,1 @@\n-    : VectorNode(in, vt) {\n-    assert(in->bottom_type()->is_vect()->element_basic_type() == T_BYTE, \"must be BYTE\");\n-  }\n+    : VectorNode(in, vt) {}\n@@ -1518,1 +1552,0 @@\n-  int GetOutShuffleSize() const { return type2aelembytes(vect_type()->element_basic_type()); }\n@@ -1549,1 +1582,0 @@\n-  static Node* makeCastNode(PhaseGVN* phase, Node* in1, const TypeVect * vt);\n@@ -1585,2 +1617,2 @@\n-  static int  opcode(int sopc, BasicType bt, bool is_signed = true);\n-  static bool implemented(BasicType bt, uint vlen);\n+  static int  opcode(int opc, BasicType bt, bool is_signed = true);\n+  static bool implemented(int opc, uint vlen, BasicType src_type, BasicType dst_type);\n@@ -1639,16 +1671,0 @@\n-class VectorCastHF2FNode : public VectorCastNode {\n- public:\n-  VectorCastHF2FNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n-    assert(in->bottom_type()->is_vect()->element_basic_type() == T_SHORT, \"must be short\");\n-  }\n-  virtual int Opcode() const;\n-};\n-\n-class VectorCastF2HFNode : public VectorCastNode {\n- public:\n-  VectorCastF2HFNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n-    assert(in->bottom_type()->is_vect()->element_basic_type() == T_FLOAT, \"must be float\");\n-  }\n-  virtual int Opcode() const;\n-};\n-\n@@ -1703,0 +1719,16 @@\n+class VectorCastHF2FNode : public VectorCastNode {\n+ public:\n+  VectorCastHF2FNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_SHORT, \"must be short\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastF2HFNode : public VectorCastNode {\n+ public:\n+  VectorCastF2HFNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_FLOAT, \"must be float\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n@@ -1735,1 +1767,1 @@\n-    : Node(NULL, box, val), _box_type(box_type), _vec_type(vt) {\n+    : Node(nullptr, box, val), _box_type(box_type), _vec_type(vt) {\n@@ -1740,2 +1772,2 @@\n-  const  TypeInstPtr* box_type() const { assert(_box_type != NULL, \"\"); return _box_type; };\n-  const  TypeVect*    vec_type() const { assert(_vec_type != NULL, \"\"); return _vec_type; };\n+  const  TypeInstPtr* box_type() const { assert(_box_type != nullptr, \"\"); return _box_type; };\n+  const  TypeVect*    vec_type() const { assert(_vec_type != nullptr, \"\"); return _vec_type; };\n@@ -1754,1 +1786,1 @@\n-    : CallStaticJavaNode(C, VectorBoxNode::vec_box_type(vbox_type), NULL, NULL) {\n+    : CallStaticJavaNode(C, VectorBoxNode::vec_box_type(vbox_type), nullptr, nullptr) {\n@@ -1766,2 +1798,0 @@\n- private:\n-  bool _shuffle_to_vector;\n@@ -1771,1 +1801,1 @@\n-  VectorUnboxNode(Compile* C, const TypeVect* vec_type, Node* obj, Node* mem, bool shuffle_to_vector)\n+  VectorUnboxNode(Compile* C, const TypeVect* vec_type, Node* obj, Node* mem)\n@@ -1773,1 +1803,0 @@\n-    _shuffle_to_vector = shuffle_to_vector;\n@@ -1784,1 +1813,0 @@\n-  bool is_shuffle_to_vector() { return _shuffle_to_vector; }\n@@ -1808,1 +1836,4 @@\n-  : VectorNode(in, vt) {}\n+  : VectorNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == vt->element_basic_type(),\n+           \"must be the same\");\n+  }\n@@ -1816,1 +1847,4 @@\n-  : VectorNode(in, vt) {}\n+  : VectorNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == vt->element_basic_type(),\n+           \"must be the same\");\n+  }\n@@ -1826,0 +1860,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1834,0 +1869,31 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n+  virtual int Opcode() const;\n+};\n+\n+class SignumVFNode : public VectorNode {\n+public:\n+  SignumVFNode(Node* in1, Node* zero, Node* one, const TypeVect* vt)\n+  : VectorNode(in1, zero, one, vt) {}\n+\n+  virtual int Opcode() const;\n+};\n+\n+class SignumVDNode : public VectorNode {\n+public:\n+  SignumVDNode(Node* in1, Node* zero, Node* one, const TypeVect* vt)\n+  : VectorNode(in1, zero, one, vt) {}\n+\n+  virtual int Opcode() const;\n+};\n+\n+class CompressBitsVNode : public VectorNode {\n+public:\n+  CompressBitsVNode(Node* in, Node* mask, const TypeVect* vt)\n+  : VectorNode(in, mask, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+class ExpandBitsVNode : public VectorNode {\n+public:\n+  ExpandBitsVNode(Node* in, Node* mask, const TypeVect* vt)\n+  : VectorNode(in, mask, vt) {}\n@@ -1836,0 +1902,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":140,"deletions":73,"binary":false,"changes":213,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,0 +69,1 @@\n+#include \"oops\/fieldInfo.hpp\"\n@@ -73,0 +74,1 @@\n+#include \"oops\/instanceStackChunkKlass.hpp\"\n@@ -93,0 +95,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -103,1 +106,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -109,1 +111,0 @@\n-#include \"utilities\/hashtable.hpp\"\n@@ -112,5 +113,0 @@\n-\n-#include CPU_HEADER(vmStructs)\n-#include OS_HEADER(vmStructs)\n-#include OS_CPU_HEADER(vmStructs)\n-\n@@ -145,0 +141,4 @@\n+#include CPU_HEADER(vmStructs)\n+#include OS_HEADER(vmStructs)\n+#include OS_CPU_HEADER(vmStructs)\n+\n@@ -224,0 +224,2 @@\n+  nonstatic_field(ConstantPoolCache,           _resolved_indy_entries,                        Array<ResolvedIndyEntry>*)             \\\n+  nonstatic_field(ResolvedIndyEntry,           _cpool_index,                                  u2)                                    \\\n@@ -229,2 +231,1 @@\n-  nonstatic_field(InstanceKlass,               _fields,                                       Array<u2>*)                            \\\n-  nonstatic_field(InstanceKlass,               _java_fields_count,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _fieldinfo_stream,                             Array<u1>*)                            \\\n@@ -239,3 +240,2 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n-  nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \\\n-  nonstatic_field(InstanceKlass,               _init_thread,                                  Thread*)                               \\\n+  volatile_nonstatic_field(InstanceKlass,      _init_state,                                   InstanceKlass::ClassState)             \\\n+  volatile_nonstatic_field(InstanceKlass,      _init_thread,                                  JavaThread*)                           \\\n@@ -290,1 +290,0 @@\n-  nonstatic_field(MethodCounters,              _nmethod_age,                                  int)                                   \\\n@@ -320,0 +319,1 @@\n+  nonstatic_field(ConstMethod,                 _num_stack_arg_slots,                          u2)                                    \\\n@@ -385,6 +385,0 @@\n-  \/******\/                                                                                                                           \\\n-  \/* os *\/                                                                                                                           \\\n-  \/******\/                                                                                                                           \\\n-                                                                                                                                     \\\n-     static_field(os,                          _polling_page,                                 address)                               \\\n-                                                                                                                                     \\\n@@ -458,0 +452,1 @@\n+     static_field(vmClasses,                   VM_CLASS_AT(Thread_FieldHolder_klass),            InstanceKlass*)                     \\\n@@ -483,0 +478,2 @@\n+  nonstatic_field(Array<ResolvedIndyEntry>,    _length,                                       int)                                   \\\n+  nonstatic_field(Array<ResolvedIndyEntry>,    _data[0],                                      ResolvedIndyEntry)                     \\\n@@ -489,1 +486,1 @@\n-  nonstatic_field(GrowableArrayBase,           _max,                                          int)                                   \\\n+  nonstatic_field(GrowableArrayBase,           _capacity,                                     int)                                   \\\n@@ -545,0 +542,1 @@\n+     static_field(StubRoutines,                _chacha20Block,                                address)                               \\\n@@ -547,0 +545,1 @@\n+     static_field(StubRoutines,                _poly1305_processBlocks,                       address)                               \\\n@@ -662,3 +661,1 @@\n-  volatile_nonstatic_field(nmethod,            _lock_count,                                   jint)                                  \\\n-  volatile_nonstatic_field(nmethod,            _stack_traversal_mark,                         int64_t)                               \\\n-  nonstatic_field(nmethod,                     _comp_level,                                   int)                                   \\\n+  nonstatic_field(nmethod,                     _comp_level,                                   CompLevel)                             \\\n@@ -712,0 +709,3 @@\n+  nonstatic_field(JavaThread,                  _vthread,                                      OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _jvmti_vthread,                                OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _scopedValueCache,                              OopHandle)                             \\\n@@ -815,1 +815,1 @@\n-  nonstatic_field(ciMethod,                    _instructions_size,                            int)                                   \\\n+  nonstatic_field(ciMethod,                    _inline_instructions_size,                     int)                                   \\\n@@ -1026,6 +1026,7 @@\n-  nonstatic_field(Array<int>,                  _length,                                       int)                                   \\\n-  unchecked_nonstatic_field(Array<int>,        _data,                                         sizeof(int))                           \\\n-  unchecked_nonstatic_field(Array<u1>,         _data,                                         sizeof(u1))                            \\\n-  unchecked_nonstatic_field(Array<u2>,         _data,                                         sizeof(u2))                            \\\n-  unchecked_nonstatic_field(Array<Method*>,    _data,                                         sizeof(Method*))                       \\\n-  unchecked_nonstatic_field(Array<Klass*>,     _data,                                         sizeof(Klass*))                        \\\n+  nonstatic_field(Array<int>,                         _length,                                int)                                   \\\n+  unchecked_nonstatic_field(Array<int>,               _data,                                  sizeof(int))                           \\\n+  unchecked_nonstatic_field(Array<u1>,                _data,                                  sizeof(u1))                            \\\n+  unchecked_nonstatic_field(Array<u2>,                _data,                                  sizeof(u2))                            \\\n+  unchecked_nonstatic_field(Array<Method*>,           _data,                                  sizeof(Method*))                       \\\n+  unchecked_nonstatic_field(Array<Klass*>,            _data,                                  sizeof(Klass*))                        \\\n+  unchecked_nonstatic_field(Array<ResolvedIndyEntry>, _data,                                  sizeof(ResolvedIndyEntry))             \\\n@@ -1048,1 +1049,1 @@\n-  CDS_ONLY(nonstatic_field(FileMapHeader,      _space[0],                 CDSFileMapRegion))                                         \\\n+  CDS_ONLY(nonstatic_field(FileMapHeader,      _regions[0],               CDSFileMapRegion))                                         \\\n@@ -1067,1 +1068,1 @@\n-  nonstatic_field(CompileTask,                 _compile_id,                                   uint)                                  \\\n+  nonstatic_field(CompileTask,                 _compile_id,                                   int)                                   \\\n@@ -1072,1 +1073,0 @@\n-  nonstatic_field(vframeArray,                 _next,                                         vframeArray*)                          \\\n@@ -1232,0 +1232,1 @@\n+        declare_type(InstanceStackChunkKlass, InstanceKlass)              \\\n@@ -1314,1 +1315,0 @@\n-        declare_type(CodeCacheSweeperThread, JavaThread)                  \\\n@@ -1496,1 +1496,0 @@\n-  declare_c2_type(CallNativeNode, CallNode)                               \\\n@@ -1568,1 +1567,1 @@\n-  declare_c2_type(BlackholeNode, MemBarNode)                              \\\n+  declare_c2_type(BlackholeNode, MultiNode)                               \\\n@@ -1572,1 +1571,0 @@\n-  declare_c2_type(Opaque2Node, Node)                                      \\\n@@ -1584,2 +1582,0 @@\n-  declare_c2_type(NoOvfDivINode, DivINode)                                \\\n-  declare_c2_type(NoOvfDivLNode, DivLNode)                                \\\n@@ -1592,2 +1588,0 @@\n-  declare_c2_type(NoOvfModINode, ModINode)                                \\\n-  declare_c2_type(NoOvfModLNode, ModLNode)                                \\\n@@ -1599,2 +1593,0 @@\n-  declare_c2_type(NoOvfDivModINode, DivModINode)                          \\\n-  declare_c2_type(NoOvfDivModLNode, DivModLNode)                          \\\n@@ -1626,1 +1618,0 @@\n-  declare_c2_type(MachCallNativeNode, MachCallNode)                       \\\n@@ -1657,1 +1648,0 @@\n-  declare_c2_type(LoadPLockedNode, LoadPNode)                             \\\n@@ -1660,2 +1650,0 @@\n-  declare_c2_type(StorePConditionalNode, LoadStoreNode)                   \\\n-  declare_c2_type(StoreLConditionalNode, LoadStoreNode)                   \\\n@@ -1712,0 +1700,1 @@\n+  declare_c2_type(CmpU3Node, CmpUNode)                                    \\\n@@ -1717,0 +1706,1 @@\n+  declare_c2_type(CmpUL3Node, CmpULNode)                                  \\\n@@ -1755,1 +1745,0 @@\n-  declare_c2_type(AddVHFNode, VectorNode)                                 \\\n@@ -1787,0 +1776,1 @@\n+  declare_c2_type(CompressMNode, VectorNode)                              \\\n@@ -1788,0 +1778,2 @@\n+  declare_c2_type(CompressBitsVNode, VectorNode)                          \\\n+  declare_c2_type(ExpandBitsVNode, VectorNode)                            \\\n@@ -1824,0 +1816,1 @@\n+  declare_c2_type(PopulateIndexNode, VectorNode)                          \\\n@@ -1857,0 +1850,4 @@\n+  declare_c2_type(IsInfiniteFNode, Node)                                  \\\n+  declare_c2_type(IsInfiniteDNode, Node)                                  \\\n+  declare_c2_type(IsFiniteFNode, Node)                                    \\\n+  declare_c2_type(IsFiniteDNode, Node)                                    \\\n@@ -1881,0 +1878,2 @@\n+  declare_c2_type(ReverseBytesVNode, VectorNode)                          \\\n+  declare_c2_type(ReverseVNode, VectorNode)                               \\\n@@ -1887,1 +1886,1 @@\n-  declare_c2_type(VectorTestNode, Node)                                   \\\n+  declare_c2_type(VectorTestNode, CmpNode)                                \\\n@@ -1970,0 +1969,1 @@\n+            declare_type(Array<ResolvedIndyEntry>, MetaspaceObj)          \\\n@@ -1978,0 +1978,2 @@\n+                                                                          \\\n+  declare_integer_type(CompLevel)                                         \\\n@@ -1984,0 +1986,1 @@\n+  declare_toplevel_type(ResolvedIndyEntry)                                \\\n@@ -2020,1 +2023,1 @@\n-  declare_type(OopMapValue, StackObj)                                     \\\n+  declare_toplevel_type(OopMapValue)                                      \\\n@@ -2108,6 +2111,0 @@\n-  declare_constant(JVM_ACC_PROMOTED_FLAGS)                                \\\n-  declare_constant(JVM_ACC_FIELD_ACCESS_WATCHED)                          \\\n-  declare_constant(JVM_ACC_FIELD_MODIFICATION_WATCHED)                    \\\n-  declare_constant(JVM_ACC_FIELD_INTERNAL)                                \\\n-  declare_constant(JVM_ACC_FIELD_STABLE)                                  \\\n-  declare_constant(JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE)                   \\\n@@ -2203,0 +2200,1 @@\n+  declare_constant(Method::_changes_current_thread)                       \\\n@@ -2244,18 +2242,0 @@\n-  \/*************************************\/                                 \\\n-  \/* FieldInfo FieldOffset enum        *\/                                 \\\n-  \/*************************************\/                                 \\\n-                                                                          \\\n-  declare_constant(FieldInfo::access_flags_offset)                        \\\n-  declare_constant(FieldInfo::name_index_offset)                          \\\n-  declare_constant(FieldInfo::signature_index_offset)                     \\\n-  declare_constant(FieldInfo::initval_index_offset)                       \\\n-  declare_constant(FieldInfo::low_packed_offset)                          \\\n-  declare_constant(FieldInfo::high_packed_offset)                         \\\n-  declare_constant(FieldInfo::field_slots)                                \\\n-                                                                          \\\n-  \/*************************************\/                                 \\\n-  \/* FieldInfo tag constants           *\/                                 \\\n-  \/*************************************\/                                 \\\n-                                                                          \\\n-  declare_preprocessor_constant(\"FIELDINFO_TAG_SIZE\", FIELDINFO_TAG_SIZE) \\\n-  declare_preprocessor_constant(\"FIELDINFO_TAG_OFFSET\", FIELDINFO_TAG_OFFSET) \\\n@@ -2285,0 +2265,1 @@\n+  declare_constant(InstanceKlass::being_linked)                           \\\n@@ -2290,16 +2271,0 @@\n-  \/***************************************\/                               \\\n-  \/* InstanceKlass enums for _misc_flags *\/                               \\\n-  \/***************************************\/                               \\\n-                                                                          \\\n-  declare_constant(InstanceKlass::_misc_rewritten)                        \\\n-  declare_constant(InstanceKlass::_misc_has_nonstatic_fields)             \\\n-  declare_constant(InstanceKlass::_misc_should_verify_class)              \\\n-  declare_constant(InstanceKlass::_misc_is_contended)                     \\\n-  declare_constant(InstanceKlass::_misc_has_nonstatic_concrete_methods)   \\\n-  declare_constant(InstanceKlass::_misc_declares_nonstatic_concrete_methods)\\\n-  declare_constant(InstanceKlass::_misc_has_been_redefined)               \\\n-  declare_constant(InstanceKlass::_misc_is_scratch_class)                 \\\n-  declare_constant(InstanceKlass::_misc_is_shared_boot_class)             \\\n-  declare_constant(InstanceKlass::_misc_is_shared_platform_class)         \\\n-  declare_constant(InstanceKlass::_misc_is_shared_app_class)              \\\n-                                                                          \\\n@@ -2346,0 +2311,11 @@\n+                                                                          \\\n+  \/******************************\/                                        \\\n+  \/* FieldFlags enum            *\/                                        \\\n+  \/******************************\/                                        \\\n+                                                                          \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_initialized)                \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_injected)                   \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_generic)                    \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_stable)                     \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_contended)                  \\\n+                                                                          \\\n@@ -2555,2 +2531,1 @@\n-  declare_constant(RegisterImpl::number_of_registers)                     \\\n-  declare_preprocessor_constant(\"REG_COUNT\", REG_COUNT)                \\\n+  declare_preprocessor_constant(\"REG_COUNT\", REG_COUNT)                   \\\n@@ -3102,1 +3077,1 @@\n-    while (types->typeName != NULL) {\n+    while (types->typeName != nullptr) {\n@@ -3123,1 +3098,1 @@\n-  const char* start = NULL;\n+  const char* start = nullptr;\n@@ -3129,1 +3104,1 @@\n-  if (start != NULL) {\n+  if (start != nullptr) {\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":71,"deletions":96,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -154,0 +154,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n@@ -166,0 +169,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n@@ -172,0 +178,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n@@ -177,0 +186,3 @@\n+    \/**\n+     * @hidden\n+     *\/\n@@ -203,3 +215,3 @@\n-    public interface ShuffleIotaOperation<S extends VectorSpecies<?>,\n-                                          SH extends VectorShuffle<?>> {\n-        SH apply(int length, int start, int step, S s);\n+    public interface IndexPartiallyInUpperRangeOperation<E,\n+                                                         M extends VectorMask<E>> {\n+        M apply(long offset, long limit);\n@@ -211,25 +223,6 @@\n-     S extends VectorSpecies<E>,\n-     SH extends VectorShuffle<E>>\n-    SH shuffleIota(Class<E> eClass, Class<? extends SH> shClass, S s,\n-                   int length,\n-                   int start, int step, int wrap,\n-                   ShuffleIotaOperation<S, SH> defaultImpl) {\n-       assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n-       return defaultImpl.apply(length, start, step, s);\n-    }\n-\n-    public interface ShuffleToVectorOperation<V extends Vector<?>,\n-                                              SH extends VectorShuffle<?>> {\n-       V apply(SH sh);\n-    }\n-\n-    @IntrinsicCandidate\n-    public static\n-    <V extends Vector<E>,\n-     SH extends VectorShuffle<E>,\n-     E>\n-    V shuffleToVector(Class<? extends Vector<E>> vClass, Class<E> eClass, Class<? extends SH> shClass, SH sh,\n-                      int length,\n-                      ShuffleToVectorOperation<V, SH> defaultImpl) {\n-      assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n-      return defaultImpl.apply(sh);\n+     M extends VectorMask<E>>\n+    M indexPartiallyInUpperRange(Class<? extends M> mClass, Class<E> eClass,\n+                                 int length, long offset, long limit,\n+                                 IndexPartiallyInUpperRangeOperation<E, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(offset, limit);\n@@ -244,1 +237,1 @@\n-    \/\/FIXME @IntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -420,3 +413,3 @@\n-                 int length,\n-                 Object base, long offset,\n-                 M m, C container, long index, S s,\n+                 int length, Object base, long offset,\n+                 M m, int offsetInRange,\n+                 C container, long index, S s,\n@@ -648,1 +641,1 @@\n-    public interface ComExpOperation<V extends Vector<?>,\n+    public interface CompressExpandOperation<V extends Vector<?>,\n@@ -658,4 +651,4 @@\n-    VectorPayload comExpOp(int opr,\n-                           Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n-                           int length, V v, M m,\n-                           ComExpOperation<V, M> defaultImpl) {\n+    VectorPayload compressExpandOp(int opr,\n+                                   Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                                   int length, V v, M m,\n+                                   CompressExpandOperation<V, M> defaultImpl) {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":31,"deletions":38,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -140,1 +140,2 @@\n-    public VectorMask<E> andNot(VectorMask<E> m) {\n+    @ForceInline\n+    public final VectorMask<E> andNot(VectorMask<E> m) {\n@@ -144,0 +145,6 @@\n+    @Override\n+    @ForceInline\n+    public final VectorMask<E> eq(VectorMask<E> m) {\n+        return xor(m.not());\n+    }\n+\n@@ -198,1 +205,1 @@\n-    @Override\n+    \/*package-private*\/\n@@ -200,1 +207,1 @@\n-    public VectorMask<E> indexInRange(int offset, int limit) {\n+    VectorMask<E> indexPartiallyInRange(int offset, int limit) {\n@@ -204,1 +211,1 @@\n-        return this.andNot(badMask);\n+        return badMask.not();\n@@ -207,1 +214,1 @@\n-    @Override\n+    \/*package-private*\/\n@@ -209,1 +216,1 @@\n-    public VectorMask<E> indexInRange(long offset, long limit) {\n+    VectorMask<E> indexPartiallyInRange(long offset, long limit) {\n@@ -213,1 +220,1 @@\n-        return this.andNot(badMask);\n+        return badMask.not();\n@@ -216,0 +223,27 @@\n+    @Override\n+    @ForceInline\n+    public VectorMask<E> indexInRange(int offset, int limit) {\n+        if (offset < 0) {\n+            return this.and(indexPartiallyInRange(offset, limit));\n+        } else if (offset >= limit) {\n+            return vectorSpecies().maskAll(false);\n+        } else if (limit - offset >= length()) {\n+            return this;\n+        }\n+        return this.and(indexPartiallyInUpperRange(offset, limit));\n+    }\n+\n+    @ForceInline\n+    public VectorMask<E> indexInRange(long offset, long limit) {\n+        if (offset < 0) {\n+            return this.and(indexPartiallyInRange(offset, limit));\n+        } else if (offset >= limit) {\n+            return vectorSpecies().maskAll(false);\n+        } else if (limit - offset >= length()) {\n+            return this;\n+        }\n+        return this.and(indexPartiallyInUpperRange(offset, limit));\n+    }\n+\n+    abstract VectorMask<E> indexPartiallyInUpperRange(long offset, long limit);\n+\n@@ -228,1 +262,1 @@\n-        \/\/ https:\/\/bugs.openjdk.java.net\/browse\/JDK-8225740\n+        \/\/ https:\/\/bugs.openjdk.org\/browse\/JDK-8225740\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractMask.java","additions":43,"deletions":9,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,1 @@\n-import jdk.incubator.foreign.MemorySegment;\n+import java.lang.foreign.MemorySegment;\n@@ -51,0 +51,2 @@\n+    final Class<? extends AbstractShuffle<E>> shuffleType;\n+    @Stable\n@@ -64,0 +66,1 @@\n+                    Class<? extends AbstractShuffle<E>> shuffleType,\n@@ -69,0 +72,1 @@\n+        this.shuffleType = shuffleType;\n@@ -143,1 +147,1 @@\n-    \/\/ FIXME: appeal to general method (see https:\/\/bugs.openjdk.java.net\/browse\/JDK-6176992)\n+    \/\/ FIXME: appeal to general method (see https:\/\/bugs.openjdk.org\/browse\/JDK-6176992)\n@@ -165,0 +169,5 @@\n+    @ForceInline\n+    final Class<? extends AbstractShuffle<E>> shuffleType() {\n+        return shuffleType;\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractSpecies.java","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,1 @@\n-import jdk.incubator.foreign.MemorySegment;\n+import java.lang.foreign.MemorySegment;\n@@ -65,0 +65,6 @@\n+    \/*package-private*\/\n+    static final int OFFSET_IN_RANGE = 1;\n+\n+    \/*package-private*\/\n+    static final int OFFSET_OUT_OF_RANGE = 0;\n+\n@@ -185,1 +191,1 @@\n-    abstract AbstractShuffle<E> iotaShuffle();\n+    abstract <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp);\n@@ -187,1 +193,15 @@\n-    abstract AbstractShuffle<E> iotaShuffle(int start, int step, boolean wrap);\n+    \/*package-private*\/\n+    @ForceInline\n+    final <F> VectorShuffle<F> toShuffleTemplate(AbstractSpecies<F> dsp) {\n+        Class<?> etype = vspecies().elementType();\n+        Class<?> dvtype = dsp.shuffleType();\n+        Class<?> dtype = dsp.asIntegral().elementType();\n+        int dlength = dsp.dummyVector().length();\n+        return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                                     getClass(), etype, length(),\n+                                     dvtype, dtype, dlength,\n+                                     this, dsp,\n+                                     AbstractVector::toShuffle0);\n+    }\n+\n+    abstract <F> VectorShuffle<F> toShuffle0(AbstractSpecies<F> dsp);\n@@ -189,2 +209,33 @@\n-    \/*do not alias this byte array*\/\n-    abstract AbstractShuffle<E> shuffleFromBytes(byte[] reorder);\n+    @ForceInline\n+    public final\n+    VectorShuffle<E> toShuffle() {\n+        return toShuffle(vspecies());\n+    }\n+\n+    abstract VectorShuffle<E> iotaShuffle();\n+\n+    @ForceInline\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    final VectorShuffle<E> iotaShuffle(int start, int step, boolean wrap) {\n+        if (start == 0 && step == 1) {\n+            return iotaShuffle();\n+        }\n+\n+        if ((length() & (length() - 1)) != 0) {\n+            return wrap ? shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i * step + start, length())))\n+                        : shuffleFromOp(i -> i * step + start);\n+        }\n+\n+        AbstractSpecies<?> species = vspecies().asIntegral();\n+        Vector iota = species.iota();\n+        iota = iota.lanewise(VectorOperators.MUL, step)\n+                   .lanewise(VectorOperators.ADD, start);\n+        Vector wrapped = iota.lanewise(VectorOperators.AND, length() - 1);\n+\n+        if (!wrap) {\n+            Vector wrappedEx = wrapped.lanewise(VectorOperators.SUB, length());\n+            VectorMask<?> mask = wrapped.compare(VectorOperators.EQ, iota);\n+            wrapped = wrappedEx.blend(wrapped, mask);\n+        }\n+        return ((AbstractVector) wrapped).toShuffle(vspecies());\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractVector.java","additions":57,"deletions":6,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Byte128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Byte128Shuffle)VectorSupport.shuffleIota(ETYPE, Byte128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Byte128Shuffle)VectorSupport.shuffleIota(ETYPE, Byte128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Byte128Shuffle shuffleFromBytes(byte[] reorder) { return new Byte128Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Byte128Shuffle shuffleFromArray(int[] indexes, int i) { return new Byte128Shuffle(indexes, i); }\n+    Byte128Shuffle shuffleFromArray(int[] indices, int i) { return new Byte128Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Byte> toShuffle() {\n-        return super.toShuffleTemplate(Byte128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -682,4 +669,5 @@\n-        public Byte128Mask eq(VectorMask<Byte> mask) {\n-            Objects.requireNonNull(mask);\n-            Byte128Mask m = (Byte128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Byte128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Byte128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Byte128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Byte128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -699,1 +687,1 @@\n-            return (Byte128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Byte128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -727,0 +715,1 @@\n+        @Override\n@@ -728,2 +717,1 @@\n-        \/* package-private *\/\n-        Byte128Mask xor(VectorMask<Byte> mask) {\n+        public Byte128Mask xor(VectorMask<Byte> mask) {\n@@ -806,2 +794,4 @@\n-        Byte128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte128Shuffle(byte[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -810,2 +800,2 @@\n-        public Byte128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -814,2 +804,2 @@\n-        public Byte128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Byte128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -818,2 +808,2 @@\n-        public Byte128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        byte[] indices() {\n+            return (byte[])getPayload();\n@@ -823,0 +813,1 @@\n+        @ForceInline\n@@ -837,3 +828,2 @@\n-        public Byte128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Byte128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Byte128Vector)(((AbstractShuffle<Byte>)(s)).toVectorTemplate())));\n+        Byte128Vector toBitsVector() {\n+            return (Byte128Vector) super.toBitsVectorTemplate();\n@@ -844,6 +834,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ByteVector toBitsVector0() {\n+            return Byte128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -852,0 +838,1 @@\n+        @Override\n@@ -853,0 +840,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -854,8 +845,24 @@\n-        public Byte128Shuffle rearrange(VectorShuffle<Byte> shuffle) {\n-            Byte128Shuffle s = (Byte128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_128;\n+            Vector<Byte> v = toBitsVector();\n+            v.convertShape(VectorOperators.B2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.B2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+            v.convertShape(VectorOperators.B2I, species, 2)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 2);\n+            v.convertShape(VectorOperators.B2I, species, 3)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 3);\n+        }\n+\n+        private static byte[] prepare(int[] indices, int offset) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n@@ -863,1 +870,28 @@\n-            return new Byte128Shuffle(r);\n+            return a;\n+        }\n+\n+        private static byte[] prepare(IntUnaryOperator f) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(byte[] indices) {\n+            int length = indices.length;\n+            for (byte si : indices) {\n+                if (si >= (byte)length || si < (byte)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -881,2 +915,2 @@\n-    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m, offsetInRange);  \/\/ specialize\n@@ -897,2 +931,2 @@\n-    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m, offsetInRange);  \/\/ specialize\n@@ -911,2 +945,2 @@\n-    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m) {\n-        return super.fromMemorySegment0Template(Byte128Mask.class, ms, offset, (Byte128Mask) m);  \/\/ specialize\n+    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Byte128Mask.class, ms, offset, (Byte128Mask) m, offsetInRange);  \/\/ specialize\n@@ -950,0 +984,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":94,"deletions":59,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Byte256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Byte256Shuffle)VectorSupport.shuffleIota(ETYPE, Byte256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Byte256Shuffle)VectorSupport.shuffleIota(ETYPE, Byte256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Byte256Shuffle shuffleFromBytes(byte[] reorder) { return new Byte256Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Byte256Shuffle shuffleFromArray(int[] indexes, int i) { return new Byte256Shuffle(indexes, i); }\n+    Byte256Shuffle shuffleFromArray(int[] indices, int i) { return new Byte256Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Byte> toShuffle() {\n-        return super.toShuffleTemplate(Byte256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -714,4 +701,5 @@\n-        public Byte256Mask eq(VectorMask<Byte> mask) {\n-            Objects.requireNonNull(mask);\n-            Byte256Mask m = (Byte256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Byte256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Byte256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Byte256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Byte256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -731,1 +719,1 @@\n-            return (Byte256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Byte256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -759,0 +747,1 @@\n+        @Override\n@@ -760,2 +749,1 @@\n-        \/* package-private *\/\n-        Byte256Mask xor(VectorMask<Byte> mask) {\n+        public Byte256Mask xor(VectorMask<Byte> mask) {\n@@ -838,2 +826,4 @@\n-        Byte256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte256Shuffle(byte[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -842,2 +832,2 @@\n-        public Byte256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -846,2 +836,2 @@\n-        public Byte256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Byte256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -850,2 +840,2 @@\n-        public Byte256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        byte[] indices() {\n+            return (byte[])getPayload();\n@@ -855,0 +845,1 @@\n+        @ForceInline\n@@ -869,3 +860,2 @@\n-        public Byte256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Byte256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Byte256Vector)(((AbstractShuffle<Byte>)(s)).toVectorTemplate())));\n+        Byte256Vector toBitsVector() {\n+            return (Byte256Vector) super.toBitsVectorTemplate();\n@@ -876,6 +866,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ByteVector toBitsVector0() {\n+            return Byte256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -884,0 +870,1 @@\n+        @Override\n@@ -885,0 +872,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -886,8 +877,24 @@\n-        public Byte256Shuffle rearrange(VectorShuffle<Byte> shuffle) {\n-            Byte256Shuffle s = (Byte256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_256;\n+            Vector<Byte> v = toBitsVector();\n+            v.convertShape(VectorOperators.B2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.B2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+            v.convertShape(VectorOperators.B2I, species, 2)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 2);\n+            v.convertShape(VectorOperators.B2I, species, 3)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 3);\n+        }\n+\n+        private static byte[] prepare(int[] indices, int offset) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n@@ -895,1 +902,28 @@\n-            return new Byte256Shuffle(r);\n+            return a;\n+        }\n+\n+        private static byte[] prepare(IntUnaryOperator f) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(byte[] indices) {\n+            int length = indices.length;\n+            for (byte si : indices) {\n+                if (si >= (byte)length || si < (byte)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -913,2 +947,2 @@\n-    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m, offsetInRange);  \/\/ specialize\n@@ -929,2 +963,2 @@\n-    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m, offsetInRange);  \/\/ specialize\n@@ -943,2 +977,2 @@\n-    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m) {\n-        return super.fromMemorySegment0Template(Byte256Mask.class, ms, offset, (Byte256Mask) m);  \/\/ specialize\n+    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Byte256Mask.class, ms, offset, (Byte256Mask) m, offsetInRange);  \/\/ specialize\n@@ -982,0 +1016,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":94,"deletions":59,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Byte512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Byte512Shuffle)VectorSupport.shuffleIota(ETYPE, Byte512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Byte512Shuffle)VectorSupport.shuffleIota(ETYPE, Byte512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Byte512Shuffle shuffleFromBytes(byte[] reorder) { return new Byte512Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Byte512Shuffle shuffleFromArray(int[] indexes, int i) { return new Byte512Shuffle(indexes, i); }\n+    Byte512Shuffle shuffleFromArray(int[] indices, int i) { return new Byte512Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Byte> toShuffle() {\n-        return super.toShuffleTemplate(Byte512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -778,4 +765,5 @@\n-        public Byte512Mask eq(VectorMask<Byte> mask) {\n-            Objects.requireNonNull(mask);\n-            Byte512Mask m = (Byte512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Byte512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Byte512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Byte512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Byte512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -795,1 +783,1 @@\n-            return (Byte512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Byte512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -823,0 +811,1 @@\n+        @Override\n@@ -824,2 +813,1 @@\n-        \/* package-private *\/\n-        Byte512Mask xor(VectorMask<Byte> mask) {\n+        public Byte512Mask xor(VectorMask<Byte> mask) {\n@@ -902,2 +890,4 @@\n-        Byte512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte512Shuffle(byte[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -906,2 +896,2 @@\n-        public Byte512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -910,2 +900,2 @@\n-        public Byte512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Byte512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -914,2 +904,2 @@\n-        public Byte512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        byte[] indices() {\n+            return (byte[])getPayload();\n@@ -919,0 +909,1 @@\n+        @ForceInline\n@@ -933,3 +924,2 @@\n-        public Byte512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Byte512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Byte512Vector)(((AbstractShuffle<Byte>)(s)).toVectorTemplate())));\n+        Byte512Vector toBitsVector() {\n+            return (Byte512Vector) super.toBitsVectorTemplate();\n@@ -940,6 +930,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ByteVector toBitsVector0() {\n+            return Byte512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -948,0 +934,1 @@\n+        @Override\n@@ -949,0 +936,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -950,8 +941,24 @@\n-        public Byte512Shuffle rearrange(VectorShuffle<Byte> shuffle) {\n-            Byte512Shuffle s = (Byte512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_512;\n+            Vector<Byte> v = toBitsVector();\n+            v.convertShape(VectorOperators.B2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.B2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+            v.convertShape(VectorOperators.B2I, species, 2)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 2);\n+            v.convertShape(VectorOperators.B2I, species, 3)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 3);\n+        }\n+\n+        private static byte[] prepare(int[] indices, int offset) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n@@ -959,1 +966,28 @@\n-            return new Byte512Shuffle(r);\n+            return a;\n+        }\n+\n+        private static byte[] prepare(IntUnaryOperator f) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(byte[] indices) {\n+            int length = indices.length;\n+            for (byte si : indices) {\n+                if (si >= (byte)length || si < (byte)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -977,2 +1011,2 @@\n-    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m, offsetInRange);  \/\/ specialize\n@@ -993,2 +1027,2 @@\n-    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m, offsetInRange);  \/\/ specialize\n@@ -1007,2 +1041,2 @@\n-    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m) {\n-        return super.fromMemorySegment0Template(Byte512Mask.class, ms, offset, (Byte512Mask) m);  \/\/ specialize\n+    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Byte512Mask.class, ms, offset, (Byte512Mask) m, offsetInRange);  \/\/ specialize\n@@ -1046,0 +1080,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":94,"deletions":59,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Byte64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Byte64Shuffle)VectorSupport.shuffleIota(ETYPE, Byte64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Byte64Shuffle)VectorSupport.shuffleIota(ETYPE, Byte64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Byte64Shuffle shuffleFromBytes(byte[] reorder) { return new Byte64Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Byte64Shuffle shuffleFromArray(int[] indexes, int i) { return new Byte64Shuffle(indexes, i); }\n+    Byte64Shuffle shuffleFromArray(int[] indices, int i) { return new Byte64Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Byte> toShuffle() {\n-        return super.toShuffleTemplate(Byte64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -666,4 +653,5 @@\n-        public Byte64Mask eq(VectorMask<Byte> mask) {\n-            Objects.requireNonNull(mask);\n-            Byte64Mask m = (Byte64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Byte64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Byte64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Byte64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Byte64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -683,1 +671,1 @@\n-            return (Byte64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Byte64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -711,0 +699,1 @@\n+        @Override\n@@ -712,2 +701,1 @@\n-        \/* package-private *\/\n-        Byte64Mask xor(VectorMask<Byte> mask) {\n+        public Byte64Mask xor(VectorMask<Byte> mask) {\n@@ -790,2 +778,4 @@\n-        Byte64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte64Shuffle(byte[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -794,2 +784,2 @@\n-        public Byte64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Byte64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -798,2 +788,2 @@\n-        public Byte64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Byte64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -802,2 +792,2 @@\n-        public Byte64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        byte[] indices() {\n+            return (byte[])getPayload();\n@@ -807,0 +797,1 @@\n+        @ForceInline\n@@ -821,3 +812,2 @@\n-        public Byte64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Byte64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Byte64Vector)(((AbstractShuffle<Byte>)(s)).toVectorTemplate())));\n+        Byte64Vector toBitsVector() {\n+            return (Byte64Vector) super.toBitsVectorTemplate();\n@@ -828,6 +818,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ByteVector toBitsVector0() {\n+            return Byte64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -836,0 +822,1 @@\n+        @Override\n@@ -837,0 +824,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -838,8 +829,24 @@\n-        public Byte64Shuffle rearrange(VectorShuffle<Byte> shuffle) {\n-            Byte64Shuffle s = (Byte64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_64;\n+            Vector<Byte> v = toBitsVector();\n+            v.convertShape(VectorOperators.B2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.B2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+            v.convertShape(VectorOperators.B2I, species, 2)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 2);\n+            v.convertShape(VectorOperators.B2I, species, 3)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 3);\n+        }\n+\n+        private static byte[] prepare(int[] indices, int offset) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n@@ -847,1 +854,28 @@\n-            return new Byte64Shuffle(r);\n+            return a;\n+        }\n+\n+        private static byte[] prepare(IntUnaryOperator f) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(byte[] indices) {\n+            int length = indices.length;\n+            for (byte si : indices) {\n+                if (si >= (byte)length || si < (byte)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -865,2 +899,2 @@\n-    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m, offsetInRange);  \/\/ specialize\n@@ -881,2 +915,2 @@\n-    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m, offsetInRange);  \/\/ specialize\n@@ -895,2 +929,2 @@\n-    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m) {\n-        return super.fromMemorySegment0Template(Byte64Mask.class, ms, offset, (Byte64Mask) m);  \/\/ specialize\n+    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Byte64Mask.class, ms, offset, (Byte64Mask) m, offsetInRange);  \/\/ specialize\n@@ -934,0 +968,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":94,"deletions":59,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    ByteMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (ByteMaxShuffle)VectorSupport.shuffleIota(ETYPE, ByteMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (ByteMaxShuffle)VectorSupport.shuffleIota(ETYPE, ByteMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    ByteMaxShuffle shuffleFromBytes(byte[] reorder) { return new ByteMaxShuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    ByteMaxShuffle shuffleFromArray(int[] indexes, int i) { return new ByteMaxShuffle(indexes, i); }\n+    ByteMaxShuffle shuffleFromArray(int[] indices, int i) { return new ByteMaxShuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Byte> toShuffle() {\n-        return super.toShuffleTemplate(ByteMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -652,4 +639,5 @@\n-        public ByteMaxMask eq(VectorMask<Byte> mask) {\n-            Objects.requireNonNull(mask);\n-            ByteMaxMask m = (ByteMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        ByteMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (ByteMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                ByteMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (ByteMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -669,1 +657,1 @@\n-            return (ByteMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (ByteMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -697,0 +685,1 @@\n+        @Override\n@@ -698,2 +687,1 @@\n-        \/* package-private *\/\n-        ByteMaxMask xor(VectorMask<Byte> mask) {\n+        public ByteMaxMask xor(VectorMask<Byte> mask) {\n@@ -776,2 +764,4 @@\n-        ByteMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        ByteMaxShuffle(byte[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -780,2 +770,2 @@\n-        public ByteMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        ByteMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -784,2 +774,2 @@\n-        public ByteMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        ByteMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -788,2 +778,2 @@\n-        public ByteMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        byte[] indices() {\n+            return (byte[])getPayload();\n@@ -793,0 +783,1 @@\n+        @ForceInline\n@@ -807,3 +798,2 @@\n-        public ByteMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, ByteMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((ByteMaxVector)(((AbstractShuffle<Byte>)(s)).toVectorTemplate())));\n+        ByteMaxVector toBitsVector() {\n+            return (ByteMaxVector) super.toBitsVectorTemplate();\n@@ -814,6 +804,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ByteVector toBitsVector0() {\n+            return ByteMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -822,0 +808,1 @@\n+        @Override\n@@ -823,0 +810,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -824,8 +815,24 @@\n-        public ByteMaxShuffle rearrange(VectorShuffle<Byte> shuffle) {\n-            ByteMaxShuffle s = (ByteMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_MAX;\n+            Vector<Byte> v = toBitsVector();\n+            v.convertShape(VectorOperators.B2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.B2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+            v.convertShape(VectorOperators.B2I, species, 2)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 2);\n+            v.convertShape(VectorOperators.B2I, species, 3)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 3);\n+        }\n+\n+        private static byte[] prepare(int[] indices, int offset) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n@@ -833,1 +840,28 @@\n-            return new ByteMaxShuffle(r);\n+            return a;\n+        }\n+\n+        private static byte[] prepare(IntUnaryOperator f) {\n+            byte[] a = new byte[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (byte)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(byte[] indices) {\n+            int length = indices.length;\n+            for (byte si : indices) {\n+                if (si >= (byte)length || si < (byte)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -851,2 +885,2 @@\n-    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -867,2 +901,2 @@\n-    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n-        return super.fromBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -881,2 +915,2 @@\n-    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m) {\n-        return super.fromMemorySegment0Template(ByteMaxMask.class, ms, offset, (ByteMaxMask) m);  \/\/ specialize\n+    ByteVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Byte> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(ByteMaxMask.class, ms, offset, (ByteMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -920,0 +954,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":94,"deletions":59,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Double128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Double128Shuffle)VectorSupport.shuffleIota(ETYPE, Double128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Double128Shuffle)VectorSupport.shuffleIota(ETYPE, Double128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Double128Shuffle shuffleFromBytes(byte[] reorder) { return new Double128Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Double128Shuffle shuffleFromArray(int[] indexes, int i) { return new Double128Shuffle(indexes, i); }\n+    Double128Shuffle shuffleFromArray(int[] indices, int i) { return new Double128Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Double> toShuffle() {\n-        return super.toShuffleTemplate(Double128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -643,4 +630,5 @@\n-        public Double128Mask eq(VectorMask<Double> mask) {\n-            Objects.requireNonNull(mask);\n-            Double128Mask m = (Double128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Double128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Double128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Double128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Double128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -660,1 +648,1 @@\n-            return (Double128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Double128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -688,0 +676,1 @@\n+        @Override\n@@ -689,2 +678,1 @@\n-        \/* package-private *\/\n-        Double128Mask xor(VectorMask<Double> mask) {\n+        public Double128Mask xor(VectorMask<Double> mask) {\n@@ -765,1 +753,1 @@\n-        static final Class<Double> ETYPE = double.class; \/\/ used by the JVM\n+        static final Class<Long> ETYPE = long.class; \/\/ used by the JVM\n@@ -767,2 +755,4 @@\n-        Double128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double128Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -771,2 +761,2 @@\n-        public Double128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -775,2 +765,2 @@\n-        public Double128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Double128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -779,2 +769,2 @@\n-        public Double128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -784,0 +774,1 @@\n+        @ForceInline\n@@ -791,2 +782,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -798,3 +789,2 @@\n-        public Double128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Double128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Double128Vector)(((AbstractShuffle<Double>)(s)).toVectorTemplate())));\n+        Long128Vector toBitsVector() {\n+            return (Long128Vector) super.toBitsVectorTemplate();\n@@ -805,6 +795,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -813,0 +799,1 @@\n+        @Override\n@@ -814,0 +801,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -815,8 +806,44 @@\n-        public Double128Shuffle rearrange(VectorShuffle<Double> shuffle) {\n-            Double128Shuffle s = (Double128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -824,1 +851,1 @@\n-            return new Double128Shuffle(r);\n+            return true;\n@@ -842,2 +869,2 @@\n-    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n-        return super.fromArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, (Double128Mask) m, offsetInRange);  \/\/ specialize\n@@ -865,2 +892,2 @@\n-    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m) {\n-        return super.fromMemorySegment0Template(Double128Mask.class, ms, offset, (Double128Mask) m);  \/\/ specialize\n+    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Double128Mask.class, ms, offset, (Double128Mask) m, offsetInRange);  \/\/ specialize\n@@ -904,0 +931,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double128Vector.java","additions":88,"deletions":60,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Double256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Double256Shuffle)VectorSupport.shuffleIota(ETYPE, Double256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Double256Shuffle)VectorSupport.shuffleIota(ETYPE, Double256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Double256Shuffle shuffleFromBytes(byte[] reorder) { return new Double256Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Double256Shuffle shuffleFromArray(int[] indexes, int i) { return new Double256Shuffle(indexes, i); }\n+    Double256Shuffle shuffleFromArray(int[] indices, int i) { return new Double256Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Double> toShuffle() {\n-        return super.toShuffleTemplate(Double256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -647,4 +634,5 @@\n-        public Double256Mask eq(VectorMask<Double> mask) {\n-            Objects.requireNonNull(mask);\n-            Double256Mask m = (Double256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Double256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Double256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Double256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Double256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -664,1 +652,1 @@\n-            return (Double256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Double256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -692,0 +680,1 @@\n+        @Override\n@@ -693,2 +682,1 @@\n-        \/* package-private *\/\n-        Double256Mask xor(VectorMask<Double> mask) {\n+        public Double256Mask xor(VectorMask<Double> mask) {\n@@ -769,1 +757,1 @@\n-        static final Class<Double> ETYPE = double.class; \/\/ used by the JVM\n+        static final Class<Long> ETYPE = long.class; \/\/ used by the JVM\n@@ -771,2 +759,4 @@\n-        Double256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double256Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -775,2 +765,2 @@\n-        public Double256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -779,2 +769,2 @@\n-        public Double256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Double256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -783,2 +773,2 @@\n-        public Double256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -788,0 +778,1 @@\n+        @ForceInline\n@@ -795,2 +786,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -802,3 +793,2 @@\n-        public Double256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Double256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Double256Vector)(((AbstractShuffle<Double>)(s)).toVectorTemplate())));\n+        Long256Vector toBitsVector() {\n+            return (Long256Vector) super.toBitsVectorTemplate();\n@@ -809,6 +799,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -817,0 +803,1 @@\n+        @Override\n@@ -818,0 +805,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -819,8 +810,44 @@\n-        public Double256Shuffle rearrange(VectorShuffle<Double> shuffle) {\n-            Double256Shuffle s = (Double256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -828,1 +855,1 @@\n-            return new Double256Shuffle(r);\n+            return true;\n@@ -846,2 +873,2 @@\n-    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n-        return super.fromArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, (Double256Mask) m, offsetInRange);  \/\/ specialize\n@@ -869,2 +896,2 @@\n-    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m) {\n-        return super.fromMemorySegment0Template(Double256Mask.class, ms, offset, (Double256Mask) m);  \/\/ specialize\n+    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Double256Mask.class, ms, offset, (Double256Mask) m, offsetInRange);  \/\/ specialize\n@@ -908,0 +935,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double256Vector.java","additions":88,"deletions":60,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Double512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Double512Shuffle)VectorSupport.shuffleIota(ETYPE, Double512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Double512Shuffle)VectorSupport.shuffleIota(ETYPE, Double512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Double512Shuffle shuffleFromBytes(byte[] reorder) { return new Double512Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Double512Shuffle shuffleFromArray(int[] indexes, int i) { return new Double512Shuffle(indexes, i); }\n+    Double512Shuffle shuffleFromArray(int[] indices, int i) { return new Double512Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Double> toShuffle() {\n-        return super.toShuffleTemplate(Double512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -655,4 +642,5 @@\n-        public Double512Mask eq(VectorMask<Double> mask) {\n-            Objects.requireNonNull(mask);\n-            Double512Mask m = (Double512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Double512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Double512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Double512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Double512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -672,1 +660,1 @@\n-            return (Double512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Double512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -700,0 +688,1 @@\n+        @Override\n@@ -701,2 +690,1 @@\n-        \/* package-private *\/\n-        Double512Mask xor(VectorMask<Double> mask) {\n+        public Double512Mask xor(VectorMask<Double> mask) {\n@@ -777,1 +765,1 @@\n-        static final Class<Double> ETYPE = double.class; \/\/ used by the JVM\n+        static final Class<Long> ETYPE = long.class; \/\/ used by the JVM\n@@ -779,2 +767,4 @@\n-        Double512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double512Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -783,2 +773,2 @@\n-        public Double512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -787,2 +777,2 @@\n-        public Double512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Double512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -791,2 +781,2 @@\n-        public Double512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -796,0 +786,1 @@\n+        @ForceInline\n@@ -803,2 +794,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -810,3 +801,2 @@\n-        public Double512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Double512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Double512Vector)(((AbstractShuffle<Double>)(s)).toVectorTemplate())));\n+        Long512Vector toBitsVector() {\n+            return (Long512Vector) super.toBitsVectorTemplate();\n@@ -817,6 +807,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -825,0 +811,1 @@\n+        @Override\n@@ -826,0 +813,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -827,8 +818,44 @@\n-        public Double512Shuffle rearrange(VectorShuffle<Double> shuffle) {\n-            Double512Shuffle s = (Double512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -836,1 +863,1 @@\n-            return new Double512Shuffle(r);\n+            return true;\n@@ -854,2 +881,2 @@\n-    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n-        return super.fromArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, (Double512Mask) m, offsetInRange);  \/\/ specialize\n@@ -877,2 +904,2 @@\n-    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m) {\n-        return super.fromMemorySegment0Template(Double512Mask.class, ms, offset, (Double512Mask) m);  \/\/ specialize\n+    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Double512Mask.class, ms, offset, (Double512Mask) m, offsetInRange);  \/\/ specialize\n@@ -916,0 +943,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double512Vector.java","additions":88,"deletions":60,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Double64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Double64Shuffle)VectorSupport.shuffleIota(ETYPE, Double64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Double64Shuffle)VectorSupport.shuffleIota(ETYPE, Double64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Double64Shuffle shuffleFromBytes(byte[] reorder) { return new Double64Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Double64Shuffle shuffleFromArray(int[] indexes, int i) { return new Double64Shuffle(indexes, i); }\n+    Double64Shuffle shuffleFromArray(int[] indices, int i) { return new Double64Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Double> toShuffle() {\n-        return super.toShuffleTemplate(Double64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -641,4 +628,5 @@\n-        public Double64Mask eq(VectorMask<Double> mask) {\n-            Objects.requireNonNull(mask);\n-            Double64Mask m = (Double64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Double64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Double64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Double64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Double64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -658,1 +646,1 @@\n-            return (Double64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Double64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -686,0 +674,1 @@\n+        @Override\n@@ -687,2 +676,1 @@\n-        \/* package-private *\/\n-        Double64Mask xor(VectorMask<Double> mask) {\n+        public Double64Mask xor(VectorMask<Double> mask) {\n@@ -763,1 +751,1 @@\n-        static final Class<Double> ETYPE = double.class; \/\/ used by the JVM\n+        static final Class<Long> ETYPE = long.class; \/\/ used by the JVM\n@@ -765,2 +753,4 @@\n-        Double64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double64Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -769,2 +759,2 @@\n-        public Double64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Double64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -773,2 +763,2 @@\n-        public Double64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Double64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -777,2 +767,2 @@\n-        public Double64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -782,0 +772,1 @@\n+        @ForceInline\n@@ -789,2 +780,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -796,3 +787,2 @@\n-        public Double64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Double64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Double64Vector)(((AbstractShuffle<Double>)(s)).toVectorTemplate())));\n+        Long64Vector toBitsVector() {\n+            return (Long64Vector) super.toBitsVectorTemplate();\n@@ -803,6 +793,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -811,0 +797,1 @@\n+        @Override\n@@ -812,0 +799,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -813,8 +804,38 @@\n-        public Double64Shuffle rearrange(VectorShuffle<Double> shuffle) {\n-            Double64Shuffle s = (Double64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            a[offset] = laneSource(0);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -822,1 +843,1 @@\n-            return new Double64Shuffle(r);\n+            return true;\n@@ -840,2 +861,2 @@\n-    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n-        return super.fromArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, (Double64Mask) m, offsetInRange);  \/\/ specialize\n@@ -863,2 +884,2 @@\n-    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m) {\n-        return super.fromMemorySegment0Template(Double64Mask.class, ms, offset, (Double64Mask) m);  \/\/ specialize\n+    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Double64Mask.class, ms, offset, (Double64Mask) m, offsetInRange);  \/\/ specialize\n@@ -902,0 +923,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double64Vector.java","additions":82,"deletions":60,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    DoubleMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (DoubleMaxShuffle)VectorSupport.shuffleIota(ETYPE, DoubleMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (DoubleMaxShuffle)VectorSupport.shuffleIota(ETYPE, DoubleMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    DoubleMaxShuffle shuffleFromBytes(byte[] reorder) { return new DoubleMaxShuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    DoubleMaxShuffle shuffleFromArray(int[] indexes, int i) { return new DoubleMaxShuffle(indexes, i); }\n+    DoubleMaxShuffle shuffleFromArray(int[] indices, int i) { return new DoubleMaxShuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Double> toShuffle() {\n-        return super.toShuffleTemplate(DoubleMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -640,4 +627,5 @@\n-        public DoubleMaxMask eq(VectorMask<Double> mask) {\n-            Objects.requireNonNull(mask);\n-            DoubleMaxMask m = (DoubleMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        DoubleMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (DoubleMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                DoubleMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (DoubleMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -657,1 +645,1 @@\n-            return (DoubleMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (DoubleMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -685,0 +673,1 @@\n+        @Override\n@@ -686,2 +675,1 @@\n-        \/* package-private *\/\n-        DoubleMaxMask xor(VectorMask<Double> mask) {\n+        public DoubleMaxMask xor(VectorMask<Double> mask) {\n@@ -762,1 +750,1 @@\n-        static final Class<Double> ETYPE = double.class; \/\/ used by the JVM\n+        static final Class<Long> ETYPE = long.class; \/\/ used by the JVM\n@@ -764,2 +752,4 @@\n-        DoubleMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        DoubleMaxShuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -768,2 +758,2 @@\n-        public DoubleMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        DoubleMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -772,2 +762,2 @@\n-        public DoubleMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        DoubleMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -776,2 +766,2 @@\n-        public DoubleMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -781,0 +771,1 @@\n+        @ForceInline\n@@ -788,2 +779,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -795,3 +786,2 @@\n-        public DoubleMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, DoubleMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((DoubleMaxVector)(((AbstractShuffle<Double>)(s)).toVectorTemplate())));\n+        LongMaxVector toBitsVector() {\n+            return (LongMaxVector) super.toBitsVectorTemplate();\n@@ -802,6 +792,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return LongMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -810,0 +796,1 @@\n+        @Override\n@@ -811,0 +798,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -812,8 +803,44 @@\n-        public DoubleMaxShuffle rearrange(VectorShuffle<Double> shuffle) {\n-            DoubleMaxShuffle s = (DoubleMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -821,1 +848,1 @@\n-            return new DoubleMaxShuffle(r);\n+            return true;\n@@ -839,2 +866,2 @@\n-    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n-        return super.fromArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -862,2 +889,2 @@\n-    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m) {\n-        return super.fromMemorySegment0Template(DoubleMaxMask.class, ms, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    DoubleVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Double> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(DoubleMaxMask.class, ms, offset, (DoubleMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -901,0 +928,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleMaxVector.java","additions":88,"deletions":60,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Float128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Float128Shuffle)VectorSupport.shuffleIota(ETYPE, Float128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Float128Shuffle)VectorSupport.shuffleIota(ETYPE, Float128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Float128Shuffle shuffleFromBytes(byte[] reorder) { return new Float128Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Float128Shuffle shuffleFromArray(int[] indexes, int i) { return new Float128Shuffle(indexes, i); }\n+    Float128Shuffle shuffleFromArray(int[] indices, int i) { return new Float128Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Float> toShuffle() {\n-        return super.toShuffleTemplate(Float128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -647,4 +634,5 @@\n-        public Float128Mask eq(VectorMask<Float> mask) {\n-            Objects.requireNonNull(mask);\n-            Float128Mask m = (Float128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Float128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Float128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Float128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Float128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -664,1 +652,1 @@\n-            return (Float128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Float128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -692,0 +680,1 @@\n+        @Override\n@@ -693,2 +682,1 @@\n-        \/* package-private *\/\n-        Float128Mask xor(VectorMask<Float> mask) {\n+        public Float128Mask xor(VectorMask<Float> mask) {\n@@ -769,1 +757,1 @@\n-        static final Class<Float> ETYPE = float.class; \/\/ used by the JVM\n+        static final Class<Integer> ETYPE = int.class; \/\/ used by the JVM\n@@ -771,2 +759,4 @@\n-        Float128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float128Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -775,2 +765,2 @@\n-        public Float128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -779,2 +769,2 @@\n-        public Float128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Float128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -783,2 +773,2 @@\n-        public Float128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -788,0 +778,1 @@\n+        @ForceInline\n@@ -795,2 +786,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -802,3 +793,2 @@\n-        public Float128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Float128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Float128Vector)(((AbstractShuffle<Float>)(s)).toVectorTemplate())));\n+        Int128Vector toBitsVector() {\n+            return (Int128Vector) super.toBitsVectorTemplate();\n@@ -809,6 +799,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -817,0 +803,1 @@\n+        @Override\n@@ -818,0 +805,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -819,8 +810,38 @@\n-        public Float128Shuffle rearrange(VectorShuffle<Float> shuffle) {\n-            Float128Shuffle s = (Float128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -828,1 +849,1 @@\n-            return new Float128Shuffle(r);\n+            return true;\n@@ -846,2 +867,2 @@\n-    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n-        return super.fromArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, (Float128Mask) m, offsetInRange);  \/\/ specialize\n@@ -869,2 +890,2 @@\n-    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m) {\n-        return super.fromMemorySegment0Template(Float128Mask.class, ms, offset, (Float128Mask) m);  \/\/ specialize\n+    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Float128Mask.class, ms, offset, (Float128Mask) m, offsetInRange);  \/\/ specialize\n@@ -908,0 +929,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float128Vector.java","additions":82,"deletions":60,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Float256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Float256Shuffle)VectorSupport.shuffleIota(ETYPE, Float256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Float256Shuffle)VectorSupport.shuffleIota(ETYPE, Float256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Float256Shuffle shuffleFromBytes(byte[] reorder) { return new Float256Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Float256Shuffle shuffleFromArray(int[] indexes, int i) { return new Float256Shuffle(indexes, i); }\n+    Float256Shuffle shuffleFromArray(int[] indices, int i) { return new Float256Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Float> toShuffle() {\n-        return super.toShuffleTemplate(Float256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -655,4 +642,5 @@\n-        public Float256Mask eq(VectorMask<Float> mask) {\n-            Objects.requireNonNull(mask);\n-            Float256Mask m = (Float256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Float256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Float256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Float256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Float256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -672,1 +660,1 @@\n-            return (Float256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Float256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -700,0 +688,1 @@\n+        @Override\n@@ -701,2 +690,1 @@\n-        \/* package-private *\/\n-        Float256Mask xor(VectorMask<Float> mask) {\n+        public Float256Mask xor(VectorMask<Float> mask) {\n@@ -777,1 +765,1 @@\n-        static final Class<Float> ETYPE = float.class; \/\/ used by the JVM\n+        static final Class<Integer> ETYPE = int.class; \/\/ used by the JVM\n@@ -779,2 +767,4 @@\n-        Float256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float256Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -783,2 +773,2 @@\n-        public Float256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -787,2 +777,2 @@\n-        public Float256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Float256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -791,2 +781,2 @@\n-        public Float256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -796,0 +786,1 @@\n+        @ForceInline\n@@ -803,2 +794,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -810,3 +801,2 @@\n-        public Float256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Float256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Float256Vector)(((AbstractShuffle<Float>)(s)).toVectorTemplate())));\n+        Int256Vector toBitsVector() {\n+            return (Int256Vector) super.toBitsVectorTemplate();\n@@ -817,6 +807,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -825,0 +811,1 @@\n+        @Override\n@@ -826,0 +813,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -827,8 +818,38 @@\n-        public Float256Shuffle rearrange(VectorShuffle<Float> shuffle) {\n-            Float256Shuffle s = (Float256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -836,1 +857,1 @@\n-            return new Float256Shuffle(r);\n+            return true;\n@@ -854,2 +875,2 @@\n-    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n-        return super.fromArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, (Float256Mask) m, offsetInRange);  \/\/ specialize\n@@ -877,2 +898,2 @@\n-    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m) {\n-        return super.fromMemorySegment0Template(Float256Mask.class, ms, offset, (Float256Mask) m);  \/\/ specialize\n+    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Float256Mask.class, ms, offset, (Float256Mask) m, offsetInRange);  \/\/ specialize\n@@ -916,0 +937,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float256Vector.java","additions":82,"deletions":60,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Float512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Float512Shuffle)VectorSupport.shuffleIota(ETYPE, Float512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Float512Shuffle)VectorSupport.shuffleIota(ETYPE, Float512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Float512Shuffle shuffleFromBytes(byte[] reorder) { return new Float512Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Float512Shuffle shuffleFromArray(int[] indexes, int i) { return new Float512Shuffle(indexes, i); }\n+    Float512Shuffle shuffleFromArray(int[] indices, int i) { return new Float512Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Float> toShuffle() {\n-        return super.toShuffleTemplate(Float512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -671,4 +658,5 @@\n-        public Float512Mask eq(VectorMask<Float> mask) {\n-            Objects.requireNonNull(mask);\n-            Float512Mask m = (Float512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Float512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Float512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Float512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Float512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -688,1 +676,1 @@\n-            return (Float512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Float512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -716,0 +704,1 @@\n+        @Override\n@@ -717,2 +706,1 @@\n-        \/* package-private *\/\n-        Float512Mask xor(VectorMask<Float> mask) {\n+        public Float512Mask xor(VectorMask<Float> mask) {\n@@ -793,1 +781,1 @@\n-        static final Class<Float> ETYPE = float.class; \/\/ used by the JVM\n+        static final Class<Integer> ETYPE = int.class; \/\/ used by the JVM\n@@ -795,2 +783,4 @@\n-        Float512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float512Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -799,2 +789,2 @@\n-        public Float512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -803,2 +793,2 @@\n-        public Float512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Float512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -807,2 +797,2 @@\n-        public Float512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -812,0 +802,1 @@\n+        @ForceInline\n@@ -819,2 +810,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -826,3 +817,2 @@\n-        public Float512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Float512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Float512Vector)(((AbstractShuffle<Float>)(s)).toVectorTemplate())));\n+        Int512Vector toBitsVector() {\n+            return (Int512Vector) super.toBitsVectorTemplate();\n@@ -833,6 +823,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -841,0 +827,1 @@\n+        @Override\n@@ -842,0 +829,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -843,8 +834,38 @@\n-        public Float512Shuffle rearrange(VectorShuffle<Float> shuffle) {\n-            Float512Shuffle s = (Float512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -852,1 +873,1 @@\n-            return new Float512Shuffle(r);\n+            return true;\n@@ -870,2 +891,2 @@\n-    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n-        return super.fromArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, (Float512Mask) m, offsetInRange);  \/\/ specialize\n@@ -893,2 +914,2 @@\n-    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m) {\n-        return super.fromMemorySegment0Template(Float512Mask.class, ms, offset, (Float512Mask) m);  \/\/ specialize\n+    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Float512Mask.class, ms, offset, (Float512Mask) m, offsetInRange);  \/\/ specialize\n@@ -932,0 +953,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float512Vector.java","additions":82,"deletions":60,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Float64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Float64Shuffle)VectorSupport.shuffleIota(ETYPE, Float64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Float64Shuffle)VectorSupport.shuffleIota(ETYPE, Float64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Float64Shuffle shuffleFromBytes(byte[] reorder) { return new Float64Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Float64Shuffle shuffleFromArray(int[] indexes, int i) { return new Float64Shuffle(indexes, i); }\n+    Float64Shuffle shuffleFromArray(int[] indices, int i) { return new Float64Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Float> toShuffle() {\n-        return super.toShuffleTemplate(Float64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -643,4 +630,5 @@\n-        public Float64Mask eq(VectorMask<Float> mask) {\n-            Objects.requireNonNull(mask);\n-            Float64Mask m = (Float64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Float64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Float64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Float64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Float64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -660,1 +648,1 @@\n-            return (Float64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Float64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -688,0 +676,1 @@\n+        @Override\n@@ -689,2 +678,1 @@\n-        \/* package-private *\/\n-        Float64Mask xor(VectorMask<Float> mask) {\n+        public Float64Mask xor(VectorMask<Float> mask) {\n@@ -765,1 +753,1 @@\n-        static final Class<Float> ETYPE = float.class; \/\/ used by the JVM\n+        static final Class<Integer> ETYPE = int.class; \/\/ used by the JVM\n@@ -767,2 +755,4 @@\n-        Float64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float64Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -771,2 +761,2 @@\n-        public Float64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Float64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -775,2 +765,2 @@\n-        public Float64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Float64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -779,2 +769,2 @@\n-        public Float64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -784,0 +774,1 @@\n+        @ForceInline\n@@ -791,2 +782,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -798,3 +789,2 @@\n-        public Float64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Float64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Float64Vector)(((AbstractShuffle<Float>)(s)).toVectorTemplate())));\n+        Int64Vector toBitsVector() {\n+            return (Int64Vector) super.toBitsVectorTemplate();\n@@ -805,6 +795,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -813,0 +799,1 @@\n+        @Override\n@@ -814,0 +801,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -815,8 +806,38 @@\n-        public Float64Shuffle rearrange(VectorShuffle<Float> shuffle) {\n-            Float64Shuffle s = (Float64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -824,1 +845,1 @@\n-            return new Float64Shuffle(r);\n+            return true;\n@@ -842,2 +863,2 @@\n-    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n-        return super.fromArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, (Float64Mask) m, offsetInRange);  \/\/ specialize\n@@ -865,2 +886,2 @@\n-    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m) {\n-        return super.fromMemorySegment0Template(Float64Mask.class, ms, offset, (Float64Mask) m);  \/\/ specialize\n+    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Float64Mask.class, ms, offset, (Float64Mask) m, offsetInRange);  \/\/ specialize\n@@ -904,0 +925,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float64Vector.java","additions":82,"deletions":60,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    FloatMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (FloatMaxShuffle)VectorSupport.shuffleIota(ETYPE, FloatMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (FloatMaxShuffle)VectorSupport.shuffleIota(ETYPE, FloatMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    FloatMaxShuffle shuffleFromBytes(byte[] reorder) { return new FloatMaxShuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    FloatMaxShuffle shuffleFromArray(int[] indexes, int i) { return new FloatMaxShuffle(indexes, i); }\n+    FloatMaxShuffle shuffleFromArray(int[] indices, int i) { return new FloatMaxShuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Float> toShuffle() {\n-        return super.toShuffleTemplate(FloatMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -640,4 +627,5 @@\n-        public FloatMaxMask eq(VectorMask<Float> mask) {\n-            Objects.requireNonNull(mask);\n-            FloatMaxMask m = (FloatMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        FloatMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (FloatMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                FloatMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (FloatMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -657,1 +645,1 @@\n-            return (FloatMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (FloatMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -685,0 +673,1 @@\n+        @Override\n@@ -686,2 +675,1 @@\n-        \/* package-private *\/\n-        FloatMaxMask xor(VectorMask<Float> mask) {\n+        public FloatMaxMask xor(VectorMask<Float> mask) {\n@@ -762,1 +750,1 @@\n-        static final Class<Float> ETYPE = float.class; \/\/ used by the JVM\n+        static final Class<Integer> ETYPE = int.class; \/\/ used by the JVM\n@@ -764,2 +752,4 @@\n-        FloatMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        FloatMaxShuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -768,2 +758,2 @@\n-        public FloatMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        FloatMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -772,2 +762,2 @@\n-        public FloatMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        FloatMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -776,2 +766,2 @@\n-        public FloatMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -781,0 +771,1 @@\n+        @ForceInline\n@@ -788,2 +779,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -795,3 +786,2 @@\n-        public FloatMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, FloatMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((FloatMaxVector)(((AbstractShuffle<Float>)(s)).toVectorTemplate())));\n+        IntMaxVector toBitsVector() {\n+            return (IntMaxVector) super.toBitsVectorTemplate();\n@@ -802,6 +792,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return IntMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -810,0 +796,1 @@\n+        @Override\n@@ -811,0 +798,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -812,8 +803,38 @@\n-        public FloatMaxShuffle rearrange(VectorShuffle<Float> shuffle) {\n-            FloatMaxShuffle s = (FloatMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -821,1 +842,1 @@\n-            return new FloatMaxShuffle(r);\n+            return true;\n@@ -839,2 +860,2 @@\n-    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n-        return super.fromArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -862,2 +883,2 @@\n-    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m) {\n-        return super.fromMemorySegment0Template(FloatMaxMask.class, ms, offset, (FloatMaxMask) m);  \/\/ specialize\n+    FloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Float> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(FloatMaxMask.class, ms, offset, (FloatMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -901,0 +922,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatMaxVector.java","additions":82,"deletions":60,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Halffloat128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Halffloat128Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Halffloat128Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Halffloat128Shuffle shuffleFromBytes(byte[] reorder) { return new Halffloat128Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Halffloat128Shuffle shuffleFromArray(int[] indexes, int i) { return new Halffloat128Shuffle(indexes, i); }\n+    Halffloat128Shuffle shuffleFromArray(int[] indices, int i) { return new Halffloat128Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Halffloat> toShuffle() {\n-        return super.toShuffleTemplate(Halffloat128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -655,4 +642,5 @@\n-        public Halffloat128Mask eq(VectorMask<Halffloat> mask) {\n-            Objects.requireNonNull(mask);\n-            Halffloat128Mask m = (Halffloat128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Halffloat128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Halffloat128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Halffloat128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Halffloat128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -672,1 +660,1 @@\n-            return (Halffloat128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Halffloat128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -700,0 +688,1 @@\n+        @Override\n@@ -701,2 +690,1 @@\n-        \/* package-private *\/\n-        Halffloat128Mask xor(VectorMask<Halffloat> mask) {\n+        public Halffloat128Mask xor(VectorMask<Halffloat> mask) {\n@@ -777,1 +765,1 @@\n-        static final Class<Halffloat> ETYPE = Halffloat.class; \/\/ used by the JVM\n+        static final Class<Short> ETYPE = short.class; \/\/ used by the JVM\n@@ -779,2 +767,4 @@\n-        Halffloat128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat128Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -783,2 +773,2 @@\n-        public Halffloat128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -787,2 +777,2 @@\n-        public Halffloat128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Halffloat128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -791,2 +781,2 @@\n-        public Halffloat128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -796,0 +786,1 @@\n+        @ForceInline\n@@ -803,2 +794,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -810,3 +801,2 @@\n-        public Halffloat128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Halffloat128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Halffloat128Vector)(((AbstractShuffle<Halffloat>)(s)).toVectorTemplate())));\n+        Short128Vector toBitsVector() {\n+            return (Short128Vector) super.toBitsVectorTemplate();\n@@ -817,6 +807,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -825,0 +811,1 @@\n+        @Override\n@@ -826,0 +813,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -827,8 +818,45 @@\n-        public Halffloat128Shuffle rearrange(VectorShuffle<Halffloat> shuffle) {\n-            Halffloat128Shuffle s = (Halffloat128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_128;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -836,1 +864,1 @@\n-            return new Halffloat128Shuffle(r);\n+            return true;\n@@ -854,2 +882,2 @@\n-    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromArray0Template(Halffloat128Mask.class, a, offset, (Halffloat128Mask) m);  \/\/ specialize\n+    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromArray0Template(Halffloat128Mask.class, a, offset, (Halffloat128Mask) m, offsetInRange);  \/\/ specialize\n@@ -869,2 +897,2 @@\n-    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromCharArray0Template(Halffloat128Mask.class, a, offset, (Halffloat128Mask) m);  \/\/ specialize\n+    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Halffloat128Mask.class, a, offset, (Halffloat128Mask) m, offsetInRange);  \/\/ specialize\n@@ -884,2 +912,2 @@\n-    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m) {\n-        return super.fromMemorySegment0Template(Halffloat128Mask.class, ms, offset, (Halffloat128Mask) m);  \/\/ specialize\n+    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Halffloat128Mask.class, ms, offset, (Halffloat128Mask) m, offsetInRange);  \/\/ specialize\n@@ -923,0 +951,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Halffloat128Vector.java","additions":91,"deletions":62,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Halffloat256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Halffloat256Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Halffloat256Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Halffloat256Shuffle shuffleFromBytes(byte[] reorder) { return new Halffloat256Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Halffloat256Shuffle shuffleFromArray(int[] indexes, int i) { return new Halffloat256Shuffle(indexes, i); }\n+    Halffloat256Shuffle shuffleFromArray(int[] indices, int i) { return new Halffloat256Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Halffloat> toShuffle() {\n-        return super.toShuffleTemplate(Halffloat256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -671,4 +658,5 @@\n-        public Halffloat256Mask eq(VectorMask<Halffloat> mask) {\n-            Objects.requireNonNull(mask);\n-            Halffloat256Mask m = (Halffloat256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Halffloat256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Halffloat256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Halffloat256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Halffloat256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -688,1 +676,1 @@\n-            return (Halffloat256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Halffloat256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -716,0 +704,1 @@\n+        @Override\n@@ -717,2 +706,1 @@\n-        \/* package-private *\/\n-        Halffloat256Mask xor(VectorMask<Halffloat> mask) {\n+        public Halffloat256Mask xor(VectorMask<Halffloat> mask) {\n@@ -793,1 +781,1 @@\n-        static final Class<Halffloat> ETYPE = Halffloat.class; \/\/ used by the JVM\n+        static final Class<Short> ETYPE = short.class; \/\/ used by the JVM\n@@ -795,2 +783,4 @@\n-        Halffloat256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat256Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -799,2 +789,2 @@\n-        public Halffloat256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -803,2 +793,2 @@\n-        public Halffloat256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Halffloat256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -807,2 +797,2 @@\n-        public Halffloat256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -812,0 +802,1 @@\n+        @ForceInline\n@@ -819,2 +810,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -826,3 +817,2 @@\n-        public Halffloat256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Halffloat256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Halffloat256Vector)(((AbstractShuffle<Halffloat>)(s)).toVectorTemplate())));\n+        Short256Vector toBitsVector() {\n+            return (Short256Vector) super.toBitsVectorTemplate();\n@@ -833,6 +823,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -841,0 +827,1 @@\n+        @Override\n@@ -842,0 +829,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -843,8 +834,45 @@\n-        public Halffloat256Shuffle rearrange(VectorShuffle<Halffloat> shuffle) {\n-            Halffloat256Shuffle s = (Halffloat256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_256;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -852,1 +880,1 @@\n-            return new Halffloat256Shuffle(r);\n+            return true;\n@@ -870,2 +898,2 @@\n-    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromArray0Template(Halffloat256Mask.class, a, offset, (Halffloat256Mask) m);  \/\/ specialize\n+    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromArray0Template(Halffloat256Mask.class, a, offset, (Halffloat256Mask) m, offsetInRange);  \/\/ specialize\n@@ -885,2 +913,2 @@\n-    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromCharArray0Template(Halffloat256Mask.class, a, offset, (Halffloat256Mask) m);  \/\/ specialize\n+    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Halffloat256Mask.class, a, offset, (Halffloat256Mask) m, offsetInRange);  \/\/ specialize\n@@ -900,2 +928,2 @@\n-    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m) {\n-        return super.fromMemorySegment0Template(Halffloat256Mask.class, ms, offset, (Halffloat256Mask) m);  \/\/ specialize\n+    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Halffloat256Mask.class, ms, offset, (Halffloat256Mask) m, offsetInRange);  \/\/ specialize\n@@ -939,0 +967,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Halffloat256Vector.java","additions":91,"deletions":62,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Halffloat512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Halffloat512Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Halffloat512Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Halffloat512Shuffle shuffleFromBytes(byte[] reorder) { return new Halffloat512Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Halffloat512Shuffle shuffleFromArray(int[] indexes, int i) { return new Halffloat512Shuffle(indexes, i); }\n+    Halffloat512Shuffle shuffleFromArray(int[] indices, int i) { return new Halffloat512Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Halffloat> toShuffle() {\n-        return super.toShuffleTemplate(Halffloat512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -703,4 +690,5 @@\n-        public Halffloat512Mask eq(VectorMask<Halffloat> mask) {\n-            Objects.requireNonNull(mask);\n-            Halffloat512Mask m = (Halffloat512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Halffloat512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Halffloat512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Halffloat512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Halffloat512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -720,1 +708,1 @@\n-            return (Halffloat512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Halffloat512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -748,0 +736,1 @@\n+        @Override\n@@ -749,2 +738,1 @@\n-        \/* package-private *\/\n-        Halffloat512Mask xor(VectorMask<Halffloat> mask) {\n+        public Halffloat512Mask xor(VectorMask<Halffloat> mask) {\n@@ -825,1 +813,1 @@\n-        static final Class<Halffloat> ETYPE = Halffloat.class; \/\/ used by the JVM\n+        static final Class<Short> ETYPE = short.class; \/\/ used by the JVM\n@@ -827,2 +815,4 @@\n-        Halffloat512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat512Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -831,2 +821,2 @@\n-        public Halffloat512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -835,2 +825,2 @@\n-        public Halffloat512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Halffloat512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -839,2 +829,2 @@\n-        public Halffloat512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -844,0 +834,1 @@\n+        @ForceInline\n@@ -851,2 +842,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -858,3 +849,2 @@\n-        public Halffloat512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Halffloat512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Halffloat512Vector)(((AbstractShuffle<Halffloat>)(s)).toVectorTemplate())));\n+        Short512Vector toBitsVector() {\n+            return (Short512Vector) super.toBitsVectorTemplate();\n@@ -865,6 +855,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -873,0 +859,1 @@\n+        @Override\n@@ -874,0 +861,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -875,8 +866,45 @@\n-        public Halffloat512Shuffle rearrange(VectorShuffle<Halffloat> shuffle) {\n-            Halffloat512Shuffle s = (Halffloat512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_512;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -884,1 +912,1 @@\n-            return new Halffloat512Shuffle(r);\n+            return true;\n@@ -902,2 +930,2 @@\n-    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromArray0Template(Halffloat512Mask.class, a, offset, (Halffloat512Mask) m);  \/\/ specialize\n+    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromArray0Template(Halffloat512Mask.class, a, offset, (Halffloat512Mask) m, offsetInRange);  \/\/ specialize\n@@ -917,2 +945,2 @@\n-    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromCharArray0Template(Halffloat512Mask.class, a, offset, (Halffloat512Mask) m);  \/\/ specialize\n+    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Halffloat512Mask.class, a, offset, (Halffloat512Mask) m, offsetInRange);  \/\/ specialize\n@@ -932,2 +960,2 @@\n-    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m) {\n-        return super.fromMemorySegment0Template(Halffloat512Mask.class, ms, offset, (Halffloat512Mask) m);  \/\/ specialize\n+    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Halffloat512Mask.class, ms, offset, (Halffloat512Mask) m, offsetInRange);  \/\/ specialize\n@@ -971,0 +999,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Halffloat512Vector.java","additions":91,"deletions":62,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Halffloat64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Halffloat64Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Halffloat64Shuffle)VectorSupport.shuffleIota(ETYPE, Halffloat64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Halffloat64Shuffle shuffleFromBytes(byte[] reorder) { return new Halffloat64Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Halffloat64Shuffle shuffleFromArray(int[] indexes, int i) { return new Halffloat64Shuffle(indexes, i); }\n+    Halffloat64Shuffle shuffleFromArray(int[] indices, int i) { return new Halffloat64Shuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Halffloat> toShuffle() {\n-        return super.toShuffleTemplate(Halffloat64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -647,4 +634,5 @@\n-        public Halffloat64Mask eq(VectorMask<Halffloat> mask) {\n-            Objects.requireNonNull(mask);\n-            Halffloat64Mask m = (Halffloat64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Halffloat64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Halffloat64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Halffloat64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Halffloat64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -664,1 +652,1 @@\n-            return (Halffloat64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Halffloat64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -692,0 +680,1 @@\n+        @Override\n@@ -693,2 +682,1 @@\n-        \/* package-private *\/\n-        Halffloat64Mask xor(VectorMask<Halffloat> mask) {\n+        public Halffloat64Mask xor(VectorMask<Halffloat> mask) {\n@@ -769,1 +757,1 @@\n-        static final Class<Halffloat> ETYPE = Halffloat.class; \/\/ used by the JVM\n+        static final Class<Short> ETYPE = short.class; \/\/ used by the JVM\n@@ -771,2 +759,4 @@\n-        Halffloat64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat64Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -775,2 +765,2 @@\n-        public Halffloat64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Halffloat64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -779,2 +769,2 @@\n-        public Halffloat64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Halffloat64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -783,2 +773,2 @@\n-        public Halffloat64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -788,0 +778,1 @@\n+        @ForceInline\n@@ -795,2 +786,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -802,3 +793,2 @@\n-        public Halffloat64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Halffloat64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Halffloat64Vector)(((AbstractShuffle<Halffloat>)(s)).toVectorTemplate())));\n+        Short64Vector toBitsVector() {\n+            return (Short64Vector) super.toBitsVectorTemplate();\n@@ -809,6 +799,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -817,0 +803,1 @@\n+        @Override\n@@ -818,0 +805,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -819,8 +810,45 @@\n-        public Halffloat64Shuffle rearrange(VectorShuffle<Halffloat> shuffle) {\n-            Halffloat64Shuffle s = (Halffloat64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_64;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -828,1 +856,1 @@\n-            return new Halffloat64Shuffle(r);\n+            return true;\n@@ -846,2 +874,2 @@\n-    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromArray0Template(Halffloat64Mask.class, a, offset, (Halffloat64Mask) m);  \/\/ specialize\n+    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromArray0Template(Halffloat64Mask.class, a, offset, (Halffloat64Mask) m, offsetInRange);  \/\/ specialize\n@@ -861,2 +889,2 @@\n-    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromCharArray0Template(Halffloat64Mask.class, a, offset, (Halffloat64Mask) m);  \/\/ specialize\n+    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Halffloat64Mask.class, a, offset, (Halffloat64Mask) m, offsetInRange);  \/\/ specialize\n@@ -876,2 +904,2 @@\n-    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m) {\n-        return super.fromMemorySegment0Template(Halffloat64Mask.class, ms, offset, (Halffloat64Mask) m);  \/\/ specialize\n+    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Halffloat64Mask.class, ms, offset, (Halffloat64Mask) m, offsetInRange);  \/\/ specialize\n@@ -915,0 +943,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Halffloat64Vector.java","additions":91,"deletions":62,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    HalffloatMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (HalffloatMaxShuffle)VectorSupport.shuffleIota(ETYPE, HalffloatMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (HalffloatMaxShuffle)VectorSupport.shuffleIota(ETYPE, HalffloatMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    HalffloatMaxShuffle shuffleFromBytes(byte[] reorder) { return new HalffloatMaxShuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    HalffloatMaxShuffle shuffleFromArray(int[] indexes, int i) { return new HalffloatMaxShuffle(indexes, i); }\n+    HalffloatMaxShuffle shuffleFromArray(int[] indices, int i) { return new HalffloatMaxShuffle(indices, i); }\n@@ -347,0 +332,1 @@\n+    @Override\n@@ -348,2 +334,3 @@\n-    public VectorShuffle<Halffloat> toShuffle() {\n-        return super.toShuffleTemplate(HalffloatMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -640,4 +627,5 @@\n-        public HalffloatMaxMask eq(VectorMask<Halffloat> mask) {\n-            Objects.requireNonNull(mask);\n-            HalffloatMaxMask m = (HalffloatMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        HalffloatMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (HalffloatMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                HalffloatMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (HalffloatMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -657,1 +645,1 @@\n-            return (HalffloatMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (HalffloatMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -685,0 +673,1 @@\n+        @Override\n@@ -686,2 +675,1 @@\n-        \/* package-private *\/\n-        HalffloatMaxMask xor(VectorMask<Halffloat> mask) {\n+        public HalffloatMaxMask xor(VectorMask<Halffloat> mask) {\n@@ -762,1 +750,1 @@\n-        static final Class<Halffloat> ETYPE = Halffloat.class; \/\/ used by the JVM\n+        static final Class<Short> ETYPE = short.class; \/\/ used by the JVM\n@@ -764,2 +752,4 @@\n-        HalffloatMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        HalffloatMaxShuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -768,2 +758,2 @@\n-        public HalffloatMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        HalffloatMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -772,2 +762,2 @@\n-        public HalffloatMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        HalffloatMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -776,2 +766,2 @@\n-        public HalffloatMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -781,0 +771,1 @@\n+        @ForceInline\n@@ -788,2 +779,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -795,3 +786,2 @@\n-        public HalffloatMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, HalffloatMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((HalffloatMaxVector)(((AbstractShuffle<Halffloat>)(s)).toVectorTemplate())));\n+        ShortMaxVector toBitsVector() {\n+            return (ShortMaxVector) super.toBitsVectorTemplate();\n@@ -802,6 +792,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return ShortMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -810,0 +796,1 @@\n+        @Override\n@@ -811,0 +798,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -812,8 +803,45 @@\n-        public HalffloatMaxShuffle rearrange(VectorShuffle<Halffloat> shuffle) {\n-            HalffloatMaxShuffle s = (HalffloatMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_MAX;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -821,1 +849,1 @@\n-            return new HalffloatMaxShuffle(r);\n+            return true;\n@@ -839,2 +867,2 @@\n-    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromArray0Template(HalffloatMaxMask.class, a, offset, (HalffloatMaxMask) m);  \/\/ specialize\n+    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromArray0Template(HalffloatMaxMask.class, a, offset, (HalffloatMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -854,2 +882,2 @@\n-    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m) {\n-        return super.fromCharArray0Template(HalffloatMaxMask.class, a, offset, (HalffloatMaxMask) m);  \/\/ specialize\n+    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromCharArray0Template(HalffloatMaxMask.class, a, offset, (HalffloatMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -869,2 +897,2 @@\n-    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m) {\n-        return super.fromMemorySegment0Template(HalffloatMaxMask.class, ms, offset, (HalffloatMaxMask) m);  \/\/ specialize\n+    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(HalffloatMaxMask.class, ms, offset, (HalffloatMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -908,0 +936,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/HalffloatMaxVector.java","additions":91,"deletions":62,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n@@ -32,3 +34,1 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ValueLayout;\n-import jdk.internal.access.foreign.MemorySegmentProxy;\n+import jdk.internal.foreign.AbstractMemorySegmentImpl;\n@@ -767,9 +767,3 @@\n-                \/\/ FIXME: Support this in the JIT.\n-                VectorMask<Short> thisNZ\n-                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n-                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n-                op = OR_UNCHECKED;\n-                \/\/ FIXME: Support OR_UNCHECKED on float\/double also!\n-                return this.viewAsIntegralLanes()\n-                    .lanewise(op, that.viewAsIntegralLanes())\n-                    .viewAsFloatingLanes();\n+                VectorMask<Short> mask\n+                    = this.viewAsIntegralLanes().compare(EQ, (short) 0);\n+                return this.blend(that, mask.cast(vspecies()));\n@@ -806,1 +800,4 @@\n-                return blend(lanewise(op, v), m);\n+                ShortVector bits = this.viewAsIntegralLanes();\n+                VectorMask<Short> mask\n+                    = bits.compare(EQ, (short) 0, m.cast(bits.vspecies()));\n+                return this.blend(that, mask.cast(vspecies()));\n@@ -957,1 +954,1 @@\n-   \/**\n+    \/**\n@@ -2326,2 +2323,2 @@\n-    private final\n-    VectorShuffle<Halffloat> toShuffle0(HalffloatSpecies dsp) {\n+    final <F>\n+    VectorShuffle<F> toShuffle0(AbstractSpecies<F> dsp) {\n@@ -2336,12 +2333,0 @@\n-    \/*package-private*\/\n-    @ForceInline\n-    final\n-    VectorShuffle<Halffloat> toShuffleTemplate(Class<?> shuffleType) {\n-        HalffloatSpecies vsp = vspecies();\n-        return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n-                                     getClass(), short.class, length(),\n-                                     shuffleType, byte.class, length(),\n-                                     this, vsp,\n-                                     HalffloatVector::toShuffle0);\n-    }\n-\n@@ -2362,3 +2347,3 @@\n-      return (HalffloatVector) VectorSupport.comExpOp(VectorSupport.VECTOR_OP_COMPRESS, getClass(), masktype,\n-                                                   Halffloat.class, length(), this, m,\n-                                                   (v1, m1) -> compressHelper(v1, m1));\n+      return (HalffloatVector) VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_COMPRESS, getClass(), masktype,\n+                                                        Halffloat.class, length(), this, m,\n+                                                        (v1, m1) -> compressHelper(v1, m1));\n@@ -2381,3 +2366,3 @@\n-      return (HalffloatVector) VectorSupport.comExpOp(VectorSupport.VECTOR_OP_EXPAND, getClass(), masktype,\n-                                                   Halffloat.class, length(), this, m,\n-                                                   (v1, m1) -> expandHelper(v1, m1));\n+      return (HalffloatVector) VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_EXPAND, getClass(), masktype,\n+                                                        Halffloat.class, length(), this, m,\n+                                                        (v1, m1) -> expandHelper(v1, m1));\n@@ -2810,2 +2795,2 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n-            return vsp.dummyVector().fromArray0(a, offset, m);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+            return vsp.dummyVector().fromArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -2814,1 +2799,0 @@\n-        \/\/ FIXME: optimize\n@@ -2816,1 +2800,1 @@\n-        return vsp.vOp(m, i -> a[offset + i]);\n+        return vsp.dummyVector().fromArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -2959,2 +2943,2 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n-            return vsp.dummyVector().fromCharArray0(a, offset, m);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+            return vsp.dummyVector().fromCharArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -2963,1 +2947,0 @@\n-        \/\/ FIXME: optimize\n@@ -2965,1 +2948,1 @@\n-        return vsp.vOp(m, i -> (short) a[offset + i]);\n+        return vsp.dummyVector().fromCharArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -3152,2 +3135,2 @@\n-        if (offset >= 0 && offset <= (ms.byteSize() - species.vectorByteSize())) {\n-            return vsp.dummyVector().fromMemorySegment0(ms, offset, m).maybeSwap(bo);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.vectorByteSize(), ms.byteSize())) {\n+            return vsp.dummyVector().fromMemorySegment0(ms, offset, m, OFFSET_IN_RANGE).maybeSwap(bo);\n@@ -3156,1 +3139,0 @@\n-        \/\/ FIXME: optimize\n@@ -3158,1 +3140,1 @@\n-        return vsp.ldLongOp(ms, offset, m, HalffloatVector::memorySegmentGet);\n+        return vsp.dummyVector().fromMemorySegment0(ms, offset, m, OFFSET_OUT_OF_RANGE).maybeSwap(bo);\n@@ -3224,1 +3206,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -3371,1 +3355,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -3496,1 +3482,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 2, ms.byteSize());\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.vectorByteSize(), ms.byteSize())) {\n+                checkMaskFromIndexSize(offset, vsp, m, 2, ms.byteSize());\n+            }\n@@ -3537,1 +3525,1 @@\n-    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m);\n+    HalffloatVector fromArray0(short[] a, int offset, VectorMask<Halffloat> m, int offsetInRange);\n@@ -3541,1 +3529,1 @@\n-    HalffloatVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+    HalffloatVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m, int offsetInRange) {\n@@ -3546,1 +3534,1 @@\n-            a, arrayAddress(a, offset), m,\n+            a, arrayAddress(a, offset), m, offsetInRange,\n@@ -3570,1 +3558,1 @@\n-    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m);\n+    HalffloatVector fromCharArray0(char[] a, int offset, VectorMask<Halffloat> m, int offsetInRange);\n@@ -3574,1 +3562,1 @@\n-    HalffloatVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+    HalffloatVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m, int offsetInRange) {\n@@ -3579,1 +3567,1 @@\n-                a, charArrayAddress(a, offset), m,\n+                a, charArrayAddress(a, offset), m, offsetInRange,\n@@ -3594,1 +3582,1 @@\n-                (MemorySegmentProxy) ms, offset, vsp,\n+                (AbstractMemorySegmentImpl) ms, offset, vsp,\n@@ -3601,1 +3589,1 @@\n-    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m);\n+    HalffloatVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Halffloat> m, int offsetInRange);\n@@ -3605,1 +3593,1 @@\n-    HalffloatVector fromMemorySegment0Template(Class<M> maskClass, MemorySegment ms, long offset, M m) {\n+    HalffloatVector fromMemorySegment0Template(Class<M> maskClass, MemorySegment ms, long offset, M m, int offsetInRange) {\n@@ -3610,1 +3598,1 @@\n-                (MemorySegmentProxy) ms, offset, m, vsp,\n+                (AbstractMemorySegmentImpl) ms, offset, m, vsp, offsetInRange,\n@@ -3661,1 +3649,1 @@\n-                (MemorySegmentProxy) ms, offset,\n+                (AbstractMemorySegmentImpl) ms, offset,\n@@ -3678,1 +3666,1 @@\n-                (MemorySegmentProxy) ms, offset,\n+                (AbstractMemorySegmentImpl) ms, offset,\n@@ -3891,0 +3879,1 @@\n+                Class<? extends AbstractShuffle<Halffloat>> shuffleType,\n@@ -3893,1 +3882,1 @@\n-                  vectorType, maskType,\n+                  vectorType, maskType, shuffleType,\n@@ -4169,0 +4158,1 @@\n+                            Halffloat64Vector.Halffloat64Shuffle.class,\n@@ -4176,0 +4166,1 @@\n+                            Halffloat128Vector.Halffloat128Shuffle.class,\n@@ -4183,0 +4174,1 @@\n+                            Halffloat256Vector.Halffloat256Shuffle.class,\n@@ -4190,0 +4182,1 @@\n+                            Halffloat512Vector.Halffloat512Shuffle.class,\n@@ -4197,0 +4190,1 @@\n+                            HalffloatMaxVector.HalffloatMaxShuffle.class,\n@@ -4206,0 +4200,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/HalffloatVector.java","additions":58,"deletions":63,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Int128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Int128Shuffle)VectorSupport.shuffleIota(ETYPE, Int128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Int128Shuffle)VectorSupport.shuffleIota(ETYPE, Int128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Int128Shuffle shuffleFromBytes(byte[] reorder) { return new Int128Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Int128Shuffle shuffleFromArray(int[] indexes, int i) { return new Int128Shuffle(indexes, i); }\n+    Int128Shuffle shuffleFromArray(int[] indices, int i) { return new Int128Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Integer> toShuffle() {\n-        return super.toShuffleTemplate(Int128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -658,4 +645,5 @@\n-        public Int128Mask eq(VectorMask<Integer> mask) {\n-            Objects.requireNonNull(mask);\n-            Int128Mask m = (Int128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Int128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Int128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Int128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Int128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -675,1 +663,1 @@\n-            return (Int128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Int128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -703,0 +691,1 @@\n+        @Override\n@@ -704,2 +693,1 @@\n-        \/* package-private *\/\n-        Int128Mask xor(VectorMask<Integer> mask) {\n+        public Int128Mask xor(VectorMask<Integer> mask) {\n@@ -782,2 +770,4 @@\n-        Int128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int128Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -786,2 +776,2 @@\n-        public Int128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -790,2 +780,2 @@\n-        public Int128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Int128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -794,2 +784,2 @@\n-        public Int128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -799,0 +789,1 @@\n+        @ForceInline\n@@ -806,2 +797,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -813,3 +804,2 @@\n-        public Int128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Int128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Int128Vector)(((AbstractShuffle<Integer>)(s)).toVectorTemplate())));\n+        Int128Vector toBitsVector() {\n+            return (Int128Vector) super.toBitsVectorTemplate();\n@@ -820,6 +810,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -828,0 +814,1 @@\n+        @Override\n@@ -829,0 +816,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -830,8 +821,38 @@\n-        public Int128Shuffle rearrange(VectorShuffle<Integer> shuffle) {\n-            Int128Shuffle s = (Int128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -839,1 +860,1 @@\n-            return new Int128Shuffle(r);\n+            return true;\n@@ -857,2 +878,2 @@\n-    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n-        return super.fromArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, (Int128Mask) m, offsetInRange);  \/\/ specialize\n@@ -880,2 +901,2 @@\n-    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m) {\n-        return super.fromMemorySegment0Template(Int128Mask.class, ms, offset, (Int128Mask) m);  \/\/ specialize\n+    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Int128Mask.class, ms, offset, (Int128Mask) m, offsetInRange);  \/\/ specialize\n@@ -919,0 +940,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int128Vector.java","additions":81,"deletions":59,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Int256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Int256Shuffle)VectorSupport.shuffleIota(ETYPE, Int256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Int256Shuffle)VectorSupport.shuffleIota(ETYPE, Int256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Int256Shuffle shuffleFromBytes(byte[] reorder) { return new Int256Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Int256Shuffle shuffleFromArray(int[] indexes, int i) { return new Int256Shuffle(indexes, i); }\n+    Int256Shuffle shuffleFromArray(int[] indices, int i) { return new Int256Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Integer> toShuffle() {\n-        return super.toShuffleTemplate(Int256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -666,4 +653,5 @@\n-        public Int256Mask eq(VectorMask<Integer> mask) {\n-            Objects.requireNonNull(mask);\n-            Int256Mask m = (Int256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Int256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Int256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Int256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Int256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -683,1 +671,1 @@\n-            return (Int256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Int256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -711,0 +699,1 @@\n+        @Override\n@@ -712,2 +701,1 @@\n-        \/* package-private *\/\n-        Int256Mask xor(VectorMask<Integer> mask) {\n+        public Int256Mask xor(VectorMask<Integer> mask) {\n@@ -790,2 +778,4 @@\n-        Int256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int256Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -794,2 +784,2 @@\n-        public Int256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -798,2 +788,2 @@\n-        public Int256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Int256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -802,2 +792,2 @@\n-        public Int256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -807,0 +797,1 @@\n+        @ForceInline\n@@ -814,2 +805,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -821,3 +812,2 @@\n-        public Int256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Int256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Int256Vector)(((AbstractShuffle<Integer>)(s)).toVectorTemplate())));\n+        Int256Vector toBitsVector() {\n+            return (Int256Vector) super.toBitsVectorTemplate();\n@@ -828,6 +818,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -836,0 +822,1 @@\n+        @Override\n@@ -837,0 +824,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -838,8 +829,38 @@\n-        public Int256Shuffle rearrange(VectorShuffle<Integer> shuffle) {\n-            Int256Shuffle s = (Int256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -847,1 +868,1 @@\n-            return new Int256Shuffle(r);\n+            return true;\n@@ -865,2 +886,2 @@\n-    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n-        return super.fromArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, (Int256Mask) m, offsetInRange);  \/\/ specialize\n@@ -888,2 +909,2 @@\n-    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m) {\n-        return super.fromMemorySegment0Template(Int256Mask.class, ms, offset, (Int256Mask) m);  \/\/ specialize\n+    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Int256Mask.class, ms, offset, (Int256Mask) m, offsetInRange);  \/\/ specialize\n@@ -927,0 +948,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int256Vector.java","additions":81,"deletions":59,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Int512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Int512Shuffle)VectorSupport.shuffleIota(ETYPE, Int512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Int512Shuffle)VectorSupport.shuffleIota(ETYPE, Int512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Int512Shuffle shuffleFromBytes(byte[] reorder) { return new Int512Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Int512Shuffle shuffleFromArray(int[] indexes, int i) { return new Int512Shuffle(indexes, i); }\n+    Int512Shuffle shuffleFromArray(int[] indices, int i) { return new Int512Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Integer> toShuffle() {\n-        return super.toShuffleTemplate(Int512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -682,4 +669,5 @@\n-        public Int512Mask eq(VectorMask<Integer> mask) {\n-            Objects.requireNonNull(mask);\n-            Int512Mask m = (Int512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Int512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Int512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Int512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Int512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -699,1 +687,1 @@\n-            return (Int512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Int512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -727,0 +715,1 @@\n+        @Override\n@@ -728,2 +717,1 @@\n-        \/* package-private *\/\n-        Int512Mask xor(VectorMask<Integer> mask) {\n+        public Int512Mask xor(VectorMask<Integer> mask) {\n@@ -806,2 +794,4 @@\n-        Int512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int512Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -810,2 +800,2 @@\n-        public Int512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -814,2 +804,2 @@\n-        public Int512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Int512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -818,2 +808,2 @@\n-        public Int512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -823,0 +813,1 @@\n+        @ForceInline\n@@ -830,2 +821,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -837,3 +828,2 @@\n-        public Int512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Int512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Int512Vector)(((AbstractShuffle<Integer>)(s)).toVectorTemplate())));\n+        Int512Vector toBitsVector() {\n+            return (Int512Vector) super.toBitsVectorTemplate();\n@@ -844,6 +834,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -852,0 +838,1 @@\n+        @Override\n@@ -853,0 +840,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -854,8 +845,38 @@\n-        public Int512Shuffle rearrange(VectorShuffle<Integer> shuffle) {\n-            Int512Shuffle s = (Int512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -863,1 +884,1 @@\n-            return new Int512Shuffle(r);\n+            return true;\n@@ -881,2 +902,2 @@\n-    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n-        return super.fromArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, (Int512Mask) m, offsetInRange);  \/\/ specialize\n@@ -904,2 +925,2 @@\n-    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m) {\n-        return super.fromMemorySegment0Template(Int512Mask.class, ms, offset, (Int512Mask) m);  \/\/ specialize\n+    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Int512Mask.class, ms, offset, (Int512Mask) m, offsetInRange);  \/\/ specialize\n@@ -943,0 +964,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int512Vector.java","additions":81,"deletions":59,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,11 +144,0 @@\n-    @ForceInline\n-    Int64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Int64Shuffle)VectorSupport.shuffleIota(ETYPE, Int64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Int64Shuffle)VectorSupport.shuffleIota(ETYPE, Int64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -157,5 +146,1 @@\n-    Int64Shuffle shuffleFromBytes(byte[] reorder) { return new Int64Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Int64Shuffle shuffleFromArray(int[] indexes, int i) { return new Int64Shuffle(indexes, i); }\n+    Int64Shuffle shuffleFromArray(int[] indices, int i) { return new Int64Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Integer> toShuffle() {\n-        return super.toShuffleTemplate(Int64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -654,4 +641,5 @@\n-        public Int64Mask eq(VectorMask<Integer> mask) {\n-            Objects.requireNonNull(mask);\n-            Int64Mask m = (Int64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Int64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Int64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Int64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Int64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -671,1 +659,1 @@\n-            return (Int64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Int64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -699,0 +687,1 @@\n+        @Override\n@@ -700,2 +689,1 @@\n-        \/* package-private *\/\n-        Int64Mask xor(VectorMask<Integer> mask) {\n+        public Int64Mask xor(VectorMask<Integer> mask) {\n@@ -778,2 +766,4 @@\n-        Int64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int64Shuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -782,2 +772,2 @@\n-        public Int64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Int64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -786,2 +776,2 @@\n-        public Int64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Int64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -790,2 +780,2 @@\n-        public Int64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -795,0 +785,1 @@\n+        @ForceInline\n@@ -802,2 +793,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -809,3 +800,2 @@\n-        public Int64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Int64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Int64Vector)(((AbstractShuffle<Integer>)(s)).toVectorTemplate())));\n+        Int64Vector toBitsVector() {\n+            return (Int64Vector) super.toBitsVectorTemplate();\n@@ -816,6 +806,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return Int64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -824,0 +810,1 @@\n+        @Override\n@@ -825,0 +812,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -826,8 +817,38 @@\n-        public Int64Shuffle rearrange(VectorShuffle<Integer> shuffle) {\n-            Int64Shuffle s = (Int64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -835,1 +856,1 @@\n-            return new Int64Shuffle(r);\n+            return true;\n@@ -853,2 +874,2 @@\n-    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n-        return super.fromArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, (Int64Mask) m, offsetInRange);  \/\/ specialize\n@@ -876,2 +897,2 @@\n-    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m) {\n-        return super.fromMemorySegment0Template(Int64Mask.class, ms, offset, (Int64Mask) m);  \/\/ specialize\n+    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Int64Mask.class, ms, offset, (Int64Mask) m, offsetInRange);  \/\/ specialize\n@@ -915,0 +936,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int64Vector.java","additions":81,"deletions":59,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    IntMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (IntMaxShuffle)VectorSupport.shuffleIota(ETYPE, IntMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (IntMaxShuffle)VectorSupport.shuffleIota(ETYPE, IntMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    IntMaxShuffle shuffleFromBytes(byte[] reorder) { return new IntMaxShuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    IntMaxShuffle shuffleFromArray(int[] indexes, int i) { return new IntMaxShuffle(indexes, i); }\n+    IntMaxShuffle shuffleFromArray(int[] indices, int i) { return new IntMaxShuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Integer> toShuffle() {\n-        return super.toShuffleTemplate(IntMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -652,4 +639,5 @@\n-        public IntMaxMask eq(VectorMask<Integer> mask) {\n-            Objects.requireNonNull(mask);\n-            IntMaxMask m = (IntMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        IntMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (IntMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                IntMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (IntMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -669,1 +657,1 @@\n-            return (IntMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (IntMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -697,0 +685,1 @@\n+        @Override\n@@ -698,2 +687,1 @@\n-        \/* package-private *\/\n-        IntMaxMask xor(VectorMask<Integer> mask) {\n+        public IntMaxMask xor(VectorMask<Integer> mask) {\n@@ -787,2 +775,4 @@\n-        IntMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        IntMaxShuffle(int[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -791,2 +781,2 @@\n-        public IntMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        IntMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -795,2 +785,2 @@\n-        public IntMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        IntMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -799,2 +789,2 @@\n-        public IntMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        int[] indices() {\n+            return (int[])getPayload();\n@@ -804,0 +794,1 @@\n+        @ForceInline\n@@ -811,2 +802,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Integer.MAX_VALUE);\n+            assert(Integer.MIN_VALUE <= -VLENGTH);\n@@ -818,3 +809,2 @@\n-        public IntMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, IntMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((IntMaxVector)(((AbstractShuffle<Integer>)(s)).toVectorTemplate())));\n+        IntMaxVector toBitsVector() {\n+            return (IntMaxVector) super.toBitsVectorTemplate();\n@@ -825,6 +815,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        IntVector toBitsVector0() {\n+            return IntMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -833,0 +819,1 @@\n+        @Override\n@@ -834,0 +821,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -835,8 +826,21 @@\n-        public IntMaxShuffle rearrange(VectorShuffle<Integer> shuffle) {\n-            IntMaxShuffle s = (IntMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            toBitsVector().intoArray(a, offset);\n+        }\n+\n+        private static int[] prepare(int[] indices, int offset) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n+            }\n+            return a;\n+        }\n+\n+        private static int[] prepare(IntUnaryOperator f) {\n+            int[] a = new int[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (int)si;\n@@ -844,1 +848,18 @@\n-            return new IntMaxShuffle(r);\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(int[] indices) {\n+            int length = indices.length;\n+            for (int si : indices) {\n+                if (si >= (int)length || si < (int)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -862,2 +883,2 @@\n-    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n-        return super.fromArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -885,2 +906,2 @@\n-    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m) {\n-        return super.fromMemorySegment0Template(IntMaxMask.class, ms, offset, (IntMaxMask) m);  \/\/ specialize\n+    IntVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Integer> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(IntMaxMask.class, ms, offset, (IntMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -924,0 +945,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntMaxVector.java","additions":81,"deletions":59,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,2 +69,7 @@\n-        this.typeChar = printName.toUpperCase().charAt(0);\n-        assert(\"FDBSILH\".indexOf(typeChar) == ordinal()) : this;\n+        this.typeChar = genericElementType.getSimpleName().charAt(0);\n+        if (basicType == T_HALFFLOAT) {\n+          assert(\"FDBSILS\".indexOf(typeChar, \"FDBSILS\".indexOf(typeChar) + 1) == ordinal()) : this;\n+        }\n+        else {\n+          assert(\"FDBSILS\".indexOf(typeChar) == ordinal()) : this;\n+        }\n@@ -76,1 +81,1 @@\n-        assert(\"....zHFDBSILoav..\".charAt(basicType) == typeChar);\n+        assert(\"....zSFDBSILSoav..\".charAt(basicType) == typeChar);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LaneType.java","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -139,15 +139,0 @@\n-    @ForceInline\n-    Long128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Long128Shuffle)VectorSupport.shuffleIota(ETYPE, Long128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Long128Shuffle)VectorSupport.shuffleIota(ETYPE, Long128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Long128Shuffle shuffleFromBytes(byte[] reorder) { return new Long128Shuffle(reorder); }\n-\n@@ -156,1 +141,1 @@\n-    Long128Shuffle shuffleFromArray(int[] indexes, int i) { return new Long128Shuffle(indexes, i); }\n+    Long128Shuffle shuffleFromArray(int[] indices, int i) { return new Long128Shuffle(indices, i); }\n@@ -355,0 +340,1 @@\n+    @Override\n@@ -356,2 +342,3 @@\n-    public VectorShuffle<Long> toShuffle() {\n-        return super.toShuffleTemplate(Long128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -644,4 +631,5 @@\n-        public Long128Mask eq(VectorMask<Long> mask) {\n-            Objects.requireNonNull(mask);\n-            Long128Mask m = (Long128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Long128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Long128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Long128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Long128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -661,1 +649,1 @@\n-            return (Long128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Long128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -689,0 +677,1 @@\n+        @Override\n@@ -690,2 +679,1 @@\n-        \/* package-private *\/\n-        Long128Mask xor(VectorMask<Long> mask) {\n+        public Long128Mask xor(VectorMask<Long> mask) {\n@@ -768,2 +756,4 @@\n-        Long128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long128Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -772,2 +762,2 @@\n-        public Long128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -776,2 +766,2 @@\n-        public Long128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Long128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -780,2 +770,2 @@\n-        public Long128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -785,0 +775,1 @@\n+        @ForceInline\n@@ -792,2 +783,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -799,3 +790,2 @@\n-        public Long128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Long128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Long128Vector)(((AbstractShuffle<Long>)(s)).toVectorTemplate())));\n+        Long128Vector toBitsVector() {\n+            return (Long128Vector) super.toBitsVectorTemplate();\n@@ -806,6 +796,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -814,0 +800,1 @@\n+        @Override\n@@ -815,0 +802,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -816,8 +807,17 @@\n-        public Long128Shuffle rearrange(VectorShuffle<Long> shuffle) {\n-            Long128Shuffle s = (Long128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n@@ -825,1 +825,28 @@\n-            return new Long128Shuffle(r);\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -843,2 +870,2 @@\n-    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n-        return super.fromArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, (Long128Mask) m, offsetInRange);  \/\/ specialize\n@@ -866,2 +893,2 @@\n-    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m) {\n-        return super.fromMemorySegment0Template(Long128Mask.class, ms, offset, (Long128Mask) m);  \/\/ specialize\n+    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Long128Mask.class, ms, offset, (Long128Mask) m, offsetInRange);  \/\/ specialize\n@@ -905,0 +932,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long128Vector.java","additions":87,"deletions":59,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -139,15 +139,0 @@\n-    @ForceInline\n-    Long256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Long256Shuffle)VectorSupport.shuffleIota(ETYPE, Long256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Long256Shuffle)VectorSupport.shuffleIota(ETYPE, Long256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Long256Shuffle shuffleFromBytes(byte[] reorder) { return new Long256Shuffle(reorder); }\n-\n@@ -156,1 +141,1 @@\n-    Long256Shuffle shuffleFromArray(int[] indexes, int i) { return new Long256Shuffle(indexes, i); }\n+    Long256Shuffle shuffleFromArray(int[] indices, int i) { return new Long256Shuffle(indices, i); }\n@@ -355,0 +340,1 @@\n+    @Override\n@@ -356,2 +342,3 @@\n-    public VectorShuffle<Long> toShuffle() {\n-        return super.toShuffleTemplate(Long256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -648,4 +635,5 @@\n-        public Long256Mask eq(VectorMask<Long> mask) {\n-            Objects.requireNonNull(mask);\n-            Long256Mask m = (Long256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Long256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Long256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Long256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Long256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -665,1 +653,1 @@\n-            return (Long256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Long256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -693,0 +681,1 @@\n+        @Override\n@@ -694,2 +683,1 @@\n-        \/* package-private *\/\n-        Long256Mask xor(VectorMask<Long> mask) {\n+        public Long256Mask xor(VectorMask<Long> mask) {\n@@ -772,2 +760,4 @@\n-        Long256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long256Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -776,2 +766,2 @@\n-        public Long256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -780,2 +770,2 @@\n-        public Long256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Long256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -784,2 +774,2 @@\n-        public Long256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -789,0 +779,1 @@\n+        @ForceInline\n@@ -796,2 +787,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -803,3 +794,2 @@\n-        public Long256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Long256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Long256Vector)(((AbstractShuffle<Long>)(s)).toVectorTemplate())));\n+        Long256Vector toBitsVector() {\n+            return (Long256Vector) super.toBitsVectorTemplate();\n@@ -810,6 +800,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -818,0 +804,1 @@\n+        @Override\n@@ -819,0 +806,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -820,8 +811,17 @@\n-        public Long256Shuffle rearrange(VectorShuffle<Long> shuffle) {\n-            Long256Shuffle s = (Long256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n@@ -829,1 +829,28 @@\n-            return new Long256Shuffle(r);\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -847,2 +874,2 @@\n-    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n-        return super.fromArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, (Long256Mask) m, offsetInRange);  \/\/ specialize\n@@ -870,2 +897,2 @@\n-    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m) {\n-        return super.fromMemorySegment0Template(Long256Mask.class, ms, offset, (Long256Mask) m);  \/\/ specialize\n+    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Long256Mask.class, ms, offset, (Long256Mask) m, offsetInRange);  \/\/ specialize\n@@ -909,0 +936,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long256Vector.java","additions":87,"deletions":59,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -139,15 +139,0 @@\n-    @ForceInline\n-    Long512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Long512Shuffle)VectorSupport.shuffleIota(ETYPE, Long512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Long512Shuffle)VectorSupport.shuffleIota(ETYPE, Long512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Long512Shuffle shuffleFromBytes(byte[] reorder) { return new Long512Shuffle(reorder); }\n-\n@@ -156,1 +141,1 @@\n-    Long512Shuffle shuffleFromArray(int[] indexes, int i) { return new Long512Shuffle(indexes, i); }\n+    Long512Shuffle shuffleFromArray(int[] indices, int i) { return new Long512Shuffle(indices, i); }\n@@ -355,0 +340,1 @@\n+    @Override\n@@ -356,2 +342,3 @@\n-    public VectorShuffle<Long> toShuffle() {\n-        return super.toShuffleTemplate(Long512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -656,4 +643,5 @@\n-        public Long512Mask eq(VectorMask<Long> mask) {\n-            Objects.requireNonNull(mask);\n-            Long512Mask m = (Long512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Long512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Long512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Long512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Long512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -673,1 +661,1 @@\n-            return (Long512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Long512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -701,0 +689,1 @@\n+        @Override\n@@ -702,2 +691,1 @@\n-        \/* package-private *\/\n-        Long512Mask xor(VectorMask<Long> mask) {\n+        public Long512Mask xor(VectorMask<Long> mask) {\n@@ -780,2 +768,4 @@\n-        Long512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long512Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -784,2 +774,2 @@\n-        public Long512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -788,2 +778,2 @@\n-        public Long512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Long512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -792,2 +782,2 @@\n-        public Long512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -797,0 +787,1 @@\n+        @ForceInline\n@@ -804,2 +795,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -811,3 +802,2 @@\n-        public Long512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Long512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Long512Vector)(((AbstractShuffle<Long>)(s)).toVectorTemplate())));\n+        Long512Vector toBitsVector() {\n+            return (Long512Vector) super.toBitsVectorTemplate();\n@@ -818,6 +808,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -826,0 +812,1 @@\n+        @Override\n@@ -827,0 +814,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -828,8 +819,17 @@\n-        public Long512Shuffle rearrange(VectorShuffle<Long> shuffle) {\n-            Long512Shuffle s = (Long512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n@@ -837,1 +837,28 @@\n-            return new Long512Shuffle(r);\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -855,2 +882,2 @@\n-    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n-        return super.fromArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, (Long512Mask) m, offsetInRange);  \/\/ specialize\n@@ -878,2 +905,2 @@\n-    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m) {\n-        return super.fromMemorySegment0Template(Long512Mask.class, ms, offset, (Long512Mask) m);  \/\/ specialize\n+    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Long512Mask.class, ms, offset, (Long512Mask) m, offsetInRange);  \/\/ specialize\n@@ -917,0 +944,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long512Vector.java","additions":87,"deletions":59,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -139,11 +139,0 @@\n-    @ForceInline\n-    Long64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Long64Shuffle)VectorSupport.shuffleIota(ETYPE, Long64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Long64Shuffle)VectorSupport.shuffleIota(ETYPE, Long64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -152,5 +141,1 @@\n-    Long64Shuffle shuffleFromBytes(byte[] reorder) { return new Long64Shuffle(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    Long64Shuffle shuffleFromArray(int[] indexes, int i) { return new Long64Shuffle(indexes, i); }\n+    Long64Shuffle shuffleFromArray(int[] indices, int i) { return new Long64Shuffle(indices, i); }\n@@ -355,0 +340,1 @@\n+    @Override\n@@ -356,2 +342,3 @@\n-    public VectorShuffle<Long> toShuffle() {\n-        return super.toShuffleTemplate(Long64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -642,4 +629,5 @@\n-        public Long64Mask eq(VectorMask<Long> mask) {\n-            Objects.requireNonNull(mask);\n-            Long64Mask m = (Long64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Long64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Long64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Long64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Long64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -659,1 +647,1 @@\n-            return (Long64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Long64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -687,0 +675,1 @@\n+        @Override\n@@ -688,2 +677,1 @@\n-        \/* package-private *\/\n-        Long64Mask xor(VectorMask<Long> mask) {\n+        public Long64Mask xor(VectorMask<Long> mask) {\n@@ -766,2 +754,4 @@\n-        Long64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long64Shuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -770,2 +760,2 @@\n-        public Long64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Long64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -774,2 +764,2 @@\n-        public Long64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Long64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -778,2 +768,2 @@\n-        public Long64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -783,0 +773,1 @@\n+        @ForceInline\n@@ -790,2 +781,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -797,3 +788,2 @@\n-        public Long64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Long64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Long64Vector)(((AbstractShuffle<Long>)(s)).toVectorTemplate())));\n+        Long64Vector toBitsVector() {\n+            return (Long64Vector) super.toBitsVectorTemplate();\n@@ -804,6 +794,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return Long64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -812,0 +798,1 @@\n+        @Override\n@@ -813,0 +800,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -814,8 +805,38 @@\n-        public Long64Shuffle rearrange(VectorShuffle<Long> shuffle) {\n-            Long64Shuffle s = (Long64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            a[offset] = laneSource(0);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n@@ -823,1 +844,1 @@\n-            return new Long64Shuffle(r);\n+            return true;\n@@ -841,2 +862,2 @@\n-    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n-        return super.fromArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, (Long64Mask) m, offsetInRange);  \/\/ specialize\n@@ -864,2 +885,2 @@\n-    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m) {\n-        return super.fromMemorySegment0Template(Long64Mask.class, ms, offset, (Long64Mask) m);  \/\/ specialize\n+    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Long64Mask.class, ms, offset, (Long64Mask) m, offsetInRange);  \/\/ specialize\n@@ -903,0 +924,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long64Vector.java","additions":81,"deletions":59,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -139,15 +139,0 @@\n-    @ForceInline\n-    LongMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (LongMaxShuffle)VectorSupport.shuffleIota(ETYPE, LongMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (LongMaxShuffle)VectorSupport.shuffleIota(ETYPE, LongMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    LongMaxShuffle shuffleFromBytes(byte[] reorder) { return new LongMaxShuffle(reorder); }\n-\n@@ -156,1 +141,1 @@\n-    LongMaxShuffle shuffleFromArray(int[] indexes, int i) { return new LongMaxShuffle(indexes, i); }\n+    LongMaxShuffle shuffleFromArray(int[] indices, int i) { return new LongMaxShuffle(indices, i); }\n@@ -355,0 +340,1 @@\n+    @Override\n@@ -356,2 +342,3 @@\n-    public VectorShuffle<Long> toShuffle() {\n-        return super.toShuffleTemplate(LongMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -642,4 +629,5 @@\n-        public LongMaxMask eq(VectorMask<Long> mask) {\n-            Objects.requireNonNull(mask);\n-            LongMaxMask m = (LongMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        LongMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (LongMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                LongMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (LongMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -659,1 +647,1 @@\n-            return (LongMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (LongMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -687,0 +675,1 @@\n+        @Override\n@@ -688,2 +677,1 @@\n-        \/* package-private *\/\n-        LongMaxMask xor(VectorMask<Long> mask) {\n+        public LongMaxMask xor(VectorMask<Long> mask) {\n@@ -766,2 +754,4 @@\n-        LongMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        LongMaxShuffle(long[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -770,2 +760,2 @@\n-        public LongMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        LongMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -774,2 +764,2 @@\n-        public LongMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        LongMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -778,2 +768,2 @@\n-        public LongMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        long[] indices() {\n+            return (long[])getPayload();\n@@ -783,0 +773,1 @@\n+        @ForceInline\n@@ -790,2 +781,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Long.MAX_VALUE);\n+            assert(Long.MIN_VALUE <= -VLENGTH);\n@@ -797,3 +788,2 @@\n-        public LongMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, LongMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((LongMaxVector)(((AbstractShuffle<Long>)(s)).toVectorTemplate())));\n+        LongMaxVector toBitsVector() {\n+            return (LongMaxVector) super.toBitsVectorTemplate();\n@@ -804,6 +794,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        LongVector toBitsVector0() {\n+            return LongMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -812,0 +798,1 @@\n+        @Override\n@@ -813,0 +800,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -814,8 +805,17 @@\n-        public LongMaxShuffle rearrange(VectorShuffle<Long> shuffle) {\n-            LongMaxShuffle s = (LongMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+        }\n+\n+        private static long[] prepare(int[] indices, int offset) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n@@ -823,1 +823,28 @@\n-            return new LongMaxShuffle(r);\n+            return a;\n+        }\n+\n+        private static long[] prepare(IntUnaryOperator f) {\n+            long[] a = new long[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (long)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(long[] indices) {\n+            int length = indices.length;\n+            for (long si : indices) {\n+                if (si >= (long)length || si < (long)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -841,2 +868,2 @@\n-    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n-        return super.fromArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -864,2 +891,2 @@\n-    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m) {\n-        return super.fromMemorySegment0Template(LongMaxMask.class, ms, offset, (LongMaxMask) m);  \/\/ specialize\n+    LongVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Long> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(LongMaxMask.class, ms, offset, (LongMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -903,0 +930,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongMaxVector.java","additions":87,"deletions":59,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Short128Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Short128Shuffle)VectorSupport.shuffleIota(ETYPE, Short128Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Short128Shuffle)VectorSupport.shuffleIota(ETYPE, Short128Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Short128Shuffle shuffleFromBytes(byte[] reorder) { return new Short128Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Short128Shuffle shuffleFromArray(int[] indexes, int i) { return new Short128Shuffle(indexes, i); }\n+    Short128Shuffle shuffleFromArray(int[] indices, int i) { return new Short128Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Short> toShuffle() {\n-        return super.toShuffleTemplate(Short128Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -666,4 +653,5 @@\n-        public Short128Mask eq(VectorMask<Short> mask) {\n-            Objects.requireNonNull(mask);\n-            Short128Mask m = (Short128Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Short128Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Short128Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Short128Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Short128Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -683,1 +671,1 @@\n-            return (Short128Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Short128Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -711,0 +699,1 @@\n+        @Override\n@@ -712,2 +701,1 @@\n-        \/* package-private *\/\n-        Short128Mask xor(VectorMask<Short> mask) {\n+        public Short128Mask xor(VectorMask<Short> mask) {\n@@ -790,2 +778,4 @@\n-        Short128Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short128Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -794,2 +784,2 @@\n-        public Short128Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short128Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -798,2 +788,2 @@\n-        public Short128Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Short128Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -802,2 +792,2 @@\n-        public Short128Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -807,0 +797,1 @@\n+        @ForceInline\n@@ -814,2 +805,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -821,3 +812,2 @@\n-        public Short128Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Short128Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Short128Vector)(((AbstractShuffle<Short>)(s)).toVectorTemplate())));\n+        Short128Vector toBitsVector() {\n+            return (Short128Vector) super.toBitsVectorTemplate();\n@@ -828,6 +818,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short128Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -836,0 +822,1 @@\n+        @Override\n@@ -837,0 +824,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -838,8 +829,18 @@\n-        public Short128Shuffle rearrange(VectorShuffle<Short> shuffle) {\n-            Short128Shuffle s = (Short128Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_128;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n@@ -847,1 +848,28 @@\n-            return new Short128Shuffle(r);\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -865,2 +893,2 @@\n-    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n-        return super.fromArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, (Short128Mask) m, offsetInRange);  \/\/ specialize\n@@ -880,2 +908,2 @@\n-    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n-        return super.fromCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m, offsetInRange);  \/\/ specialize\n@@ -895,2 +923,2 @@\n-    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m) {\n-        return super.fromMemorySegment0Template(Short128Mask.class, ms, offset, (Short128Mask) m);  \/\/ specialize\n+    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Short128Mask.class, ms, offset, (Short128Mask) m, offsetInRange);  \/\/ specialize\n@@ -934,0 +962,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":90,"deletions":61,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Short256Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Short256Shuffle)VectorSupport.shuffleIota(ETYPE, Short256Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Short256Shuffle)VectorSupport.shuffleIota(ETYPE, Short256Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Short256Shuffle shuffleFromBytes(byte[] reorder) { return new Short256Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Short256Shuffle shuffleFromArray(int[] indexes, int i) { return new Short256Shuffle(indexes, i); }\n+    Short256Shuffle shuffleFromArray(int[] indices, int i) { return new Short256Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Short> toShuffle() {\n-        return super.toShuffleTemplate(Short256Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -682,4 +669,5 @@\n-        public Short256Mask eq(VectorMask<Short> mask) {\n-            Objects.requireNonNull(mask);\n-            Short256Mask m = (Short256Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Short256Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Short256Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Short256Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Short256Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -699,1 +687,1 @@\n-            return (Short256Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Short256Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -727,0 +715,1 @@\n+        @Override\n@@ -728,2 +717,1 @@\n-        \/* package-private *\/\n-        Short256Mask xor(VectorMask<Short> mask) {\n+        public Short256Mask xor(VectorMask<Short> mask) {\n@@ -806,2 +794,4 @@\n-        Short256Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short256Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -810,2 +800,2 @@\n-        public Short256Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short256Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -814,2 +804,2 @@\n-        public Short256Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Short256Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -818,2 +808,2 @@\n-        public Short256Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -823,0 +813,1 @@\n+        @ForceInline\n@@ -830,2 +821,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -837,3 +828,2 @@\n-        public Short256Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Short256Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Short256Vector)(((AbstractShuffle<Short>)(s)).toVectorTemplate())));\n+        Short256Vector toBitsVector() {\n+            return (Short256Vector) super.toBitsVectorTemplate();\n@@ -844,6 +834,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short256Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -852,0 +838,1 @@\n+        @Override\n@@ -853,0 +840,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -854,8 +845,18 @@\n-        public Short256Shuffle rearrange(VectorShuffle<Short> shuffle) {\n-            Short256Shuffle s = (Short256Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_256;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n@@ -863,1 +864,28 @@\n-            return new Short256Shuffle(r);\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -881,2 +909,2 @@\n-    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n-        return super.fromArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, (Short256Mask) m, offsetInRange);  \/\/ specialize\n@@ -896,2 +924,2 @@\n-    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n-        return super.fromCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m, offsetInRange);  \/\/ specialize\n@@ -911,2 +939,2 @@\n-    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m) {\n-        return super.fromMemorySegment0Template(Short256Mask.class, ms, offset, (Short256Mask) m);  \/\/ specialize\n+    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Short256Mask.class, ms, offset, (Short256Mask) m, offsetInRange);  \/\/ specialize\n@@ -950,0 +978,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":90,"deletions":61,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Short512Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Short512Shuffle)VectorSupport.shuffleIota(ETYPE, Short512Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Short512Shuffle)VectorSupport.shuffleIota(ETYPE, Short512Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Short512Shuffle shuffleFromBytes(byte[] reorder) { return new Short512Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Short512Shuffle shuffleFromArray(int[] indexes, int i) { return new Short512Shuffle(indexes, i); }\n+    Short512Shuffle shuffleFromArray(int[] indices, int i) { return new Short512Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Short> toShuffle() {\n-        return super.toShuffleTemplate(Short512Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -714,4 +701,5 @@\n-        public Short512Mask eq(VectorMask<Short> mask) {\n-            Objects.requireNonNull(mask);\n-            Short512Mask m = (Short512Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Short512Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Short512Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Short512Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Short512Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -731,1 +719,1 @@\n-            return (Short512Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Short512Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -759,0 +747,1 @@\n+        @Override\n@@ -760,2 +749,1 @@\n-        \/* package-private *\/\n-        Short512Mask xor(VectorMask<Short> mask) {\n+        public Short512Mask xor(VectorMask<Short> mask) {\n@@ -838,2 +826,4 @@\n-        Short512Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short512Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -842,2 +832,2 @@\n-        public Short512Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short512Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -846,2 +836,2 @@\n-        public Short512Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Short512Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -850,2 +840,2 @@\n-        public Short512Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -855,0 +845,1 @@\n+        @ForceInline\n@@ -862,2 +853,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -869,3 +860,2 @@\n-        public Short512Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Short512Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Short512Vector)(((AbstractShuffle<Short>)(s)).toVectorTemplate())));\n+        Short512Vector toBitsVector() {\n+            return (Short512Vector) super.toBitsVectorTemplate();\n@@ -876,6 +866,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short512Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -884,0 +870,1 @@\n+        @Override\n@@ -885,0 +872,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -886,8 +877,18 @@\n-        public Short512Shuffle rearrange(VectorShuffle<Short> shuffle) {\n-            Short512Shuffle s = (Short512Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_512;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n@@ -895,1 +896,28 @@\n-            return new Short512Shuffle(r);\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -913,2 +941,2 @@\n-    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n-        return super.fromArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, (Short512Mask) m, offsetInRange);  \/\/ specialize\n@@ -928,2 +956,2 @@\n-    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n-        return super.fromCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m, offsetInRange);  \/\/ specialize\n@@ -943,2 +971,2 @@\n-    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m) {\n-        return super.fromMemorySegment0Template(Short512Mask.class, ms, offset, (Short512Mask) m);  \/\/ specialize\n+    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Short512Mask.class, ms, offset, (Short512Mask) m, offsetInRange);  \/\/ specialize\n@@ -982,0 +1010,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":90,"deletions":61,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    Short64Shuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (Short64Shuffle)VectorSupport.shuffleIota(ETYPE, Short64Shuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (Short64Shuffle)VectorSupport.shuffleIota(ETYPE, Short64Shuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    Short64Shuffle shuffleFromBytes(byte[] reorder) { return new Short64Shuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    Short64Shuffle shuffleFromArray(int[] indexes, int i) { return new Short64Shuffle(indexes, i); }\n+    Short64Shuffle shuffleFromArray(int[] indices, int i) { return new Short64Shuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Short> toShuffle() {\n-        return super.toShuffleTemplate(Short64Shuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -658,4 +645,5 @@\n-        public Short64Mask eq(VectorMask<Short> mask) {\n-            Objects.requireNonNull(mask);\n-            Short64Mask m = (Short64Mask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        Short64Mask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (Short64Mask) VectorSupport.indexPartiallyInUpperRange(\n+                Short64Mask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (Short64Mask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -675,1 +663,1 @@\n-            return (Short64Mask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (Short64Mask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -703,0 +691,1 @@\n+        @Override\n@@ -704,2 +693,1 @@\n-        \/* package-private *\/\n-        Short64Mask xor(VectorMask<Short> mask) {\n+        public Short64Mask xor(VectorMask<Short> mask) {\n@@ -782,2 +770,4 @@\n-        Short64Shuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short64Shuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -786,2 +776,2 @@\n-        public Short64Shuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        Short64Shuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -790,2 +780,2 @@\n-        public Short64Shuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        Short64Shuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -794,2 +784,2 @@\n-        public Short64Shuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -799,0 +789,1 @@\n+        @ForceInline\n@@ -806,2 +797,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -813,3 +804,2 @@\n-        public Short64Vector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, Short64Shuffle.class, this, VLENGTH,\n-                                                    (s) -> ((Short64Vector)(((AbstractShuffle<Short>)(s)).toVectorTemplate())));\n+        Short64Vector toBitsVector() {\n+            return (Short64Vector) super.toBitsVectorTemplate();\n@@ -820,6 +810,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return Short64Vector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -828,0 +814,1 @@\n+        @Override\n@@ -829,0 +816,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -830,8 +821,18 @@\n-        public Short64Shuffle rearrange(VectorShuffle<Short> shuffle) {\n-            Short64Shuffle s = (Short64Shuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_64;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n@@ -839,1 +840,28 @@\n-            return new Short64Shuffle(r);\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -857,2 +885,2 @@\n-    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n-        return super.fromArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, (Short64Mask) m, offsetInRange);  \/\/ specialize\n@@ -872,2 +900,2 @@\n-    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n-        return super.fromCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m, offsetInRange);  \/\/ specialize\n@@ -887,2 +915,2 @@\n-    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m) {\n-        return super.fromMemorySegment0Template(Short64Mask.class, ms, offset, (Short64Mask) m);  \/\/ specialize\n+    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(Short64Mask.class, ms, offset, (Short64Mask) m, offsetInRange);  \/\/ specialize\n@@ -926,0 +954,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":90,"deletions":61,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -144,15 +144,0 @@\n-    @ForceInline\n-    ShortMaxShuffle iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return (ShortMaxShuffle)VectorSupport.shuffleIota(ETYPE, ShortMaxShuffle.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return (ShortMaxShuffle)VectorSupport.shuffleIota(ETYPE, ShortMaxShuffle.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n-    @Override\n-    @ForceInline\n-    ShortMaxShuffle shuffleFromBytes(byte[] reorder) { return new ShortMaxShuffle(reorder); }\n-\n@@ -161,1 +146,1 @@\n-    ShortMaxShuffle shuffleFromArray(int[] indexes, int i) { return new ShortMaxShuffle(indexes, i); }\n+    ShortMaxShuffle shuffleFromArray(int[] indices, int i) { return new ShortMaxShuffle(indices, i); }\n@@ -360,0 +345,1 @@\n+    @Override\n@@ -361,2 +347,3 @@\n-    public VectorShuffle<Short> toShuffle() {\n-        return super.toShuffleTemplate(ShortMaxShuffle.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -652,4 +639,5 @@\n-        public ShortMaxMask eq(VectorMask<Short> mask) {\n-            Objects.requireNonNull(mask);\n-            ShortMaxMask m = (ShortMaxMask)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        ShortMaxMask indexPartiallyInUpperRange(long offset, long limit) {\n+            return (ShortMaxMask) VectorSupport.indexPartiallyInUpperRange(\n+                ShortMaxMask.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> (ShortMaxMask) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -669,1 +657,1 @@\n-            return (ShortMaxMask)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return (ShortMaxMask)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -697,0 +685,1 @@\n+        @Override\n@@ -698,2 +687,1 @@\n-        \/* package-private *\/\n-        ShortMaxMask xor(VectorMask<Short> mask) {\n+        public ShortMaxMask xor(VectorMask<Short> mask) {\n@@ -776,2 +764,4 @@\n-        ShortMaxShuffle(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        ShortMaxShuffle(short[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -780,2 +770,2 @@\n-        public ShortMaxShuffle(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        ShortMaxShuffle(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -784,2 +774,2 @@\n-        public ShortMaxShuffle(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        ShortMaxShuffle(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -788,2 +778,2 @@\n-        public ShortMaxShuffle(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        short[] indices() {\n+            return (short[])getPayload();\n@@ -793,0 +783,1 @@\n+        @ForceInline\n@@ -800,2 +791,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < Short.MAX_VALUE);\n+            assert(Short.MIN_VALUE <= -VLENGTH);\n@@ -807,3 +798,2 @@\n-        public ShortMaxVector toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, ShortMaxShuffle.class, this, VLENGTH,\n-                                                    (s) -> ((ShortMaxVector)(((AbstractShuffle<Short>)(s)).toVectorTemplate())));\n+        ShortMaxVector toBitsVector() {\n+            return (ShortMaxVector) super.toBitsVectorTemplate();\n@@ -814,6 +804,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        ShortVector toBitsVector0() {\n+            return ShortMaxVector.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -822,0 +808,1 @@\n+        @Override\n@@ -823,0 +810,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -824,8 +815,18 @@\n-        public ShortMaxShuffle rearrange(VectorShuffle<Short> shuffle) {\n-            ShortMaxShuffle s = (ShortMaxShuffle) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+            VectorSpecies<Integer> species = IntVector.SPECIES_MAX;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+        }\n+\n+        private static short[] prepare(int[] indices, int offset) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n@@ -833,1 +834,28 @@\n-            return new ShortMaxShuffle(r);\n+            return a;\n+        }\n+\n+        private static short[] prepare(IntUnaryOperator f) {\n+            short[] a = new short[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = (short)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange(short[] indices) {\n+            int length = indices.length;\n+            for (short si : indices) {\n+                if (si >= (short)length || si < (short)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -851,2 +879,2 @@\n-    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n-        return super.fromArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -866,2 +894,2 @@\n-    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n-        return super.fromCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -881,2 +909,2 @@\n-    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m) {\n-        return super.fromMemorySegment0Template(ShortMaxMask.class, ms, offset, (ShortMaxMask) m);  \/\/ specialize\n+    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template(ShortMaxMask.class, ms, offset, (ShortMaxMask) m, offsetInRange);  \/\/ specialize\n@@ -920,0 +948,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":90,"deletions":61,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n@@ -32,3 +34,1 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ValueLayout;\n-import jdk.internal.access.foreign.MemorySegmentProxy;\n+import jdk.internal.foreign.AbstractMemorySegmentImpl;\n@@ -762,5 +762,3 @@\n-                \/\/ FIXME: Support this in the JIT.\n-                VectorMask<Short> thisNZ\n-                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n-                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n-                op = OR_UNCHECKED;\n+                VectorMask<Short> mask\n+                    = this.compare(EQ, (short) 0);\n+                return this.blend(that, mask);\n@@ -812,5 +810,3 @@\n-                \/\/ FIXME: Support this in the JIT.\n-                VectorMask<Short> thisNZ\n-                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n-                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n-                op = OR_UNCHECKED;\n+                VectorMask<Short> mask\n+                    = this.compare(EQ, (short) 0, m);\n+                return this.blend(that, mask);\n@@ -1078,1 +1074,1 @@\n-   \/**\n+    \/**\n@@ -2488,2 +2484,2 @@\n-    private final\n-    VectorShuffle<Short> toShuffle0(ShortSpecies dsp) {\n+    final <F>\n+    VectorShuffle<F> toShuffle0(AbstractSpecies<F> dsp) {\n@@ -2498,12 +2494,0 @@\n-    \/*package-private*\/\n-    @ForceInline\n-    final\n-    VectorShuffle<Short> toShuffleTemplate(Class<?> shuffleType) {\n-        ShortSpecies vsp = vspecies();\n-        return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n-                                     getClass(), short.class, length(),\n-                                     shuffleType, byte.class, length(),\n-                                     this, vsp,\n-                                     ShortVector::toShuffle0);\n-    }\n-\n@@ -2524,3 +2508,3 @@\n-      return (ShortVector) VectorSupport.comExpOp(VectorSupport.VECTOR_OP_COMPRESS, getClass(), masktype,\n-                                                   short.class, length(), this, m,\n-                                                   (v1, m1) -> compressHelper(v1, m1));\n+      return (ShortVector) VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_COMPRESS, getClass(), masktype,\n+                                                        short.class, length(), this, m,\n+                                                        (v1, m1) -> compressHelper(v1, m1));\n@@ -2543,3 +2527,3 @@\n-      return (ShortVector) VectorSupport.comExpOp(VectorSupport.VECTOR_OP_EXPAND, getClass(), masktype,\n-                                                   short.class, length(), this, m,\n-                                                   (v1, m1) -> expandHelper(v1, m1));\n+      return (ShortVector) VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_EXPAND, getClass(), masktype,\n+                                                        short.class, length(), this, m,\n+                                                        (v1, m1) -> expandHelper(v1, m1));\n@@ -3012,2 +2996,2 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n-            return vsp.dummyVector().fromArray0(a, offset, m);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+            return vsp.dummyVector().fromArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -3016,2 +3000,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.vOp(m, i -> a[offset + i]);\n+        return vsp.dummyVector().fromArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -3161,2 +3144,2 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n-            return vsp.dummyVector().fromCharArray0(a, offset, m);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+            return vsp.dummyVector().fromCharArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -3165,2 +3148,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.vOp(m, i -> (short) a[offset + i]);\n+        return vsp.dummyVector().fromCharArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -3354,2 +3336,2 @@\n-        if (offset >= 0 && offset <= (ms.byteSize() - species.vectorByteSize())) {\n-            return vsp.dummyVector().fromMemorySegment0(ms, offset, m).maybeSwap(bo);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.vectorByteSize(), ms.byteSize())) {\n+            return vsp.dummyVector().fromMemorySegment0(ms, offset, m, OFFSET_IN_RANGE).maybeSwap(bo);\n@@ -3358,2 +3340,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.ldLongOp(ms, offset, m, ShortVector::memorySegmentGet);\n+        return vsp.dummyVector().fromMemorySegment0(ms, offset, m, OFFSET_OUT_OF_RANGE).maybeSwap(bo);\n@@ -3426,1 +3407,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -3573,1 +3556,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -3698,1 +3683,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 2, ms.byteSize());\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.vectorByteSize(), ms.byteSize())) {\n+                checkMaskFromIndexSize(offset, vsp, m, 2, ms.byteSize());\n+            }\n@@ -3739,1 +3726,1 @@\n-    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m);\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m, int offsetInRange);\n@@ -3743,1 +3730,1 @@\n-    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m, int offsetInRange) {\n@@ -3748,1 +3735,1 @@\n-            a, arrayAddress(a, offset), m,\n+            a, arrayAddress(a, offset), m, offsetInRange,\n@@ -3772,1 +3759,1 @@\n-    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m, int offsetInRange);\n@@ -3776,1 +3763,1 @@\n-    ShortVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+    ShortVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m, int offsetInRange) {\n@@ -3781,1 +3768,1 @@\n-                a, charArrayAddress(a, offset), m,\n+                a, charArrayAddress(a, offset), m, offsetInRange,\n@@ -3796,1 +3783,1 @@\n-                (MemorySegmentProxy) ms, offset, vsp,\n+                (AbstractMemorySegmentImpl) ms, offset, vsp,\n@@ -3803,1 +3790,1 @@\n-    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m);\n+    ShortVector fromMemorySegment0(MemorySegment ms, long offset, VectorMask<Short> m, int offsetInRange);\n@@ -3807,1 +3794,1 @@\n-    ShortVector fromMemorySegment0Template(Class<M> maskClass, MemorySegment ms, long offset, M m) {\n+    ShortVector fromMemorySegment0Template(Class<M> maskClass, MemorySegment ms, long offset, M m, int offsetInRange) {\n@@ -3812,1 +3799,1 @@\n-                (MemorySegmentProxy) ms, offset, m, vsp,\n+                (AbstractMemorySegmentImpl) ms, offset, m, vsp, offsetInRange,\n@@ -3863,1 +3850,1 @@\n-                (MemorySegmentProxy) ms, offset,\n+                (AbstractMemorySegmentImpl) ms, offset,\n@@ -3880,1 +3867,1 @@\n-                (MemorySegmentProxy) ms, offset,\n+                (AbstractMemorySegmentImpl) ms, offset,\n@@ -4093,0 +4080,1 @@\n+                Class<? extends AbstractShuffle<Short>> shuffleType,\n@@ -4095,1 +4083,1 @@\n-                  vectorType, maskType,\n+                  vectorType, maskType, shuffleType,\n@@ -4371,0 +4359,1 @@\n+                            Short64Vector.Short64Shuffle.class,\n@@ -4378,0 +4367,1 @@\n+                            Short128Vector.Short128Shuffle.class,\n@@ -4385,0 +4375,1 @@\n+                            Short256Vector.Short256Shuffle.class,\n@@ -4392,0 +4383,1 @@\n+                            Short512Vector.Short512Shuffle.class,\n@@ -4399,0 +4391,1 @@\n+                            ShortMaxVector.ShortMaxShuffle.class,\n@@ -4408,0 +4401,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":57,"deletions":63,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-import jdk.incubator.foreign.MemorySegment;\n+import java.lang.foreign.MemorySegment;\n@@ -95,1 +95,1 @@\n- * overloadings and overrides (commonly the unmasked varient with scalar-broadcast\n+ * overloadings and overrides (commonly the unmasked variant with scalar-broadcast\n@@ -690,1 +690,1 @@\n- * general way to know which neighbor is the the more significant.\n+ * general way to know which neighbor is the more significant.\n@@ -767,1 +767,1 @@\n- * in a {@link jdk.incubator.foreign.MemorySegment}.\n+ * in a {@link java.lang.foreign.MemorySegment}.\n@@ -2262,1 +2262,1 @@\n-     * when the the result is represented using the vector\n+     * when the result is represented using the vector\n@@ -2452,1 +2452,1 @@\n-     *         placed starting in the first lane of the ouput,\n+     *         placed starting in the first lane of the output,\n@@ -2959,2 +2959,2 @@\n-     * @see IntVector#intoMemorySegment(jdk.incubator.foreign.MemorySegment, long, java.nio.ByteOrder)\n-     * @see FloatVector#intoMemorySegment(jdk.incubator.foreign.MemorySegment, long, java.nio.ByteOrder)\n+     * @see IntVector#intoMemorySegment(java.lang.foreign.MemorySegment, long, java.nio.ByteOrder)\n+     * @see FloatVector#intoMemorySegment(java.lang.foreign.MemorySegment, long, java.nio.ByteOrder)\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Vector.java","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,5 @@\n+    @ForceInline\n+    static boolean indexInRange(long ix, long vlen, long length) {\n+        return ix >= 0 && ix <= (length - vlen);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorIntrinsics.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+ * <li>{@code EMASK} &mdash; the bit mask of the operand type, where {@code EMASK=(1<<(ESIZE*8))-1}\n+ *\n@@ -571,1 +573,1 @@\n-    \/** Produce {@code a>>>(n&(ESIZE*8-1))}.  Integral only. *\/\n+    \/** Produce {@code (a&EMASK)>>>(n&(ESIZE*8-1))}.  Integral only. *\/\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorOperators.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -127,0 +127,1 @@\n+    @ForceInline\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorShape.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n@@ -32,3 +34,1 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ValueLayout;\n-import jdk.internal.access.foreign.MemorySegmentProxy;\n+import jdk.internal.foreign.AbstractMemorySegmentImpl;\n@@ -883,11 +883,3 @@\n-                \/\/ FIXME: Support this in the JIT.\n-                VectorMask<$Boxbitstype$> thisNZ\n-                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n-                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n-                op = OR_UNCHECKED;\n-#if[FP]\n-                \/\/ FIXME: Support OR_UNCHECKED on float\/double also!\n-                return this.viewAsIntegralLanes()\n-                    .lanewise(op, that.viewAsIntegralLanes())\n-                    .viewAsFloatingLanes();\n-#end[FP]\n+                VectorMask<$Boxbitstype$> mask\n+                    = this{#if[FP]?.viewAsIntegralLanes()}.compare(EQ, ($bitstype$) 0);\n+                return this.blend(that, mask{#if[FP]?.cast(vspecies())});\n@@ -944,1 +936,4 @@\n-                return blend(lanewise(op, v), m);\n+                $Bitstype$Vector bits = this.viewAsIntegralLanes();\n+                VectorMask<$Boxbitstype$> mask\n+                    = bits.compare(EQ, ($bitstype$) 0, m.cast(bits.vspecies()));\n+                return this.blend(that, mask.cast(vspecies()));\n@@ -946,5 +941,3 @@\n-                \/\/ FIXME: Support this in the JIT.\n-                VectorMask<$Boxbitstype$> thisNZ\n-                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n-                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n-                op = OR_UNCHECKED;\n+                VectorMask<$Boxtype$> mask\n+                    = this.compare(EQ, ($type$) 0, m);\n+                return this.blend(that, mask);\n@@ -1279,1 +1272,1 @@\n-   \/**\n+    \/**\n@@ -2960,2 +2953,2 @@\n-    private final\n-    VectorShuffle<$Boxtype$> toShuffle0($Type$Species dsp) {\n+    final <F>\n+    VectorShuffle<F> toShuffle0(AbstractSpecies<F> dsp) {\n@@ -2970,12 +2963,0 @@\n-    \/*package-private*\/\n-    @ForceInline\n-    final\n-    VectorShuffle<$Boxtype$> toShuffleTemplate(Class<?> shuffleType) {\n-        $Type$Species vsp = vspecies();\n-        return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n-                                     getClass(), $type$.class, length(),\n-                                     shuffleType, byte.class, length(),\n-                                     this, vsp,\n-                                     $Type$Vector::toShuffle0);\n-    }\n-\n@@ -2996,3 +2977,3 @@\n-      return ($Type$Vector) VectorSupport.comExpOp(VectorSupport.VECTOR_OP_COMPRESS, getClass(), masktype,\n-                                                   $elemtype$.class, length(), this, m,\n-                                                   (v1, m1) -> compressHelper(v1, m1));\n+      return ($Type$Vector) VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_COMPRESS, getClass(), masktype,\n+                                                        $elemtype$.class, length(), this, m,\n+                                                        (v1, m1) -> compressHelper(v1, m1));\n@@ -3015,3 +2996,3 @@\n-      return ($Type$Vector) VectorSupport.comExpOp(VectorSupport.VECTOR_OP_EXPAND, getClass(), masktype,\n-                                                   $elemtype$.class, length(), this, m,\n-                                                   (v1, m1) -> expandHelper(v1, m1));\n+      return ($Type$Vector) VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_EXPAND, getClass(), masktype,\n+                                                        $elemtype$.class, length(), this, m,\n+                                                        (v1, m1) -> expandHelper(v1, m1));\n@@ -3698,2 +3679,2 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n-            return vsp.dummyVector().fromArray0(a, offset, m);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+            return vsp.dummyVector().fromArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -3702,2 +3683,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.vOp(m, i -> a[offset + i]);\n+        return vsp.dummyVector().fromArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -3919,2 +3899,2 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n-            return vsp.dummyVector().fromCharArray0(a, offset, m);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+            return vsp.dummyVector().fromCharArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -3923,2 +3903,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.vOp(m, i -> (short) a[offset + i]);\n+        return vsp.dummyVector().fromCharArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -4078,1 +4057,1 @@\n-        if (offset >= 0 && offset <= (a.length - species.length())) {\n+        if (VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n@@ -4080,1 +4059,1 @@\n-            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m, OFFSET_IN_RANGE);\n@@ -4083,2 +4062,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.vOp(m, i -> (byte) (a[offset + i] ? 1 : 0));\n+        return vsp.dummyVector().fromBooleanArray0(a, offset, m, OFFSET_OUT_OF_RANGE);\n@@ -4276,2 +4254,2 @@\n-        if (offset >= 0 && offset <= (ms.byteSize() - species.vectorByteSize())) {\n-            return vsp.dummyVector().fromMemorySegment0(ms, offset, m).maybeSwap(bo);\n+        if (VectorIntrinsics.indexInRange(offset, vsp.vectorByteSize(), ms.byteSize())) {\n+            return vsp.dummyVector().fromMemorySegment0(ms, offset, m, OFFSET_IN_RANGE).maybeSwap(bo);\n@@ -4280,2 +4258,1 @@\n-        \/\/ FIXME: optimize\n-        return vsp.ldLongOp(ms, offset, m, $abstractvectortype$::memorySegmentGet);\n+        return vsp.dummyVector().fromMemorySegment0(ms, offset, m, OFFSET_OUT_OF_RANGE).maybeSwap(bo);\n@@ -4348,1 +4325,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -4566,1 +4545,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -4728,1 +4709,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.length(), a.length)) {\n+                checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            }\n@@ -4859,1 +4842,3 @@\n-            checkMaskFromIndexSize(offset, vsp, m, $sizeInBytes$, ms.byteSize());\n+            if (!VectorIntrinsics.indexInRange(offset, vsp.vectorByteSize(), ms.byteSize())) {\n+                checkMaskFromIndexSize(offset, vsp, m, $sizeInBytes$, ms.byteSize());\n+            }\n@@ -4900,1 +4885,1 @@\n-    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m, int offsetInRange);\n@@ -4904,1 +4889,1 @@\n-    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset, M m, int offsetInRange) {\n@@ -4909,1 +4894,1 @@\n-            a, arrayAddress(a, offset), m,\n+            a, arrayAddress(a, offset), m, offsetInRange,\n@@ -4994,1 +4979,1 @@\n-    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m, int offsetInRange);\n@@ -4998,1 +4983,1 @@\n-    $abstractvectortype$ fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+    $abstractvectortype$ fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m, int offsetInRange) {\n@@ -5003,1 +4988,1 @@\n-                a, charArrayAddress(a, offset), m,\n+                a, charArrayAddress(a, offset), m, offsetInRange,\n@@ -5028,1 +5013,1 @@\n-    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m, int offsetInRange);\n@@ -5032,1 +5017,1 @@\n-    $abstractvectortype$ fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+    $abstractvectortype$ fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m, int offsetInRange) {\n@@ -5037,1 +5022,1 @@\n-            a, booleanArrayAddress(a, offset), m,\n+            a, booleanArrayAddress(a, offset), m, offsetInRange,\n@@ -5052,1 +5037,1 @@\n-                (MemorySegmentProxy) ms, offset, vsp,\n+                (AbstractMemorySegmentImpl) ms, offset, vsp,\n@@ -5059,1 +5044,1 @@\n-    $abstractvectortype$ fromMemorySegment0(MemorySegment ms, long offset, VectorMask<$Boxtype$> m);\n+    $abstractvectortype$ fromMemorySegment0(MemorySegment ms, long offset, VectorMask<$Boxtype$> m, int offsetInRange);\n@@ -5063,1 +5048,1 @@\n-    $abstractvectortype$ fromMemorySegment0Template(Class<M> maskClass, MemorySegment ms, long offset, M m) {\n+    $abstractvectortype$ fromMemorySegment0Template(Class<M> maskClass, MemorySegment ms, long offset, M m, int offsetInRange) {\n@@ -5068,1 +5053,1 @@\n-                (MemorySegmentProxy) ms, offset, m, vsp,\n+                (AbstractMemorySegmentImpl) ms, offset, m, vsp, offsetInRange,\n@@ -5199,1 +5184,1 @@\n-                (MemorySegmentProxy) ms, offset,\n+                (AbstractMemorySegmentImpl) ms, offset,\n@@ -5216,1 +5201,1 @@\n-                (MemorySegmentProxy) ms, offset,\n+                (AbstractMemorySegmentImpl) ms, offset,\n@@ -5478,0 +5463,1 @@\n+                Class<? extends AbstractShuffle<$Boxtype$>> shuffleType,\n@@ -5480,1 +5466,1 @@\n-                  vectorType, maskType,\n+                  vectorType, maskType, shuffleType,\n@@ -5771,0 +5757,1 @@\n+                            $Type$64Vector.$Type$64Shuffle.class,\n@@ -5778,0 +5765,1 @@\n+                            $Type$128Vector.$Type$128Shuffle.class,\n@@ -5785,0 +5773,1 @@\n+                            $Type$256Vector.$Type$256Shuffle.class,\n@@ -5792,0 +5781,1 @@\n+                            $Type$512Vector.$Type$512Shuffle.class,\n@@ -5799,0 +5789,1 @@\n+                            $Type$MaxVector.$Type$MaxShuffle.class,\n@@ -5808,0 +5799,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":70,"deletions":78,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+import java.lang.foreign.MemorySegment;\n@@ -31,1 +32,0 @@\n-import jdk.incubator.foreign.MemorySegment;\n@@ -146,11 +146,0 @@\n-    @ForceInline\n-    $shuffletype$ iotaShuffle(int start, int step, boolean wrap) {\n-      if (wrap) {\n-        return ($shuffletype$)VectorSupport.shuffleIota(ETYPE, $shuffletype$.class, VSPECIES, VLENGTH, start, step, 1,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (VectorIntrinsics.wrapToRange(i*lstep + lstart, l))));\n-      } else {\n-        return ($shuffletype$)VectorSupport.shuffleIota(ETYPE, $shuffletype$.class, VSPECIES, VLENGTH, start, step, 0,\n-                (l, lstart, lstep, s) -> s.shuffleFromOp(i -> (i*lstep + lstart)));\n-      }\n-    }\n-\n@@ -159,5 +148,1 @@\n-    $shuffletype$ shuffleFromBytes(byte[] reorder) { return new $shuffletype$(reorder); }\n-\n-    @Override\n-    @ForceInline\n-    $shuffletype$ shuffleFromArray(int[] indexes, int i) { return new $shuffletype$(indexes, i); }\n+    $shuffletype$ shuffleFromArray(int[] indices, int i) { return new $shuffletype$(indices, i); }\n@@ -364,0 +349,1 @@\n+    @Override\n@@ -365,2 +351,3 @@\n-    public VectorShuffle<$Boxtype$> toShuffle() {\n-        return super.toShuffleTemplate($shuffletype$.class); \/\/ specialize\n+    public final\n+    <F> VectorShuffle<F> toShuffle(AbstractSpecies<F> dsp) {\n+        return super.toShuffleTemplate(dsp);\n@@ -961,4 +948,5 @@\n-        public $masktype$ eq(VectorMask<$Boxtype$> mask) {\n-            Objects.requireNonNull(mask);\n-            $masktype$ m = ($masktype$)mask;\n-            return xor(m.not());\n+        \/*package-private*\/\n+        $masktype$ indexPartiallyInUpperRange(long offset, long limit) {\n+            return ($masktype$) VectorSupport.indexPartiallyInUpperRange(\n+                $masktype$.class, ETYPE, VLENGTH, offset, limit,\n+                (o, l) -> ($masktype$) TRUE_MASK.indexPartiallyInRange(o, l));\n@@ -978,1 +966,1 @@\n-            return ($masktype$)VectorSupport.comExpOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n+            return ($masktype$)VectorSupport.compressExpandOp(VectorSupport.VECTOR_OP_MASK_COMPRESS,\n@@ -1010,0 +998,1 @@\n+        @Override\n@@ -1011,2 +1000,1 @@\n-        \/* package-private *\/\n-        $masktype$ xor(VectorMask<$Boxtype$> mask) {\n+        public $masktype$ xor(VectorMask<$Boxtype$> mask) {\n@@ -1102,1 +1090,1 @@\n-        static final Class<$Boxtype$> ETYPE = $elemtype$.class; \/\/ used by the JVM\n+        static final Class<$Boxbitstype$> ETYPE = $bitstype$.class; \/\/ used by the JVM\n@@ -1104,2 +1092,4 @@\n-        $shuffletype$(byte[] reorder) {\n-            super(VLENGTH, reorder);\n+        $shuffletype$($bitstype$[] indices) {\n+            super(indices);\n+            assert(VLENGTH == indices.length);\n+            assert(indicesInRange(indices));\n@@ -1108,2 +1098,2 @@\n-        public $shuffletype$(int[] reorder) {\n-            super(VLENGTH, reorder);\n+        $shuffletype$(int[] indices, int i) {\n+            this(prepare(indices, i));\n@@ -1112,2 +1102,2 @@\n-        public $shuffletype$(int[] reorder, int i) {\n-            super(VLENGTH, reorder, i);\n+        $shuffletype$(IntUnaryOperator fn) {\n+            this(prepare(fn));\n@@ -1116,2 +1106,2 @@\n-        public $shuffletype$(IntUnaryOperator fn) {\n-            super(VLENGTH, fn);\n+        $bitstype$[] indices() {\n+            return ($bitstype$[])getPayload();\n@@ -1121,0 +1111,1 @@\n+        @ForceInline\n@@ -1128,2 +1119,2 @@\n-            assert(VLENGTH < Byte.MAX_VALUE);\n-            assert(Byte.MIN_VALUE <= -VLENGTH);\n+            assert(VLENGTH < $Boxbitstype$.MAX_VALUE);\n+            assert($Boxbitstype$.MIN_VALUE <= -VLENGTH);\n@@ -1135,3 +1126,2 @@\n-        public $vectortype$ toVector() {\n-            return VectorSupport.shuffleToVector(VCLASS, ETYPE, $shuffletype$.class, this, VLENGTH,\n-                                                    (s) -> (($vectortype$)(((AbstractShuffle<$Boxtype$>)(s)).toVectorTemplate())));\n+        $bitsvectortype$ toBitsVector() {\n+            return ($bitsvectortype$) super.toBitsVectorTemplate();\n@@ -1142,6 +1132,2 @@\n-        public <F> VectorShuffle<F> cast(VectorSpecies<F> s) {\n-            AbstractSpecies<F> species = (AbstractSpecies<F>) s;\n-            if (length() != species.laneCount())\n-                throw new IllegalArgumentException(\"VectorShuffle length and species length differ\");\n-            int[] shuffleArray = toArray();\n-            return s.shuffleFromArray(shuffleArray, 0).check(s);\n+        $Bitstype$Vector toBitsVector0() {\n+            return $bitsvectortype$.VSPECIES.dummyVector().vectorFactory(indices());\n@@ -1150,0 +1136,1 @@\n+        @Override\n@@ -1151,0 +1138,4 @@\n+        public int laneSource(int i) {\n+            return (int)toBitsVector().lane(i);\n+        }\n+\n@@ -1152,8 +1143,53 @@\n-        public $shuffletype$ rearrange(VectorShuffle<$Boxtype$> shuffle) {\n-            $shuffletype$ s = ($shuffletype$) shuffle;\n-            byte[] reorder1 = reorder();\n-            byte[] reorder2 = s.reorder();\n-            byte[] r = new byte[reorder1.length];\n-            for (int i = 0; i < reorder1.length; i++) {\n-                int ssi = reorder2[i];\n-                r[i] = reorder1[ssi];  \/\/ throws on exceptional index\n+        @ForceInline\n+        public void intoArray(int[] a, int offset) {\n+#if[byte]\n+            VectorSpecies<Integer> species = IntVector.SPECIES_$BITS$;\n+            Vector<Byte> v = toBitsVector();\n+            v.convertShape(VectorOperators.B2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.B2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+            v.convertShape(VectorOperators.B2I, species, 2)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 2);\n+            v.convertShape(VectorOperators.B2I, species, 3)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length() * 3);\n+#end[byte]\n+#if[short]\n+            VectorSpecies<Integer> species = IntVector.SPECIES_$BITS$;\n+            Vector<Short> v = toBitsVector();\n+            v.convertShape(VectorOperators.S2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+            v.convertShape(VectorOperators.S2I, species, 1)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset + species.length());\n+#end[short]\n+#if[intOrFloat]\n+            toBitsVector().intoArray(a, offset);\n+#end[intOrFloat]\n+#if[longOrDouble]\n+#if[!1L]\n+            VectorSpecies<Integer> species = VectorSpecies.of(\n+                    int.class,\n+                    VectorShape.forBitSize(length() * Integer.SIZE));\n+            Vector<Long> v = toBitsVector();\n+            v.convertShape(VectorOperators.L2I, species, 0)\n+                    .reinterpretAsInts()\n+                    .intoArray(a, offset);\n+#end[!1L]\n+#if[1L]\n+            a[offset] = laneSource(0);\n+#end[1L]\n+#end[longOrDouble]\n+        }\n+\n+        private static $bitstype$[] prepare(int[] indices, int offset) {\n+            $bitstype$[] a = new $bitstype$[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = indices[offset + i];\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = ($bitstype$)si;\n@@ -1161,1 +1197,28 @@\n-            return new $shuffletype$(r);\n+            return a;\n+        }\n+\n+        private static $bitstype$[] prepare(IntUnaryOperator f) {\n+            $bitstype$[] a = new $bitstype$[VLENGTH];\n+            for (int i = 0; i < VLENGTH; i++) {\n+                int si = f.applyAsInt(i);\n+                si = partiallyWrapIndex(si, VLENGTH);\n+                a[i] = ($bitstype$)si;\n+            }\n+            return a;\n+        }\n+\n+        private static boolean indicesInRange($bitstype$[] indices) {\n+            int length = indices.length;\n+            for ($bitstype$ si : indices) {\n+                if (si >= ($bitstype$)length || si < ($bitstype$)(-length)) {\n+                    boolean assertsEnabled = false;\n+                    assert(assertsEnabled = true);\n+                    if (assertsEnabled) {\n+                        String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(indices));\n+                        throw new AssertionError(msg);\n+                    }\n+                    return false;\n+                }\n+            }\n+            return true;\n@@ -1179,2 +1242,2 @@\n-    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n-        return super.fromArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m, int offsetInRange) {\n+        return super.fromArray0Template($masktype$.class, a, offset, ($masktype$) m, offsetInRange);  \/\/ specialize\n@@ -1203,2 +1266,2 @@\n-    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n-        return super.fromCharArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m, int offsetInRange) {\n+        return super.fromCharArray0Template($masktype$.class, a, offset, ($masktype$) m, offsetInRange);  \/\/ specialize\n@@ -1219,2 +1282,2 @@\n-    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n-        return super.fromBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m, int offsetInRange) {\n+        return super.fromBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m, offsetInRange);  \/\/ specialize\n@@ -1234,2 +1297,2 @@\n-    $abstractvectortype$ fromMemorySegment0(MemorySegment ms, long offset, VectorMask<$Boxtype$> m) {\n-        return super.fromMemorySegment0Template($masktype$.class, ms, offset, ($masktype$) m);  \/\/ specialize\n+    $abstractvectortype$ fromMemorySegment0(MemorySegment ms, long offset, VectorMask<$Boxtype$> m, int offsetInRange) {\n+        return super.fromMemorySegment0Template($masktype$.class, ms, offset, ($masktype$) m, offsetInRange);  \/\/ specialize\n@@ -1291,0 +1354,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":128,"deletions":64,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -0,0 +1,333 @@\n+\/*\n+ * Copyright (c) 2009, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.vm.ci.amd64;\n+\n+import static jdk.vm.ci.code.MemoryBarriers.LOAD_LOAD;\n+import static jdk.vm.ci.code.MemoryBarriers.LOAD_STORE;\n+import static jdk.vm.ci.code.MemoryBarriers.STORE_STORE;\n+import static jdk.vm.ci.code.Register.SPECIAL;\n+\n+import java.nio.ByteOrder;\n+import java.util.EnumSet;\n+\n+import jdk.vm.ci.code.Architecture;\n+import jdk.vm.ci.code.CPUFeatureName;\n+import jdk.vm.ci.code.Register;\n+import jdk.vm.ci.code.Register.RegisterCategory;\n+import jdk.vm.ci.code.RegisterArray;\n+import jdk.vm.ci.meta.JavaKind;\n+import jdk.vm.ci.meta.PlatformKind;\n+\n+\/**\n+ * Represents the AMD64 architecture.\n+ *\/\n+public class AMD64 extends Architecture {\n+\n+    public static final RegisterCategory CPU = new RegisterCategory(\"CPU\");\n+\n+    \/\/ @formatter:off\n+\n+    \/\/ General purpose CPU registers\n+    public static final Register rax = new Register(0, 0, \"rax\", CPU);\n+    public static final Register rcx = new Register(1, 1, \"rcx\", CPU);\n+    public static final Register rdx = new Register(2, 2, \"rdx\", CPU);\n+    public static final Register rbx = new Register(3, 3, \"rbx\", CPU);\n+    public static final Register rsp = new Register(4, 4, \"rsp\", CPU);\n+    public static final Register rbp = new Register(5, 5, \"rbp\", CPU);\n+    public static final Register rsi = new Register(6, 6, \"rsi\", CPU);\n+    public static final Register rdi = new Register(7, 7, \"rdi\", CPU);\n+\n+    public static final Register r8  = new Register(8,  8,  \"r8\", CPU);\n+    public static final Register r9  = new Register(9,  9,  \"r9\", CPU);\n+    public static final Register r10 = new Register(10, 10, \"r10\", CPU);\n+    public static final Register r11 = new Register(11, 11, \"r11\", CPU);\n+    public static final Register r12 = new Register(12, 12, \"r12\", CPU);\n+    public static final Register r13 = new Register(13, 13, \"r13\", CPU);\n+    public static final Register r14 = new Register(14, 14, \"r14\", CPU);\n+    public static final Register r15 = new Register(15, 15, \"r15\", CPU);\n+\n+    public static final Register[] cpuRegisters = {\n+        rax, rcx, rdx, rbx, rsp, rbp, rsi, rdi,\n+        r8, r9, r10, r11, r12, r13, r14, r15\n+    };\n+\n+    public static final RegisterCategory XMM = new RegisterCategory(\"XMM\");\n+\n+    \/\/ XMM registers\n+    public static final Register xmm0 = new Register(16, 0, \"xmm0\", XMM);\n+    public static final Register xmm1 = new Register(17, 1, \"xmm1\", XMM);\n+    public static final Register xmm2 = new Register(18, 2, \"xmm2\", XMM);\n+    public static final Register xmm3 = new Register(19, 3, \"xmm3\", XMM);\n+    public static final Register xmm4 = new Register(20, 4, \"xmm4\", XMM);\n+    public static final Register xmm5 = new Register(21, 5, \"xmm5\", XMM);\n+    public static final Register xmm6 = new Register(22, 6, \"xmm6\", XMM);\n+    public static final Register xmm7 = new Register(23, 7, \"xmm7\", XMM);\n+\n+    public static final Register xmm8  = new Register(24,  8, \"xmm8\",  XMM);\n+    public static final Register xmm9  = new Register(25,  9, \"xmm9\",  XMM);\n+    public static final Register xmm10 = new Register(26, 10, \"xmm10\", XMM);\n+    public static final Register xmm11 = new Register(27, 11, \"xmm11\", XMM);\n+    public static final Register xmm12 = new Register(28, 12, \"xmm12\", XMM);\n+    public static final Register xmm13 = new Register(29, 13, \"xmm13\", XMM);\n+    public static final Register xmm14 = new Register(30, 14, \"xmm14\", XMM);\n+    public static final Register xmm15 = new Register(31, 15, \"xmm15\", XMM);\n+\n+    public static final Register xmm16 = new Register(32, 16, \"xmm16\", XMM);\n+    public static final Register xmm17 = new Register(33, 17, \"xmm17\", XMM);\n+    public static final Register xmm18 = new Register(34, 18, \"xmm18\", XMM);\n+    public static final Register xmm19 = new Register(35, 19, \"xmm19\", XMM);\n+    public static final Register xmm20 = new Register(36, 20, \"xmm20\", XMM);\n+    public static final Register xmm21 = new Register(37, 21, \"xmm21\", XMM);\n+    public static final Register xmm22 = new Register(38, 22, \"xmm22\", XMM);\n+    public static final Register xmm23 = new Register(39, 23, \"xmm23\", XMM);\n+\n+    public static final Register xmm24 = new Register(40, 24, \"xmm24\", XMM);\n+    public static final Register xmm25 = new Register(41, 25, \"xmm25\", XMM);\n+    public static final Register xmm26 = new Register(42, 26, \"xmm26\", XMM);\n+    public static final Register xmm27 = new Register(43, 27, \"xmm27\", XMM);\n+    public static final Register xmm28 = new Register(44, 28, \"xmm28\", XMM);\n+    public static final Register xmm29 = new Register(45, 29, \"xmm29\", XMM);\n+    public static final Register xmm30 = new Register(46, 30, \"xmm30\", XMM);\n+    public static final Register xmm31 = new Register(47, 31, \"xmm31\", XMM);\n+\n+    public static final Register[] xmmRegistersSSE = {\n+        xmm0, xmm1, xmm2,  xmm3,  xmm4,  xmm5,  xmm6,  xmm7,\n+        xmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15\n+    };\n+\n+    public static final Register[] xmmRegistersAVX512 = {\n+        xmm0, xmm1, xmm2,  xmm3,  xmm4,  xmm5,  xmm6,  xmm7,\n+        xmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15,\n+        xmm16, xmm17, xmm18, xmm19, xmm20, xmm21, xmm22, xmm23,\n+        xmm24, xmm25, xmm26, xmm27, xmm28, xmm29, xmm30, xmm31\n+    };\n+\n+    public static final RegisterCategory MASK = new RegisterCategory(\"MASK\", false);\n+\n+    public static final Register k0 = new Register(48, 0, \"k0\", MASK);\n+    public static final Register k1 = new Register(49, 1, \"k1\", MASK);\n+    public static final Register k2 = new Register(50, 2, \"k2\", MASK);\n+    public static final Register k3 = new Register(51, 3, \"k3\", MASK);\n+    public static final Register k4 = new Register(52, 4, \"k4\", MASK);\n+    public static final Register k5 = new Register(53, 5, \"k5\", MASK);\n+    public static final Register k6 = new Register(54, 6, \"k6\", MASK);\n+    public static final Register k7 = new Register(55, 7, \"k7\", MASK);\n+\n+    public static final RegisterArray valueRegistersSSE = new RegisterArray(\n+        rax,  rcx,  rdx,   rbx,   rsp,   rbp,   rsi,   rdi,\n+        r8,   r9,   r10,   r11,   r12,   r13,   r14,   r15,\n+        xmm0, xmm1, xmm2,  xmm3,  xmm4,  xmm5,  xmm6,  xmm7,\n+        xmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15\n+    );\n+\n+    public static final RegisterArray valueRegistersAVX512 = new RegisterArray(\n+        rax,  rcx,  rdx,   rbx,   rsp,   rbp,   rsi,   rdi,\n+        r8,   r9,   r10,   r11,   r12,   r13,   r14,   r15,\n+        xmm0, xmm1, xmm2,  xmm3,  xmm4,  xmm5,  xmm6,  xmm7,\n+        xmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15,\n+        xmm16, xmm17, xmm18, xmm19, xmm20, xmm21, xmm22, xmm23,\n+        xmm24, xmm25, xmm26, xmm27, xmm28, xmm29, xmm30, xmm31,\n+        k0, k1, k2, k3, k4, k5, k6, k7\n+    );\n+\n+    \/**\n+     * Register used to construct an instruction-relative address.\n+     *\/\n+    public static final Register rip = new Register(56, -1, \"rip\", SPECIAL);\n+\n+    public static final RegisterArray allRegisters = new RegisterArray(\n+        rax,  rcx,  rdx,   rbx,   rsp,   rbp,   rsi,   rdi,\n+        r8,   r9,   r10,   r11,   r12,   r13,   r14,   r15,\n+        xmm0, xmm1, xmm2,  xmm3,  xmm4,  xmm5,  xmm6,  xmm7,\n+        xmm8, xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15,\n+        xmm16, xmm17, xmm18, xmm19, xmm20, xmm21, xmm22, xmm23,\n+        xmm24, xmm25, xmm26, xmm27, xmm28, xmm29, xmm30, xmm31,\n+        k0, k1, k2, k3, k4, k5, k6, k7,\n+        rip\n+    );\n+\n+    \/\/ @formatter:on\n+\n+    \/**\n+     * Basic set of CPU features mirroring what is returned from the cpuid instruction. See:\n+     * {@code VM_Version::cpuFeatureFlags}.\n+     *\/\n+    public enum CPUFeature implements CPUFeatureName {\n+        CX8,\n+        CMOV,\n+        FXSR,\n+        HT,\n+        MMX,\n+        AMD_3DNOW_PREFETCH,\n+        SSE,\n+        SSE2,\n+        SSE3,\n+        SSSE3,\n+        SSE4A,\n+        SSE4_1,\n+        SSE4_2,\n+        POPCNT,\n+        LZCNT,\n+        TSC,\n+        TSCINV,\n+        TSCINV_BIT,\n+        AVX,\n+        AVX2,\n+        AES,\n+        ERMS,\n+        CLMUL,\n+        BMI1,\n+        BMI2,\n+        RTM,\n+        ADX,\n+        AVX512F,\n+        AVX512DQ,\n+        AVX512PF,\n+        AVX512ER,\n+        AVX512CD,\n+        AVX512BW,\n+        AVX512VL,\n+        SHA,\n+        FMA,\n+        VZEROUPPER,\n+        AVX512_VPOPCNTDQ,\n+        AVX512_VPCLMULQDQ,\n+        AVX512_VAES,\n+        AVX512_VNNI,\n+        FLUSH,\n+        FLUSHOPT,\n+        CLWB,\n+        AVX512_VBMI2,\n+        AVX512_VBMI,\n+        HV,\n+        SERIALIZE,\n+        RDTSCP,\n+        RDPID,\n+        FSRM,\n+        GFNI,\n+        AVX512_BITALG,\n+        F16C,\n+        PKU,\n+        OSPKE,\n+        CET_IBT,\n+        CET_SS,\n+        AVX512_IFMA,\n+        AVX512_FP16,\n+    }\n+\n+    private final EnumSet<CPUFeature> features;\n+\n+    \/**\n+     * Set of flags to control code emission.\n+     *\/\n+    public enum Flag {\n+        UseCountLeadingZerosInstruction,\n+        UseCountTrailingZerosInstruction\n+    }\n+\n+    private final EnumSet<Flag> flags;\n+\n+    private final AMD64Kind largestKind;\n+\n+    public AMD64(EnumSet<CPUFeature> features, EnumSet<Flag> flags) {\n+        super(\"AMD64\", AMD64Kind.QWORD, ByteOrder.LITTLE_ENDIAN, true, allRegisters, LOAD_LOAD | LOAD_STORE | STORE_STORE, 1, 8);\n+        this.features = features;\n+        this.flags = flags;\n+        assert features.contains(CPUFeature.SSE2) : \"minimum config for x64\";\n+\n+        if (features.contains(CPUFeature.AVX512F)) {\n+            largestKind = AMD64Kind.V512_QWORD;\n+        } else if (features.contains(CPUFeature.AVX)) {\n+            largestKind = AMD64Kind.V256_QWORD;\n+        } else {\n+            largestKind = AMD64Kind.V128_QWORD;\n+        }\n+    }\n+\n+    @Override\n+    public EnumSet<CPUFeature> getFeatures() {\n+        return features;\n+    }\n+\n+    public EnumSet<Flag> getFlags() {\n+        return flags;\n+    }\n+\n+    @Override\n+    public RegisterArray getAvailableValueRegisters() {\n+        if (features.contains(CPUFeature.AVX512F)) {\n+            return valueRegistersAVX512;\n+        } else {\n+            return valueRegistersSSE;\n+        }\n+    }\n+\n+    @Override\n+    public PlatformKind getPlatformKind(JavaKind javaKind) {\n+        switch (javaKind) {\n+            case Boolean:\n+            case Byte:\n+                return AMD64Kind.BYTE;\n+            case Short:\n+            case Char:\n+                return AMD64Kind.WORD;\n+            case Int:\n+                return AMD64Kind.DWORD;\n+            case Long:\n+            case Object:\n+                return AMD64Kind.QWORD;\n+            case Float:\n+                return AMD64Kind.SINGLE;\n+            case Double:\n+                return AMD64Kind.DOUBLE;\n+            default:\n+                return null;\n+        }\n+    }\n+\n+    @Override\n+    public boolean canStoreValue(RegisterCategory category, PlatformKind platformKind) {\n+        AMD64Kind kind = (AMD64Kind) platformKind;\n+        if (kind.isInteger()) {\n+            return category.equals(CPU);\n+        } else if (kind.isXMM()) {\n+            return category.equals(XMM);\n+        } else {\n+            assert kind.isMask();\n+            return category.equals(MASK);\n+        }\n+    }\n+\n+    @Override\n+    public AMD64Kind getLargestStorableKind(RegisterCategory category) {\n+        if (category.equals(CPU)) {\n+            return AMD64Kind.QWORD;\n+        } else if (category.equals(XMM)) {\n+            return largestKind;\n+        } else if (category.equals(MASK)) {\n+            return AMD64Kind.MASK64;\n+        } else {\n+            return null;\n+        }\n+    }\n+}\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/amd64\/AMD64.java","additions":333,"deletions":0,"binary":false,"changes":333,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8294588\n+ * @summary Auto-vectorize Float.floatToFloat16, Float.float16ToFloat APIs\n+ * @requires vm.compiler2.enabled\n+ * @requires (os.simpleArch == \"x64\" & vm.cpu.features ~= \".*avx512_fp16.*\") | os.arch == \"aarch64\"\n+ * @library \/test\/lib \/\n+ * @run driver compiler.vectorization.TestFloatConversionsVector\n+ *\/\n+\n+package compiler.vectorization;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.test.lib.Asserts;\n+\n+public class TestFloatConversionsVector {\n+    private static final int ARRLEN = 1024;\n+    private static final int ITERS  = 11000;\n+    private static float  [] finp;\n+    private static short  [] sout;\n+    private static short  [] sinp;\n+    private static float  [] fout;\n+\n+    public static void main(String args[]) {\n+        TestFramework.runWithFlags(\"-XX:-TieredCompilation\",\n+                                   \"-XX:CompileThresholdScaling=0.3\");\n+        System.out.println(\"PASSED\");\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.VECTOR_CAST_F2HF, \"> 0\"})\n+    public void test_float_float16(short[] sout, float[] finp) {\n+        for (int i = 0; i < finp.length; i++) {\n+            sout[i] = Float.floatToFloat16(finp[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"test_float_float16\"}, mode = RunMode.STANDALONE)\n+    public void kernel_test_float_float16() {\n+        finp = new float[ARRLEN];\n+        sout = new short[ARRLEN];\n+\n+        for (int i = 0; i < ARRLEN; i++) {\n+            finp[i] = (float) i * 1.4f;\n+        }\n+\n+        for (int i = 0; i < ITERS; i++) {\n+            test_float_float16(sout, finp);\n+        }\n+\n+        \/\/ Verifying the result\n+        for (int i = 0; i < ARRLEN; i++) {\n+            Asserts.assertEquals(Float.floatToFloat16(finp[i]), sout[i]);\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.VECTOR_CAST_HF2F, \"> 0\"})\n+    public void test_float16_float(float[] fout, short[] sinp) {\n+        for (int i = 0; i < sinp.length; i++) {\n+            fout[i] = Float.float16ToFloat(sinp[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"test_float16_float\"}, mode = RunMode.STANDALONE)\n+    public void kernel_test_float16_float() {\n+        sinp = new short[ARRLEN];\n+        fout = new float[ARRLEN];\n+\n+        for (int i = 0; i < ARRLEN; i++) {\n+            sinp[i] = (short)i;\n+        }\n+\n+        for (int i = 0; i < ITERS; i++) {\n+            test_float16_float(fout, sinp);\n+        }\n+\n+        \/\/ Verifying the result\n+        for (int i = 0; i < ARRLEN; i++) {\n+            Asserts.assertEquals(Float.float16ToFloat(sinp[i]), fout[i]);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVector.java","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -5856,0 +5856,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndByte128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrByte128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorByte128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotByte128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5856,0 +5856,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndByte256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrByte256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorByte256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotByte256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5856,0 +5856,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndByte512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrByte512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorByte512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotByte512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5856,0 +5856,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndByte64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrByte64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorByte64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotByte64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Byte64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5861,0 +5861,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndByteMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ByteMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrByteMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ByteMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorByteMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ByteMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotByteMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ByteMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/ByteMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4891,0 +4891,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndDouble128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrDouble128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorDouble128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotDouble128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4891,0 +4891,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndDouble256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrDouble256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorDouble256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotDouble256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4891,0 +4891,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndDouble512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrDouble512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorDouble512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotDouble512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4891,0 +4891,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndDouble64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrDouble64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorDouble64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotDouble64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Double64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4896,0 +4896,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndDoubleMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, DoubleMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrDoubleMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, DoubleMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorDoubleMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, DoubleMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotDoubleMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, DoubleMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/DoubleMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4870,0 +4870,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndFloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrFloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorFloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotFloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4870,0 +4870,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndFloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrFloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorFloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotFloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4870,0 +4870,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndFloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrFloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorFloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotFloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4870,0 +4870,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndFloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrFloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorFloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotFloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Float64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -4875,0 +4875,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndFloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, FloatMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrFloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, FloatMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorFloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, FloatMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotFloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, FloatMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/FloatMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n- * @modules jdk.incubator.foreign jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @modules jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n@@ -33,3 +34,3 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ResourceScope;\n-import jdk.incubator.foreign.ValueLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.ValueLayout;\n@@ -58,0 +59,2 @@\n+    static final ValueLayout.OfShort ELEMENT_LAYOUT = ValueLayout.JAVA_SHORT.withBitAlignment(8);\n+\n@@ -224,1 +227,1 @@\n-            ms.set(ValueLayout.JAVA_SHORT, i * SPECIES.elementSize() \/ 8 , a[i]);\n+            ms.set(ELEMENT_LAYOUT, i * SPECIES.elementSize() \/ 8 , a[i]);\n@@ -230,1 +233,1 @@\n-        return ms.toArray(ValueLayout.JAVA_SHORT);\n+        return ms.toArray(ELEMENT_LAYOUT);\n@@ -481,2 +484,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -510,2 +513,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -574,2 +577,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -605,2 +608,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat128VectorLoadStoreTests.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2790,0 +2790,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndHalffloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrHalffloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorHalffloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotHalffloat128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n- * @modules jdk.incubator.foreign jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @modules jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n@@ -33,3 +34,3 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ResourceScope;\n-import jdk.incubator.foreign.ValueLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.ValueLayout;\n@@ -58,0 +59,2 @@\n+    static final ValueLayout.OfShort ELEMENT_LAYOUT = ValueLayout.JAVA_SHORT.withBitAlignment(8);\n+\n@@ -224,1 +227,1 @@\n-            ms.set(ValueLayout.JAVA_SHORT, i * SPECIES.elementSize() \/ 8 , a[i]);\n+            ms.set(ELEMENT_LAYOUT, i * SPECIES.elementSize() \/ 8 , a[i]);\n@@ -230,1 +233,1 @@\n-        return ms.toArray(ValueLayout.JAVA_SHORT);\n+        return ms.toArray(ELEMENT_LAYOUT);\n@@ -481,2 +484,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -510,2 +513,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -574,2 +577,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -605,2 +608,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat256VectorLoadStoreTests.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2790,0 +2790,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndHalffloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrHalffloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorHalffloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotHalffloat256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n- * @modules jdk.incubator.foreign jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @modules jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n@@ -33,3 +34,3 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ResourceScope;\n-import jdk.incubator.foreign.ValueLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.ValueLayout;\n@@ -58,0 +59,2 @@\n+    static final ValueLayout.OfShort ELEMENT_LAYOUT = ValueLayout.JAVA_SHORT.withBitAlignment(8);\n+\n@@ -224,1 +227,1 @@\n-            ms.set(ValueLayout.JAVA_SHORT, i * SPECIES.elementSize() \/ 8 , a[i]);\n+            ms.set(ELEMENT_LAYOUT, i * SPECIES.elementSize() \/ 8 , a[i]);\n@@ -230,1 +233,1 @@\n-        return ms.toArray(ValueLayout.JAVA_SHORT);\n+        return ms.toArray(ELEMENT_LAYOUT);\n@@ -481,2 +484,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -510,2 +513,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -574,2 +577,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -605,2 +608,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat512VectorLoadStoreTests.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2790,0 +2790,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndHalffloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrHalffloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorHalffloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotHalffloat512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n- * @modules jdk.incubator.foreign jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @modules jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n@@ -33,3 +34,3 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ResourceScope;\n-import jdk.incubator.foreign.ValueLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.ValueLayout;\n@@ -58,0 +59,2 @@\n+    static final ValueLayout.OfShort ELEMENT_LAYOUT = ValueLayout.JAVA_SHORT.withBitAlignment(8);\n+\n@@ -224,1 +227,1 @@\n-            ms.set(ValueLayout.JAVA_SHORT, i * SPECIES.elementSize() \/ 8 , a[i]);\n+            ms.set(ELEMENT_LAYOUT, i * SPECIES.elementSize() \/ 8 , a[i]);\n@@ -230,1 +233,1 @@\n-        return ms.toArray(ValueLayout.JAVA_SHORT);\n+        return ms.toArray(ELEMENT_LAYOUT);\n@@ -481,2 +484,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -510,2 +513,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -574,2 +577,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -605,2 +608,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat64VectorLoadStoreTests.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2790,0 +2790,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndHalffloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrHalffloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorHalffloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotHalffloat64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Halffloat64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Halffloat64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n- * @modules jdk.incubator.foreign jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @modules jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n@@ -34,3 +35,3 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ResourceScope;\n-import jdk.incubator.foreign.ValueLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.ValueLayout;\n@@ -60,0 +61,2 @@\n+    static final ValueLayout.OfShort ELEMENT_LAYOUT = ValueLayout.JAVA_SHORT.withBitAlignment(8);\n+\n@@ -231,1 +234,1 @@\n-            ms.set(ValueLayout.JAVA_SHORT, i * SPECIES.elementSize() \/ 8 , a[i]);\n+            ms.set(ELEMENT_LAYOUT, i * SPECIES.elementSize() \/ 8 , a[i]);\n@@ -237,1 +240,1 @@\n-        return ms.toArray(ValueLayout.JAVA_SHORT);\n+        return ms.toArray(ELEMENT_LAYOUT);\n@@ -488,2 +491,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -517,2 +520,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -581,2 +584,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n@@ -612,2 +615,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, Halffloat.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), Halffloat.SIZE, SegmentScope.auto());\n","filename":"test\/jdk\/jdk\/incubator\/vector\/HalffloatMaxVectorLoadStoreTests.java","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2795,0 +2795,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndHalffloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, HalffloatMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrHalffloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, HalffloatMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorHalffloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, HalffloatMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotHalffloatMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, HalffloatMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/HalffloatMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5889,0 +5889,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndInt128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrInt128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorInt128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotInt128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5889,0 +5889,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndInt256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrInt256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorInt256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotInt256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5889,0 +5889,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndInt512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrInt512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorInt512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotInt512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5889,0 +5889,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndInt64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrInt64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorInt64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotInt64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Int64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5894,0 +5894,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndIntMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, IntMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrIntMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, IntMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorIntMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, IntMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotIntMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, IntMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/IntMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5775,0 +5775,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndLong128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrLong128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorLong128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotLong128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5775,0 +5775,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndLong256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrLong256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorLong256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotLong256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5775,0 +5775,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndLong512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrLong512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorLong512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotLong512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5775,0 +5775,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndLong64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrLong64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorLong64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotLong64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Long64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5780,0 +5780,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndLongMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, LongMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrLongMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, LongMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorLongMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, LongMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotLongMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, LongMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/LongMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5841,0 +5841,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndShort128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short128VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrShort128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short128VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorShort128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short128VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotShort128VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short128VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short128VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5841,0 +5841,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndShort256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short256VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrShort256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short256VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorShort256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short256VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotShort256VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short256VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short256VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5841,0 +5841,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndShort512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short512VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrShort512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short512VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorShort512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short512VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotShort512VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short512VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short512VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5841,0 +5841,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndShort64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short64VectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrShort64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short64VectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorShort64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short64VectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotShort64VectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, Short64VectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short64VectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -5846,0 +5846,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndShortMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ShortMaxVectorTests::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOrShortMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ShortMaxVectorTests::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXorShortMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ShortMaxVectorTests::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNotShortMaxVectorTestsSmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, ShortMaxVectorTests::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/ShortMaxVectorTests.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -229,0 +229,1 @@\n+  gen_perf_tests=true\n","filename":"test\/jdk\/jdk\/incubator\/vector\/gen-template.sh","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -68,1 +68,0 @@\n-    LayoutType=$TYPE\n@@ -117,1 +116,0 @@\n-        LayoutType=SHORT\n@@ -129,1 +127,1 @@\n-    args=\"$args -Dfptype=$fptype -DFptype=$Fptype -DBoxfptype=$Boxfptype -DLayoutType=$LayoutType\"\n+    args=\"$args -Dfptype=$fptype -DFptype=$Fptype -DBoxfptype=$Boxfptype\"\n","filename":"test\/jdk\/jdk\/incubator\/vector\/gen-tests.sh","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -397,0 +397,76 @@\n+    static boolean band(boolean a, boolean b) {\n+        return a & b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAnd$vectorteststype$SmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.and(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, $vectorteststype$::band);\n+    }\n+\n+    static boolean bor(boolean a, boolean b) {\n+        return a | b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskOr$vectorteststype$SmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.or(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, $vectorteststype$::bor);\n+    }\n+\n+    static boolean bxor(boolean a, boolean b) {\n+        return a != b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskXor$vectorteststype$SmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.xor(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, $vectorteststype$::bxor);\n+    }\n+\n+    static boolean bandNot(boolean a, boolean b) {\n+        return a & !b;\n+    }\n+\n+    @Test(dataProvider = \"maskCompareOpProvider\")\n+    static void maskAndNot$vectorteststype$SmokeTest(IntFunction<boolean[]> fa, IntFunction<boolean[]> fb) {\n+        boolean[] a = fa.apply(SPECIES.length());\n+        boolean[] b = fb.apply(SPECIES.length());\n+        boolean[] r = new boolean[a.length];\n+\n+        for (int i = 0; i < a.length; i += SPECIES.length()) {\n+            var av = SPECIES.loadMask(a, i);\n+            var bv = SPECIES.loadMask(b, i);\n+            var cv = av.andNot(bv);\n+            cv.intoArray(r, i);\n+        }\n+        assertArraysEquals(r, a, b, $vectorteststype$::bandNot);\n+    }\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/templates\/Unit-Miscellaneous.template","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -30,1 +30,2 @@\n- * @modules jdk.incubator.foreign jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @modules jdk.incubator.vector java.base\/jdk.internal.vm.annotation\n@@ -42,3 +43,3 @@\n-import jdk.incubator.foreign.MemorySegment;\n-import jdk.incubator.foreign.ResourceScope;\n-import jdk.incubator.foreign.ValueLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.ValueLayout;\n@@ -77,0 +78,6 @@\n+#if[Halffloat]\n+    static final ValueLayout.OfShort ELEMENT_LAYOUT = ValueLayout.JAVA_SHORT.withBitAlignment(8);\n+#else[Halffloat]\n+    static final ValueLayout.Of$Type$ ELEMENT_LAYOUT = ValueLayout.JAVA_$TYPE$.withBitAlignment(8);\n+#end[Halffloat]\n+\n@@ -250,1 +257,1 @@\n-            ms.set(ValueLayout.JAVA_$LayoutType$, i * SPECIES.elementSize() \/ 8 , a[i]);\n+            ms.set(ELEMENT_LAYOUT, i * SPECIES.elementSize() \/ 8 , a[i]);\n@@ -256,1 +263,1 @@\n-        return ms.toArray(ValueLayout.JAVA_$LayoutType$);\n+        return ms.toArray(ELEMENT_LAYOUT);\n@@ -507,2 +514,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, $Boxtype$.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), $Boxtype$.SIZE, SegmentScope.auto());\n@@ -536,2 +543,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, $Boxtype$.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), $Boxtype$.SIZE, SegmentScope.auto());\n@@ -600,2 +607,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, $Boxtype$.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), $Boxtype$.SIZE, SegmentScope.auto());\n@@ -631,2 +638,2 @@\n-        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, ResourceScope.newImplicitScope()));\n-        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), ResourceScope.newImplicitScope());\n+        MemorySegment a = toSegment(fa.apply(SPECIES.length()), i -> MemorySegment.allocateNative(i, $Boxtype$.SIZE, SegmentScope.auto()));\n+        MemorySegment r = MemorySegment.allocateNative(a.byteSize(), $Boxtype$.SIZE, SegmentScope.auto());\n","filename":"test\/jdk\/jdk\/incubator\/vector\/templates\/X-LoadStoreTest.java.template","additions":21,"deletions":14,"binary":false,"changes":35,"status":"modified"}]}