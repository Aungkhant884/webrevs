{"files":[{"patch":"@@ -2437,0 +2437,8 @@\n+    case Op_PopCountI:\n+    case Op_PopCountL:\n+    case Op_PopCountVI:\n+    case Op_PopCountVL:\n+      if (!UsePopCountInstruction) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -2477,0 +2485,3 @@\n+    case Op_CompressV:\n+    case Op_CompressM:\n+    case Op_ExpandV:\n@@ -8644,1 +8655,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8655,0 +8665,1 @@\n+    assert(UsePopCountInstruction, \"unsupported\");\n@@ -8666,1 +8677,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8676,0 +8686,1 @@\n+    assert(UsePopCountInstruction, \"unsupported\");\n@@ -8689,1 +8700,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8699,0 +8709,1 @@\n+    assert(UsePopCountInstruction, \"unsupported\");\n@@ -8709,1 +8720,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8719,0 +8729,1 @@\n+    assert(UsePopCountInstruction, \"unsupported\");\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -5686,2 +5686,2 @@\n-instruct vpopcount4I(vecX dst, vecX src) %{\n-  predicate(UsePopCountInstruction && n->as_Vector()->length() == 4);\n+instruct vpopcountID(vecD dst, vecD src) %{\n+  predicate(n->as_Vector()->length_in_bytes() < 16);\n@@ -5689,4 +5689,15 @@\n-  format %{\n-    \"cnt     $dst, $src\\t# vector (16B)\\n\\t\"\n-    \"uaddlp  $dst, $dst\\t# vector (16B)\\n\\t\"\n-    \"uaddlp  $dst, $dst\\t# vector (8H)\"\n+  ins_cost(3 * INSN_COST);\n+  format %{ \"vpopcountI  $dst, $src\\t# vector (8B\/4H\/2S)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ cnt(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg));\n+    if (bt == T_SHORT || bt == T_INT) {\n+      __ uaddlp(as_FloatRegister($dst$$reg), __ T8B,\n+                as_FloatRegister($dst$$reg));\n+    }\n+    if (bt == T_INT) {\n+      __ uaddlp(as_FloatRegister($dst$$reg), __ T4H,\n+                as_FloatRegister($dst$$reg));\n+    }\n@@ -5694,0 +5705,8 @@\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vpopcountIX(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (PopCountVI src));\n+  ins_cost(3 * INSN_COST);\n+  format %{ \"vpopcountI  $dst, $src\\t# vector (16B\/8H\/4S)\" %}\n@@ -5695,0 +5714,28 @@\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ cnt(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg));\n+    if (bt == T_SHORT || bt == T_INT) {\n+      __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n+                as_FloatRegister($dst$$reg));\n+    }\n+    if (bt == T_INT) {\n+      __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n+                as_FloatRegister($dst$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ If the PopCountVL is generated by auto-vectorization, the dst basic\n+\/\/ type is T_INT. And once we have unified the type definition for\n+\/\/ Vector API and auto-vectorization, this rule can be merged with\n+\/\/ \"vpopcountLX\" rule.\n+instruct vpopcountLD(vecD dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() < 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (PopCountVL src));\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"vpopcountL  $dst, $src\\t# vector (2S)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");\n@@ -5701,0 +5748,4 @@\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T4S,\n+              as_FloatRegister($dst$$reg));\n+    __ xtn(as_FloatRegister($dst$$reg), __ T2S,\n+           as_FloatRegister($dst$$reg), __ T2D);\n@@ -5705,8 +5756,6 @@\n-instruct vpopcount2I(vecD dst, vecD src) %{\n-  predicate(UsePopCountInstruction && n->as_Vector()->length() == 2);\n-  match(Set dst (PopCountVI src));\n-  format %{\n-    \"cnt     $dst, $src\\t# vector (8B)\\n\\t\"\n-    \"uaddlp  $dst, $dst\\t# vector (8B)\\n\\t\"\n-    \"uaddlp  $dst, $dst\\t# vector (4H)\"\n-  %}\n+instruct vpopcountLX(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (PopCountVL src));\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"vpopcountL  $dst, $src\\t# vector (2D)\" %}\n@@ -5714,1 +5763,2 @@\n-    __ cnt(as_FloatRegister($dst$$reg), __ T8B,\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    __ cnt(as_FloatRegister($dst$$reg), __ T16B,\n@@ -5716,1 +5766,1 @@\n-    __ uaddlp(as_FloatRegister($dst$$reg), __ T8B,\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n@@ -5718,1 +5768,3 @@\n-    __ uaddlp(as_FloatRegister($dst$$reg), __ T4H,\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n+              as_FloatRegister($dst$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T4S,\n@@ -5924,0 +5976,128 @@\n+\n+\/\/------------------------- CountLeadingZerosV -----------------------------\n+\n+instruct countLeadingZerosVD(vecD dst, vecD src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (CountLeadingZerosV src));\n+  ins_cost(INSN_COST);\n+  format %{ \"countLeadingZerosV $dst, $src\\t# vector (8B\/4H\/2S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), false);\n+    __ clz(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct countLeadingZerosVX(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (CountLeadingZerosV src));\n+  ins_cost(INSN_COST);\n+  format %{ \"countLeadingZerosV $dst, $src\\t# vector (16B\/8H\/4S\/2D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), true);\n+    if (bt != T_LONG) {\n+      __ clz(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg));\n+    } else {\n+      __ umov(rscratch1, as_FloatRegister($src$$reg), __ D, 0);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov(as_FloatRegister($dst$$reg), __ D, 0, rscratch1);\n+      __ umov(rscratch1, as_FloatRegister($src$$reg), __ D, 1);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov(as_FloatRegister($dst$$reg), __ D, 1, rscratch1);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/------------------------- CountTrailingZerosV ----------------------------\n+\n+instruct countTrailingZerosVD(vecD dst, vecD src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (CountTrailingZerosV src));\n+  ins_cost(3 * INSN_COST);\n+  format %{ \"countTrailingZerosV $dst, $src\\t# vector (8B\/4H\/2S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), false);\n+    __ neon_reverse_bits(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, false);\n+    __ clz(as_FloatRegister($dst$$reg), size, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct countTrailingZerosVX(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (CountTrailingZerosV src));\n+  ins_cost(3 * INSN_COST);\n+  format %{ \"countTrailingZerosV $dst, $src\\t# vector (16B\/8H\/4S\/2D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), true);\n+    __ neon_reverse_bits(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, true);\n+    if (bt != T_LONG) {\n+      __ clz(as_FloatRegister($dst$$reg), size, as_FloatRegister($dst$$reg));\n+    } else {\n+      __ umov(rscratch1, as_FloatRegister($dst$$reg), __ D, 0);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov(as_FloatRegister($dst$$reg), __ D, 0, rscratch1);\n+      __ umov(rscratch1, as_FloatRegister($dst$$reg), __ D, 1);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov(as_FloatRegister($dst$$reg), __ D, 1, rscratch1);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/------------------------------ ReverseV -----------------------------------\n+\n+instruct vreverseD(vecD dst, vecD src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (ReverseV src));\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"ReverseV $dst, $src\\t# vector (D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ neon_reverse_bits(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vreverseX(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (ReverseV src));\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"ReverseV $dst, $src\\t# vector (X)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ neon_reverse_bits(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/---------------------------- ReverseBytesV --------------------------------\n+\n+instruct vreverseBytesD(vecD dst, vecD src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (ReverseBytesV src));\n+  ins_cost(INSN_COST);\n+  format %{ \"ReverseBytesV $dst, $src\\t# vector (D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ neon_reverse_bytes(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vreverseBytesX(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (ReverseBytesV src));\n+  ins_cost(INSN_COST);\n+  format %{ \"ReverseBytesV $dst, $src\\t# vector (X)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ neon_reverse_bytes(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":197,"deletions":17,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -2448,13 +2448,21 @@\n-define(`VPOPCOUNT', `\n-instruct vpopcount$1$2`'(vec$5 dst, vec$5 src) %{\n-  predicate(UsePopCountInstruction && n->as_Vector()->length() == $1);\n-  match(Set dst (PopCountVI src));\n-  format %{\n-    \"cnt     $dst, $src\\t# vector ($3B)\\n\\t\"\n-    \"uaddlp  $dst, $dst\\t# vector ($3B)\\n\\t\"\n-    \"uaddlp  $dst, $dst\\t# vector ($4H)\"\n-  %}\n-  ins_encode %{\n-    __ cnt(as_FloatRegister($dst$$reg), __ T$3B,\n-           as_FloatRegister($src$$reg));\n-    __ uaddlp(as_FloatRegister($dst$$reg), __ T$3B,\n+define(`VPOPCOUNT', `dnl\n+ifelse($1$2, `LD', `\n+\/\/ If the PopCountVL is generated by auto-vectorization, the dst basic\n+\/\/ type is T_INT. And once we have unified the type definition for\n+\/\/ Vector API and auto-vectorization, this rule can be merged with\n+\/\/ \"vpopcountLX\" rule.', `')\n+instruct vpopcount$1$2`'(vec$2 dst, vec$3 src) %{\n+  predicate(n->as_Vector()->length_in_bytes() $4 16`'ifelse($1$2, `LD', ` &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_INT', $1$2, `LX', ` &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG', `'));\n+  match(Set dst (PopCountV$1 src));\n+  ins_cost($5 * INSN_COST);\n+  format %{ \"vpopcount$1  $dst, $src\\t# vector ($6)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");dnl\n+ifelse($1, `I', `\n+    BasicType bt = Matcher::vector_element_basic_type(this);', `')\n+    __ cnt(as_FloatRegister($dst$$reg), __ T`'ifelse($3, D, 8, 16)B,\n+           as_FloatRegister($src$$reg));dnl\n+ifelse($1, `L', `\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n@@ -2462,1 +2470,1 @@\n-    __ uaddlp(as_FloatRegister($dst$$reg), __ T$4H,\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n@@ -2464,0 +2472,13 @@\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T4S,\n+              as_FloatRegister($dst$$reg));', `\n+    if (bt == T_SHORT || bt == T_INT) {\n+      __ uaddlp(as_FloatRegister($dst$$reg), __ T`'ifelse($2, D, 8, 16)B,\n+                as_FloatRegister($dst$$reg));\n+    }\n+    if (bt == T_INT) {\n+      __ uaddlp(as_FloatRegister($dst$$reg), __ T`'ifelse($2, D, 4, 8)H,\n+                as_FloatRegister($dst$$reg));\n+    }')dnl\n+ifelse($1$2, `LD', `\n+    __ xtn(as_FloatRegister($dst$$reg), __ T2S,\n+           as_FloatRegister($dst$$reg), __ T2D);', `')\n@@ -2467,3 +2488,5 @@\n-dnl       $1 $2 $3  $4 $5\n-VPOPCOUNT(4, I, 16, 8, X)\n-VPOPCOUNT(2, I, 8,  4, D)\n+dnl       $1 $2 $3 $4  $5 $6\n+VPOPCOUNT(I, D, D, <,  3, 8B\/4H\/2S)\n+VPOPCOUNT(I, X, X, ==, 3, 16B\/8H\/4S)\n+VPOPCOUNT(L, D, X, <,  5, 2S)\n+VPOPCOUNT(L, X, X, ==, 4, 2D)\n@@ -2650,0 +2673,78 @@\n+\n+dnl\n+dnl CLTZ_D($1     )\n+dnl CLTZ_D(op_name)\n+define(`CLTZ_D', `\n+instruct count$1D(vecD dst, vecD src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (Count$1 src));\n+  ins_cost(ifelse($1, `TrailingZerosV', `3 * ', `')INSN_COST);\n+  format %{ \"count$1 $dst, $src\\t# vector (8B\/4H\/2S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), false);dnl\n+ifelse($1, `TrailingZerosV', `\n+    __ neon_reverse_bits(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, false);', `')\n+    __ clz(as_FloatRegister($dst$$reg), size, as_FloatRegister($ifelse($1, `TrailingZerosV', dst, src)$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl CLTZ_X($1     )\n+dnl CLTZ_X(op_name)\n+define(`CLTZ_X', `\n+instruct count$1X(vecX dst, vecX src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (Count$1 src));\n+  ins_cost(ifelse($1, `TrailingZerosV', `3 * ', `')INSN_COST);\n+  format %{ \"count$1 $dst, $src\\t# vector (16B\/8H\/4S\/2D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), true);dnl\n+ifelse($1, `TrailingZerosV', `\n+    __ neon_reverse_bits(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, true);', `')\n+    if (bt != T_LONG) {\n+      __ clz(as_FloatRegister($dst$$reg), size, as_FloatRegister($ifelse($1, `TrailingZerosV', dst, src)$$reg));\n+    } else {\n+      __ umov(rscratch1, as_FloatRegister($ifelse($1, `TrailingZerosV', dst, src)$$reg), __ D, 0);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov(as_FloatRegister($dst$$reg), __ D, 0, rscratch1);\n+      __ umov(rscratch1, as_FloatRegister($ifelse($1, `TrailingZerosV', dst, src)$$reg), __ D, 1);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov(as_FloatRegister($dst$$reg), __ D, 1, rscratch1);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/------------------------- CountLeadingZerosV -----------------------------\n+CLTZ_D(LeadingZerosV)\n+CLTZ_X(LeadingZerosV)\n+\n+\/\/------------------------- CountTrailingZerosV ----------------------------\n+CLTZ_D(TrailingZerosV)\n+CLTZ_X(TrailingZerosV)\n+\n+dnl\n+dnl REVERSE($1,        $2,      $3,   $4  )\n+dnl REVERSE(insn_name, op_name, type, insn)\n+define(`REVERSE', `\n+instruct $1(vec$3 dst, vec$3 src) %{\n+  predicate(n->as_Vector()->length_in_bytes() == ifelse($3, D, 8, 16));\n+  match(Set dst ($2 src));\n+  ins_cost(ifelse($2, `ReverseV', `2 * ', `')INSN_COST);\n+  format %{ \"$2 $dst, $src\\t# vector ($3)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ $4(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), bt, ifelse($3, D, false, true));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/------------------------------ ReverseV -----------------------------------\n+REVERSE(vreverseD, ReverseV, D, neon_reverse_bits)\n+REVERSE(vreverseX, ReverseV, X, neon_reverse_bits)\n+\n+\/\/---------------------------- ReverseBytesV --------------------------------\n+REVERSE(vreverseBytesD, ReverseBytesV, D, neon_reverse_bytes)\n+REVERSE(vreverseBytesX, ReverseBytesV, X, neon_reverse_bytes)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":118,"deletions":17,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -152,0 +152,2 @@\n+      case Op_ExpandV:\n+        if (UseSVE < 2 || is_subword_type(bt)) return false;\n@@ -2200,1 +2202,1 @@\n-\/\/ popcount vector\n+\/\/ vector popcount\n@@ -2203,1 +2205,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2205,1 +2208,40 @@\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_cnt(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vpopcountL(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector() &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (PopCountVL src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    __ sve_cnt(as_FloatRegister($dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ If the PopCountVL is generated by auto-vectorization, the dst basic\n+\/\/ type is T_INT. And once we have unified the type definition for\n+\/\/ Vector API and auto-vectorization, this rule can be merged with\n+\/\/ \"vpopcountL\" rule.\n+instruct vpopcountLI(vReg dst, vReg src, vReg vtmp) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector() &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (PopCountVL src));\n+  effect(TEMP_DEF dst, TEMP vtmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_cnt $dst, $src\\n\\t\"\n+            \"sve_dup $vtmp, #0\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $vtmp\\t# vector (sve) (S)\" %}\n@@ -2207,1 +2249,35 @@\n-     __ sve_cnt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg));\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    __ sve_cnt(as_FloatRegister($dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector popcount - predicated\n+\n+instruct vpopcountI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (PopCountVI dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cnt $dst_src, $pg, $dst_src\\t# vector (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_cnt(as_FloatRegister($dst_src$$reg), __ elemType_to_regVariant(bt),\n+         as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vpopcountL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst_src (PopCountVL dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cnt $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    __ sve_cnt(as_FloatRegister($dst_src$$reg), __ D,\n+         as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n@@ -5653,0 +5729,98 @@\n+\/\/ ---------------------------- Compress\/Expand Operations ---------------------------\n+\n+instruct mcompress(pReg dst, pReg pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (CompressM pg));\n+  effect(KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cntp rscratch1, $pg\\n\\t\"\n+            \"sve_whilelo $dst, zr, rscratch1\\t# mask compress (B\/H\/S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp(rscratch1, size, ptrue, as_PRegister($pg$$reg));\n+    __ sve_whilelo(as_PRegister($dst$$reg), size, zr, rscratch1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompress(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (CompressV src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_compact $dst, $src, $pg\\t# vector compress (S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_compact(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg), as_PRegister($pg$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompressB(vReg dst, vReg src, pReg pg, vReg vtmp1, vReg vtmp2, vReg vtmp3, vReg vtmp4,\n+                    pReg ptmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP ptmp, TEMP pgtmp);\n+  match(Set dst (CompressV src pg));\n+  ins_cost(13 * SVE_COST);\n+  format %{ \"sve_compact $dst, $src, $pg\\t# vector compress (B)\" %}\n+  ins_encode %{\n+    __ sve_compress_byte(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                         as_FloatRegister($vtmp1$$reg),as_FloatRegister($vtmp2$$reg),\n+                         as_FloatRegister($vtmp3$$reg),as_FloatRegister($vtmp4$$reg),\n+                         as_PRegister($ptmp$$reg), as_PRegister($pgtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompressS(vReg dst, vReg src, pReg pg, vReg vtmp1, vReg vtmp2, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp);\n+  match(Set dst (CompressV src pg));\n+  ins_cost(38 * SVE_COST);\n+  format %{ \"sve_compact $dst, $src, $pg\\t# vector compress (H)\" %}\n+  ins_encode %{\n+    __ sve_compress_short(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                          as_FloatRegister($vtmp1$$reg),as_FloatRegister($vtmp2$$reg), as_PRegister($pgtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vexpand(vReg dst, vReg src, pRegGov pg) %{\n+  match(Set dst (ExpandV src pg));\n+  effect(TEMP_DEF dst);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_dup $dst, S\/D, 0\\n\\t\"\n+            \"sve_histcnt $dst, S\/D, $pg, $dst, $dst\\n\\t\"\n+            \"sve_sub $dst, S\/D, 1\\n\\t\"\n+            \"sve_tbl $dst, S\/D, $src, $dst\\t# vector expand (S\/D)\" %}\n+  ins_encode %{\n+    \/\/ Example input:   src   = 1 2 3 4 5 6 7 8\n+    \/\/                  pg    = 1 0 0 1 1 0 1 1\n+    \/\/ Expected result: dst   = 4 0 0 5 6 0 7 8\n+\n+    \/\/ The basic idea is to use TBL which can shuffle the elements in the given\n+    \/\/ vector flexibly. HISTCNT + SUB is used to generate the second source input\n+    \/\/ for TBL whose value is used to select the indexed element from src vector.\n+\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    assert(UseSVE == 2 && !is_subword_type(bt), \"unsupported\");\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    \/\/ dst = 0 0 0 0 0 0 0 0\n+    __ sve_dup(as_FloatRegister($dst$$reg), size, 0);\n+    \/\/ dst = 5 0 0 4 3 0 2 1\n+    __ sve_histcnt(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+                   as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+    \/\/ dst = 4 -1 -1 3 2 -1 1 0\n+    __ sve_sub(as_FloatRegister($dst$$reg), size, 1);\n+    \/\/ dst = 4 0 0 5 6 0 7 8\n+    __ sve_tbl(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg),\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -5666,0 +5840,144 @@\n+\n+\/\/ ------------------------------ CountLeadingZerosV ------------------------------\n+\n+instruct vcountLeadingZeros(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (CountLeadingZerosV src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_clz $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_clz(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vcountLeadingZeros_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (CountLeadingZerosV dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_clz $dst_src, $pg, $dst_src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_clz(as_FloatRegister($dst_src$$reg), size,\n+        as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ CountTrailingZerosV -----------------------------\n+\n+instruct vcountTrailingZeros(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (CountTrailingZerosV src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_rbit $dst, $src\\n\\t\"\n+            \"sve_clz  $dst, $dst\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_rbit(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_clz(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vcountTrailingZeros_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (CountTrailingZerosV dst_src pg));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_rbit $dst_src, $pg, $dst_src\\n\\t\"\n+            \"sve_clz  $dst_src, $pg, $dst_src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_rbit(as_FloatRegister($dst_src$$reg), size,\n+        as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+    __ sve_clz(as_FloatRegister($dst_src$$reg), size,\n+        as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ---------------------------------- ReverseV ------------------------------------\n+\n+instruct vreverse(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (ReverseV src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_rbit $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_rbit(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vreverse_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (ReverseV dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_rbit $dst_src, $pg, $dst_src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_rbit(as_FloatRegister($dst_src$$reg), size,\n+        as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ -------------------------------- ReverseBytesV ---------------------------------\n+\n+instruct vreverseBytes(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (ReverseBytesV src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_revb $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (bt == T_BYTE) {\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+    } else {\n+      __ sve_revb(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($src$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vreverseBytes_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (ReverseBytesV dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_revb $dst_src, $pg, $dst_src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (bt == T_BYTE) {\n+      \/\/ do nothing\n+    } else {\n+      __ sve_revb(as_FloatRegister($dst_src$$reg), size,\n+          as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":322,"deletions":4,"binary":false,"changes":326,"status":"modified"},{"patch":"@@ -147,0 +147,2 @@\n+      case Op_ExpandV:\n+        if (UseSVE < 2 || is_subword_type(bt)) return false;\n@@ -1173,1 +1175,24 @@\n-\/\/ popcount vector\n+dnl\n+dnl VPOPCOUNT($1,          $2  )\n+dnl VPOPCOUNT(name_suffix, size)\n+define(`VPOPCOUNT', `\n+instruct vpopcount$1(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector()`'ifelse($1, `L', ` &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG', `'));\n+  match(Set dst (PopCountV$1 src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) ($2)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");dnl\n+ifelse($1, `I', `\n+    BasicType bt = Matcher::vector_element_basic_type(this);', `')\n+    __ sve_cnt(as_FloatRegister($dst$$reg), ifelse($1, `I', `__ elemType_to_regVariant(bt)', `__ D'),\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector popcount\n+VPOPCOUNT(I, B\/H\/S)\n+VPOPCOUNT(L, D)\n@@ -1175,4 +1200,14 @@\n-instruct vpopcountI(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (PopCountVI src));\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n+\/\/ If the PopCountVL is generated by auto-vectorization, the dst basic\n+\/\/ type is T_INT. And once we have unified the type definition for\n+\/\/ Vector API and auto-vectorization, this rule can be merged with\n+\/\/ \"vpopcountL\" rule.\n+instruct vpopcountLI(vReg dst, vReg src, vReg vtmp) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector() &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (PopCountVL src));\n+  effect(TEMP_DEF dst, TEMP vtmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_cnt $dst, $src\\n\\t\"\n+            \"sve_dup $vtmp, #0\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $vtmp\\t# vector (sve) (S)\" %}\n@@ -1180,1 +1215,5 @@\n-     __ sve_cnt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg));\n+    assert(UsePopCountInstruction, \"unsupported\");\n+    __ sve_cnt(as_FloatRegister($dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($vtmp$$reg));\n@@ -1185,0 +1224,23 @@\n+dnl\n+dnl VPOPCOUNT_PREDICATE($1,          $2  )\n+dnl VPOPCOUNT_PREDICATE(name_suffix, size)\n+define(`VPOPCOUNT_PREDICATE', `\n+instruct vpopcount$1_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0`'ifelse($1, `L', ` &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG', `'));\n+  match(Set dst_src (PopCountV$1 dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cnt $dst_src, $pg, $dst_src\\t# vector (sve) ($2)\" %}\n+  ins_encode %{\n+    assert(UsePopCountInstruction, \"unsupported\");dnl\n+ifelse($1, `I', `\n+    BasicType bt = Matcher::vector_element_basic_type(this);', `')\n+    __ sve_cnt(as_FloatRegister($dst_src$$reg), ifelse($1, `I', `__ elemType_to_regVariant(bt)', `__ D'),\n+         as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\/\/ vector popcount - predicated\n+VPOPCOUNT_PREDICATE(I, B\/H\/S)\n+VPOPCOUNT_PREDICATE(L, D)\n+\n@@ -3183,0 +3245,98 @@\n+\/\/ ---------------------------- Compress\/Expand Operations ---------------------------\n+\n+instruct mcompress(pReg dst, pReg pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (CompressM pg));\n+  effect(KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cntp rscratch1, $pg\\n\\t\"\n+            \"sve_whilelo $dst, zr, rscratch1\\t# mask compress (B\/H\/S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp(rscratch1, size, ptrue, as_PRegister($pg$$reg));\n+    __ sve_whilelo(as_PRegister($dst$$reg), size, zr, rscratch1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompress(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (CompressV src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_compact $dst, $src, $pg\\t# vector compress (S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_compact(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg), as_PRegister($pg$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompressB(vReg dst, vReg src, pReg pg, vReg vtmp1, vReg vtmp2, vReg vtmp3, vReg vtmp4,\n+                    pReg ptmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP ptmp, TEMP pgtmp);\n+  match(Set dst (CompressV src pg));\n+  ins_cost(13 * SVE_COST);\n+  format %{ \"sve_compact $dst, $src, $pg\\t# vector compress (B)\" %}\n+  ins_encode %{\n+    __ sve_compress_byte(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                         as_FloatRegister($vtmp1$$reg),as_FloatRegister($vtmp2$$reg),\n+                         as_FloatRegister($vtmp3$$reg),as_FloatRegister($vtmp4$$reg),\n+                         as_PRegister($ptmp$$reg), as_PRegister($pgtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompressS(vReg dst, vReg src, pReg pg, vReg vtmp1, vReg vtmp2, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp);\n+  match(Set dst (CompressV src pg));\n+  ins_cost(38 * SVE_COST);\n+  format %{ \"sve_compact $dst, $src, $pg\\t# vector compress (H)\" %}\n+  ins_encode %{\n+    __ sve_compress_short(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                          as_FloatRegister($vtmp1$$reg),as_FloatRegister($vtmp2$$reg), as_PRegister($pgtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vexpand(vReg dst, vReg src, pRegGov pg) %{\n+  match(Set dst (ExpandV src pg));\n+  effect(TEMP_DEF dst);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_dup $dst, S\/D, 0\\n\\t\"\n+            \"sve_histcnt $dst, S\/D, $pg, $dst, $dst\\n\\t\"\n+            \"sve_sub $dst, S\/D, 1\\n\\t\"\n+            \"sve_tbl $dst, S\/D, $src, $dst\\t# vector expand (S\/D)\" %}\n+  ins_encode %{\n+    \/\/ Example input:   src   = 1 2 3 4 5 6 7 8\n+    \/\/                  pg    = 1 0 0 1 1 0 1 1\n+    \/\/ Expected result: dst   = 4 0 0 5 6 0 7 8\n+\n+    \/\/ The basic idea is to use TBL which can shuffle the elements in the given\n+    \/\/ vector flexibly. HISTCNT + SUB is used to generate the second source input\n+    \/\/ for TBL whose value is used to select the indexed element from src vector.\n+\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    assert(UseSVE == 2 && !is_subword_type(bt), \"unsupported\");\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    \/\/ dst = 0 0 0 0 0 0 0 0\n+    __ sve_dup(as_FloatRegister($dst$$reg), size, 0);\n+    \/\/ dst = 5 0 0 4 3 0 2 1\n+    __ sve_histcnt(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+                   as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+    \/\/ dst = 4 -1 -1 3 2 -1 1 0\n+    __ sve_sub(as_FloatRegister($dst$$reg), size, 1);\n+    \/\/ dst = 4 0 0 5 6 0 7 8\n+    __ sve_tbl(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg),\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -3196,0 +3356,76 @@\n+\n+dnl\n+dnl BITWISE_UNARY($1,        $2,      $3  )\n+dnl BITWISE_UNARY(insn_name, op_name, insn)\n+define(`BITWISE_UNARY', `\n+instruct $1(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst ($2 src));\n+  ins_cost(ifelse($2, `CountTrailingZerosV', `2 * ', `')SVE_COST);\n+  format %{ ifelse($2, `CountTrailingZerosV', `\"sve_rbit $dst, $src\\n\\t\"\n+            \"$3  $dst, $dst', `\"$3 $dst, $src')\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);dnl\n+ifelse($2, `CountTrailingZerosV', `\n+    __ sve_rbit(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($src$$reg));', `')dnl\n+ifelse($2, `ReverseBytesV', `\n+    if (bt == T_BYTE) {\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+    } else {\n+      __ $3(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($src$$reg));\n+    }', `\n+    __ $3(as_FloatRegister($dst$$reg), size, ptrue, as_FloatRegister($ifelse($2, `CountTrailingZerosV', dst, src)$$reg));')\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl BITWISE_UNARY_PREDICATE($1,        $2,      $3  )\n+dnl BITWISE_UNARY_PREDICATE(insn_name, op_name, insn)\n+define(`BITWISE_UNARY_PREDICATE', `\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct $1_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src pg));\n+  ins_cost(ifelse($2, `CountTrailingZerosV', `2 * ', `')SVE_COST);\n+  format %{ ifelse($2, `CountTrailingZerosV', `\"sve_rbit $dst_src, $pg, $dst_src\\n\\t\"\n+            \"$3  $dst_src, $pg, $dst_src', `\"$3 $dst_src, $pg, $dst_src')\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);dnl\n+ifelse($2, `CountTrailingZerosV', `\n+    __ sve_rbit(as_FloatRegister($dst_src$$reg), size,\n+        as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));', `')dnl\n+ifelse($2, `ReverseBytesV', `\n+    if (bt == T_BYTE) {\n+      \/\/ do nothing\n+    } else {\n+      __ $3(as_FloatRegister($dst_src$$reg), size,\n+          as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));\n+    }', `\n+    __ $3(as_FloatRegister($dst_src$$reg), size,\n+        as_PRegister($pg$$reg), as_FloatRegister($dst_src$$reg));')\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ ------------------------------ CountLeadingZerosV ------------------------------\n+BITWISE_UNARY(vcountLeadingZeros, CountLeadingZerosV, sve_clz)\n+BITWISE_UNARY_PREDICATE(vcountLeadingZeros, CountLeadingZerosV, sve_clz)\n+\n+\/\/ ------------------------------ CountTrailingZerosV -----------------------------\n+BITWISE_UNARY(vcountTrailingZeros, CountTrailingZerosV, sve_clz)\n+BITWISE_UNARY_PREDICATE(vcountTrailingZeros, CountTrailingZerosV, sve_clz)\n+\n+\/\/ ---------------------------------- ReverseV ------------------------------------\n+BITWISE_UNARY(vreverse, ReverseV, sve_rbit)\n+BITWISE_UNARY_PREDICATE(vreverse, ReverseV, sve_rbit)\n+\n+\/\/ -------------------------------- ReverseBytesV ---------------------------------\n+BITWISE_UNARY(vreverseBytes, ReverseBytesV, sve_revb)\n+BITWISE_UNARY_PREDICATE(vreverseBytes, ReverseBytesV, sve_revb)\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":242,"deletions":6,"binary":false,"changes":248,"status":"modified"},{"patch":"@@ -3137,0 +3137,1 @@\n+  INSN(sve_clz,  0b00000100, 0b011001101); \/\/ vector count leading zero bits\n@@ -3796,1 +3797,15 @@\n-  \/\/ SVE create index starting from and incremented by immediate\n+\/\/ SVE reverse within elements\n+#define INSN(NAME, opc, cond)                                                        \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg,  FloatRegister Zn) {  \\\n+    starti;                                                                          \\\n+    assert(cond, \"invalid size\");                                                    \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1001, 21, 18), f(opc, 17, 16);          \\\n+    f(0b100, 15, 13), pgrf(Pg, 10), rf(Zn, 5), rf(Zd, 0);                            \\\n+  }\n+\n+  INSN(sve_revb, 0b00, T == H || T == S || T == D);\n+  INSN(sve_rbit, 0b11, T != Q);\n+#undef INSN\n+\n+  \/\/ SVE Index Generation:\n+  \/\/ Create index starting from and incremented by immediate\n@@ -3799,0 +3814,1 @@\n+    assert(T != Q, \"invalid size\");\n@@ -3804,0 +3820,10 @@\n+  \/\/ SVE Index Generation:\n+  \/\/ Create index starting from general-purpose register and incremented by immediate\n+  void sve_index(FloatRegister Zd, SIMD_RegVariant T, Register Rn, int imm) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    f(0b00000100, 31, 24), f(T, 23, 22), f(0b1, 21);\n+    sf(imm, 20, 16), f(0b010001, 15, 10);\n+    zrf(Rn, 5), rf(Zd, 0);\n+  }\n+\n@@ -3812,0 +3838,17 @@\n+  \/\/ Shuffle active elements of vector to the right and fill with zero\n+  void sve_compact(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn, PRegister Pg) {\n+    starti;\n+    assert(T == S || T == D, \"invalid size\");\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b100001100, 21, 13);\n+    pgrf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE2 Count matching elements in vector\n+  void sve_histcnt(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg,\n+                   FloatRegister Zn, FloatRegister Zm) {\n+    starti;\n+    assert(T == S || T == D, \"invalid size\");\n+    f(0b01000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);\n+    f(0b110, 15, 13), pgrf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":44,"deletions":1,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -1073,0 +1073,1 @@\n+      assert_different_registers(dst, tmp);\n@@ -1077,0 +1078,1 @@\n+      assert_different_registers(dst, tmp);\n@@ -1088,0 +1090,1 @@\n+      assert_different_registers(dst, tmp);\n@@ -1271,0 +1274,148 @@\n+\/\/ Pack active elements of src, under the control of mask, into the lowest-numbered elements of dst.\n+\/\/ Any remaining elements of dst will be filled with zero.\n+\/\/ Clobbers: rscratch1\n+\/\/ Preserves: src, mask\n+void C2_MacroAssembler::sve_compress_short(FloatRegister dst, FloatRegister src, PRegister mask,\n+                                           FloatRegister vtmp1, FloatRegister vtmp2,\n+                                           PRegister pgtmp) {\n+  assert(pgtmp->is_governing(), \"This register has to be a governing predicate register\");\n+  assert_different_registers(dst, src, vtmp1, vtmp2);\n+  assert_different_registers(mask, pgtmp);\n+\n+  \/\/ Example input:   src   = 8888 7777 6666 5555 4444 3333 2222 1111\n+  \/\/                  mask  = 0001 0000 0000 0001 0001 0000 0001 0001\n+  \/\/ Expected result: dst   = 0000 0000 0000 8888 5555 4444 2222 1111\n+  sve_dup(vtmp2, H, 0);\n+\n+  \/\/ Extend lowest half to type INT.\n+  \/\/ dst = 00004444 00003333 00002222 00001111\n+  sve_uunpklo(dst, S, src);\n+  \/\/ pgtmp = 00000001 00000000 00000001 00000001\n+  sve_punpklo(pgtmp, mask);\n+  \/\/ Pack the active elements in size of type INT to the right,\n+  \/\/ and fill the remainings with zero.\n+  \/\/ dst = 00000000 00004444 00002222 00001111\n+  sve_compact(dst, S, dst, pgtmp);\n+  \/\/ Narrow the result back to type SHORT.\n+  \/\/ dst = 0000 0000 0000 0000 0000 4444 2222 1111\n+  sve_uzp1(dst, H, dst, vtmp2);\n+  \/\/ Count the active elements of lowest half.\n+  \/\/ rscratch1 = 3\n+  sve_cntp(rscratch1, S, ptrue, pgtmp);\n+\n+  \/\/ Repeat to the highest half.\n+  \/\/ pgtmp = 00000001 00000000 00000000 00000001\n+  sve_punpkhi(pgtmp, mask);\n+  \/\/ vtmp1 = 00008888 00007777 00006666 00005555\n+  sve_uunpkhi(vtmp1, S, src);\n+  \/\/ vtmp1 = 00000000 00000000 00008888 00005555\n+  sve_compact(vtmp1, S, vtmp1, pgtmp);\n+  \/\/ vtmp1 = 0000 0000 0000 0000 0000 0000 8888 5555\n+  sve_uzp1(vtmp1, H, vtmp1, vtmp2);\n+\n+  \/\/ Compressed low:   dst   = 0000 0000 0000 0000 0000 4444 2222 1111\n+  \/\/ Compressed high:  vtmp1 = 0000 0000 0000 0000 0000 0000 8888  5555\n+  \/\/ Left shift(cross lane) compressed high with TRUE_CNT lanes,\n+  \/\/ TRUE_CNT is the number of active elements in the compressed low.\n+  neg(rscratch1, rscratch1);\n+  \/\/ vtmp2 = {4 3 2 1 0 -1 -2 -3}\n+  sve_index(vtmp2, H, rscratch1, 1);\n+  \/\/ vtmp1 = 0000 0000 0000 8888 5555 0000 0000 0000\n+  sve_tbl(vtmp1, H, vtmp1, vtmp2);\n+\n+  \/\/ Combine the compressed high(after shifted) with the compressed low.\n+  \/\/ dst = 0000 0000 0000 8888 5555 4444 2222 1111\n+  sve_orr(dst, dst, vtmp1);\n+}\n+\n+\/\/ Clobbers: rscratch1, rscratch2\n+\/\/ Preserves: src, mask\n+void C2_MacroAssembler::sve_compress_byte(FloatRegister dst, FloatRegister src, PRegister mask,\n+                                          FloatRegister vtmp1, FloatRegister vtmp2,\n+                                          FloatRegister vtmp3, FloatRegister vtmp4,\n+                                          PRegister ptmp, PRegister pgtmp) {\n+  assert(pgtmp->is_governing(), \"This register has to be a governing predicate register\");\n+  assert_different_registers(dst, src, vtmp1, vtmp2, vtmp3, vtmp4);\n+  assert_different_registers(mask, ptmp, pgtmp);\n+  \/\/ Example input:   src   = 88 77 66 45 44 33 22 11\n+  \/\/                  mask  = 01 00 00 01 01 00 01 01\n+  \/\/ Expected result: dst   = 00 00 00 88 55 44 22 11\n+\n+  sve_dup(vtmp4, B, 0);\n+  \/\/ Extend lowest half to type SHORT.\n+  \/\/ vtmp1 = 0044 0033 0022 0011\n+  sve_uunpklo(vtmp1, H, src);\n+  \/\/ ptmp = 0001 0000 0001 0001\n+  sve_punpklo(ptmp, mask);\n+  \/\/ Count the active elements of lowest half.\n+  \/\/ rscratch2 = 3\n+  sve_cntp(rscratch2, H, ptrue, ptmp);\n+  \/\/ Pack the active elements in size of type SHORT to the right,\n+  \/\/ and fill the remainings with zero.\n+  \/\/ dst = 0000 0044 0022 0011\n+  sve_compress_short(dst, vtmp1, ptmp, vtmp2, vtmp3, pgtmp);\n+  \/\/ Narrow the result back to type BYTE.\n+  \/\/ dst = 00 00 00 00 00 44 22 11\n+  sve_uzp1(dst, B, dst, vtmp4);\n+\n+  \/\/ Repeat to the highest half.\n+  \/\/ ptmp = 0001 0000 0000 0001\n+  sve_punpkhi(ptmp, mask);\n+  \/\/ vtmp1 = 0088 0077 0066 0055\n+  sve_uunpkhi(vtmp2, H, src);\n+  \/\/ vtmp1 = 0000 0000 0088 0055\n+  sve_compress_short(vtmp1, vtmp2, ptmp, vtmp3, vtmp4, pgtmp);\n+\n+  sve_dup(vtmp4, B, 0);\n+  \/\/ vtmp1 = 00 00 00 00 00 00 88 55\n+  sve_uzp1(vtmp1, B, vtmp1, vtmp4);\n+\n+  \/\/ Compressed low:   dst   = 00 00 00 00 00 44 22 11\n+  \/\/ Compressed high:  vtmp1 = 00 00 00 00 00 00 88 55\n+  \/\/ Left shift(cross lane) compressed high with TRUE_CNT lanes,\n+  \/\/ TRUE_CNT is the number of active elements in the compressed low.\n+  neg(rscratch2, rscratch2);\n+  \/\/ vtmp2 = {4 3 2 1 0 -1 -2 -3}\n+  sve_index(vtmp2, B, rscratch2, 1);\n+  \/\/ vtmp1 = 00 00 00 88 55 00 00 00\n+  sve_tbl(vtmp1, B, vtmp1, vtmp2);\n+  \/\/ Combine the compressed high(after shifted) with the compressed low.\n+  \/\/ dst = 00 00 00 88 55 44 22 11\n+  sve_orr(dst, dst, vtmp1);\n+}\n+\n+void C2_MacroAssembler::neon_reverse_bits(FloatRegister dst, FloatRegister src, BasicType bt, bool isQ) {\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported basic type\");\n+  SIMD_Arrangement size = isQ ? T16B : T8B;\n+  if (bt == T_BYTE) {\n+    rbit(dst, size, src);\n+  } else {\n+    neon_reverse_bytes(dst, src, bt, isQ);\n+    rbit(dst, size, dst);\n+  }\n+}\n+\n+void C2_MacroAssembler::neon_reverse_bytes(FloatRegister dst, FloatRegister src, BasicType bt, bool isQ) {\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported basic type\");\n+  SIMD_Arrangement size = isQ ? T16B : T8B;\n+  switch (bt) {\n+    case T_BYTE:\n+      if (dst != src) {\n+        orr(dst, size, src, src);\n+      }\n+      break;\n+    case T_SHORT:\n+      rev16(dst, size, src);\n+      break;\n+    case T_INT:\n+      rev32(dst, size, src);\n+      break;\n+    case T_LONG:\n+      rev64(dst, size, src);\n+      break;\n+    default:\n+      assert(false, \"unsupported\");\n+      ShouldNotReachHere();\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":151,"deletions":0,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -107,0 +107,16 @@\n+  \/\/ Pack active elements of src, under the control of mask, into the\n+  \/\/ lowest-numbered elements of dst. Any remaining elements of dst will\n+  \/\/ be filled with zero.\n+  void sve_compress_byte(FloatRegister dst, FloatRegister src, PRegister mask,\n+                         FloatRegister vtmp1, FloatRegister vtmp2,\n+                         FloatRegister vtmp3, FloatRegister vtmp4,\n+                         PRegister ptmp, PRegister pgtmp);\n+\n+  void sve_compress_short(FloatRegister dst, FloatRegister src, PRegister mask,\n+                          FloatRegister vtmp1, FloatRegister vtmp2,\n+                          PRegister pgtmp);\n+\n+  void neon_reverse_bits(FloatRegister dst, FloatRegister src, BasicType bt, bool isQ);\n+\n+  void neon_reverse_bytes(FloatRegister dst, FloatRegister src, BasicType bt, bool isQ);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -4890,1 +4890,27 @@\n-void Assembler::vpopcntd(XMMRegister dst, XMMRegister src, int vector_len) {\n+void Assembler::evpopcntb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_bitalg(), \"must support avx512bitalg feature\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x54, (0xC0 | encode));\n+}\n+\n+void Assembler::evpopcntw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_bitalg(), \"must support avx512bitalg feature\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x54, (0xC0 | encode));\n+}\n+\n+void Assembler::evpopcntd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -4892,1 +4918,2 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -4894,0 +4921,4 @@\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -4898,1 +4929,1 @@\n-void Assembler::vpopcntq(XMMRegister dst, XMMRegister src, int vector_len) {\n+void Assembler::evpopcntq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -4900,1 +4931,2 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -4902,0 +4934,4 @@\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -7946,0 +7982,24 @@\n+void Assembler::evplzcntd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512cd() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n+void Assembler::evplzcntq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512cd() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n@@ -7982,0 +8042,78 @@\n+void Assembler::evexpandps(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x88, (0xC0 | encode));\n+}\n+\n+void Assembler::evexpandpd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x88, (0xC0 | encode));\n+}\n+\n+void Assembler::evpexpandb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi2(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x62, (0xC0 | encode));\n+}\n+\n+void Assembler::evpexpandw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi2(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x62, (0xC0 | encode));\n+}\n+\n+void Assembler::evpexpandd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x89, (0xC0 | encode));\n+}\n+\n+void Assembler::evpexpandq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x89, (0xC0 | encode));\n+}\n+\n@@ -8025,1 +8163,1 @@\n-  assert(VM_Version::supports_avx(), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8084,1 +8222,1 @@\n-  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8097,1 +8235,1 @@\n-  assert(VM_Version::supports_avx(), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8398,0 +8536,14 @@\n+void Assembler::vpunpckhwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x69, (0xC0 | encode));\n+}\n+\n+void Assembler::vpunpcklwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x61, (0xC0 | encode));\n+}\n+\n@@ -9914,0 +10066,8 @@\n+void Assembler::vgf2p8affineqb(XMMRegister dst, XMMRegister src2, XMMRegister src3, int imm8, int vector_len) {\n+  assert(VM_Version::supports_gfni(), \"requires GFNI support\");\n+  assert(VM_Version::supports_sse(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src3->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24((unsigned char)0xCE, (unsigned char)(0xC0 | encode), imm8);\n+}\n+\n@@ -11607,0 +11767,73 @@\n+\n+void Assembler::evpcompressb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi2() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x63, (0xC0 | encode));\n+}\n+\n+void Assembler::evpcompressw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi2() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x63, (0xC0 | encode));\n+}\n+\n+void Assembler::evpcompressd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8B, (0xC0 | encode));\n+}\n+\n+void Assembler::evpcompressq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8B, (0xC0 | encode));\n+}\n+\n+void Assembler::evcompressps(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8A, (0xC0 | encode));\n+}\n+\n+void Assembler::evcompresspd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8A, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":240,"deletions":7,"binary":false,"changes":247,"status":"modified"},{"patch":"@@ -1873,2 +1873,4 @@\n-  void vpopcntd(XMMRegister dst, XMMRegister src, int vector_len);\n-  void vpopcntq(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evpopcntb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpopcntw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpopcntd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpopcntq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -1940,0 +1942,6 @@\n+  \/\/ Interleave High Word\n+  void vpunpckhwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n+  \/\/ Interleave Low Word\n+  void vpunpcklwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n@@ -2201,2 +2209,1 @@\n-  void pdep(Register dst, Register src1, Register src2);\n-\n+  void pdep(Register dst, Register src1, Register src2);\n@@ -2432,0 +2439,2 @@\n+  void evplzcntd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evplzcntq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -2576,0 +2585,15 @@\n+  \/\/ Vector compress\/expand instructions.\n+  void evpcompressb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpcompressw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpcompressd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpcompressq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evcompressps(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evcompresspd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+\n+  void evpexpandb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpexpandw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpexpandd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpexpandq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evexpandps(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evexpandpd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+\n@@ -2732,0 +2756,4 @@\n+\n+  \/\/ Galois field affine transformation instructions.\n+  void vgf2p8affineqb(XMMRegister dst, XMMRegister src2, XMMRegister src3, int imm8, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -4421,0 +4421,65 @@\n+\n+void C2_MacroAssembler::vector_mask_compress(KRegister dst, KRegister src, Register rtmp1,\n+                                             Register rtmp2, int mask_len) {\n+  kmov(rtmp1, src);\n+  andq(rtmp1, (0xFFFFFFFFFFFFFFFFUL >> (64 - mask_len)));\n+  mov64(rtmp2, -1L);\n+  pext(rtmp2, rtmp2, rtmp1);\n+  kmov(dst, rtmp2);\n+}\n+\n+void C2_MacroAssembler::vector_compress_expand(int opcode, XMMRegister dst, XMMRegister src, KRegister mask,\n+                                               bool merge, BasicType bt, int vec_enc) {\n+  if (opcode == Op_CompressV) {\n+    switch(bt) {\n+    case T_BYTE:\n+      evpcompressb(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpcompressw(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evpcompressd(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_FLOAT:\n+      evcompressps(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_LONG:\n+      evpcompressq(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_DOUBLE:\n+      evcompresspd(dst, mask, src, merge, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type\");\n+      break;\n+    }\n+  } else {\n+    assert(opcode == Op_ExpandV, \"\");\n+    switch(bt) {\n+    case T_BYTE:\n+      evpexpandb(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpexpandw(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evpexpandd(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_FLOAT:\n+      evexpandps(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_LONG:\n+      evpexpandq(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_DOUBLE:\n+      evexpandpd(dst, mask, src, merge, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type\");\n+      break;\n+    }\n+  }\n+}\n@@ -4442,0 +4507,26 @@\n+void C2_MacroAssembler::vbroadcast(BasicType bt, XMMRegister dst, int imm32, Register rtmp, int vec_enc) {\n+  int lane_size = type2aelembytes(bt);\n+  bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if ((is_LP64 || lane_size < 8) &&\n+      ((is_non_subword_integral_type(bt) && VM_Version::supports_avx512vl()) ||\n+       (is_subword_type(bt) && VM_Version::supports_avx512vlbw()))) {\n+    movptr(rtmp, imm32);\n+    switch(lane_size) {\n+      case 1 : evpbroadcastb(dst, rtmp, vec_enc); break;\n+      case 2 : evpbroadcastw(dst, rtmp, vec_enc); break;\n+      case 4 : evpbroadcastd(dst, rtmp, vec_enc); break;\n+      case 8 : evpbroadcastq(dst, rtmp, vec_enc); break;\n+      default : ShouldNotReachHere(); break;\n+    }\n+  } else {\n+    movptr(rtmp, imm32);\n+    LP64_ONLY(movq(dst, rtmp)) NOT_LP64(movdl(dst, rtmp));\n+    switch(lane_size) {\n+      case 1 : vpbroadcastb(dst, dst, vec_enc); break;\n+      case 2 : vpbroadcastw(dst, dst, vec_enc); break;\n+      case 4 : vpbroadcastd(dst, dst, vec_enc); break;\n+      case 8 : vpbroadcastq(dst, dst, vec_enc); break;\n+      default : ShouldNotReachHere(); break;\n+    }\n+  }\n+}\n@@ -4472,0 +4563,14 @@\n+\n+void C2_MacroAssembler::vector_popcount_byte(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                             XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  assert((vec_enc == Assembler::AVX_512bit && VM_Version::supports_avx512bw()) || VM_Version::supports_avx2(), \"\");\n+  vbroadcast(T_INT, xtmp1, 0x0F0F0F0F, rtmp, vec_enc);\n+  vpsrlw(dst, src, 4, vec_enc);\n+  vpand(dst, dst, xtmp1, vec_enc);\n+  vpand(xtmp1, src, xtmp1, vec_enc);\n+  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp, vec_enc);\n+  vpshufb(xtmp1, xtmp2, xtmp1, vec_enc);\n+  vpshufb(dst, xtmp2, dst, vec_enc);\n+  vpaddb(dst, dst, xtmp1, vec_enc);\n+}\n+\n@@ -4473,27 +4578,19 @@\n-                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                                            int vec_enc) {\n-  if (VM_Version::supports_avx512_vpopcntdq()) {\n-    vpopcntd(dst, src, vec_enc);\n-  } else {\n-    assert((vec_enc == Assembler::AVX_512bit && VM_Version::supports_avx512bw()) || VM_Version::supports_avx2(), \"\");\n-    movl(rtmp, 0x0F0F0F0F);\n-    movdl(xtmp1, rtmp);\n-    vpbroadcastd(xtmp1, xtmp1, vec_enc);\n-    if (Assembler::AVX_512bit == vec_enc) {\n-      evmovdqul(xtmp2, k0, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), false, vec_enc, rtmp);\n-    } else {\n-      vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp);\n-    }\n-    vpand(xtmp3, src, xtmp1, vec_enc);\n-    vpshufb(xtmp3, xtmp2, xtmp3, vec_enc);\n-    vpsrlw(dst, src, 4, vec_enc);\n-    vpand(dst, dst, xtmp1, vec_enc);\n-    vpshufb(dst, xtmp2, dst, vec_enc);\n-    vpaddb(xtmp3, dst, xtmp3, vec_enc);\n-    vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n-    vpunpckhdq(dst, xtmp3, xtmp1, vec_enc);\n-    vpsadbw(dst, dst, xtmp1, vec_enc);\n-    vpunpckldq(xtmp2, xtmp3, xtmp1, vec_enc);\n-    vpsadbw(xtmp2, xtmp2, xtmp1, vec_enc);\n-    vpackuswb(dst, xtmp2, dst, vec_enc);\n-  }\n+                                            XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  vector_popcount_byte(xtmp1, src, dst, xtmp2, rtmp, vec_enc);\n+  \/\/ Following code is as per steps e,f,g and h of above algorithm.\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vpunpckhdq(dst, xtmp1, xtmp2, vec_enc);\n+  vpsadbw(dst, dst, xtmp2, vec_enc);\n+  vpunpckldq(xtmp1, xtmp1, xtmp2, vec_enc);\n+  vpsadbw(xtmp1, xtmp1, xtmp2, vec_enc);\n+  vpackuswb(dst, xtmp1, dst, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_popcount_short(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                              XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  vector_popcount_byte(xtmp1, src, dst, xtmp2, rtmp, vec_enc);\n+  \/\/ Add the popcount of upper and lower bytes of word.\n+  vbroadcast(T_INT, xtmp2, 0x00FF00FF, rtmp, vec_enc);\n+  vpsrlw(dst, xtmp1, 8, vec_enc);\n+  vpand(xtmp1, xtmp1, xtmp2, vec_enc);\n+  vpaddw(dst, dst, xtmp1, vec_enc);\n@@ -4503,23 +4600,52 @@\n-                                             XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                                             int vec_enc) {\n-  if (VM_Version::supports_avx512_vpopcntdq()) {\n-    vpopcntq(dst, src, vec_enc);\n-  } else if (vec_enc == Assembler::AVX_512bit) {\n-    assert(VM_Version::supports_avx512bw(), \"\");\n-    movl(rtmp, 0x0F0F0F0F);\n-    movdl(xtmp1, rtmp);\n-    vpbroadcastd(xtmp1, xtmp1, vec_enc);\n-    evmovdqul(xtmp2, k0, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), true, vec_enc, rtmp);\n-    vpandq(xtmp3, src, xtmp1, vec_enc);\n-    vpshufb(xtmp3, xtmp2, xtmp3, vec_enc);\n-    vpsrlw(dst, src, 4, vec_enc);\n-    vpandq(dst, dst, xtmp1, vec_enc);\n-    vpshufb(dst, xtmp2, dst, vec_enc);\n-    vpaddb(xtmp3, dst, xtmp3, vec_enc);\n-    vpxorq(xtmp1, xtmp1, xtmp1, vec_enc);\n-    vpsadbw(dst, xtmp3, xtmp1, vec_enc);\n-  } else {\n-    \/\/ We do not see any performance benefit of running\n-    \/\/ above instruction sequence on 256 bit vector which\n-    \/\/ can operate over maximum 4 long elements.\n-    ShouldNotReachHere();\n+                                             XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  vector_popcount_byte(xtmp1, src, dst, xtmp2, rtmp, vec_enc);\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vpsadbw(dst, xtmp1, xtmp2, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_popcount_integral(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                 XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  switch(bt) {\n+    case T_LONG:\n+      vector_popcount_long(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vector_popcount_int(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      vector_popcount_short(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+      vector_popcount_byte(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_popcount_integral_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                      KRegister mask, bool merge, int vec_enc) {\n+  assert(VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit, \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      assert(VM_Version::supports_avx512_vpopcntdq(), \"\");\n+      evpopcntq(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      assert(VM_Version::supports_avx512_vpopcntdq(), \"\");\n+      evpopcntd(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      assert(VM_Version::supports_avx512_bitalg(), \"\");\n+      evpopcntw(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+      assert(VM_Version::supports_avx512_bitalg(), \"\");\n+      evpopcntb(dst, mask, src, merge, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -4527,1 +4653,0 @@\n-  evpmovqd(dst, dst, vec_enc);\n@@ -4538,0 +4663,390 @@\n+\/\/ Bit reversal algorithm first reverses the bits of each byte followed by\n+\/\/ a byte level reversal for multi-byte primitive types (short\/int\/long).\n+\/\/ Algorithm performs a lookup table access to get reverse bit sequence\n+\/\/ corresponding to a 4 bit value. Thus a reverse bit sequence for a byte\n+\/\/ is obtained by swapping the reverse bit sequences of upper and lower\n+\/\/ nibble of a byte.\n+void C2_MacroAssembler::vector_reverse_bit(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  if (VM_Version::supports_avx512vlbw()) {\n+\n+    \/\/ Get the reverse bit sequence of lower nibble of each byte.\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vbroadcast(T_INT, xtmp2, 0x0F0F0F0F, rtmp, vec_enc);\n+    vpandq(dst, xtmp2, src, vec_enc);\n+    vpshufb(dst, xtmp1, dst, vec_enc);\n+    vpsllq(dst, dst, 4, vec_enc);\n+\n+    \/\/ Get the reverse bit sequence of upper nibble of each byte.\n+    vpandn(xtmp2, xtmp2, src, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 4, vec_enc);\n+    vpshufb(xtmp2, xtmp1, xtmp2, vec_enc);\n+\n+    \/\/ Perform logical OR operation b\/w left shifted reverse bit sequence of lower nibble and\n+    \/\/ right shifted reverse bit sequence of upper nibble to obtain the reverse bit sequence of each byte.\n+    vporq(xtmp2, dst, xtmp2, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+\n+  } else if(!VM_Version::supports_avx512vlbw() && vec_enc == Assembler::AVX_512bit) {\n+\n+    \/\/ Shift based bit reversal.\n+    assert(bt == T_LONG || bt == T_INT, \"\");\n+    vbroadcast(T_INT, xtmp1, 0x0F0F0F0F, rtmp, vec_enc);\n+\n+    \/\/ Swap lower and upper nibble of each byte.\n+    vpandq(dst, xtmp1, src, vec_enc);\n+    vpsllq(dst, dst, 4, vec_enc);\n+    vpandn(xtmp2, xtmp1, src, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 4, vec_enc);\n+    vporq(xtmp1, dst, xtmp2, vec_enc);\n+\n+    \/\/ Swap two least and most significant bits of each nibble.\n+    vbroadcast(T_INT, xtmp2, 0x33333333, rtmp, vec_enc);\n+    vpandq(dst, xtmp2, xtmp1, vec_enc);\n+    vpsllq(dst, dst, 2, vec_enc);\n+    vpandn(xtmp2, xtmp2, xtmp1, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 2, vec_enc);\n+    vporq(xtmp1, dst, xtmp2, vec_enc);\n+\n+    \/\/ Swap adjacent pair of bits.\n+    vbroadcast(T_INT, xtmp2, 0x55555555, rtmp, vec_enc);\n+    vpandq(dst, xtmp2, xtmp1, vec_enc);\n+    vpsllq(dst, dst, 1, vec_enc);\n+    vpandn(xtmp2, xtmp2, xtmp1, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 1, vec_enc);\n+    vporq(xtmp1, dst, xtmp2, vec_enc);\n+\n+    vector_reverse_byte64(bt, dst, xtmp1, xtmp1, xtmp2, rtmp, vec_enc);\n+\n+  } else {\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vbroadcast(T_INT, xtmp2, 0x0F0F0F0F, rtmp, vec_enc);\n+\n+    \/\/ Get the reverse bit sequence of lower nibble of each byte.\n+    vpand(dst, xtmp2, src, vec_enc);\n+    vpshufb(dst, xtmp1, dst, vec_enc);\n+    vpsllq(dst, dst, 4, vec_enc);\n+\n+    \/\/ Get the reverse bit sequence of upper nibble of each byte.\n+    vpandn(xtmp2, xtmp2, src, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 4, vec_enc);\n+    vpshufb(xtmp2, xtmp1, xtmp2, vec_enc);\n+\n+    \/\/ Perform logical OR operation b\/w left shifted reverse bit sequence of lower nibble and\n+    \/\/ right shifted reverse bit sequence of upper nibble to obtain the reverse bit sequence of each byte.\n+    vpor(xtmp2, dst, xtmp2, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                XMMRegister xtmp, AddressLiteral mask, Register rtmp, int vec_enc) {\n+  \/\/ Galois field instruction based bit reversal based on following algorithm.\n+  \/\/ http:\/\/0x80.pl\/articles\/avx512-galois-field-for-bit-shuffling.html\n+  assert(VM_Version::supports_gfni(), \"\");\n+  vpbroadcastq(xtmp, mask, vec_enc, rtmp);\n+  vgf2p8affineqb(xtmp, src, xtmp, 0, vec_enc);\n+  vector_reverse_byte(bt, dst, xtmp, rtmp, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_reverse_byte64(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                              XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  \/\/ Shift based bit reversal.\n+  assert(VM_Version::supports_evex(), \"\");\n+  evmovdqul(xtmp1, k0, src, true, vec_enc);\n+  switch(bt) {\n+    case T_LONG:\n+      \/\/ Swap upper and lower double word of each quad word.\n+      evprorq(xtmp1, k0, xtmp1, 32, true, vec_enc);\n+    case T_INT:\n+      \/\/ Swap upper and lower word of each double word.\n+      evprord(xtmp1, k0, xtmp1, 16, true, vec_enc);\n+    case T_SHORT:\n+      \/\/ Swap upper and lower byte of each word.\n+      vbroadcast(T_INT, dst, 0x00FF00FF, rtmp, vec_enc);\n+      vpandq(xtmp2, dst, xtmp1, vec_enc);\n+      vpsllq(xtmp2, xtmp2, 8, vec_enc);\n+      vpandn(xtmp1, dst, xtmp1, vec_enc);\n+      vpsrlq(dst, xtmp1, 8, vec_enc);\n+      vporq(dst, dst, xtmp2, vec_enc);\n+      break;\n+    case T_BYTE:\n+      evmovdquq(dst, k0, src, true, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc) {\n+  if (bt == T_BYTE) {\n+    if (VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit) {\n+      evmovdquq(dst, k0, src, true, vec_enc);\n+    } else {\n+      vmovdqu(dst, src);\n+    }\n+    return;\n+  }\n+  \/\/ Perform byte reversal by shuffling the bytes of a multi-byte primitive type using\n+  \/\/ pre-computed shuffle indices.\n+  switch(bt) {\n+    case T_LONG:\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), rtmp, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), rtmp, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type\");\n+      break;\n+  }\n+  vpshufb(dst, src, dst, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                        XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                        KRegister ktmp, Register rtmp, bool merge, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  assert(VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit, \"\");\n+  assert(VM_Version::supports_avx512cd(), \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      evplzcntq(dst, ktmp, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evplzcntd(dst, ktmp, src, merge, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vpternlogd(xtmp1, 0xff, xtmp1, xtmp1, vec_enc);\n+      vpunpcklwd(xtmp2, xtmp1, src, vec_enc);\n+      evplzcntd(xtmp2, ktmp, xtmp2, merge, vec_enc);\n+      vpunpckhwd(dst, xtmp1, src, vec_enc);\n+      evplzcntd(dst, ktmp, dst, merge, vec_enc);\n+      vpackusdw(dst, xtmp2, dst, vec_enc);\n+      break;\n+    case T_BYTE:\n+      \/\/ T1 = Compute leading zero counts of 4 LSB bits of each byte by\n+      \/\/ accessing the lookup table.\n+      \/\/ T2 = Compute leading zero counts of 4 MSB bits of each byte by\n+      \/\/ accessing the lookup table.\n+      \/\/ Add T1 to T2 if 4 MSB bits of byte are all zeros.\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      evmovdquq(xtmp1, ExternalAddress(StubRoutines::x86::vector_count_leading_zeros_lut()), vec_enc, rtmp);\n+      vbroadcast(T_INT, dst, 0x0F0F0F0F, rtmp, vec_enc);\n+      vpand(xtmp2, dst, src, vec_enc);\n+      vpshufb(xtmp2, xtmp1, xtmp2, vec_enc);\n+      vpsrlw(xtmp3, src, 4, vec_enc);\n+      vpand(xtmp3, dst, xtmp3, vec_enc);\n+      vpshufb(dst, xtmp1, xtmp3, vec_enc);\n+      vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n+      evpcmpeqb(ktmp, xtmp1, xtmp3, vec_enc);\n+      evpaddb(dst, ktmp, dst, xtmp2, true, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_byte_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_count_leading_zeros_lut()), rtmp);\n+  vbroadcast(T_INT, xtmp2, 0x0F0F0F0F, rtmp, vec_enc);\n+  \/\/ T1 = Compute leading zero counts of 4 LSB bits of each byte by\n+  \/\/ accessing the lookup table.\n+  vpand(dst, xtmp2, src, vec_enc);\n+  vpshufb(dst, xtmp1, dst, vec_enc);\n+  \/\/ T2 = Compute leading zero counts of 4 MSB bits of each byte by\n+  \/\/ accessing the lookup table.\n+  vpsrlw(xtmp3, src, 4, vec_enc);\n+  vpand(xtmp3, xtmp2, xtmp3, vec_enc);\n+  vpshufb(xtmp2, xtmp1, xtmp3, vec_enc);\n+  \/\/ Add T1 to T2 if 4 MSB bits of byte are all zeros.\n+  vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n+  vpcmpeqb(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpaddb(dst, dst, xtmp2, vec_enc);\n+  vpblendvb(dst, xtmp2, dst, xtmp3, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_short_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                             XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vector_count_leading_zeros_byte_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+  \/\/ Add zero counts of lower byte and upper byte of a word if\n+  \/\/ upper byte holds a zero value.\n+  vpsrlw(xtmp3, src, 8, vec_enc);\n+  \/\/ xtmp1 is set to all zeros by vector_count_leading_zeros_byte_avx.\n+  vpcmpeqw(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpsllw(xtmp2, dst, 8, vec_enc);\n+  vpaddw(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrlw(dst, dst, 8, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_int_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                           XMMRegister xtmp2, XMMRegister xtmp3, int vec_enc) {\n+  \/\/ Since IEEE 754 floating point format represents mantissa in 1.0 format\n+  \/\/ hence biased exponent can be used to compute leading zero count as per\n+  \/\/ following formula:-\n+  \/\/ LZCNT = 32 - (biased_exp - 127)\n+  \/\/ Special handling has been introduced for Zero, Max_Int and -ve source values.\n+\n+  \/\/ Broadcast 0xFF\n+  vpcmpeqd(xtmp1, xtmp1, xtmp1, vec_enc);\n+  vpsrld(xtmp1, xtmp1, 24, vec_enc);\n+\n+  \/\/ Extract biased exponent.\n+  vcvtdq2ps(dst, src, vec_enc);\n+  vpsrld(dst, dst, 23, vec_enc);\n+  vpand(dst, dst, xtmp1, vec_enc);\n+\n+  \/\/ Broadcast 127.\n+  vpsrld(xtmp1, xtmp1, 1, vec_enc);\n+  \/\/ Exponent = biased_exp - 127\n+  vpsubd(dst, dst, xtmp1, vec_enc);\n+\n+  \/\/ Exponent = Exponent  + 1\n+  vpsrld(xtmp3, xtmp1, 6, vec_enc);\n+  vpaddd(dst, dst, xtmp3, vec_enc);\n+\n+  \/\/ Replace -ve exponent with zero, exponent is -ve when src\n+  \/\/ lane contains a zero value.\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vblendvps(dst, dst, xtmp2, dst, vec_enc);\n+\n+  \/\/ Rematerialize broadcast 32.\n+  vpslld(xtmp1, xtmp3, 5, vec_enc);\n+  \/\/ Exponent is 32 if corresponding source lane contains max_int value.\n+  vpcmpeqd(xtmp2, dst, xtmp1, vec_enc);\n+  \/\/ LZCNT = 32 - exponent\n+  vpsubd(dst, xtmp1, dst, vec_enc);\n+\n+  \/\/ Replace LZCNT with a value 1 if corresponding source lane\n+  \/\/ contains max_int value.\n+  vpblendvb(dst, dst, xtmp3, xtmp2, vec_enc);\n+\n+  \/\/ Replace biased_exp with 0 if source lane value is less than zero.\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vblendvps(dst, dst, xtmp2, src, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_long_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+  \/\/ Add zero counts of lower word and upper word of a double word if\n+  \/\/ upper word holds a zero value.\n+  vpsrld(xtmp3, src, 16, vec_enc);\n+  \/\/ xtmp1 is set to all zeros by vector_count_leading_zeros_byte_avx.\n+  vpcmpeqd(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpslld(xtmp2, dst, 16, vec_enc);\n+  vpaddd(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrld(dst, dst, 16, vec_enc);\n+  \/\/ Add zero counts of lower doubleword and upper doubleword of a\n+  \/\/ quadword if upper doubleword holds a zero value.\n+  vpsrlq(xtmp3, src, 32, vec_enc);\n+  vpcmpeqq(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpsllq(xtmp2, dst, 32, vec_enc);\n+  vpaddq(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrlq(dst, dst, 32, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                       Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"unexpected type\");\n+  assert(vec_enc < Assembler::AVX_512bit, \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      vector_count_leading_zeros_long_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vector_count_leading_zeros_int_avx(dst, src, xtmp1, xtmp2, xtmp3, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    case T_BYTE:\n+      vector_count_leading_zeros_byte_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::vpsub(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc) {\n+  switch(bt) {\n+    case T_BYTE:\n+      vpsubb(dst, src1, src2, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vpsubw(dst, src1, src2, vec_enc);\n+      break;\n+    case T_INT:\n+      vpsubd(dst, src1, src2, vec_enc);\n+      break;\n+    case T_LONG:\n+      vpsubq(dst, src1, src2, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::vpadd(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc) {\n+  switch(bt) {\n+    case T_BYTE:\n+      vpaddb(dst, src1, src2, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vpaddw(dst, src1, src2, vec_enc);\n+      break;\n+    case T_INT:\n+      vpaddd(dst, src1, src2, vec_enc);\n+      break;\n+    case T_LONG:\n+      vpaddq(dst, src1, src2, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Trailing zero count computation is based on leading zero count operation as per\n+\/\/ following equation. All AVX3 targets support AVX512CD feature which offers\n+\/\/ direct vector instruction to compute leading zero count.\n+\/\/      CTZ = PRIM_TYPE_WIDHT - CLZ((x - 1) & ~x)\n+void C2_MacroAssembler::vector_count_trailing_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                         XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                         XMMRegister xtmp4, KRegister ktmp, Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  \/\/ xtmp = -1\n+  vpternlogd(xtmp4, 0xff, xtmp4, xtmp4, vec_enc);\n+  \/\/ xtmp = xtmp + src\n+  vpadd(bt, xtmp4, xtmp4, src, vec_enc);\n+  \/\/ xtmp = xtmp & ~src\n+  vpternlogd(xtmp4, 0x40, xtmp4, src, vec_enc);\n+  vector_count_leading_zeros_evex(bt, dst, xtmp4, xtmp1, xtmp2, xtmp3, ktmp, rtmp, true, vec_enc);\n+  vbroadcast(bt, xtmp4, 8 * type2aelembytes(bt), rtmp, vec_enc);\n+  vpsub(bt, dst, xtmp4, dst, vec_enc);\n+}\n+\n+\/\/ Trailing zero count computation for AVX2 targets is based on popcount operation as per following equation\n+\/\/      CTZ = PRIM_TYPE_WIDHT - POPC(x | -x)\n+void C2_MacroAssembler::vector_count_trailing_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                        XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  \/\/ xtmp = 0\n+  vpxor(xtmp3 , xtmp3, xtmp3, vec_enc);\n+  \/\/ xtmp = 0 - src\n+  vpsub(bt, xtmp3, xtmp3, src, vec_enc);\n+  \/\/ xtmp = xtmp | src\n+  vpor(xtmp3, xtmp3, src, vec_enc);\n+  vector_popcount_integral(bt, dst, xtmp3, xtmp1, xtmp2, rtmp, vec_enc);\n+  vbroadcast(bt, xtmp1, 8 * type2aelembytes(bt), rtmp, vec_enc);\n+  vpsub(bt, dst, xtmp1, dst, vec_enc);\n+}\n+\n@@ -4700,1 +5215,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":566,"deletions":52,"binary":false,"changes":618,"status":"modified"},{"patch":"@@ -91,0 +91,5 @@\n+  void vector_compress_expand(int opcode, XMMRegister dst, XMMRegister src, KRegister mask,\n+                              bool merge, BasicType bt, int vec_enc);\n+\n+  void vector_mask_compress(KRegister dst, KRegister src, Register rtmp1, Register rtmp2, int mask_len);\n+\n@@ -343,0 +348,8 @@\n+  void vector_reverse_bit(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                          XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp,\n+                               AddressLiteral mask, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc);\n+\n@@ -347,1 +360,1 @@\n-  #ifdef _LP64\n+#ifdef _LP64\n@@ -351,1 +364,2 @@\n-  #endif\n+#endif\n+\n@@ -353,2 +367,1 @@\n-                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                           int vec_enc);\n+                           XMMRegister xtmp2, Register rtmp, int vec_enc);\n@@ -357,2 +370,49 @@\n-                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                            int vec_enc);\n+                            XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_short(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                             XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_byte(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                            XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_integral(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_integral_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                     KRegister mask, bool merge, int vec_enc);\n+\n+  void vbroadcast(BasicType bt, XMMRegister dst, int imm32, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_byte64(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                             XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+\n+  void vector_count_leading_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                       KRegister ktmp, Register rtmp, bool merge, int vec_enc);\n+\n+  void vector_count_leading_zeros_byte_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_short_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_int_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                          XMMRegister xtmp2, XMMRegister xtmp3, int vec_enc);\n+\n+  void vector_count_leading_zeros_long_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                      XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vpadd(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc);\n+\n+  void vpsub(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc);\n+\n+  void vector_count_trailing_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                        XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4, KRegister ktmp,\n+                                        Register rtmp, int vec_enc);\n+\n+  void vector_count_trailing_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                       XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":66,"deletions":6,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -2535,2 +2535,4 @@\n-  assert(vector_len <= AVX_256bit, \"AVX2 vector length\");\n-  if (vector_len == AVX_256bit) {\n+  assert(vector_len <= AVX_512bit, \"unexpected vector length\");\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+  } else if (vector_len == AVX_256bit) {\n@@ -3148,0 +3150,9 @@\n+void MacroAssembler::vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  if (reachable(src)) {\n+    Assembler::vpbroadcastq(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpbroadcastq(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1318,0 +1318,5 @@\n+  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n+  void vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+  void vpbroadcastq(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -190,2 +190,9 @@\n-      case Op_PopCountVI: return VM_Version::supports_avx512_vpopcntdq() ? 0 : 50;\n-      case Op_PopCountVL: return VM_Version::supports_avx512_vpopcntdq() ? 0 : 40;\n+      case Op_CountTrailingZerosV:\n+      case Op_CountLeadingZerosV:\n+         return VM_Version::supports_avx512cd() && (ety == T_INT || ety == T_LONG) ? 0 : 40;\n+      case Op_PopCountVI:\n+        return ((ety == T_INT && VM_Version::supports_avx512_vpopcntdq()) ||\n+           (is_subword_type(ety) && VM_Version::supports_avx512_bitalg())) ? 0 : 50;\n+      case Op_PopCountVL:\n+        return VM_Version::supports_avx512_vpopcntdq() ? 0 : 40;\n+      case Op_ReverseV:  return VM_Version::supports_gfni() ? 0 : 30;\n@@ -198,0 +205,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -591,0 +591,24 @@\n+  address generate_count_leading_zeros_lut(const char *stub_name) {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    return start;\n+  }\n+\n+\n@@ -638,0 +662,92 @@\n+  address generate_vector_reverse_bit_lut(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x0C040800, relocInfo::none, 0);\n+    __ emit_data(0x0E060A02, relocInfo::none, 0);\n+    __ emit_data(0x0D050901, relocInfo::none, 0);\n+    __ emit_data(0x0F070B03, relocInfo::none, 0);\n+    __ emit_data(0x0C040800, relocInfo::none, 0);\n+    __ emit_data(0x0E060A02, relocInfo::none, 0);\n+    __ emit_data(0x0D050901, relocInfo::none, 0);\n+    __ emit_data(0x0F070B03, relocInfo::none, 0);\n+    __ emit_data(0x0C040800, relocInfo::none, 0);\n+    __ emit_data(0x0E060A02, relocInfo::none, 0);\n+    __ emit_data(0x0D050901, relocInfo::none, 0);\n+    __ emit_data(0x0F070B03, relocInfo::none, 0);\n+    __ emit_data(0x0C040800, relocInfo::none, 0);\n+    __ emit_data(0x0E060A02, relocInfo::none, 0);\n+    __ emit_data(0x0D050901, relocInfo::none, 0);\n+    __ emit_data(0x0F070B03, relocInfo::none, 0);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    __ emit_data(0x00010203, relocInfo::none, 0);\n+    __ emit_data(0x04050607, relocInfo::none, 0);\n+    __ emit_data(0x08090A0B, relocInfo::none, 0);\n+    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x02030001, relocInfo::none, 0);\n+    __ emit_data(0x06070405, relocInfo::none, 0);\n+    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n+    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n+    __ emit_data(0x02030001, relocInfo::none, 0);\n+    __ emit_data(0x06070405, relocInfo::none, 0);\n+    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n+    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n+    __ emit_data(0x02030001, relocInfo::none, 0);\n+    __ emit_data(0x06070405, relocInfo::none, 0);\n+    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n+    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n+    __ emit_data(0x02030001, relocInfo::none, 0);\n+    __ emit_data(0x06070405, relocInfo::none, 0);\n+    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n+    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n+    return start;\n+  }\n+\n@@ -3988,0 +4104,5 @@\n+    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n@@ -3989,1 +4110,1 @@\n-    if (UsePopCountInstruction && VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":122,"deletions":1,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -798,0 +798,15 @@\n+  address generate_count_leading_zeros_lut(const char *stub_name) {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -828,0 +843,60 @@\n+  address generate_vector_reverse_bit_lut(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -7682,0 +7757,5 @@\n+    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n@@ -7683,1 +7763,1 @@\n-    if (UsePopCountInstruction && VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":81,"deletions":1,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -925,0 +925,1 @@\n+    _features &= ~CPU_AVX512_BITALG;\n@@ -954,0 +955,2 @@\n+      _features &= ~CPU_GFNI;\n+      _features &= ~CPU_AVX512_BITALG;\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -364,1 +364,3 @@\n-    decl(SERIALIZE,         \"serialize\",         47) \/* CPU SERIALIZE *\/\n+    decl(SERIALIZE,         \"serialize\",         47) \/* CPU SERIALIZE *\/ \\\n+    decl(GFNI,              \"gfni\",              48) \/* Vector GFNI instructions *\/ \\\n+    decl(AVX512_BITALG,     \"avx512_bitalg\",     49) \/* Vector sub-word popcount and bit gather instructions *\/\n@@ -594,0 +596,2 @@\n+        if (_cpuid_info.sef_cpuid7_ecx.bits.gfni != 0)\n+          result |= CPU_GFNI;\n@@ -596,0 +600,2 @@\n+        if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_bitalg != 0)\n+          result |= CPU_AVX512_BITALG;\n@@ -900,0 +906,1 @@\n+  static bool supports_gfni()         { return (_features & CPU_GFNI) != 0; }\n@@ -901,0 +908,1 @@\n+  static bool supports_avx512_bitalg()  { return (_features & CPU_AVX512_BITALG) != 0; }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1244,0 +1244,5 @@\n+static inline bool is_vector_popcount_predicate(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n@@ -1248,0 +1253,5 @@\n+static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n+  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n+           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+}\n+\n@@ -1408,1 +1418,1 @@\n-      if (!UsePopCountInstruction || (UseAVX < 2)) {\n+      if (UseAVX < 2) {\n@@ -1413,1 +1423,1 @@\n-      if (!UsePopCountInstruction || (UseAVX <= 2)) {\n+      if (UseAVX < 2) {\n@@ -1628,0 +1638,11 @@\n+    case Op_CompressM:\n+      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (!VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -1649,0 +1670,5 @@\n+static inline bool is_pop_count_instr_target(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n@@ -1854,1 +1880,1 @@\n-      if(is_subword_type(bt)) {\n+      if (is_subword_type(bt)) {\n@@ -1881,0 +1907,17 @@\n+    case Op_CompressM:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 ) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -1890,2 +1933,10 @@\n-      if (!VM_Version::supports_avx512_vpopcntdq() &&\n-          (vlen == 16) && !VM_Version::supports_avx512bw()) {\n+    case Op_PopCountVL: {\n+        if (!is_pop_count_instr_target(bt) &&\n+            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n+          return false;\n+        }\n+      }\n+      break;\n+    case Op_ReverseV:\n+    case Op_ReverseBytesV:\n+      if (UseAVX < 2) {\n@@ -1895,3 +1946,3 @@\n-    case Op_PopCountVL:\n-      if (!VM_Version::supports_avx512_vpopcntdq() &&\n-          ((vlen <= 4) || ((vlen == 8) && !VM_Version::supports_avx512bw()))) {\n+    case Op_CountTrailingZerosV:\n+    case Op_CountLeadingZerosV:\n+      if (UseAVX < 2) {\n@@ -2045,0 +2096,7 @@\n+    case Op_PopCountVI:\n+    case Op_PopCountVL:\n+      if (!is_pop_count_instr_target(bt)) {\n+        return false;\n+      }\n+      return true;\n+\n@@ -2048,0 +2106,4 @@\n+    case Op_CountLeadingZerosV:\n+      if ((bt == T_INT || bt == T_LONG) && VM_Version::supports_avx512cd()) {\n+        return true;\n+      }\n@@ -8645,2 +8707,2 @@\n-instruct vpopcountI_popcntd(vec dst, vec src) %{\n-  predicate(VM_Version::supports_avx512_vpopcntdq());\n+instruct vpopcount_integral_reg_evex(vec dst, vec src) %{\n+  predicate(is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));\n@@ -8648,1 +8710,3 @@\n-  format %{ \"vector_popcount_int $dst, $src\\t! vector popcount packedI\" %}\n+  match(Set dst (PopCountVL src));\n+  ins_cost(400);\n+  format %{ \"vector_popcount_integral $dst, $src\" %}\n@@ -8650,3 +8714,10 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_popcount_int($dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg, xnoreg, noreg, vlen_enc);\n+    int opcode = this->ideal_Opcode();\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_popcount_integral_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, k0, true, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (opcode == Op_PopCountVL && Matcher::vector_element_basic_type(this) == T_INT) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -8657,5 +8728,5 @@\n-instruct vpopcountI(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp, rFlagsReg cc) %{\n-  predicate(!VM_Version::supports_avx512_vpopcntdq());\n-  match(Set dst (PopCountVI src));\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, KILL cc);\n-  format %{ \"vector_popcount_int  $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+instruct vpopcount_integral_reg_evex_masked(vec dst, vec src, kReg mask) %{\n+  predicate(is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));\n+  match(Set dst (PopCountVI src mask));\n+  match(Set dst (PopCountVL src mask));\n+  format %{ \"vector_popcount_integral_masked $dst, $src, $mask\" %}\n@@ -8663,4 +8734,4 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_popcount_int($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister,\n-                           $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+    __ vector_popcount_integral_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $mask$$KRegister, true, vlen_enc);\n@@ -8671,2 +8742,3 @@\n-instruct vpopcountL_popcntd(vec dst, vec src) %{\n-  predicate(VM_Version::supports_avx512_vpopcntdq());\n+instruct vpopcount_avx_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegP rtmp) %{\n+  predicate(!is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));\n+  match(Set dst (PopCountVI src));\n@@ -8674,1 +8746,2 @@\n-  format %{ \"vector_popcount_long  $dst, $src\\t! vector popcount packedL\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);\n+  format %{ \"vector_popcount_integral $dst, $src\\t! using $xtmp1, $xtmp2, and $rtmp as TEMP\" %}\n@@ -8676,1 +8749,1 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n+    int opcode = this->ideal_Opcode();\n@@ -8678,1 +8751,15 @@\n-    __ vector_popcount_long($dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg, xnoreg, noreg, vlen_enc);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_popcount_integral(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                $xtmp2$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (opcode == Op_PopCountVL && Matcher::vector_element_basic_type(this) == T_INT) {\n+      if (VM_Version::supports_avx512vl()) {\n+        __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      } else {\n+        assert(VM_Version::supports_avx2(), \"\");\n+        __ vpshufd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+        __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+      }\n+    }\n@@ -8683,5 +8770,63 @@\n-instruct vpopcountL(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp, rFlagsReg cc) %{\n-  predicate(!VM_Version::supports_avx512_vpopcntdq());\n-  match(Set dst (PopCountVL src));\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, KILL cc);\n-  format %{ \"vector_popcount_long  $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+\/\/ --------------------------------- Vector Trailing Zeros Count --------------------------------------\n+\n+instruct vcount_trailing_zeros_reg_evex(vec dst, vec src, vec xtmp, rRegP rtmp) %{\n+  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),\n+                                              Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n+  ins_cost(400);\n+  format %{ \"vector_count_trailing_zeros $dst, $src!\\t using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,\n+                                        xnoreg, xnoreg, $xtmp$$XMMRegister, k0, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountTrailingZerosV\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (bt == T_LONG && rbt == T_INT) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_trailing_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+            VM_Version::supports_avx512cd() &&\n+            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  ins_cost(400);\n+  format %{ \"vector_count_trailing_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3 and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                        $xtmp2$$XMMRegister, xnoreg, $xtmp3$$XMMRegister, k0, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_trailing_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, kReg ktmp, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE && VM_Version::supports_avx512vlbw());\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP ktmp, TEMP rtmp);\n+  ins_cost(400);\n+  format %{ \"vector_count_trailing_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3, $xtmp4, $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                        $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,\n+                                        $ktmp$$KRegister, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_trailing_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  format %{ \"vector_count_trailing_zeros $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n@@ -8689,3 +8834,16 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    __ vector_popcount_long($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister,\n-                           $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_trailing_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (bt == T_LONG && rbt == T_INT) {\n+      if (VM_Version::supports_avx512vl()) {\n+        __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      } else {\n+        assert(VM_Version::supports_avx2(), \"\");\n+        __ vpshufd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+        __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+      }\n+    }\n@@ -8697,0 +8855,1 @@\n+\n@@ -8971,0 +9130,28 @@\n+\n+\/\/ --------------------------------- Compress\/Expand Operations ---------------------------\n+\n+instruct vcompress_expand_reg_evex(vec dst, vec src, kReg mask) %{\n+  match(Set dst (CompressV src mask));\n+  match(Set dst (ExpandV src mask));\n+  format %{ \"vector_compress_expand $dst, $src, $mask\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vector_len = vector_length_encoding(this);\n+    BasicType bt  = Matcher::vector_element_basic_type(this);\n+    __ vector_compress_expand(opcode, $dst$$XMMRegister, $src$$XMMRegister, $mask$$KRegister, false, bt, vector_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcompress_mask_reg_evex(kReg dst, kReg mask, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n+  match(Set dst (CompressM mask));\n+  effect(TEMP rtmp1, TEMP rtmp2, KILL cr);\n+  format %{ \"mask_compress_evex $dst, $mask\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    assert(this->in(1)->bottom_type()->isa_vectmask(), \"\");\n+    int mask_len = Matcher::vector_length(this);\n+    __ vector_mask_compress($dst$$KRegister, $mask$$KRegister, $rtmp1$$Register, $rtmp2$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -8973,0 +9160,164 @@\n+\/\/ -------------------------------- Bit and Byte Reversal Vector Operations ------------------------\n+\n+instruct vreverse_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegI rtmp) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);\n+  format %{ \"vector_reverse_bit_evex $dst, $src!\\t using $xtmp1, $xtmp2 and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_reverse_bit(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                          $xtmp2$$XMMRegister, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp, rRegI rtmp) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseV src));\n+  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n+  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $rtmp and $xtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt  = Matcher::vector_element_basic_type(this);\n+    InternalAddress addr = $constantaddress(T_LONG, vreplicate_imm(T_LONG, 0x8040201008040201L, 1));\n+    __ vector_reverse_bit_gfni(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                               addr, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vreverse_byte_reg(vec dst, vec src, rRegI rtmp) %{\n+  predicate(VM_Version::supports_avx512bw() || Matcher::vector_length_in_bytes(n) < 64);\n+  match(Set dst (ReverseBytesV src));\n+  effect(TEMP dst, TEMP rtmp);\n+  format %{ \"vector_reverse_byte $dst, $src!\\t using $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vreverse_byte64_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegI rtmp) %{\n+  predicate(!VM_Version::supports_avx512bw() && Matcher::vector_length_in_bytes(n) == 64);\n+  match(Set dst (ReverseBytesV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);\n+  format %{ \"vector_reverse_byte $dst, $src!\\t using $xtmp1, $xtmp2 and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_reverse_byte64(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                             $xtmp2$$XMMRegister, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ ---------------------------------- Vector Count Leading Zeros -----------------------------------\n+\n+instruct vcount_leading_zeros_IL_reg_evex(vec dst, vec src) %{\n+  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),\n+                                              Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (CountLeadingZerosV src));\n+  format %{ \"vector_count_leading_zeros $dst, $src\" %}\n+  ins_encode %{\n+     int vlen_enc = vector_length_encoding(this, $src);\n+     BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+     BasicType rbt = Matcher::vector_element_basic_type(this);\n+     __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,\n+                                        xnoreg, xnoreg, k0, noreg, true, vlen_enc);\n+     \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n+     \/\/ should be succeeded by its corresponding vector IR and following\n+     \/\/ special handling should be removed.\n+     if (rbt == T_INT && bt == T_LONG) {\n+       __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+     }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_IL_reg_evex_masked(vec dst, vec src, kReg mask) %{\n+  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),\n+                                              Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (CountLeadingZerosV src mask));\n+  format %{ \"vector_count_leading_zeros $dst, $src, $mask\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg,\n+                                       xnoreg, $mask$$KRegister, noreg, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+            VM_Version::supports_avx512cd() &&\n+            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vector_count_leading_zeros $dst, $src!\\t using $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, xnoreg, k0, noreg, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, kReg ktmp, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE && VM_Version::supports_avx512vlbw());\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP ktmp, TEMP rtmp);\n+  format %{ \"vector_count_leading_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3, $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $ktmp$$KRegister,\n+                                       $rtmp$$Register, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_int_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_INT &&\n+            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3);\n+  format %{ \"vector_count_leading_zeros $dst, $src\\t! using $xtmp1, $xtmp2 and $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, noreg, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) != T_INT &&\n+            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  format %{ \"vector_count_leading_zeros $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (rbt == T_INT && bt == T_LONG) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":387,"deletions":36,"binary":false,"changes":423,"status":"modified"},{"patch":"@@ -4219,0 +4219,1 @@\n+    \"CompressV\", \"ExpandV\", \"CompressM\",\n@@ -4230,1 +4231,1 @@\n-    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\",\n+    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\", \"ReverseV\", \"ReverseBytesV\",\n@@ -4240,0 +4241,1 @@\n+    \"CountLeadingZerosV\", \"CountTrailingZerosV\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -916,1 +916,1 @@\n-                                     \"I\"                                                                                                       \\\n+                                     \"J\"                                                                                                       \\\n@@ -931,1 +931,1 @@\n-                                            \"I\"                                                                                                \\\n+                                            \"J\"                                                                                                \\\n@@ -943,2 +943,4 @@\n-                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n-                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)\"                        \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                   \\\n+                                      \"Ljava\/lang\/Object;\"                                                                                     \\\n+                                      \"J\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)\"                                           \\\n@@ -958,1 +960,1 @@\n-                                             \"I\"                                                                                               \\\n+                                             \"J\"                                                                                               \\\n@@ -1118,0 +1120,11 @@\n+  do_intrinsic(_VectorComExp, jdk_internal_vm_vector_VectorSupport, vector_comexp_op_name, vector_comexp_op_sig, F_S)                           \\\n+   do_signature(vector_comexp_op_sig, \"(I\"                                                                                                     \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$ComExpOperation;)\"                                                \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                  \\\n+   do_name(vector_comexp_op_name,     \"comExpOp\")                                                                                              \\\n@@ -1226,1 +1239,1 @@\n-  LAST_COMPILER_INLINE = _VectorMaskOp,\n+  LAST_COMPILER_INLINE = _VectorComExp,\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":19,"deletions":6,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+macro(ReverseBytesV)\n@@ -78,0 +79,2 @@\n+macro(CompressBits)\n+macro(ExpandBits)\n@@ -156,0 +159,1 @@\n+macro(CountLeadingZerosV)\n@@ -158,0 +162,1 @@\n+macro(CountTrailingZerosV)\n@@ -294,0 +299,3 @@\n+macro(ReverseI)\n+macro(ReverseL)\n+macro(ReverseV)\n@@ -431,0 +439,3 @@\n+macro(CompressV)\n+macro(CompressM)\n+macro(ExpandV)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3414,0 +3414,15 @@\n+  case Op_ReverseBytesV:\n+  case Op_ReverseV: {\n+    if ((uint)n->in(1)->Opcode() == nop) {\n+      if (n->is_predicated_vector() && n->in(1)->is_predicated_vector() &&\n+          n->in(2) == n->in(1)->in(2)) {\n+        \/\/ Node (Node X , Mask) Mask => X\n+        n->subsume_by(n->in(1)->in(1), this);\n+      } else if (!n->is_predicated_using_blend() && !n->in(1)->is_predicated_using_blend()) {\n+        \/\/ Node (Node X) =>  X\n+        n->subsume_by(n->in(1)->in(1), this);\n+      }\n+    }\n+    break;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -700,0 +700,2 @@\n+  case vmIntrinsics::_VectorComExp:\n+    return inline_vector_compress_expand();\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -977,0 +977,3 @@\n+      case Op_CountTrailingZerosV:\n+      case Op_CountLeadingZerosV:\n+      case Op_ReverseV:\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2266,0 +2266,3 @@\n+    case Op_CompressV:\n+    case Op_CompressM:\n+    case Op_ExpandV:\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -179,0 +179,3 @@\n+class ExpandVNode;\n+class CompressVNode;\n+class CompressMNode;\n@@ -710,0 +713,3 @@\n+        DEFINE_CLASS_ID(CompressV, Vector, 4)\n+        DEFINE_CLASS_ID(ExpandV, Vector, 5)\n+        DEFINE_CLASS_ID(CompressM, Vector, 6)\n@@ -779,5 +785,7 @@\n-    Flag_is_expensive                = 1 << 13,\n-    Flag_is_predicated_vector        = 1 << 14,\n-    Flag_for_post_loop_opts_igvn     = 1 << 15,\n-    Flag_is_removed_by_peephole      = 1 << 16,\n-    _last_flag                       = Flag_is_removed_by_peephole\n+    Flag_has_vector_mask_set         = 1 << 13,\n+    Flag_is_expensive                = 1 << 14,\n+    Flag_is_predicated_vector        = 1 << 15,\n+    Flag_for_post_loop_opts_igvn     = 1 << 16,\n+    Flag_is_removed_by_peephole      = 1 << 17,\n+    Flag_is_predicated_using_blend   = 1 << 18,\n+    _last_flag                       = Flag_is_predicated_using_blend\n@@ -939,1 +947,4 @@\n-  DEFINE_CLASS_QUERY(VectorReinterpret);\n+  DEFINE_CLASS_QUERY(VectorReinterpret)\n+  DEFINE_CLASS_QUERY(CompressV)\n+  DEFINE_CLASS_QUERY(ExpandV)\n+  DEFINE_CLASS_QUERY(CompressM)\n@@ -997,0 +1008,5 @@\n+  bool is_predicated_using_blend() const { return (_flags & Flag_is_predicated_using_blend) != 0; }\n+\n+  \/\/ The node is a CountedLoopEnd with a mask annotation so as to emit a restore context\n+  bool has_vector_mask_set() const { return (_flags & Flag_has_vector_mask_set) != 0; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -551,0 +551,20 @@\n+\/\/-------------------------------ReverseINode--------------------------------\n+\/\/ reverse bits of an int\n+class ReverseINode : public Node {\n+public:\n+  ReverseINode(Node *c, Node *in1) : Node(c, in1) {}\n+  virtual int Opcode() const;\n+  const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual uint ideal_reg() const { return Op_RegI; }\n+};\n+\n+\/\/-------------------------------ReverseLNode--------------------------------\n+\/\/ reverse bits of a long\n+class ReverseLNode : public Node {\n+public:\n+  ReverseLNode(Node *c, Node *in1) : Node(c, in1) {}\n+  virtual int Opcode() const;\n+  const Type *bottom_type() const { return TypeLong::LONG; }\n+  virtual uint ideal_reg() const { return Op_RegL; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2567,1 +2567,3 @@\n-                 opc == Op_PopCountI || opc == Op_PopCountL) {\n+                 opc == Op_PopCountI || opc == Op_PopCountL ||\n+                 opc == Op_CountLeadingZerosI || opc == Op_CountLeadingZerosL ||\n+                 opc == Op_CountTrailingZerosI || opc == Op_CountTrailingZerosL) {\n@@ -3035,3 +3037,3 @@\n-  if (VectorNode::is_vpopcnt_long(use)) {\n-    \/\/ VPOPCNT_LONG takes long and produces int - hence the special checks\n-    \/\/ on alignment and size.\n+  if (VectorNode::is_type_transition_long_to_int(use)) {\n+    \/\/ PopCountL\/CountLeadingZerosL\/CountTrailingZerosL takes long and produces\n+    \/\/ int - hence the special checks on alignment and size.\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -85,1 +85,2 @@\n-      if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, elem_bt)) {\n+      if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, elem_bt) ||\n+          !Matcher::match_rule_supported_vector(Op_LoadVector, num_elem, T_BOOLEAN)) {\n@@ -265,1 +266,2 @@\n-    if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, type)) {\n+    if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, type) ||\n+        !Matcher::match_rule_supported_vector(Op_LoadVector, num_elem, T_BOOLEAN)) {\n@@ -278,1 +280,2 @@\n-    if (!Matcher::match_rule_supported_vector(Op_VectorStoreMask, num_elem, type)) {\n+    if (!Matcher::match_rule_supported_vector(Op_VectorStoreMask, num_elem, type) ||\n+        !Matcher::match_rule_supported_vector(Op_StoreVector, num_elem, T_BOOLEAN)) {\n@@ -564,0 +567,1 @@\n+      operation->add_flag(Node::Flag_is_predicated_using_blend);\n@@ -698,9 +702,1 @@\n-  if (!arch_supports_vector(Op_LoadVector, num_elem, T_BOOLEAN, VecMaskNotUsed)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=1 op=cast#%d\/3 vlen2=%d etype2=%s\",\n-                    Op_LoadVector, num_elem, type2name(T_BOOLEAN));\n-    }\n-    return false; \/\/ not supported\n-  }\n-\n-  if (!arch_supports_vector(mopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+  if (!arch_supports_vector(mopc, num_elem, elem_bt, VecMaskUseLoad)) {\n@@ -940,1 +936,1 @@\n-\/\/         C container, int index, S s,     \/\/ Arguments for default implementation\n+\/\/         C container, long index, S s,     \/\/ Arguments for default implementation\n@@ -949,1 +945,1 @@\n-\/\/            C container, int index,      \/\/ Arguments for default implementation\n+\/\/            C container, long index,      \/\/ Arguments for default implementation\n@@ -1052,10 +1048,0 @@\n-    if (!arch_supports_vector(Op_LoadVector, num_elem, T_BOOLEAN, VecMaskNotUsed)) {\n-      if (C->print_intrinsics()) {\n-        tty->print_cr(\"  ** not supported: arity=%d op=%s\/mask vlen=%d etype=bit ismask=no\",\n-                      is_store, is_store ? \"store\" : \"load\",\n-                      num_elem);\n-      }\n-      set_map(old_map);\n-      set_sp(old_sp);\n-      return false; \/\/ not supported\n-    }\n@@ -1099,1 +1085,3 @@\n-\n+    if (is_mask) {\n+      val = gvn().transform(VectorStoreMaskNode::make(gvn(), val, elem_bt, num_elem));\n+    }\n@@ -1141,1 +1129,1 @@\n-\/\/              C container, int index, S s,  \/\/ Arguments for default implementation\n+\/\/              C container, long index, S s,  \/\/ Arguments for default implementation\n@@ -1152,1 +1140,1 @@\n-\/\/                  C container, int index,  \/\/ Arguments for default implementation\n+\/\/                  C container, long index,  \/\/ Arguments for default implementation\n@@ -2739,0 +2727,94 @@\n+\/\/ public static\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/  V comExpOp(int opr,\n+\/\/             Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+\/\/             int length, V v, M m,\n+\/\/             CmpExpOperation<V, M> defaultImpl)\n+bool LibraryCallKit::inline_vector_compress_expand() {\n+  const TypeInt*     opr          = gvn().type(argument(0))->isa_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n+\n+  if (vector_klass == NULL || elem_klass == NULL || mask_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+\n+  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  int num_elem = vlen->get_con();\n+  BasicType elem_bt = elem_type->basic_type();\n+  int opc = VectorSupport::vop2ideal(opr->get_con(), elem_bt);\n+\n+  if (!arch_supports_vector(opc, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: opc=%d vlen=%d etype=%s ismask=useload\",\n+                    opc, num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* opd1 = NULL;\n+  const TypeInstPtr* vbox_type = NULL;\n+  if (opc != Op_CompressM) {\n+    ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+    opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+    if (opd1 == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(5)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(is_vector_mask(mbox_klass), \"argument(6) should be a mask class\");\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* mask = unbox_vector(argument(6), mbox_type, elem_bt, num_elem);\n+  if (mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    NodeClassNames[argument(6)->Opcode()]);\n+    }\n+    return false;\n+  }\n+\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, opc == Op_CompressM);\n+  Node* operation = gvn().transform(VectorNode::make(opc, opd1, mask, vt));\n+\n+  \/\/ Wrap it up in VectorBox to keep object type information.\n+  const TypeInstPtr* box_type = opc == Op_CompressM ? mbox_type : vbox_type;\n+  Node* vbox = box_vector(operation, box_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":109,"deletions":27,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -165,3 +165,1 @@\n-    \/\/ Unimplemented for subword types since bit count changes\n-    \/\/ depending on size of lane (and sign bit).\n-    return (bt == T_INT ? Op_PopCountVI : 0);\n+    return Op_PopCountVI;\n@@ -170,0 +168,13 @@\n+  case Op_ReverseI:\n+  case Op_ReverseL:\n+    return (is_integral_type(bt) ? Op_ReverseV : 0);\n+  case Op_ReverseBytesS:\n+  case Op_ReverseBytesI:\n+  case Op_ReverseBytesL:\n+    return (is_integral_type(bt) ? Op_ReverseBytesV : 0);\n+  case Op_CompressBits:\n+    \/\/ Not implemented. Returning 0 temporarily\n+    return 0;\n+  case Op_ExpandBits:\n+    \/\/ Not implemented. Returning 0 temporarily\n+    return 0;\n@@ -248,0 +259,6 @@\n+  case Op_CountLeadingZerosI:\n+  case Op_CountLeadingZerosL:\n+    return Op_CountLeadingZerosV;\n+  case Op_CountTrailingZerosI:\n+  case Op_CountTrailingZerosL:\n+    return Op_CountTrailingZerosV;\n@@ -316,3 +333,8 @@\n-bool VectorNode::is_vpopcnt_long(Node* n) {\n-  if (n->Opcode() == Op_PopCountL) {\n-    return true;\n+bool VectorNode::is_type_transition_long_to_int(Node* n) {\n+  switch(n->Opcode()) {\n+    case Op_PopCountL:\n+    case Op_CountLeadingZerosL:\n+    case Op_CountTrailingZerosL:\n+       return true;\n+    default:\n+       return false;\n@@ -320,1 +342,0 @@\n-  return false;\n@@ -323,3 +344,0 @@\n-\n-\n-\n@@ -589,0 +607,3 @@\n+  case Op_ReverseV: return new ReverseVNode(n1, vt);\n+  case Op_ReverseBytesV: return new ReverseBytesVNode(n1, vt);\n+\n@@ -622,0 +643,6 @@\n+\n+  case Op_ExpandV: return new ExpandVNode(n1, n2, vt);\n+  case Op_CompressV: return new CompressVNode(n1, n2, vt);\n+  case Op_CompressM: assert(n1 == NULL, \"\"); return new CompressMNode(n2, vt);\n+  case Op_CountLeadingZerosV: return new CountLeadingZerosVNode(n1, vt);\n+  case Op_CountTrailingZerosV: return new CountTrailingZerosVNode(n1, vt);\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":37,"deletions":10,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -1779,0 +1779,2 @@\n+  declare_c2_type(CompressVNode, VectorNode)                              \\\n+  declare_c2_type(ExpandVNode, VectorNode)                                \\\n@@ -1869,0 +1871,2 @@\n+  declare_c2_type(CountLeadingZerosVNode, VectorNode)                     \\\n+  declare_c2_type(CountTrailingZerosVNode, VectorNode)                    \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -724,0 +724,4 @@\n+inline bool is_non_subword_integral_type(BasicType t) {\n+  return t == T_INT || t == T_LONG;\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1772,0 +1772,4 @@\n+                        [\"index\",   \"__ sve_index(z6, __ B, r5, 2);\",                     \"index\\tz6.b, w5, #2\"],\n+                        [\"index\",   \"__ sve_index(z6, __ H, r5, 3);\",                     \"index\\tz6.h, w5, #3\"],\n+                        [\"index\",   \"__ sve_index(z6, __ S, r5, 4);\",                     \"index\\tz6.s, w5, #4\"],\n+                        [\"index\",   \"__ sve_index(z7, __ D, r5, 5);\",                     \"index\\tz7.d, x5, #5\"],\n@@ -1814,0 +1818,2 @@\n+                        [\"compact\", \"__ sve_compact(z16, __ S, z16, p1);\",                \"compact\\tz16.s, p1, z16.s\"],\n+                        [\"compact\", \"__ sve_compact(z16, __ D, z16, p1);\",                \"compact\\tz16.d, p1, z16.d\"],\n@@ -1815,0 +1821,3 @@\n+                        # SVE2 instructions\n+                        [\"histcnt\", \"__ sve_histcnt(z16, __ S, p0, z16, z16);\",           \"histcnt\\tz16.s, p0\/z, z16.s, z16.s\"],\n+                        [\"histcnt\", \"__ sve_histcnt(z17, __ D, p0, z17, z17);\",           \"histcnt\\tz17.d, p0\/z, z17.d, z17.d\"],\n@@ -1858,0 +1867,1 @@\n+                       [\"clz\", \"ZPZ\", \"m\"],\n@@ -1866,0 +1876,2 @@\n+                       [\"rbit\", \"ZPZ\", \"m\"],\n+                       [\"revb\", \"ZPZ\", \"m\"],\n@@ -1907,2 +1919,3 @@\n-# compile for sve with 8.3 and sha3 because of SHA3 crypto extension.\n-subprocess.check_call([AARCH64_AS, \"-march=armv8.3-a+sha3+sve\", \"aarch64ops.s\", \"-o\", \"aarch64ops.o\"])\n+# compile for sve with armv9-a+sha3 because of SHA3 crypto extension and SVE2 instructions.\n+# armv9-a enables sve and sve2 by default.\n+subprocess.check_call([AARCH64_AS, \"-march=armv9-a+sha3\", \"aarch64ops.s\", \"-o\", \"aarch64ops.o\"])\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":15,"deletions":2,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -916,0 +916,4 @@\n+    __ sve_index(z6, __ B, r5, 2);                     \/\/       index   z6.b, w5, #2\n+    __ sve_index(z6, __ H, r5, 3);                     \/\/       index   z6.h, w5, #3\n+    __ sve_index(z6, __ S, r5, 4);                     \/\/       index   z6.s, w5, #4\n+    __ sve_index(z7, __ D, r5, 5);                     \/\/       index   z7.d, x5, #5\n@@ -958,0 +962,2 @@\n+    __ sve_compact(z16, __ S, z16, p1);                \/\/       compact z16.s, p1, z16.s\n+    __ sve_compact(z16, __ D, z16, p1);                \/\/       compact z16.d, p1, z16.d\n@@ -959,0 +965,2 @@\n+    __ sve_histcnt(z16, __ S, p0, z16, z16);           \/\/       histcnt z16.s, p0\/z, z16.s, z16.s\n+    __ sve_histcnt(z17, __ D, p0, z17, z17);           \/\/       histcnt z17.d, p0\/z, z17.d, z17.d\n@@ -1147,39 +1155,42 @@\n-    __ sve_cnt(z11, __ S, p3, z11);                    \/\/       cnt     z11.s, p3\/m, z11.s\n-    __ sve_eor(z1, __ S, p6, z8);                      \/\/       eor     z1.s, p6\/m, z1.s, z8.s\n-    __ sve_lsl(z13, __ S, p4, z17);                    \/\/       lsl     z13.s, p4\/m, z13.s, z17.s\n-    __ sve_lsr(z4, __ H, p0, z3);                      \/\/       lsr     z4.h, p0\/m, z4.h, z3.h\n-    __ sve_mul(z7, __ S, p3, z14);                     \/\/       mul     z7.s, p3\/m, z7.s, z14.s\n-    __ sve_neg(z4, __ B, p3, z29);                     \/\/       neg     z4.b, p3\/m, z29.b\n-    __ sve_not(z0, __ D, p2, z21);                     \/\/       not     z0.d, p2\/m, z21.d\n-    __ sve_orr(z3, __ S, p0, z9);                      \/\/       orr     z3.s, p0\/m, z3.s, z9.s\n-    __ sve_smax(z28, __ B, p2, z24);                   \/\/       smax    z28.b, p2\/m, z28.b, z24.b\n-    __ sve_smin(z19, __ D, p1, z23);                   \/\/       smin    z19.d, p1\/m, z19.d, z23.d\n-    __ sve_sub(z13, __ D, p5, z10);                    \/\/       sub     z13.d, p5\/m, z13.d, z10.d\n-    __ sve_fabs(z12, __ D, p4, z30);                   \/\/       fabs    z12.d, p4\/m, z30.d\n-    __ sve_fadd(z14, __ D, p0, z29);                   \/\/       fadd    z14.d, p0\/m, z14.d, z29.d\n-    __ sve_fdiv(z21, __ D, p5, z7);                    \/\/       fdiv    z21.d, p5\/m, z21.d, z7.d\n-    __ sve_fmax(z2, __ D, p0, z26);                    \/\/       fmax    z2.d, p0\/m, z2.d, z26.d\n-    __ sve_fmin(z9, __ D, p4, z17);                    \/\/       fmin    z9.d, p4\/m, z9.d, z17.d\n-    __ sve_fmul(z0, __ D, p1, z2);                     \/\/       fmul    z0.d, p1\/m, z0.d, z2.d\n-    __ sve_fneg(z14, __ D, p1, z11);                   \/\/       fneg    z14.d, p1\/m, z11.d\n-    __ sve_frintm(z14, __ S, p4, z29);                 \/\/       frintm  z14.s, p4\/m, z29.s\n-    __ sve_frintn(z3, __ S, p0, z22);                  \/\/       frintn  z3.s, p0\/m, z22.s\n-    __ sve_frintp(z3, __ S, p6, z27);                  \/\/       frintp  z3.s, p6\/m, z27.s\n-    __ sve_fsqrt(z19, __ D, p5, z7);                   \/\/       fsqrt   z19.d, p5\/m, z7.d\n-    __ sve_fsub(z21, __ S, p3, z5);                    \/\/       fsub    z21.s, p3\/m, z21.s, z5.s\n-    __ sve_fmad(z25, __ D, p1, z21, z17);              \/\/       fmad    z25.d, p1\/m, z21.d, z17.d\n-    __ sve_fmla(z0, __ S, p0, z9, z19);                \/\/       fmla    z0.s, p0\/m, z9.s, z19.s\n-    __ sve_fmls(z7, __ D, p3, z14, z17);               \/\/       fmls    z7.d, p3\/m, z14.d, z17.d\n-    __ sve_fmsb(z11, __ D, p3, z24, z17);              \/\/       fmsb    z11.d, p3\/m, z24.d, z17.d\n-    __ sve_fnmad(z17, __ D, p2, z15, z14);             \/\/       fnmad   z17.d, p2\/m, z15.d, z14.d\n-    __ sve_fnmsb(z22, __ S, p7, z22, z7);              \/\/       fnmsb   z22.s, p7\/m, z22.s, z7.s\n-    __ sve_fnmla(z5, __ S, p7, z27, z10);              \/\/       fnmla   z5.s, p7\/m, z27.s, z10.s\n-    __ sve_fnmls(z14, __ S, p6, z21, z20);             \/\/       fnmls   z14.s, p6\/m, z21.s, z20.s\n-    __ sve_mla(z3, __ D, p5, z25, z5);                 \/\/       mla     z3.d, p5\/m, z25.d, z5.d\n-    __ sve_mls(z29, __ H, p4, z17, z1);                \/\/       mls     z29.h, p4\/m, z17.h, z1.h\n-    __ sve_and(z14, z29, z13);                         \/\/       and     z14.d, z29.d, z13.d\n-    __ sve_eor(z17, z2, z30);                          \/\/       eor     z17.d, z2.d, z30.d\n-    __ sve_orr(z22, z21, z29);                         \/\/       orr     z22.d, z21.d, z29.d\n-    __ sve_bic(z8, z2, z0);                            \/\/       bic     z8.d, z2.d, z0.d\n-    __ sve_uzp1(z23, __ S, z22, z0);                   \/\/       uzp1    z23.s, z22.s, z0.s\n-    __ sve_uzp2(z25, __ H, z26, z23);                  \/\/       uzp2    z25.h, z26.h, z23.h\n+    __ sve_clz(z11, __ S, p3, z11);                    \/\/       clz     z11.s, p3\/m, z11.s\n+    __ sve_cnt(z1, __ S, p6, z8);                      \/\/       cnt     z1.s, p6\/m, z8.s\n+    __ sve_eor(z13, __ S, p4, z17);                    \/\/       eor     z13.s, p4\/m, z13.s, z17.s\n+    __ sve_lsl(z4, __ H, p0, z3);                      \/\/       lsl     z4.h, p0\/m, z4.h, z3.h\n+    __ sve_lsr(z7, __ S, p3, z14);                     \/\/       lsr     z7.s, p3\/m, z7.s, z14.s\n+    __ sve_mul(z4, __ B, p3, z29);                     \/\/       mul     z4.b, p3\/m, z4.b, z29.b\n+    __ sve_neg(z0, __ D, p2, z21);                     \/\/       neg     z0.d, p2\/m, z21.d\n+    __ sve_not(z3, __ S, p0, z9);                      \/\/       not     z3.s, p0\/m, z9.s\n+    __ sve_orr(z28, __ B, p2, z24);                    \/\/       orr     z28.b, p2\/m, z28.b, z24.b\n+    __ sve_rbit(z19, __ D, p1, z23);                   \/\/       rbit    z19.d, p1\/m, z23.d\n+    __ sve_revb(z13, __ D, p5, z10);                   \/\/       revb    z13.d, p5\/m, z10.d\n+    __ sve_smax(z12, __ S, p4, z30);                   \/\/       smax    z12.s, p4\/m, z12.s, z30.s\n+    __ sve_smin(z14, __ S, p0, z29);                   \/\/       smin    z14.s, p0\/m, z14.s, z29.s\n+    __ sve_sub(z21, __ S, p5, z7);                     \/\/       sub     z21.s, p5\/m, z21.s, z7.s\n+    __ sve_fabs(z2, __ D, p0, z26);                    \/\/       fabs    z2.d, p0\/m, z26.d\n+    __ sve_fadd(z9, __ D, p4, z17);                    \/\/       fadd    z9.d, p4\/m, z9.d, z17.d\n+    __ sve_fdiv(z0, __ D, p1, z2);                     \/\/       fdiv    z0.d, p1\/m, z0.d, z2.d\n+    __ sve_fmax(z14, __ D, p1, z11);                   \/\/       fmax    z14.d, p1\/m, z14.d, z11.d\n+    __ sve_fmin(z14, __ S, p4, z29);                   \/\/       fmin    z14.s, p4\/m, z14.s, z29.s\n+    __ sve_fmul(z3, __ S, p0, z22);                    \/\/       fmul    z3.s, p0\/m, z3.s, z22.s\n+    __ sve_fneg(z3, __ S, p6, z27);                    \/\/       fneg    z3.s, p6\/m, z27.s\n+    __ sve_frintm(z19, __ D, p5, z7);                  \/\/       frintm  z19.d, p5\/m, z7.d\n+    __ sve_frintn(z21, __ S, p3, z5);                  \/\/       frintn  z21.s, p3\/m, z5.s\n+    __ sve_frintp(z25, __ D, p1, z21);                 \/\/       frintp  z25.d, p1\/m, z21.d\n+    __ sve_fsqrt(z17, __ S, p0, z3);                   \/\/       fsqrt   z17.s, p0\/m, z3.s\n+    __ sve_fsub(z19, __ S, p3, z7);                    \/\/       fsub    z19.s, p3\/m, z19.s, z7.s\n+    __ sve_fmad(z14, __ S, p4, z17, z11);              \/\/       fmad    z14.s, p4\/m, z17.s, z11.s\n+    __ sve_fmla(z24, __ S, p4, z30, z17);              \/\/       fmla    z24.s, p4\/m, z30.s, z17.s\n+    __ sve_fmls(z15, __ D, p3, z26, z22);              \/\/       fmls    z15.d, p3\/m, z26.d, z22.d\n+    __ sve_fmsb(z22, __ D, p2, z8, z5);                \/\/       fmsb    z22.d, p2\/m, z8.d, z5.d\n+    __ sve_fnmad(z27, __ D, p2, z0, z14);              \/\/       fnmad   z27.d, p2\/m, z0.d, z14.d\n+    __ sve_fnmsb(z21, __ D, p5, z0, z3);               \/\/       fnmsb   z21.d, p5\/m, z0.d, z3.d\n+    __ sve_fnmla(z25, __ D, p1, z25, z29);             \/\/       fnmla   z25.d, p1\/m, z25.d, z29.d\n+    __ sve_fnmls(z17, __ D, p0, z12, z14);             \/\/       fnmls   z17.d, p0\/m, z12.d, z14.d\n+    __ sve_mla(z13, __ D, p0, z17, z2);                \/\/       mla     z13.d, p0\/m, z17.d, z2.d\n+    __ sve_mls(z20, __ H, p5, z21, z29);               \/\/       mls     z20.h, p5\/m, z21.h, z29.h\n+    __ sve_and(z8, z2, z0);                            \/\/       and     z8.d, z2.d, z0.d\n+    __ sve_eor(z23, z22, z0);                          \/\/       eor     z23.d, z22.d, z0.d\n+    __ sve_orr(z25, z26, z23);                         \/\/       orr     z25.d, z26.d, z23.d\n+    __ sve_bic(z21, z21, z1);                          \/\/       bic     z21.d, z21.d, z1.d\n+    __ sve_uzp1(z10, __ S, z19, z11);                  \/\/       uzp1    z10.s, z19.s, z11.s\n+    __ sve_uzp2(z23, __ D, z23, z8);                   \/\/       uzp2    z23.d, z23.d, z8.d\n@@ -1188,9 +1199,9 @@\n-    __ sve_andv(v21, __ B, p5, z1);                    \/\/       andv b21, p5, z1.b\n-    __ sve_orv(v10, __ S, p5, z11);                    \/\/       orv s10, p5, z11.s\n-    __ sve_eorv(v23, __ D, p6, z8);                    \/\/       eorv d23, p6, z8.d\n-    __ sve_smaxv(v17, __ S, p5, z19);                  \/\/       smaxv s17, p5, z19.s\n-    __ sve_sminv(v4, __ D, p5, z13);                   \/\/       sminv d4, p5, z13.d\n-    __ sve_fminv(v22, __ D, p7, z30);                  \/\/       fminv d22, p7, z30.d\n-    __ sve_fmaxv(v17, __ S, p4, z14);                  \/\/       fmaxv s17, p4, z14.s\n-    __ sve_fadda(v12, __ S, p7, z20);                  \/\/       fadda s12, p7, s12, z20.s\n-    __ sve_uaddv(v1, __ B, p3, z13);                   \/\/       uaddv d1, p3, z13.b\n+    __ sve_andv(v17, __ S, p5, z19);                   \/\/       andv s17, p5, z19.s\n+    __ sve_orv(v4, __ D, p5, z13);                     \/\/       orv d4, p5, z13.d\n+    __ sve_eorv(v22, __ D, p7, z30);                   \/\/       eorv d22, p7, z30.d\n+    __ sve_smaxv(v17, __ H, p4, z14);                  \/\/       smaxv h17, p4, z14.h\n+    __ sve_sminv(v12, __ B, p7, z20);                  \/\/       sminv b12, p7, z20.b\n+    __ sve_fminv(v1, __ S, p3, z13);                   \/\/       fminv s1, p3, z13.s\n+    __ sve_fmaxv(v7, __ D, p2, z11);                   \/\/       fmaxv d7, p2, z11.d\n+    __ sve_fadda(v4, __ S, p6, z15);                   \/\/       fadda s4, p6, s4, z15.s\n+    __ sve_uaddv(v3, __ S, p7, z0);                    \/\/       uaddv d3, p7, z0.s\n@@ -1215,7 +1226,7 @@\n-    0x14000000,     0x17ffffd7,     0x140003e4,     0x94000000,\n-    0x97ffffd4,     0x940003e1,     0x3400000a,     0x34fffa2a,\n-    0x34007bca,     0x35000008,     0x35fff9c8,     0x35007b68,\n-    0xb400000b,     0xb4fff96b,     0xb4007b0b,     0xb500001d,\n-    0xb5fff91d,     0xb5007abd,     0x10000013,     0x10fff8b3,\n-    0x10007a53,     0x90000013,     0x36300016,     0x3637f836,\n-    0x363079d6,     0x3758000c,     0x375ff7cc,     0x3758796c,\n+    0x14000000,     0x17ffffd7,     0x140003ef,     0x94000000,\n+    0x97ffffd4,     0x940003ec,     0x3400000a,     0x34fffa2a,\n+    0x34007d2a,     0x35000008,     0x35fff9c8,     0x35007cc8,\n+    0xb400000b,     0xb4fff96b,     0xb4007c6b,     0xb500001d,\n+    0xb5fff91d,     0xb5007c1d,     0x10000013,     0x10fff8b3,\n+    0x10007bb3,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36307b36,     0x3758000c,     0x375ff7cc,     0x37587acc,\n@@ -1226,13 +1237,13 @@\n-    0x54007740,     0x54000001,     0x54fff541,     0x540076e1,\n-    0x54000002,     0x54fff4e2,     0x54007682,     0x54000002,\n-    0x54fff482,     0x54007622,     0x54000003,     0x54fff423,\n-    0x540075c3,     0x54000003,     0x54fff3c3,     0x54007563,\n-    0x54000004,     0x54fff364,     0x54007504,     0x54000005,\n-    0x54fff305,     0x540074a5,     0x54000006,     0x54fff2a6,\n-    0x54007446,     0x54000007,     0x54fff247,     0x540073e7,\n-    0x54000008,     0x54fff1e8,     0x54007388,     0x54000009,\n-    0x54fff189,     0x54007329,     0x5400000a,     0x54fff12a,\n-    0x540072ca,     0x5400000b,     0x54fff0cb,     0x5400726b,\n-    0x5400000c,     0x54fff06c,     0x5400720c,     0x5400000d,\n-    0x54fff00d,     0x540071ad,     0x5400000e,     0x54ffefae,\n-    0x5400714e,     0x5400000f,     0x54ffef4f,     0x540070ef,\n+    0x540078a0,     0x54000001,     0x54fff541,     0x54007841,\n+    0x54000002,     0x54fff4e2,     0x540077e2,     0x54000002,\n+    0x54fff482,     0x54007782,     0x54000003,     0x54fff423,\n+    0x54007723,     0x54000003,     0x54fff3c3,     0x540076c3,\n+    0x54000004,     0x54fff364,     0x54007664,     0x54000005,\n+    0x54fff305,     0x54007605,     0x54000006,     0x54fff2a6,\n+    0x540075a6,     0x54000007,     0x54fff247,     0x54007547,\n+    0x54000008,     0x54fff1e8,     0x540074e8,     0x54000009,\n+    0x54fff189,     0x54007489,     0x5400000a,     0x54fff12a,\n+    0x5400742a,     0x5400000b,     0x54fff0cb,     0x540073cb,\n+    0x5400000c,     0x54fff06c,     0x5400736c,     0x5400000d,\n+    0x54fff00d,     0x5400730d,     0x5400000e,     0x54ffefae,\n+    0x540072ae,     0x5400000f,     0x54ffef4f,     0x5400724f,\n@@ -1403,1 +1414,2 @@\n-    0x052281e0,     0x05238601,     0x04a14026,     0x0568aca7,\n+    0x052281e0,     0x05238601,     0x04a14026,     0x042244a6,\n+    0x046344a6,     0x04a444a6,     0x04e544a7,     0x0568aca7,\n@@ -1414,1 +1426,2 @@\n-    0x05314001,     0x05271e11,     0x1e601000,     0x1e603000,\n+    0x05314001,     0x05a18610,     0x05e18610,     0x05271e11,\n+    0x45b0c210,     0x45f1c231,     0x1e601000,     0x1e603000,\n@@ -1452,13 +1465,14 @@\n-    0x049089bc,     0x045b1787,     0x049aad6b,     0x04991901,\n-    0x0493922d,     0x04518064,     0x04900dc7,     0x0417afa4,\n-    0x04deaaa0,     0x04980123,     0x04080b1c,     0x04ca06f3,\n-    0x04c1154d,     0x04dcb3cc,     0x65c083ae,     0x65cd94f5,\n-    0x65c68342,     0x65c79229,     0x65c28440,     0x04dda56e,\n-    0x6582b3ae,     0x6580a2c3,     0x6581bb63,     0x65cdb4f3,\n-    0x65818cb5,     0x65f186b9,     0x65b30120,     0x65f12dc7,\n-    0x65f1af0b,     0x65eec9f1,     0x65a7fed6,     0x65aa5f65,\n-    0x65b47aae,     0x04c55723,     0x0441723d,     0x042d33ae,\n-    0x04be3051,     0x047d32b6,     0x04e03048,     0x05a06ad7,\n-    0x05776f59,     0x041a3435,     0x0498356a,     0x04d93917,\n-    0x04883671,     0x04ca35a4,     0x65c73fd6,     0x658631d1,\n-    0x65983e8c,     0x04012da1,\n+    0x049089bc,     0x045b1787,     0x0499ad6b,     0x049ab901,\n+    0x0499122d,     0x04538064,     0x04918dc7,     0x04100fa4,\n+    0x04d7aaa0,     0x049ea123,     0x04180b1c,     0x05e786f3,\n+    0x05e4954d,     0x048813cc,     0x048a03ae,     0x048114f5,\n+    0x04dca342,     0x65c09229,     0x65cd8440,     0x65c6856e,\n+    0x658793ae,     0x658282c3,     0x049dbb63,     0x65c2b4f3,\n+    0x6580acb5,     0x65c1a6b9,     0x658da071,     0x65818cf3,\n+    0x65ab922e,     0x65b113d8,     0x65f62f4f,     0x65e5a916,\n+    0x65eec81b,     0x65e3f415,     0x65fd4739,     0x65ee6191,\n+    0x04c2422d,     0x045d76b4,     0x04203048,     0x04a032d7,\n+    0x04773359,     0x04e132b5,     0x05ab6a6a,     0x05e86ef7,\n+    0x049a3671,     0x04d835a4,     0x04d93fd6,     0x044831d1,\n+    0x040a3e8c,     0x65872da1,     0x65c62967,     0x659839e4,\n+    0x04813c03,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":97,"deletions":83,"binary":false,"changes":180,"status":"modified"}]}