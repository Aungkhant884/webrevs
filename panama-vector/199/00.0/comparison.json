{"files":[{"patch":"@@ -31,1 +31,1 @@\n-\/\/ archtecture.\n+\/\/ architecture.\n@@ -491,1 +491,1 @@\n-\/\/ the AArch64 CSPR status flag register is not directly acessible as\n+\/\/ the AArch64 CSPR status flag register is not directly accessible as\n@@ -1511,1 +1511,1 @@\n-  \/\/ MemBarAcquire, possibly thorugh an optional DecodeN, is still\n+  \/\/ MemBarAcquire, possibly through an optional DecodeN, is still\n@@ -2810,0 +2810,4 @@\n+bool Parse::do_one_bytecode_targeted() {\n+  return false;\n+}\n+\n@@ -4131,1 +4135,1 @@\n-\/\/         are owned by the CALLEE.  Holes should not be nessecary in the\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n@@ -4134,1 +4138,1 @@\n-\/\/         avoid holes.  Holes in the outgoing arguments may be nessecary for\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n@@ -15151,0 +15155,24 @@\n+instruct round_double_reg(iRegLNoSp dst, vRegD src, vRegD ftmp, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP_DEF dst, TEMP ftmp, KILL cr);\n+  format %{ \"java_round_double $dst,$src\"%}\n+  ins_encode %{\n+    __ java_round_double($dst$$Register, as_FloatRegister($src$$reg),\n+                         as_FloatRegister($ftmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(iRegINoSp dst, vRegF src, vRegF ftmp, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP_DEF dst, TEMP ftmp, KILL cr);\n+  format %{ \"java_round_float $dst,$src\"%}\n+  ins_encode %{\n+    __ java_round_float($dst$$Register, as_FloatRegister($src$$reg),\n+                        as_FloatRegister($ftmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17358,1 +17386,1 @@\n-\/\/   \/\/ increment preceeded by register-register move\n+\/\/   \/\/ increment preceded by register-register move\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -573,0 +573,46 @@\n+\n+instruct vroundvecD2Fto2I(vecD dst, vecD src, vecD tmp1, vecD tmp2, vecD tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, T2S, $src\\t# round vecD 2F to 2I vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ T2S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vroundvecX4Fto4I(vecX dst, vecX src, vecX tmp1, vecX tmp2, vecX tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, T4S, $src\\t# round vecX 4F to 4I vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ T4S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vroundvecX2Dto2L(vecX dst, vecX src, vecX tmp1, vecX tmp2, vecX tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (RoundVD src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, T2D, $src\\t# round vecX 2D to 2L vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ T2D);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n@@ -1707,35 +1753,1 @@\n-instruct insert8B(vecD dst, vecD src, iRegIorL2I val, immI idx)\n-%{\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorInsert (Binary src val) idx));\n-  ins_cost(INSN_COST);\n-  format %{ \"orr    $dst, T8B, $src, $src\\n\\t\"\n-            \"mov    $dst, B, $idx, $val\\t# insert into vector(8B)\" %}\n-  ins_encode %{\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n-             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    }\n-    __ mov(as_FloatRegister($dst$$reg), __ B, $idx$$constant, $val$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct insert16B(vecX dst, vecX src, iRegIorL2I val, immI idx)\n-%{\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorInsert (Binary src val) idx));\n-  ins_cost(INSN_COST);\n-  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n-            \"mov    $dst, B, $idx, $val\\t# insert into vector(16B)\" %}\n-  ins_encode %{\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n-             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    }\n-    __ mov(as_FloatRegister($dst$$reg), __ B, $idx$$constant, $val$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct insert4S(vecD dst, vecD src, iRegIorL2I val, immI idx)\n+instruct insertID(vecD dst, vecD src, iRegIorL2I val, immI idx)\n@@ -1743,1 +1755,3 @@\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  predicate((n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -1745,35 +1759,1 @@\n-  ins_cost(INSN_COST);\n-  format %{ \"orr    $dst, T8B, $src, $src\\n\\t\"\n-            \"mov    $dst, H, $idx, $val\\t# insert into vector(4S)\" %}\n-  ins_encode %{\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n-             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    }\n-    __ mov(as_FloatRegister($dst$$reg), __ H, $idx$$constant, $val$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct insert8S(vecX dst, vecX src, iRegIorL2I val, immI idx)\n-%{\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorInsert (Binary src val) idx));\n-  ins_cost(INSN_COST);\n-  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n-            \"mov    $dst, H, $idx, $val\\t# insert into vector(8S)\" %}\n-  ins_encode %{\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n-             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    }\n-    __ mov(as_FloatRegister($dst$$reg), __ H, $idx$$constant, $val$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct insert2I(vecD dst, vecD src, iRegIorL2I val, immI idx)\n-%{\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorInsert (Binary src val) idx));\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -1781,1 +1761,1 @@\n-            \"mov    $dst, S, $idx, $val\\t# insert into vector(2I)\" %}\n+            \"mov    $dst, B\/H\/S, $idx, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -1787,1 +1767,2 @@\n-    __ mov(as_FloatRegister($dst$$reg), __ S, $idx$$constant, $val$$Register);\n+    __ mov(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(Matcher::vector_element_basic_type(this)),\n+           $idx$$constant, $val$$Register);\n@@ -1792,1 +1773,1 @@\n-instruct insert4I(vecX dst, vecX src, iRegIorL2I val, immI idx)\n+instruct insertIX(vecX dst, vecX src, iRegIorL2I val, immI idx)\n@@ -1794,1 +1775,3 @@\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  predicate((n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -1796,1 +1779,1 @@\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -1798,1 +1781,1 @@\n-            \"mov    $dst, S, $idx, $val\\t# insert into vector(4I)\" %}\n+            \"mov    $dst, B\/H\/S, $idx, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -1804,1 +1787,2 @@\n-    __ mov(as_FloatRegister($dst$$reg), __ S, $idx$$constant, $val$$Register);\n+    __ mov(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(Matcher::vector_element_basic_type(this)),\n+           $idx$$constant, $val$$Register);\n@@ -1813,1 +1797,1 @@\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -1815,1 +1799,1 @@\n-            \"mov    $dst, D, $idx, $val\\t# insert into vector(2L)\" %}\n+            \"mov    $dst, D, $idx, $val\\t# insert into vector (D)\" %}\n@@ -1821,1 +1805,2 @@\n-    __ mov(as_FloatRegister($dst$$reg), __ D, $idx$$constant, $val$$Register);\n+    __ mov(as_FloatRegister($dst$$reg), __ D,\n+           $idx$$constant, $val$$Register);\n@@ -1830,1 +1815,1 @@\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -1847,1 +1832,1 @@\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -1864,1 +1849,1 @@\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -1970,2 +1955,9 @@\n-    __ ins(as_FloatRegister($dst$$reg), __ S,\n-           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    if ((0 == $idx$$constant) &&\n+        (as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg))) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovs(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else {\n+      __ ins(as_FloatRegister($dst$$reg), __ S,\n+             as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    }\n@@ -1983,2 +1975,9 @@\n-    __ ins(as_FloatRegister($dst$$reg), __ S,\n-           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    if ((0 == $idx$$constant) &&\n+        (as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg))) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovs(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else {\n+      __ ins(as_FloatRegister($dst$$reg), __ S,\n+             as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    }\n@@ -1996,2 +1995,9 @@\n-    __ ins(as_FloatRegister($dst$$reg), __ D,\n-           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    if ((0 == $idx$$constant) &&\n+        (as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg))) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovd(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else {\n+      __ ins(as_FloatRegister($dst$$reg), __ D,\n+             as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    }\n@@ -5899,1 +5905,1 @@\n-    \/\/ substracting it by 7 (VLENGTH - 1).\n+    \/\/ subtracting it by 7 (VLENGTH - 1).\n@@ -5931,1 +5937,1 @@\n-    \/\/ Count the leading zero bytes and substract it by 15 (VLENGTH - 1).\n+    \/\/ Count the leading zero bytes and subtract it by 15 (VLENGTH - 1).\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":98,"deletions":92,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -352,0 +352,19 @@\n+define(`VECTOR_JAVA_FROUND', `\n+instruct vround$7$2to$5$3($7 dst, $7 src, $7 tmp1, $7 tmp2, $7 tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == $5 && n->bottom_type()->is_vect()->element_basic_type() == T_$6);\n+  match(Set dst (RoundV$1 src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, $4, $src\\t# round $7 $2 to $5$3 vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ $4);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl           $1  $2  $3   $4 $5    $6    $7\n+VECTOR_JAVA_FROUND(F, 2F,  I, T2S, 2,  INT, vecD)\n+VECTOR_JAVA_FROUND(F, 4F,  I, T4S, 4,  INT, vecX)\n+VECTOR_JAVA_FROUND(D, 2D,  L, T2D, 2, LONG, vecX)\n+\n@@ -871,0 +890,2 @@\n+dnl VECTOR_INSERT_I($1,        $2,                     $3,          $4,   $5)\n+dnl VECTOR_INSERT_I(rule_name, vector_length_in_bytes, reg_variant, vreg, ireg)\n@@ -872,1 +893,1 @@\n-instruct insert$1$2`'(vec$3 dst, vec$3 src, iReg$4`'ORL2I($4) val, immI idx)\n+instruct $1($4 dst, $4 src, $5 val, immI idx)\n@@ -874,1 +895,4 @@\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n+  predicate(ifelse($3, D, n->bottom_type()->is_vect()->element_basic_type() == T_LONG,\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT)));\n@@ -876,3 +900,3 @@\n-  ins_cost(INSN_COST);\n-  format %{ \"orr    $dst, T$5, $src, $src\\n\\t\"\n-            \"mov    $dst, iTYPE2SIMD($2), $idx, $val\\t# insert into vector($1$2)\" %}\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"orr    $dst, T$2B, $src, $src\\n\\t\"\n+            \"mov    $dst, $3, $idx, $val\\t`#' insert into vector ($3)\" %}\n@@ -881,1 +905,1 @@\n-      __ orr(as_FloatRegister($dst$$reg), __ T$5,\n+      __ orr(as_FloatRegister($dst$$reg), __ T$2B,\n@@ -884,1 +908,2 @@\n-    __ mov(as_FloatRegister($dst$$reg), __ iTYPE2SIMD($2), $idx$$constant, $val$$Register);\n+    __ mov(as_FloatRegister($dst$$reg), __ ifelse($3, D, D, elemType_to_regVariant(Matcher::vector_element_basic_type(this))),\n+           $idx$$constant, $val$$Register);\n@@ -888,8 +913,4 @@\n-dnl             $1  $2 $3 $4 $5\n-VECTOR_INSERT_I(8,  B, D, I, 8B)\n-VECTOR_INSERT_I(16, B, X, I, 16B)\n-VECTOR_INSERT_I(4,  S, D, I, 8B)\n-VECTOR_INSERT_I(8,  S, X, I, 16B)\n-VECTOR_INSERT_I(2,  I, D, I, 8B)\n-VECTOR_INSERT_I(4,  I, X, I, 16B)\n-VECTOR_INSERT_I(2,  L, X, L, 16B)\n+dnl             $1        $2  $3     $4    $5\n+VECTOR_INSERT_I(insertID, 8,  B\/H\/S, vecD, iRegIorL2I)\n+VECTOR_INSERT_I(insertIX, 16, B\/H\/S, vecX, iRegIorL2I)\n+VECTOR_INSERT_I(insert2L, 16, D,     vecX, iRegL)\n@@ -898,1 +919,1 @@\n-instruct insert$1`'(vec$2 dst, vec$2 src, vReg$3 val, immI idx)\n+instruct insert$3`'(vec$2 dst, vec$2 src, vReg$1 val, immI idx)\n@@ -900,1 +921,1 @@\n-  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($1));\n@@ -902,1 +923,1 @@\n-  ins_cost(INSN_COST);\n+  ins_cost(2 * INSN_COST);\n@@ -904,2 +925,2 @@\n-  format %{ \"orr    $dst, T$4, $src, $src\\n\\t\"\n-            \"ins    $dst, $5, $val, $idx, 0\\t# insert into vector($1)\" %}\n+  format %{ \"orr    $dst, ifelse($2, D, T8B, T16B), $src, $src\\n\\t\"\n+            \"ins    $dst, ifelse($1, F, S, D), $val, $idx, 0\\t# insert into vector($3)\" %}\n@@ -907,1 +928,1 @@\n-    __ orr(as_FloatRegister($dst$$reg), __ T$4,\n+    __ orr(as_FloatRegister($dst$$reg), __ ifelse($2, D, T8B, T16B),\n@@ -909,1 +930,1 @@\n-    __ ins(as_FloatRegister($dst$$reg), __ $5,\n+    __ ins(as_FloatRegister($dst$$reg), __ ifelse($1, F, S, D),\n@@ -914,4 +935,4 @@\n-dnl             $1  $2 $3 $4   $5\n-VECTOR_INSERT_F(2F, D, F, 8B,  S)\n-VECTOR_INSERT_F(4F, X, F, 16B, S)\n-VECTOR_INSERT_F(2D, X, D, 16B, D)\n+dnl             $1 $2 $3\n+VECTOR_INSERT_F(F, D, 2F)\n+VECTOR_INSERT_F(F, X, 4F)\n+VECTOR_INSERT_F(D, X, 2D)\n@@ -950,2 +971,9 @@\n-    __ ins(as_FloatRegister($dst$$reg), __ $4,\n-           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    if ((0 == $idx$$constant) &&\n+        (as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg))) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ ifelse($2, F, fmovs, fmovd)(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else {\n+      __ ins(as_FloatRegister($dst$$reg), __ $4,\n+             as_FloatRegister($src$$reg), 0, $idx$$constant);\n+    }\n@@ -2574,1 +2602,1 @@\n-    \/\/ substracting it by 7 (VLENGTH - 1).\n+    \/\/ subtracting it by 7 (VLENGTH - 1).\n@@ -2606,1 +2634,1 @@\n-    \/\/ Count the leading zero bytes and substract it by 15 (VLENGTH - 1).\n+    \/\/ Count the leading zero bytes and subtract it by 15 (VLENGTH - 1).\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":58,"deletions":30,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -167,1 +167,0 @@\n-\n@@ -3356,0 +3355,48 @@\n+instruct vroundFtoI(vReg dst, vReg src, vReg tmp1, vReg tmp2, vReg tmp3, pRegGov ptmp)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP ptmp);\n+  format %{ \"sve_vround  $dst, S, $src\\t# round F to I vector\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    if (vlen > 16) {\n+      __ vector_round_sve(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                          as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                          as_PRegister($ptmp$$reg), __ S);\n+    } else {\n+      __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                           as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                           as_FloatRegister($tmp3$$reg),\n+                           __ esize2arrangement(type2aelembytes(bt),\n+                              \/*isQ*\/ vlen == 16));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vroundDtoL(vReg dst, vReg src, vReg tmp1, vReg tmp2, vReg tmp3, pRegGov ptmp)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (RoundVD src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP ptmp);\n+  format %{ \"sve_vround  $dst, D, $src\\t# round D to L vector\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    if (vlen > 16) {\n+      __ vector_round_sve(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                          as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                          as_PRegister($ptmp$$reg), __ D);\n+    } else {\n+      __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                           as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                           as_FloatRegister($tmp3$$reg),\n+                           __ esize2arrangement(type2aelembytes(bt),\n+                              \/*isQ*\/ vlen == 16));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n@@ -4739,1 +4786,1 @@\n-instruct extractB(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+instruct extractB(iRegINoSp dst, vReg src, immI idx, vReg vtmp)\n@@ -4741,1 +4788,1 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 16);\n@@ -4743,1 +4790,1 @@\n-  effect(TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp);\n@@ -4745,2 +4792,1 @@\n-  format %{ \"sve_extract $dst, B, $pgtmp, $src, $idx\\n\\t\"\n-            \"sbfmw $dst, $dst, 0U, 7U\\t# extract from vector(B)\" %}\n+  format %{ \"sve_extract_integral $dst, B, $src, $idx\\t# extract from vector(B)\" %}\n@@ -4748,3 +4794,2 @@\n-    __ sve_extract(as_Register($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n-    __ sbfmw(as_Register($dst$$reg), as_Register($dst$$reg), 0U, 7U);\n+    __ sve_extract_integral(as_Register($dst$$reg), __ B, as_FloatRegister($src$$reg),\n+                            (int)($idx$$constant), \/* is_signed *\/ true, as_FloatRegister($vtmp$$reg));\n@@ -4755,1 +4800,1 @@\n-instruct extractS(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+instruct extractS(iRegINoSp dst, vReg src, immI idx, vReg vtmp)\n@@ -4757,1 +4802,1 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 8);\n@@ -4759,1 +4804,1 @@\n-  effect(TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp);\n@@ -4761,2 +4806,1 @@\n-  format %{ \"sve_extract $dst, H, $pgtmp, $src, $idx\\n\\t\"\n-            \"sbfmw $dst, $dst, 0U, 15U\\t# extract from vector(S)\" %}\n+  format %{ \"sve_extract_integral $dst, H, $src, $idx\\t# extract from vector(S)\" %}\n@@ -4764,3 +4808,2 @@\n-    __ sve_extract(as_Register($dst$$reg), __ H, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n-    __ sbfmw(as_Register($dst$$reg), as_Register($dst$$reg), 0U, 15U);\n+    __ sve_extract_integral(as_Register($dst$$reg), __ H, as_FloatRegister($src$$reg),\n+                            (int)($idx$$constant), \/* is_signed *\/ true, as_FloatRegister($vtmp$$reg));\n@@ -4771,2 +4814,1 @@\n-\n-instruct extractI(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+instruct extractI(iRegINoSp dst, vReg src, immI idx, vReg vtmp)\n@@ -4774,1 +4816,1 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 4);\n@@ -4776,1 +4818,1 @@\n-  effect(TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp);\n@@ -4778,1 +4820,1 @@\n-  format %{ \"sve_extract $dst, S, $pgtmp, $src, $idx\\t# extract from vector(I)\" %}\n+  format %{ \"sve_extract_integral $dst, S, $src, $idx\\t# extract from vector(I)\" %}\n@@ -4780,2 +4822,2 @@\n-    __ sve_extract(as_Register($dst$$reg), __ S, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n+    __ sve_extract_integral(as_Register($dst$$reg), __ S, as_FloatRegister($src$$reg),\n+                            (int)($idx$$constant), \/* is_signed *\/ true, as_FloatRegister($vtmp$$reg));\n@@ -4786,1 +4828,1 @@\n-instruct extractL(iRegLNoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+instruct extractL(iRegLNoSp dst, vReg src, immI idx, vReg vtmp)\n@@ -4788,1 +4830,1 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 2);\n@@ -4790,1 +4832,1 @@\n-  effect(TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp);\n@@ -4792,1 +4834,1 @@\n-  format %{ \"sve_extract $dst, D, $pgtmp, $src, $idx\\t# extract from vector(L)\" %}\n+  format %{ \"sve_extract_integral $dst, D, $src, $idx\\t# extract from vector(L)\" %}\n@@ -4794,2 +4836,2 @@\n-    __ sve_extract(as_Register($dst$$reg), __ D, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n+    __ sve_extract_integral(as_Register($dst$$reg), __ D, as_FloatRegister($src$$reg),\n+                            (int)($idx$$constant), \/* is_signed *\/ false, as_FloatRegister($vtmp$$reg));\n@@ -4800,1 +4842,49 @@\n-instruct extractF(vRegF dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+instruct extractB_LT16(iRegINoSp dst, vReg src, immI idx)\n+%{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 16);\n+  match(Set dst (ExtractB src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov $dst, B, $src, $idx\\t# extract from vector(B)\" %}\n+  ins_encode %{\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($src$$reg), __ B, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extractS_LT8(iRegINoSp dst, vReg src, immI idx)\n+%{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 8);\n+  match(Set dst (ExtractS src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov $dst, H, $src, $idx\\t# extract from vector(S)\" %}\n+  ins_encode %{\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($src$$reg), __ H, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extractI_LT4(iRegINoSp dst, vReg src, immI idx)\n+%{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 4);\n+  match(Set dst (ExtractI src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov $dst, S, $src, $idx\\t# extract from vector(I)\" %}\n+  ins_encode %{\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($src$$reg), __ S, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extractL_LT2(iRegLNoSp dst, vReg src, immI idx)\n+%{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 2);\n+  match(Set dst (ExtractL src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"umov $dst, D, $src, $idx\\t# extract from vector(L)\" %}\n+  ins_encode %{\n+    __ umov(as_Register($dst$$reg), as_FloatRegister($src$$reg), __ D, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extractF(vRegF dst, vReg src, immI idx)\n@@ -4804,5 +4894,12 @@\n-  effect(TEMP pgtmp, KILL cr);\n-  format %{ \"sve_extract $dst, S, $pgtmp, $src, $idx\\t# extract from vector(F)\" %}\n-  ins_encode %{\n-    __ sve_extract(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n+  format %{ \"sve_extract_f $dst, S, $src, $idx\\t# extract from vector(F)\" %}\n+  ins_encode %{\n+    if ((as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg)) && ($idx$$constant == 0)) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovs(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else if ($idx$$constant < 4) {\n+      __ ins(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), 0, (int)($idx$$constant));\n+    } else {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      __ sve_ext(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), $idx$$constant << 2);\n+    }\n@@ -4814,1 +4911,1 @@\n-instruct extractD(vRegD dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+instruct extractD(vRegD dst, vReg src, immI idx)\n@@ -4818,5 +4915,12 @@\n-  effect(TEMP pgtmp, KILL cr);\n-  format %{ \"sve_extract $dst, D, $pgtmp, $src, $idx\\t# extract from vector(D)\" %}\n-  ins_encode %{\n-    __ sve_extract(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n+  format %{ \"sve_extract_d $dst, D, $src, $idx\\t# extract from vector(D)\" %}\n+  ins_encode %{\n+    if ((as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg)) && ($idx$$constant == 0)) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovd(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else if ($idx$$constant == 1) {\n+      __ ins(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), 0, 1);\n+    } else {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      __ sve_ext(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), $idx$$constant << 3);\n+    }\n@@ -4865,3 +4969,3 @@\n-instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() <= 32 &&\n+instruct insertI_le128bits(vReg dst, vReg src, iRegIorL2I val, immI idx) %{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_length_in_bytes(n) == 8 || Matcher::vector_length_in_bytes(n) == 16) &&\n@@ -4872,6 +4976,3 @@\n-  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_index $dst, -16, 1\\t# (B\/H\/S)\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n-            \"sve_orr $dst, $src, $src\\n\\t\"\n-            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"orr    $dst, T8\/16B, $src, $src\\n\\t\"\n+            \"mov    $dst, B\/H\/S, $idx, $val\\t# insertI into vector(64\/128bits)\" %}\n@@ -4879,7 +4980,7 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_index(as_FloatRegister($dst$$reg), size, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), Matcher::vector_length_in_bytes(this) == 8 ? __ T8B : __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(Matcher::vector_element_basic_type(this)),\n+           $idx$$constant, $val$$Register);\n@@ -4890,4 +4991,6 @@\n-instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() <= 32 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+instruct insertI_small_index(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg vtmp, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -4895,1 +4998,1 @@\n-  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp, TEMP pgtmp, KILL cr);\n@@ -4897,2 +5000,2 @@\n-  format %{ \"sve_index $dst, S, -16, 1\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $vtmp, -16, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $vtmp, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -4900,1 +5003,1 @@\n-            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -4902,5 +5005,11 @@\n-    __ sve_index(as_FloatRegister($dst$$reg), __ S, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ block_comment(\"insert into vector (B\/H\/S) {\");\n+      __ sve_index(as_FloatRegister($vtmp$$reg), size, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n+                 as_FloatRegister($vtmp$$reg), (int)($idx$$constant) - 16);\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+      __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), $val$$Register);\n+    __ block_comment(\"} insert into vector (B\/H\/S)\");\n@@ -4911,3 +5020,3 @@\n-instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() > 32 &&\n+instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n@@ -4918,1 +5027,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n@@ -4920,3 +5029,3 @@\n-  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n-            \"sve_dup $dst, $idx\\t# (B\/H\/S)\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n+  format %{ \"sve_index $vtmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_dup $vtmp2, $idx\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $vtmp1, $vtmp2\\n\\t\"\n@@ -4928,6 +5037,10 @@\n-    __ sve_index(as_FloatRegister($tmp1$$reg), size, 0, 1);\n-    __ sve_dup(as_FloatRegister($dst$$reg), size, (int)($idx$$constant));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n-               as_FloatRegister($tmp1$$reg), as_FloatRegister($dst$$reg));\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n+    __ block_comment(\"insert into vector (B\/H\/S) {\");\n+      __ sve_index(as_FloatRegister($vtmp1$$reg), size, 0, 1);\n+      __ sve_dup(as_FloatRegister($vtmp2$$reg), size, (int)($idx$$constant));\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n+                 as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg));\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+      __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), $val$$Register);\n+    __ block_comment(\"} insert into vector (B\/H\/S)\");\n@@ -4938,2 +5051,19 @@\n-instruct insertL(vReg dst, vReg src, iRegL val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n+instruct insertL_128bits(vReg dst, vReg src, iRegL val, immI idx) %{\n+  predicate(UseSVE > 0 && Matcher::vector_length_in_bytes(n) == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"mov    $dst, D, $idx, $val\\t# insertL into vector(128bits)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ D, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertL(vReg dst, vReg src, iRegL val, immI idx, vReg vtmp, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -4941,0 +5071,1 @@\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n@@ -4943,1 +5074,1 @@\n-  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp, TEMP pgtmp, KILL cr);\n@@ -4945,2 +5076,2 @@\n-  format %{ \"sve_index $dst, D, -16, 1\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $vtmp, D, -16, 1\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $vtmp, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -4950,5 +5081,10 @@\n-    __ sve_index(as_FloatRegister($dst$$reg), __ D, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n+    __ block_comment(\"insert into vector (L) {\");\n+      __ sve_index(as_FloatRegister($vtmp$$reg), __ D, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n+                 as_FloatRegister($vtmp$$reg), (int)($idx$$constant) - 16);\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ D,\n+                 as_PRegister($pgtmp$$reg), $val$$Register);\n+    __ block_comment(\"} insert into vector (L)\");\n@@ -4959,4 +5095,21 @@\n-instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+instruct insertF_le128bits(vReg dst, vReg src, vRegF val, immI idx) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            (Matcher::vector_length_in_bytes(n) == 8 || Matcher::vector_length_in_bytes(n) == 16));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(2 * INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T8\/16B, $src, $src\\n\\t\"\n+            \"ins    $dst, S, $val, $idx, 0\\t# insertF into vector(64\/128bits)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), Matcher::vector_length_in_bytes(this) == 8 ? __ T8B : __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ S,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertF_small_index(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n@@ -4966,1 +5119,1 @@\n-  format %{ \"sve_index $dst, D, -16, 1\\n\\t\"\n+  format %{ \"sve_index $dst, S, -16, 1\\n\\t\"\n@@ -4969,1 +5122,1 @@\n-            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (D)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -4971,5 +5124,7 @@\n-    __ sve_index(as_FloatRegister($dst$$reg), __ D, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"insert into vector (F) {\");\n+      __ sve_index(as_FloatRegister($dst$$reg), __ S, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n+                 as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"} insert into vector (F)\");\n@@ -4980,3 +5135,3 @@\n-instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() > 32 &&\n+instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n@@ -4993,9 +5148,52 @@\n-    __ sve_index(as_FloatRegister($tmp1$$reg), __ S, 0, 1);\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ S, (int)($idx$$constant));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n-               as_FloatRegister($tmp1$$reg), as_FloatRegister($dst$$reg));\n-    __ sve_orr(as_FloatRegister($dst$$reg),\n-               as_FloatRegister($src$$reg),\n-               as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S,\n-               as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"insert into vector (F) {\");\n+      __ sve_index(as_FloatRegister($tmp1$$reg), __ S, 0, 1);\n+      __ sve_dup(as_FloatRegister($dst$$reg), __ S, (int)($idx$$constant));\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n+                 as_FloatRegister($tmp1$$reg), as_FloatRegister($dst$$reg));\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                 as_FloatRegister($src$$reg));\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ S,\n+                 as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"} insert into vector (F)\");\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertD_128bits(vReg dst, vReg src, vRegD val, immI idx) %{\n+  predicate(UseSVE > 0 && Matcher::vector_length_in_bytes(n) == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(2 * INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"ins    $dst, D, $val, $idx, 0\\t# insertD into vector(128bits)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ D,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_index $dst, D, -16, 1\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_orr $dst, $src, $src\\n\\t\"\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (D)\" %}\n+  ins_encode %{\n+    __ block_comment(\"insert into vector (D) {\");\n+      __ sve_index(as_FloatRegister($dst$$reg), __ D, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n+                 as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                 as_FloatRegister($src$$reg));\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ D,\n+                 as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"} insert into vector (D)\");\n@@ -5352,1 +5550,1 @@\n-instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2) %{\n@@ -5356,1 +5554,1 @@\n-  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp1, TEMP vtmp2);\n@@ -5363,2 +5561,1 @@\n-                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg),\n-                        as_PRegister($pgtmp$$reg));\n+                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":318,"deletions":121,"binary":false,"changes":439,"status":"modified"},{"patch":"@@ -162,1 +162,0 @@\n-\n@@ -1858,0 +1857,26 @@\n+define(`VECTOR_JAVA_FROUND', `\n+instruct vround$1to$3($7 dst, $7 src, $7 tmp1, $7 tmp2, $7 tmp3, pRegGov ptmp)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (RoundV$1 src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP ptmp);\n+  format %{ \"sve_vround  $dst, $4, $src\\t# round $1 to $3 vector\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    if (vlen > 16) {\n+      __ vector_round_sve(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                          as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                          as_PRegister($ptmp$$reg), __ $4);\n+    } else {\n+      __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                           as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                           as_FloatRegister($tmp3$$reg),\n+                           __ esize2arrangement(type2aelembytes(bt),\n+                              \/*isQ*\/ vlen == 16));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl           $1  $2  $3 $4 $5    $6    $7\n+VECTOR_JAVA_FROUND(F, 8F,  I, S, 8,  INT, vReg)\n+VECTOR_JAVA_FROUND(D, 4D,  L, D, 4, LONG, vReg)\n@@ -2393,2 +2418,3 @@\n-define(`VECTOR_EXTRACT_SXT', `\n-instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+dnl\n+define(`VECTOR_EXTRACT_I', `\n+instruct extract$1`'($3 dst, vReg src, immI idx, vReg vtmp)\n@@ -2396,1 +2422,1 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= $2);\n@@ -2398,1 +2424,1 @@\n-  effect(TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp);\n@@ -2400,2 +2426,1 @@\n-  format %{ \"sve_extract $dst, $3, $pgtmp, $src, $idx\\n\\t\"\n-            \"sbfmw $dst, $dst, 0U, $5\\t# extract from vector($1)\" %}\n+  format %{ \"sve_extract_integral $dst, $4, $src, $idx\\t# extract from vector($1)\" %}\n@@ -2403,3 +2428,2 @@\n-    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n-    __ sbfmw(as_$4($dst$$reg), as_$4($dst$$reg), 0U, $5);\n+    __ sve_extract_integral(as_Register($dst$$reg), __ $4, as_FloatRegister($src$$reg),\n+                            (int)($idx$$constant), \/* is_signed *\/ ifelse($1, L, false, true), as_FloatRegister($vtmp$$reg));\n@@ -2409,4 +2433,5 @@\n-dnl                $1 $2         $3 $4        $5\n-VECTOR_EXTRACT_SXT(B, iRegINoSp, B, Register, 7U)\n-VECTOR_EXTRACT_SXT(S, iRegINoSp, H, Register, 15U)\n-\n+dnl              $1 $2  $3         $4\n+VECTOR_EXTRACT_I(B, 16, iRegINoSp, B)\n+VECTOR_EXTRACT_I(S, 8,  iRegINoSp, H)\n+VECTOR_EXTRACT_I(I, 4,  iRegINoSp, S)\n+VECTOR_EXTRACT_I(L, 2,  iRegLNoSp, D)\n@@ -2414,2 +2439,2 @@\n-define(`VECTOR_EXTRACT', `\n-instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n+define(`VECTOR_EXTRACT_I_LT', `\n+instruct extract$1_LT$2`'($3 dst, vReg src, immI idx)\n@@ -2417,1 +2442,1 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < $2);\n@@ -2419,3 +2444,2 @@\n-  effect(TEMP pgtmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_extract $dst, $3, $pgtmp, $src, $idx\\t# extract from vector($1)\" %}\n+  ins_cost(INSN_COST);\n+  format %{ \"ifelse($4, D, umov, smov) $dst, $4, $src, $idx\\t# extract from vector($1)\" %}\n@@ -2423,2 +2447,1 @@\n-    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pgtmp$$reg),\n-                   as_FloatRegister($src$$reg), (int)($idx$$constant));\n+    __ ifelse($4, D, umov, smov)(as_Register($dst$$reg), as_FloatRegister($src$$reg), __ $4, $idx$$constant);\n@@ -2426,1 +2449,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(pipe_class_default);\n@@ -2428,5 +2451,47 @@\n-dnl            $1 $2         $3 $4\n-VECTOR_EXTRACT(I, iRegINoSp, S, Register)\n-VECTOR_EXTRACT(L, iRegLNoSp, D, Register)\n-VECTOR_EXTRACT(F, vRegF,     S, FloatRegister)\n-VECTOR_EXTRACT(D, vRegD,     D, FloatRegister)\n+dnl                 $1  $2  $3         $4\n+VECTOR_EXTRACT_I_LT(B,  16, iRegINoSp, B)\n+VECTOR_EXTRACT_I_LT(S,  8,  iRegINoSp, H)\n+VECTOR_EXTRACT_I_LT(I,  4,  iRegINoSp, S)\n+VECTOR_EXTRACT_I_LT(L,  2,  iRegLNoSp, D)\n+\n+instruct extractF(vRegF dst, vReg src, immI idx)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (ExtractF src idx));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_extract_f $dst, S, $src, $idx\\t# extract from vector(F)\" %}\n+  ins_encode %{\n+    if ((as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg)) && ($idx$$constant == 0)) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovs(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else if ($idx$$constant < 4) {\n+      __ ins(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), 0, (int)($idx$$constant));\n+    } else {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      __ sve_ext(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), $idx$$constant << 2);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct extractD(vRegD dst, vReg src, immI idx)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (ExtractD src idx));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_extract_d $dst, D, $src, $idx\\t# extract from vector(D)\" %}\n+  ins_encode %{\n+    if ((as_FloatRegister($dst$$reg) == as_FloatRegister($src$$reg)) && ($idx$$constant == 0)) {\n+      \/* empty *\/\n+    } else if ($idx$$constant == 0) {\n+      __ fmovd(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    } else if ($idx$$constant == 1) {\n+      __ ins(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), 0, 1);\n+    } else {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      __ sve_ext(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), $idx$$constant << 3);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2472,3 +2537,3 @@\n-instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() <= 32 &&\n+instruct insertI_le128bits(vReg dst, vReg src, iRegIorL2I val, immI idx) %{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_length_in_bytes(n) == 8 || Matcher::vector_length_in_bytes(n) == 16) &&\n@@ -2479,6 +2544,3 @@\n-  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_index $dst, -16, 1\\t# (B\/H\/S)\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n-            \"sve_orr $dst, $src, $src\\n\\t\"\n-            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"orr    $dst, T8\/16B, $src, $src\\n\\t\"\n+            \"mov    $dst, B\/H\/S, $idx, $val\\t# insertI into vector(64\/128bits)\" %}\n@@ -2486,7 +2548,7 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_index(as_FloatRegister($dst$$reg), size, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), Matcher::vector_length_in_bytes(this) == 8 ? __ T8B : __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(Matcher::vector_element_basic_type(this)),\n+           $idx$$constant, $val$$Register);\n@@ -2497,4 +2559,6 @@\n-instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() <= 32 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+instruct insertI_small_index(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg vtmp, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -2502,1 +2566,1 @@\n-  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp, TEMP pgtmp, KILL cr);\n@@ -2504,2 +2568,2 @@\n-  format %{ \"sve_index $dst, S, -16, 1\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $vtmp, -16, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $vtmp, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2507,1 +2571,1 @@\n-            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -2509,5 +2573,11 @@\n-    __ sve_index(as_FloatRegister($dst$$reg), __ S, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ block_comment(\"insert into vector (B\/H\/S) {\");\n+      __ sve_index(as_FloatRegister($vtmp$$reg), size, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n+                 as_FloatRegister($vtmp$$reg), (int)($idx$$constant) - 16);\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+      __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), $val$$Register);\n+    __ block_comment(\"} insert into vector (B\/H\/S)\");\n@@ -2518,3 +2588,3 @@\n-instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() > 32 &&\n+instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n@@ -2525,1 +2595,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n@@ -2527,3 +2597,3 @@\n-  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n-            \"sve_dup $dst, $idx\\t# (B\/H\/S)\\n\\t\"\n-            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n+  format %{ \"sve_index $vtmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_dup $vtmp2, $idx\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $vtmp1, $vtmp2\\n\\t\"\n@@ -2535,6 +2605,10 @@\n-    __ sve_index(as_FloatRegister($tmp1$$reg), size, 0, 1);\n-    __ sve_dup(as_FloatRegister($dst$$reg), size, (int)($idx$$constant));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n-               as_FloatRegister($tmp1$$reg), as_FloatRegister($dst$$reg));\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n+    __ block_comment(\"insert into vector (B\/H\/S) {\");\n+      __ sve_index(as_FloatRegister($vtmp1$$reg), size, 0, 1);\n+      __ sve_dup(as_FloatRegister($vtmp2$$reg), size, (int)($idx$$constant));\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n+                 as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg));\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+      __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), $val$$Register);\n+    __ block_comment(\"} insert into vector (B\/H\/S)\");\n@@ -2544,5 +2618,20 @@\n-dnl\n-dnl\n-define(`VECTOR_INSERT_D', `\n-instruct insert$1`'(vReg dst, vReg src, $2 val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n-%{\n+\n+instruct insertL_128bits(vReg dst, vReg src, iRegL val, immI idx) %{\n+  predicate(UseSVE > 0 && Matcher::vector_length_in_bytes(n) == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(2 * INSN_COST);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"mov    $dst, D, $idx, $val\\t# insertL into vector(128bits)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ D, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertL(vReg dst, vReg src, iRegL val, immI idx, vReg vtmp, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -2550,1 +2639,45 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($1));\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP vtmp, TEMP pgtmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_index $vtmp, D, -16, 1\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $vtmp, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_orr $dst, $src, $src\\n\\t\"\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (L)\" %}\n+  ins_encode %{\n+    __ block_comment(\"insert into vector (L) {\");\n+      __ sve_index(as_FloatRegister($vtmp$$reg), __ D, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n+                 as_FloatRegister($vtmp$$reg), (int)($idx$$constant) - 16);\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      }\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ D,\n+                 as_PRegister($pgtmp$$reg), $val$$Register);\n+    __ block_comment(\"} insert into vector (L)\");\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertF_le128bits(vReg dst, vReg src, vRegF val, immI idx) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            (Matcher::vector_length_in_bytes(n) == 8 || Matcher::vector_length_in_bytes(n) == 16));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(2 * INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T8\/16B, $src, $src\\n\\t\"\n+            \"ins    $dst, S, $val, $idx, 0\\t# insertF into vector(64\/128bits)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), Matcher::vector_length_in_bytes(this) == 8 ? __ T8B : __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ S,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertF_small_index(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() < 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n@@ -2554,1 +2687,1 @@\n-  format %{ \"sve_index $dst, $3, -16, 1\\n\\t\"\n+  format %{ \"sve_index $dst, S, -16, 1\\n\\t\"\n@@ -2557,1 +2690,1 @@\n-            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector ($1)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -2559,5 +2692,7 @@\n-    __ sve_index(as_FloatRegister($dst$$reg), __ $3, -16, 1);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ $3, ptrue,\n-               as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n-    __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ $3, as_PRegister($pgtmp$$reg), as_$4($val$$reg));\n+    __ block_comment(\"insert into vector (F) {\");\n+      __ sve_index(as_FloatRegister($dst$$reg), __ S, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n+                 as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"} insert into vector (F)\");\n@@ -2566,4 +2701,1 @@\n-%}')dnl\n-dnl             $1 $2     $3 $4\n-VECTOR_INSERT_D(L, iRegL, D, Register)\n-VECTOR_INSERT_D(D, vRegD, D, FloatRegister)\n+%}\n@@ -2571,3 +2703,3 @@\n-instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() > 32 &&\n+instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->get_int() >= 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n@@ -2584,9 +2716,52 @@\n-    __ sve_index(as_FloatRegister($tmp1$$reg), __ S, 0, 1);\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ S, (int)($idx$$constant));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n-               as_FloatRegister($tmp1$$reg), as_FloatRegister($dst$$reg));\n-    __ sve_orr(as_FloatRegister($dst$$reg),\n-               as_FloatRegister($src$$reg),\n-               as_FloatRegister($src$$reg));\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S,\n-               as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"insert into vector (F) {\");\n+      __ sve_index(as_FloatRegister($tmp1$$reg), __ S, 0, 1);\n+      __ sve_dup(as_FloatRegister($dst$$reg), __ S, (int)($idx$$constant));\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n+                 as_FloatRegister($tmp1$$reg), as_FloatRegister($dst$$reg));\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                 as_FloatRegister($src$$reg));\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ S,\n+                 as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"} insert into vector (F)\");\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertD_128bits(vReg dst, vReg src, vRegD val, immI idx) %{\n+  predicate(UseSVE > 0 && Matcher::vector_length_in_bytes(n) == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(2 * INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"ins    $dst, D, $val, $idx, 0\\t# insertD into vector(128bits)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ D,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_index $dst, D, -16, 1\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_orr $dst, $src, $src\\n\\t\"\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (D)\" %}\n+  ins_encode %{\n+    __ block_comment(\"insert into vector (D) {\");\n+      __ sve_index(as_FloatRegister($dst$$reg), __ D, -16, 1);\n+      __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n+                 as_FloatRegister($dst$$reg), (int)($idx$$constant) - 16);\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                 as_FloatRegister($src$$reg));\n+      __ sve_cpy(as_FloatRegister($dst$$reg), __ D,\n+                 as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n+    __ block_comment(\"} insert into vector (D)\");\n@@ -2929,1 +3104,1 @@\n-instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2) %{\n@@ -2933,1 +3108,1 @@\n-  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n+  effect(TEMP vtmp1, TEMP vtmp2);\n@@ -2940,2 +3115,1 @@\n-                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg),\n-                        as_PRegister($pgtmp$$reg));\n+                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":277,"deletions":103,"binary":false,"changes":380,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-\/\/ Use MSVC instrinsic: https:\/\/docs.microsoft.com\/en-us\/cpp\/intrinsics\/arm64-intrinsics?view=vs-2019#I\n+\/\/ Use MSVC intrinsic: https:\/\/docs.microsoft.com\/en-us\/cpp\/intrinsics\/arm64-intrinsics?view=vs-2019#I\n@@ -309,8 +309,0 @@\n-\n-  void fixed(unsigned value, unsigned mask) {\n-    assert_cond ((mask & bits) == 0);\n-#ifdef ASSERT\n-    bits |= mask;\n-#endif\n-    insn |= value;\n-  }\n@@ -596,1 +588,1 @@\n-\/\/ Convience classes\n+\/\/ Convenience classes\n@@ -701,1 +693,0 @@\n-#define fixed current_insn.fixed\n@@ -1088,1 +1079,1 @@\n-    \/\/ We can use ISH for a barrier because the ARM ARM says \"This\n+    \/\/ We can use ISH for a barrier because the Arm ARM says \"This\n@@ -2085,1 +2076,1 @@\n-  void float_int_convert(unsigned op31, unsigned type,\n+  void float_int_convert(unsigned sflag, unsigned ftype,\n@@ -2089,1 +2080,2 @@\n-    f(op31, 31, 29);\n+    f(sflag, 31);\n+    f(0b00, 30, 29);\n@@ -2091,1 +2083,1 @@\n-    f(type, 23, 22), f(1, 21), f(rmode, 20, 19);\n+    f(ftype, 23, 22), f(1, 21), f(rmode, 20, 19);\n@@ -2096,3 +2088,3 @@\n-#define INSN(NAME, op31, type, rmode, opcode)                           \\\n-  void NAME(Register Rd, FloatRegister Vn) {                            \\\n-    float_int_convert(op31, type, rmode, opcode, Rd, as_Register(Vn));  \\\n+#define INSN(NAME, sflag, ftype, rmode, opcode)                          \\\n+  void NAME(Register Rd, FloatRegister Vn) {                             \\\n+    float_int_convert(sflag, ftype, rmode, opcode, Rd, as_Register(Vn)); \\\n@@ -2101,4 +2093,12 @@\n-  INSN(fcvtzsw, 0b000, 0b00, 0b11, 0b000);\n-  INSN(fcvtzs,  0b100, 0b00, 0b11, 0b000);\n-  INSN(fcvtzdw, 0b000, 0b01, 0b11, 0b000);\n-  INSN(fcvtzd,  0b100, 0b01, 0b11, 0b000);\n+  INSN(fcvtzsw, 0b0, 0b00, 0b11, 0b000);\n+  INSN(fcvtzs,  0b1, 0b00, 0b11, 0b000);\n+  INSN(fcvtzdw, 0b0, 0b01, 0b11, 0b000);\n+  INSN(fcvtzd,  0b1, 0b01, 0b11, 0b000);\n+\n+  \/\/ RoundToNearestTiesAway\n+  INSN(fcvtassw, 0b0, 0b00, 0b00, 0b100);  \/\/ float -> signed word\n+  INSN(fcvtasd,  0b1, 0b01, 0b00, 0b100);  \/\/ double -> signed xword\n+\n+  \/\/ RoundTowardsNegative\n+  INSN(fcvtmssw, 0b0, 0b00, 0b10, 0b000);  \/\/ float -> signed word\n+  INSN(fcvtmsd,  0b1, 0b01, 0b10, 0b000);  \/\/ double -> signed xword\n@@ -2106,2 +2106,2 @@\n-  INSN(fmovs, 0b000, 0b00, 0b00, 0b110);\n-  INSN(fmovd, 0b100, 0b01, 0b00, 0b110);\n+  INSN(fmovs, 0b0, 0b00, 0b00, 0b110);\n+  INSN(fmovd, 0b1, 0b01, 0b00, 0b110);\n@@ -2109,1 +2109,1 @@\n-  INSN(fmovhid, 0b100, 0b10, 0b01, 0b110);\n+  INSN(fmovhid, 0b1, 0b10, 0b01, 0b110);\n@@ -2113,1 +2113,1 @@\n-#define INSN(NAME, op31, type, rmode, opcode)                           \\\n+#define INSN(NAME, sflag, type, rmode, opcode)                           \\\n@@ -2115,1 +2115,1 @@\n-    float_int_convert(op31, type, rmode, opcode, as_Register(Vd), Rn);  \\\n+    float_int_convert(sflag, type, rmode, opcode, as_Register(Vd), Rn);  \\\n@@ -2118,2 +2118,2 @@\n-  INSN(fmovs, 0b000, 0b00, 0b00, 0b111);\n-  INSN(fmovd, 0b100, 0b01, 0b00, 0b111);\n+  INSN(fmovs, 0b0, 0b00, 0b00, 0b111);\n+  INSN(fmovd, 0b1, 0b01, 0b00, 0b111);\n@@ -2121,4 +2121,4 @@\n-  INSN(scvtfws, 0b000, 0b00, 0b00, 0b010);\n-  INSN(scvtfs,  0b100, 0b00, 0b00, 0b010);\n-  INSN(scvtfwd, 0b000, 0b01, 0b00, 0b010);\n-  INSN(scvtfd,  0b100, 0b01, 0b00, 0b010);\n+  INSN(scvtfws, 0b0, 0b00, 0b00, 0b010);\n+  INSN(scvtfs,  0b1, 0b00, 0b00, 0b010);\n+  INSN(scvtfwd, 0b0, 0b01, 0b00, 0b010);\n+  INSN(scvtfd,  0b1, 0b01, 0b00, 0b010);\n@@ -2513,0 +2513,1 @@\n+\/\/ Advanced SIMD modified immediate\n@@ -2540,1 +2541,16 @@\n-#define INSN(NAME, op1, op2, op3) \\\n+#define INSN(NAME, op, cmode)                                           \\\n+  void NAME(FloatRegister Vd, SIMD_Arrangement T, double imm) {         \\\n+    unsigned imm8 = pack(imm);                                          \\\n+    starti;                                                             \\\n+    f(0, 31), f((int)T & 1, 30), f(op, 29), f(0b0111100000, 28, 19);    \\\n+    f(imm8 >> 5, 18, 16), f(cmode, 15, 12), f(0x01, 11, 10), f(imm8 & 0b11111, 9, 5); \\\n+    rf(Vd, 0);                                                          \\\n+  }\n+\n+  INSN(fmovs, 0, 0b1111);\n+  INSN(fmovd, 1, 0b1111);\n+\n+#undef INSN\n+\n+\/\/ Advanced SIMD three same\n+#define INSN(NAME, op1, op2, op3)                                                       \\\n@@ -2987,0 +3003,1 @@\n+  INSN(fcvtas, 0, 0b00, 0b01, 0b11100);\n@@ -2988,0 +3005,1 @@\n+  INSN(fcvtms, 0, 0b00, 0b01, 0b11011);\n@@ -3158,0 +3176,1 @@\n+  INSN(sve_frinta,  0b01100101, 0b000100101); \/\/ floating-point round to integral value, nearest with ties to away\n@@ -3453,2 +3472,3 @@\n-  \/\/ SVE copy signed integer immediate to vector elements (predicated)\n-  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8, bool isMerge) {\n+private:\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8,\n+               bool isMerge, bool isFloat) {\n@@ -3468,1 +3488,11 @@\n-    prf(Pg, 16), f(0b0, 15), f(m, 14), f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+    prf(Pg, 16), f(isFloat ? 1 : 0, 15), f(m, 14), f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+  }\n+\n+public:\n+  \/\/ SVE copy signed integer immediate to vector elements (predicated)\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8, bool isMerge) {\n+    sve_cpy(Zd, T, Pg, imm8, isMerge, \/*isFloat*\/false);\n+  }\n+  \/\/ SVE copy floating-point immediate to vector elements (predicated)\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, double d) {\n+    sve_cpy(Zd, T, Pg, checked_cast<int8_t>(pack(d)), \/*isMerge*\/true, \/*isFloat*\/true);\n@@ -3480,0 +3510,7 @@\n+  \/\/ SVE Permute Vector - Extract\n+  void sve_ext(FloatRegister Zdn, FloatRegister Zm, int imm8) {\n+    starti;\n+    f(0b00000101001, 31, 21), f(imm8 >> 3, 20, 16), f(0b000, 15, 13);\n+    f(imm8 & 0b111, 12, 10), rf(Zm, 5), rf(Zdn, 0);\n+  }\n+\n@@ -3532,0 +3569,23 @@\n+\/\/ SVE Floating-point compare vector with zero\n+void sve_fcm(Condition cond, PRegister Pd, SIMD_RegVariant T,\n+             PRegister Pg, FloatRegister Zn, double d) {\n+  starti;\n+  assert(T != Q, \"invalid size\");\n+  guarantee(d == 0.0, \"invalid immediate\");\n+  int cond_op;\n+  switch(cond) {\n+    case EQ: cond_op = 0b100; break;\n+    case GT: cond_op = 0b001; break;\n+    case GE: cond_op = 0b000; break;\n+    case LT: cond_op = 0b010; break;\n+    case LE: cond_op = 0b011; break;\n+    case NE: cond_op = 0b110; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  f(0b01100101, 31, 24), f(T, 23, 22), f(0b0100, 21, 18),\n+  f((cond_op >> 1) & 0x3, 17, 16), f(0b001, 15, 13),\n+  pgrf(Pg, 10), rf(Zn, 5);\n+  f(cond_op & 0x1, 4), prf(Pd, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":97,"deletions":37,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  \/\/ in str1, cnt1. Find the 1st occurence of pattern in source or return -1.\n+  \/\/ in str1, cnt1. Find the 1st occurrence of pattern in source or return -1.\n@@ -963,2 +963,1 @@\n-                                         FloatRegister vtmp1, FloatRegister vtmp2, PRegister pgtmp) {\n-  assert(pgtmp->is_governing(), \"This register has to be a governing predicate register.\");\n+                                         FloatRegister vtmp1, FloatRegister vtmp2) {\n@@ -967,0 +966,1 @@\n+  assert_different_registers(vtmp1, vtmp2);\n@@ -984,1 +984,1 @@\n-    idx == 1 ? fmovhid(rscratch1, vtmp1) : sve_extract(rscratch1, D, pgtmp, vtmp1, idx);\n+    sve_extract_integral(rscratch1, D, vtmp1, idx, \/* is_signed *\/ false, vtmp2);\n@@ -1421,0 +1421,85 @@\n+\n+\/\/ Extract a scalar element from an sve vector at position 'idx'.\n+\/\/ The input elements in src are expected to be of integral type.\n+void C2_MacroAssembler::sve_extract_integral(Register dst, SIMD_RegVariant size, FloatRegister src, int idx,\n+                                             bool is_signed, FloatRegister vtmp) {\n+  assert(UseSVE > 0 && size != Q, \"unsupported\");\n+  assert(!(is_signed && size == D), \"signed extract (D) not supported.\");\n+  if (regVariant_to_elemBits(size) * idx < 128) { \/\/ generate lower cost NEON instruction\n+    is_signed ? smov(dst, src, size, idx) : umov(dst, src, size, idx);\n+  } else {\n+    sve_orr(vtmp, src, src);\n+    sve_ext(vtmp, vtmp, idx << size);\n+    is_signed ? smov(dst, vtmp, size, 0) : umov(dst, vtmp, size, 0);\n+  }\n+}\n+\n+\/\/ java.lang.Math::round intrinsics\n+\n+void C2_MacroAssembler::vector_round_neon(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                                       FloatRegister tmp2, FloatRegister tmp3, SIMD_Arrangement T) {\n+  assert_different_registers(tmp1, tmp2, tmp3, src, dst);\n+  switch (T) {\n+    case T2S:\n+    case T4S:\n+      fmovs(tmp1, T, 0.5f);\n+      mov(rscratch1, jint_cast(0x1.0p23f));\n+      break;\n+    case T2D:\n+      fmovd(tmp1, T, 0.5);\n+      mov(rscratch1, julong_cast(0x1.0p52));\n+      break;\n+    default:\n+      assert(T == T2S || T == T4S || T == T2D, \"invalid arrangement\");\n+  }\n+  fadd(tmp1, T, tmp1, src);\n+  fcvtms(tmp1, T, tmp1);\n+  \/\/ tmp1 = floor(src + 0.5, ties to even)\n+\n+  fcvtas(dst, T, src);\n+  \/\/ dst = round(src), ties to away\n+\n+  fneg(tmp3, T, src);\n+  dup(tmp2, T, rscratch1);\n+  cmhs(tmp3, T, tmp3, tmp2);\n+  \/\/ tmp3 is now a set of flags\n+\n+  bif(dst, T16B, tmp1, tmp3);\n+  \/\/ result in dst\n+}\n+\n+void C2_MacroAssembler::vector_round_sve(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                                      FloatRegister tmp2, PRegister ptmp, SIMD_RegVariant T) {\n+  assert_different_registers(tmp1, tmp2, src, dst);\n+\n+  switch (T) {\n+    case S:\n+      mov(rscratch1, jint_cast(0x1.0p23f));\n+      break;\n+    case D:\n+      mov(rscratch1, julong_cast(0x1.0p52));\n+      break;\n+    default:\n+      assert(T == S || T == D, \"invalid arrangement\");\n+  }\n+\n+  sve_frinta(dst, T, ptrue, src);\n+  \/\/ dst = round(src), ties to away\n+\n+  Label none;\n+\n+  sve_fneg(tmp1, T, ptrue, src);\n+  sve_dup(tmp2, T, rscratch1);\n+  sve_cmp(HS, ptmp, T, ptrue, tmp2, tmp1);\n+  br(EQ, none);\n+  {\n+    sve_cpy(tmp1, T, ptmp, 0.5);\n+    sve_fadd(tmp1, T, ptmp, src);\n+    sve_frintm(dst, T, ptmp, tmp1);\n+    \/\/ dst = floor(src + 0.5, ties to even)\n+  }\n+  bind(none);\n+\n+  sve_fcvtzs(dst, T, ptrue, dst, T);\n+  \/\/ result in dst\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":89,"deletions":4,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-                        FloatRegister vtmp1, FloatRegister vtmp2, PRegister pgtmp);\n+                        FloatRegister vtmp1, FloatRegister vtmp2);\n@@ -95,10 +95,11 @@\n-  \/\/ rscratch1 will be clobbered.\n-  \/\/ T could be FloatRegister or Register.\n-  template<class T>\n-  inline void sve_extract(T dst, SIMD_RegVariant size, PRegister pg, FloatRegister src, int idx) {\n-    assert(UseSVE > 0, \"not supported\");\n-    assert(pg->is_governing(), \"This register has to be a governing predicate register\");\n-    mov(rscratch1, idx);\n-    sve_whilele(pg, size, zr, rscratch1);\n-    sve_lastb(dst, size, pg, src);\n-  }\n+  \/\/ The input elements in src are expected to be of integral type.\n+  void sve_extract_integral(Register dst, SIMD_RegVariant size, FloatRegister src, int idx,\n+                            bool is_signed, FloatRegister vtmp);\n+\n+  \/\/ java.lang.Math::round intrinsics\n+  void vector_round_neon(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                         FloatRegister tmp2, FloatRegister tmp3,\n+                         SIMD_Arrangement T);\n+  void vector_round_sve(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                        FloatRegister tmp2, PRegister ptmp,\n+                        SIMD_RegVariant T);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -362,1 +362,1 @@\n-  \/\/ apply the compression to the displacment iff the result is8bit.\n+  \/\/ apply the compression to the displacement iff the result is8bit.\n@@ -447,1 +447,1 @@\n-  \/\/ apply the compression to the displacment iff the result is8bit.\n+  \/\/ apply the compression to the displacement iff the result is8bit.\n@@ -1050,1 +1050,1 @@\n-    ip++; \/\/ skip P0 and exmaine W in P1\n+    ip++; \/\/ skip P0 and examine W in P1\n@@ -1768,1 +1768,1 @@\n-  \/\/ 0x66 is there. Strangly ucomisd comes out correct\n+  \/\/ 0x66 is there. Strangely ucomisd comes out correct\n@@ -2238,0 +2238,7 @@\n+void Assembler::idivl(Address src) {\n+  InstructionMark im(this);\n+  prefix(src);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(as_Register(7), src);\n+}\n+\n@@ -3595,1 +3602,1 @@\n-      \/\/ For Intel don't generate consecutive addess nops (mix with regular nops)\n+      \/\/ For Intel don't generate consecutive address nops (mix with regular nops)\n@@ -3765,1 +3772,1 @@\n-      \/\/ For ZX don't generate consecutive addess nops (mix with regular nops)\n+      \/\/ For ZX don't generate consecutive address nops (mix with regular nops)\n@@ -5337,1 +5344,1 @@\n-\/\/ scans rcx pointer sized words at [edi] for occurance of rax,\n+\/\/ scans rcx pointer sized words at [edi] for occurrence of rax,\n@@ -5346,1 +5353,1 @@\n-\/\/ scans rcx 4 byte words at [edi] for occurance of rax,\n+\/\/ scans rcx 4 byte words at [edi] for occurrence of rax,\n@@ -12602,0 +12609,7 @@\n+void Assembler::idivq(Address src) {\n+  InstructionMark im(this);\n+  prefixq(src);\n+  emit_int8((unsigned char)0xF7);\n+  emit_operand(as_Register(7), src);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":22,"deletions":8,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -139,1 +139,1 @@\n-\/\/ rscratch1 will apear in 32bit code that is dead but of course must compile\n+\/\/ rscratch1 will appear in 32bit code that is dead but of course must compile\n@@ -391,1 +391,1 @@\n-\/\/ Convience classes\n+\/\/ Convenience classes\n@@ -445,1 +445,1 @@\n-\/\/ 64-bit refect the fxsave size which is 512 bytes and the new xsave area on EVEX which is another 2176 bytes\n+\/\/ 64-bit reflect the fxsave size which is 512 bytes and the new xsave area on EVEX which is another 2176 bytes\n@@ -1199,0 +1199,3 @@\n+  void divl(Register src);\n+  void divq(Register src);\n+\n@@ -1367,3 +1370,1 @@\n-  void divl(Register src); \/\/ Unsigned division\n-\n-#ifdef _LP64\n+  void idivl(Address src);\n@@ -1371,2 +1372,1 @@\n-  void divq(Register src); \/\/ Unsigned division\n-#endif\n+  void idivq(Address src);\n@@ -2849,1 +2849,1 @@\n-  \/\/ Set the current instuction to be encoded as an EVEX instuction\n+  \/\/ Set the current instruction to be encoded as an EVEX instruction\n@@ -2859,1 +2859,1 @@\n-  \/\/ Map back to current asembler so that we can manage object level assocation\n+  \/\/ Map back to current assembler so that we can manage object level association\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -774,1 +774,1 @@\n-  \/\/ coherence traffic on the lock *and* artifically extended the critical section\n+  \/\/ coherence traffic on the lock *and* artificially extended the critical section\n@@ -2915,1 +2915,1 @@\n-  jcc(Assembler::less, SCAN_TO_CHAR_INIT);\/\/less than 16 entires left\n+  jcc(Assembler::less, SCAN_TO_CHAR_INIT);\/\/less than 16 entries left\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-  \/\/ See full desription in macroAssembler_x86.cpp.\n+  \/\/ See full description in macroAssembler_x86.cpp.\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1588,1 +1588,1 @@\n-  \/\/ stack pointer as the user finsihed with it. This allows\n+  \/\/ stack pointer as the user finished with it. This allows\n@@ -4795,1 +4795,1 @@\n-\/\/ Doesn't do verfication, generates fixed size code\n+\/\/ Doesn't do verification, generates fixed size code\n@@ -6121,1 +6121,1 @@\n- * Code for BigInteger::multiplyToLen() instrinsic.\n+ * Code for BigInteger::multiplyToLen() intrinsic.\n@@ -6330,1 +6330,1 @@\n-    \/\/ AVX512 code to compare upto 63 byte vectors.\n+    \/\/ AVX512 code to compare up to 63 byte vectors.\n@@ -6611,1 +6611,1 @@\n- * Add 64 bit long carry into z[] with carry propogation.\n+ * Add 64 bit long carry into z[] with carry propagation.\n@@ -6791,1 +6791,1 @@\n-  \/\/ Add 64 bit long carry into z with carry propogation.\n+  \/\/ Add 64 bit long carry into z with carry propagation.\n@@ -7701,1 +7701,1 @@\n-\/\/ Subtract from a lenght of a buffer\n+\/\/ Subtract from a length of a buffer\n@@ -8110,1 +8110,1 @@\n-    \/\/ First check whether a character is compressable ( <= 0xFF).\n+    \/\/ First check whether a character is compressible ( <= 0xFF).\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -543,1 +543,1 @@\n-  \/\/ Additonal registers can be excluded in a passed RegSet.\n+  \/\/ Additional registers can be excluded in a passed RegSet.\n@@ -858,1 +858,1 @@\n-  \/\/ NOTE: these jumps tranfer to the effective address of dst NOT\n+  \/\/ NOTE: these jumps transfer to the effective address of dst NOT\n@@ -865,1 +865,1 @@\n-  \/\/ to be installed in the Address class. This jump will tranfers to the address\n+  \/\/ to be installed in the Address class. This jump will transfer to the address\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -132,2 +132,2 @@\n-  \/\/ On x64 it is stored without convertion so we can use normal access.\n-  \/\/ On x32 it is stored with convertion only when FPU is used for floats.\n+  \/\/ On x64 it is stored without conversion so we can use normal access.\n+  \/\/ On x32 it is stored with conversion only when FPU is used for floats.\n@@ -177,1 +177,1 @@\n-  \/\/ true means we have fast l2f convers\n+  \/\/ true means we have fast l2f conversion\n@@ -186,1 +186,1 @@\n-  \/\/ Returns pre-selection estimated cost of a vector operation.\n+  \/\/ Returns pre-selection estimated size of a vector operation.\n@@ -199,0 +199,15 @@\n+      case Op_RoundVF: \/\/ fall through\n+      case Op_RoundVD: {\n+        return 30;\n+      }\n+    }\n+  }\n+\n+  \/\/ Returns pre-selection estimated size of a scalar operation.\n+  static int scalar_op_pre_select_sz_estimate(int vopc, BasicType ety) {\n+    switch(vopc) {\n+      default: return 0;\n+      case Op_RoundF: \/\/ fall through\n+      case Op_RoundD: {\n+        return 30;\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":19,"deletions":4,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -3439,1 +3439,1 @@\n-   * Ouput:\n+   * Output:\n@@ -3495,1 +3495,1 @@\n-  * Ouput:\n+  * Output:\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -2325,1 +2325,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -2439,1 +2439,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -4025,1 +4025,1 @@\n-        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes upto 0xe0\n+        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n@@ -4117,2 +4117,2 @@\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes upto 0xc0\n-        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes upto 0xc0\n+        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n@@ -4121,1 +4121,1 @@\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes upto 0xe0\n+        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n@@ -5683,1 +5683,1 @@\n-      \/\/ bits respecively.  This is done using vpmullw.  We end up\n+      \/\/ bits respectively.  This is done using vpmullw.  We end up\n@@ -6560,1 +6560,1 @@\n-   * Ouput:\n+   * Output:\n@@ -6616,1 +6616,1 @@\n-  * Ouput:\n+  * Output:\n@@ -6836,1 +6836,1 @@\n-  \/\/    c_rarg3   - z lenth\n+  \/\/    c_rarg3   - z length\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -411,1 +411,1 @@\n-    \/\/ If UseAVX is unitialized or is set by the user to include EVEX\n+    \/\/ If UseAVX is uninitialized or is set by the user to include EVEX\n@@ -501,1 +501,1 @@\n-    \/\/ If UseAVX is unitialized or is set by the user to include EVEX\n+    \/\/ If UseAVX is uninitialized or is set by the user to include EVEX\n@@ -2148,1 +2148,1 @@\n-  \"Pentium Pro\",   \/\/or Pentium-M\/Woodcrest depeding on model\n+  \"Pentium Pro\",   \/\/or Pentium-M\/Woodcrest depending on model\n@@ -2263,1 +2263,1 @@\n-\/* Brand ID is for back compability\n+\/* Brand ID is for back compatibility\n@@ -2799,1 +2799,1 @@\n-    \/\/ Compute freqency (in Hz) from brand string.\n+    \/\/ Compute frequency (in Hz) from brand string.\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1055,1 +1055,1 @@\n-  \/\/ Old CPUs perform lea on AGU which causes additional latency transfering the\n+  \/\/ Old CPUs perform lea on AGU which causes additional latency transferring the\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-\/\/ archtecture.\n+\/\/ architecture.\n@@ -6728,1 +6728,1 @@\n-    \/\/ Shift lower half, with result in vtmp2 usign vtmp1 as TEMP\n+    \/\/ Shift lower half, with result in vtmp2 using vtmp1 as TEMP\n@@ -6734,1 +6734,1 @@\n-    \/\/ Shift upper half, with result in dst usign vtmp1 as TEMP\n+    \/\/ Shift upper half, with result in dst using vtmp1 as TEMP\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1357,1 +1357,1 @@\n-\/\/ Seach through operands to determine parameters unique positions.\n+\/\/ Search through operands to determine parameters unique positions.\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-\/\/ add the declaration of the intrinsic to the approriate section of the list.\n+\/\/ add the declaration of the intrinsic to the appropriate section of the list.\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -170,0 +170,2 @@\n+macro(NoOvfDivI)\n+macro(NoOvfDivL)\n@@ -175,0 +177,2 @@\n+macro(NoOvfDivModI)\n+macro(NoOvfDivModL)\n@@ -243,0 +247,2 @@\n+macro(NoOvfModI)\n+macro(NoOvfModL)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -251,0 +251,1 @@\n+    PhaseStringOpts::print_statistics();\n@@ -1296,1 +1297,1 @@\n-    assert(InlineUnsafeOps, \"indeterminate pointers come only from unsafe ops\");\n+    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n@@ -1839,1 +1840,1 @@\n-    PhaseStringOpts pso(initial_gvn(), for_igvn());\n+    PhaseStringOpts pso(initial_gvn());\n@@ -3527,0 +3528,5 @@\n+        } else {\n+          \/\/ replace a%b with a-((a\/b)*b)\n+          Node* mult = new MulINode(d, d->in(2));\n+          Node* sub  = new SubINode(d->in(1), mult);\n+          n->subsume_by(sub, this);\n@@ -3542,0 +3548,45 @@\n+        } else {\n+          \/\/ replace a%b with a-((a\/b)*b)\n+          Node* mult = new MulLNode(d, d->in(2));\n+          Node* sub  = new SubLNode(d->in(1), mult);\n+          n->subsume_by(sub, this);\n+        }\n+      }\n+    }\n+    break;\n+\n+  case Op_NoOvfModI:\n+    if (UseDivMod) {\n+      \/\/ Check if a%b and a\/b both exist\n+      Node* d = n->find_similar(Op_NoOvfDivI);\n+      if (d) {\n+        \/\/ Replace them with a fused divmod if supported\n+        if (Matcher::has_match_rule(Op_NoOvfDivModI)) {\n+          NoOvfDivModINode* divmod = NoOvfDivModINode::make(n);\n+          d->subsume_by(divmod->div_proj(), this);\n+          n->subsume_by(divmod->mod_proj(), this);\n+        } else {\n+          \/\/ replace a%b with a-((a\/b)*b)\n+          Node* mult = new MulINode(d, d->in(2));\n+          Node* sub  = new SubINode(d->in(1), mult);\n+          n->subsume_by(sub, this);\n+        }\n+      }\n+    }\n+    break;\n+\n+  case Op_NoOvfModL:\n+    if (UseDivMod) {\n+      \/\/ Check if a%b and a\/b both exist\n+      Node* d = n->find_similar(Op_NoOvfDivL);\n+      if (d) {\n+        \/\/ Replace them with a fused divmod if supported\n+        if (Matcher::has_match_rule(Op_NoOvfDivModL)) {\n+          NoOvfDivModLNode* divmod = NoOvfDivModLNode::make(n);\n+          d->subsume_by(divmod->div_proj(), this);\n+          n->subsume_by(divmod->mod_proj(), this);\n+        } else {\n+          \/\/ replace a%b with a-((a\/b)*b)\n+          Node* mult = new MulLNode(d, d->in(2));\n+          Node* sub  = new SubLNode(d->in(1), mult);\n+          n->subsume_by(sub, this);\n@@ -3931,1 +3982,1 @@\n-  \/\/ check if the optimizer has made it homogenous, item (3).\n+  \/\/ check if the optimizer has made it homogeneous, item (3).\n@@ -4298,1 +4349,1 @@\n-\/\/ _print_inlining_stream and transfered into the _print_inlining_list\n+\/\/ _print_inlining_stream and transferred into the _print_inlining_list\n@@ -4389,1 +4440,1 @@\n-  assert(!_print_inlining || _print_inlining_stream->size() == 0, \"loosing data\");\n+  assert(!_print_inlining || _print_inlining_stream->size() == 0, \"losing data\");\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":56,"deletions":5,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -757,1 +757,1 @@\n-    set_control(top()); \/\/ No fast path instrinsic\n+    set_control(top()); \/\/ No fast path intrinsic\n@@ -1070,1 +1070,1 @@\n-  \/\/ length is now known postive, add a cast node to make this explicit\n+  \/\/ length is now known positive, add a cast node to make this explicit\n@@ -1591,1 +1591,1 @@\n-    ch = access_load_at(value, adr, TypeAryPtr::BYTES, TypeInt::CHAR, T_CHAR, IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD);\n+    ch = access_load_at(value, adr, TypeAryPtr::BYTES, TypeInt::CHAR, T_CHAR, IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n@@ -2272,1 +2272,1 @@\n-  \/\/ contraint in place.\n+  \/\/ constraint in place.\n@@ -4480,1 +4480,1 @@\n-\/\/ unitialized array will escape the compiled method. To prevent that\n+\/\/ uninitialized array will escape the compiled method. To prevent that\n@@ -5754,1 +5754,1 @@\n-  assert(UseAdler32Intrinsics, \"Adler32 Instrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n+  assert(UseAdler32Intrinsics, \"Adler32 Intrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n@@ -5800,1 +5800,1 @@\n-  assert(UseAdler32Intrinsics, \"Adler32 Instrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n+  assert(UseAdler32Intrinsics, \"Adler32 Intrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n@@ -6290,1 +6290,1 @@\n-  \/\/ Intel's extention is based on this optimization and AESCrypt generates round keys by preprocessing MixColumns.\n+  \/\/ Intel's extension is based on this optimization and AESCrypt generates round keys by preprocessing MixColumns.\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -973,4 +973,4 @@\n-      case Op_RoundF: body_size += 30; break;\n-      case Op_RoundD: body_size += 30; break;\n-      case Op_RoundVF: body_size += 30; break;\n-      case Op_RoundVD: body_size += 30; break;\n+      case Op_RoundF:\n+      case Op_RoundD: {\n+          body_size += Matcher::scalar_op_pre_select_sz_estimate(n->Opcode(), n->bottom_type()->basic_type());\n+      } break;\n@@ -980,0 +980,2 @@\n+      case Op_RoundVF:\n+      case Op_RoundVD:\n@@ -2210,0 +2212,14 @@\n+      if (loop_head->unrolled_count() == 1) {\n+        \/\/ The Opaque2 node created above (in the case of the first unrolling) hides the type of the loop limit.\n+        \/\/ As a result, if the iv Phi constant folds (because it captured the iteration range), the exit test won't\n+        \/\/ constant fold and the graph contains a broken counted loop.\n+        const Type* new_limit_t;\n+        if (stride_con > 0) {\n+          new_limit_t = TypeInt::make(min_jint, limit_type->_hi, limit_type->_widen);\n+        } else {\n+          assert(stride_con < 0, \"stride can't be 0\");\n+          new_limit_t = TypeInt::make(limit_type->_lo, max_jint, limit_type->_widen);\n+        }\n+        new_limit = new CastIINode(new_limit, new_limit_t);\n+        register_new_node(new_limit, ctrl);\n+      }\n@@ -3594,0 +3610,2 @@\n+    } else if (phase->duplicate_loop_backedge(this, old_new)) {\n+      return false;\n@@ -3622,0 +3640,3 @@\n+    if (StressDuplicateBackedge && phase->duplicate_loop_backedge(this, old_new)) {\n+      return false;\n+    }\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2233,1 +2233,1 @@\n-    case Op_BoxLock:         \/\/ Cant match until we get stack-regs in ADLC\n+    case Op_BoxLock:         \/\/ Can't match until we get stack-regs in ADLC\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1044,1 +1044,1 @@\n-  \/\/ treatise in node.cpp above the default implemention AND TEST WITH\n+  \/\/ treatise in node.cpp above the default implementation AND TEST WITH\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -379,1 +379,1 @@\n-\/\/ implemention on most chips.  Since a naive graph involves control flow, we\n+\/\/ implementation on most chips.  Since a naive graph involves control flow, we\n@@ -391,1 +391,1 @@\n-\/\/ implemention on most chips.  Since a naive graph involves control flow, we\n+\/\/ implementation on most chips.  Since a naive graph involves control flow, we\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -418,1 +418,1 @@\n-\/\/    This simplies the upcoming \"independence\" checker.\n+\/\/    This simplifies the upcoming \"independence\" checker.\n@@ -2128,1 +2128,1 @@\n-  \/\/ remove current_store from its current position in the memmory graph\n+  \/\/ remove current_store from its current position in the memory graph\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -815,1 +815,1 @@\n-  \/\/ MODE_BROADCAST for vector Vector.boradcast and VectorMask.maskAll operations.\n+  \/\/ MODE_BROADCAST for vector Vector.broadcast and VectorMask.maskAll operations.\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -362,1 +362,1 @@\n-  \/\/ Adding a pessimistic check to avoid complex pattern mathing which\n+  \/\/ Adding a pessimistic check to avoid complex pattern matching which\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1584,0 +1584,2 @@\n+  declare_c2_type(NoOvfDivINode, DivINode)                                \\\n+  declare_c2_type(NoOvfDivLNode, DivLNode)                                \\\n@@ -1590,0 +1592,2 @@\n+  declare_c2_type(NoOvfModINode, ModINode)                                \\\n+  declare_c2_type(NoOvfModLNode, ModLNode)                                \\\n@@ -1595,0 +1599,2 @@\n+  declare_c2_type(NoOvfDivModINode, DivModINode)                          \\\n+  declare_c2_type(NoOvfDivModLNode, DivModLNode)                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n-\/\/ The caller is responsible for considering overlow.\n+\/\/ The caller is responsible for considering overflow.\n@@ -475,1 +475,1 @@\n-\/\/ Unsigned one, two, four and eigth byte quantities used for describing\n+\/\/ Unsigned one, two, four and eight byte quantities used for describing\n@@ -963,1 +963,1 @@\n-  _thread_new_trans         =  3, \/\/ corresponding transition state (not used, included for completness)\n+  _thread_new_trans         =  3, \/\/ corresponding transition state (not used, included for completeness)\n@@ -969,1 +969,1 @@\n-  _thread_in_Java_trans     =  9, \/\/ corresponding transition state (not used, included for completness)\n+  _thread_in_Java_trans     =  9, \/\/ corresponding transition state (not used, included for completeness)\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -465,0 +465,23 @@\n+class SVEComparisonWithZero(Instruction):\n+     def __init__(self, arg):\n+          Instruction.__init__(self, \"fcm\")\n+          self.condition = arg\n+          self.dest = OperandFactory.create('p').generate()\n+          self.reg = SVEVectorRegister().generate()\n+          self._width = RegVariant(2, 3)\n+          self.preg = OperandFactory.create('P').generate()\n+\n+     def generate(self):\n+          return Instruction.generate(self)\n+\n+     def cstr(self):\n+          return (\"%s(%s, %s, %s, %s, %s, 0.0);\"\n+                  % (\"__ sve_\" + self._name, \"Assembler::\" + self.condition,\n+                     str(self.dest), self._width.cstr(), str(self.preg), str(self.reg)))\n+\n+     def astr(self):\n+          val = (\"%s%s\\t%s%s, %s\/z, %s%s, #0.0\"\n+                 % (self._name, self.condition.lower(), str(self.dest), self._width.astr(),\n+                    str(self.preg), str(self.reg), self._width.astr()))\n+          return val\n+\n@@ -1447,0 +1470,2 @@\n+                          [\"fcvtassw\", \"fcvtas\", \"ws\"], [\"fcvtasd\", \"fcvtas\", \"xd\"],\n+                          [\"fcvtmssw\", \"fcvtms\", \"ws\"], [\"fcvtmsd\", \"fcvtms\", \"xd\"],\n@@ -1593,0 +1618,2 @@\n+generate(SVEComparisonWithZero, [\"EQ\", \"GT\", \"GE\", \"LT\", \"LE\", \"NE\"])\n+\n@@ -1616,0 +1643,2 @@\n+                        [\"fmov\",   \"__ fmovs(v9, __ T2S, 0.5f);\",                        \"fmov\\tv9.2s, 0.5\"],\n+                        [\"fmov\",   \"__ fmovd(v14, __ T2D, 0.5f);\",                       \"fmov\\tv14.2d, 0.5\"],\n@@ -1617,1 +1646,3 @@\n-                        [\"fcvtzs\", \"__ fcvtzs(v0, __ T4S, v1);\",                         \"fcvtzs\\tv0.4s, v1.4s\"],\n+                        [\"fcvtzs\", \"__ fcvtzs(v0, __ T2S, v1);\",                         \"fcvtzs\\tv0.2s, v1.2s\"],\n+                        [\"fcvtas\", \"__ fcvtas(v2, __ T4S, v3);\",                         \"fcvtas\\tv2.4s, v3.4s\"],\n+                        [\"fcvtms\", \"__ fcvtms(v4, __ T2D, v5);\",                         \"fcvtms\\tv4.2d, v5.2d\"],\n@@ -1789,0 +1820,1 @@\n+                        [\"ext\",     \"__ sve_ext(z17, z16, 63);\",                          \"ext\\tz17.b, z17.b, z16.b, #63\"],\n@@ -1877,2 +1909,0 @@\n-                       # SVE2 instructions\n-                       [\"histcnt\", \"ZPZZ\", \"z\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":33,"deletions":3,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -511,4 +511,8 @@\n-    __ fmovs(r7, v14);                                 \/\/       fmov    w7, s14\n-    __ fmovd(r7, v8);                                  \/\/       fmov    x7, d8\n-    __ fmovs(v20, r17);                                \/\/       fmov    s20, w17\n-    __ fmovd(v28, r30);                                \/\/       fmov    d28, x30\n+    __ fcvtassw(r7, v14);                              \/\/       fcvtas  w7, s14\n+    __ fcvtasd(r7, v8);                                \/\/       fcvtas  x7, d8\n+    __ fcvtmssw(r20, v17);                             \/\/       fcvtms  w20, s17\n+    __ fcvtmsd(r28, v30);                              \/\/       fcvtms  x28, d30\n+    __ fmovs(r16, v2);                                 \/\/       fmov    w16, s2\n+    __ fmovd(r9, v16);                                 \/\/       fmov    x9, d16\n+    __ fmovs(v20, r29);                                \/\/       fmov    s20, w29\n+    __ fmovd(v4, r1);                                  \/\/       fmov    d4, x1\n@@ -517,4 +521,4 @@\n-    __ fcmps(v16, v2);                                 \/\/       fcmp    s16, s2\n-    __ fcmpd(v9, v16);                                 \/\/       fcmp    d9, d16\n-    __ fcmps(v20, 0.0);                                \/\/       fcmp    s20, #0.0\n-    __ fcmpd(v29, 0.0);                                \/\/       fcmp    d29, #0.0\n+    __ fcmps(v26, v24);                                \/\/       fcmp    s26, s24\n+    __ fcmpd(v23, v14);                                \/\/       fcmp    d23, d14\n+    __ fcmps(v21, 0.0);                                \/\/       fcmp    s21, #0.0\n+    __ fcmpd(v12, 0.0);                                \/\/       fcmp    d12, #0.0\n@@ -523,5 +527,5 @@\n-    __ stpw(r1, r26, Address(r24, -208));              \/\/       stp     w1, w26, [x24, #-208]\n-    __ ldpw(r5, r11, Address(r12, 48));                \/\/       ldp     w5, w11, [x12, #48]\n-    __ ldpsw(r21, r15, Address(r27, 48));              \/\/       ldpsw   x21, x15, [x27, #48]\n-    __ stp(r5, r28, Address(r22, 32));                 \/\/       stp     x5, x28, [x22, #32]\n-    __ ldp(r27, r17, Address(r19, -32));               \/\/       ldp     x27, x17, [x19, #-32]\n+    __ stpw(r12, r24, Address(r24, -192));             \/\/       stp     w12, w24, [x24, #-192]\n+    __ ldpw(r22, r5, Address(r16, 128));               \/\/       ldp     w22, w5, [x16, #128]\n+    __ ldpsw(r20, r19, Address(r13, 112));             \/\/       ldpsw   x20, x19, [x13, #112]\n+    __ stp(r17, r6, Address(r13, 96));                 \/\/       stp     x17, x6, [x13, #96]\n+    __ ldp(r5, r1, Address(r17, -160));                \/\/       ldp     x5, x1, [x17, #-160]\n@@ -530,5 +534,5 @@\n-    __ stpw(r13, r7, Address(__ pre(r26, -176)));      \/\/       stp     w13, w7, [x26, #-176]!\n-    __ ldpw(r13, r21, Address(__ pre(r6, -48)));       \/\/       ldp     w13, w21, [x6, #-48]!\n-    __ ldpsw(r20, r30, Address(__ pre(r27, 16)));      \/\/       ldpsw   x20, x30, [x27, #16]!\n-    __ stp(r21, r5, Address(__ pre(r10, -128)));       \/\/       stp     x21, x5, [x10, #-128]!\n-    __ ldp(r14, r4, Address(__ pre(r23, -96)));        \/\/       ldp     x14, x4, [x23, #-96]!\n+    __ stpw(r13, r20, Address(__ pre(r22, -208)));     \/\/       stp     w13, w20, [x22, #-208]!\n+    __ ldpw(r30, r27, Address(__ pre(r10, 80)));       \/\/       ldp     w30, w27, [x10, #80]!\n+    __ ldpsw(r13, r20, Address(__ pre(r26, 16)));      \/\/       ldpsw   x13, x20, [x26, #16]!\n+    __ stp(r4, r23, Address(__ pre(r29, -80)));        \/\/       stp     x4, x23, [x29, #-80]!\n+    __ ldp(r22, r0, Address(__ pre(r6, -112)));        \/\/       ldp     x22, x0, [x6, #-112]!\n@@ -537,5 +541,5 @@\n-    __ stpw(r29, r12, Address(__ post(r16, 32)));      \/\/       stp     w29, w12, [x16], #32\n-    __ ldpw(r26, r17, Address(__ post(r27, 96)));      \/\/       ldp     w26, w17, [x27], #96\n-    __ ldpsw(r4, r20, Address(__ post(r14, -96)));     \/\/       ldpsw   x4, x20, [x14], #-96\n-    __ stp(r16, r2, Address(__ post(r14, -112)));      \/\/       stp     x16, x2, [x14], #-112\n-    __ ldp(r23, r24, Address(__ post(r7, -256)));      \/\/       ldp     x23, x24, [x7], #-256\n+    __ stpw(r17, r27, Address(__ post(r5, 80)));       \/\/       stp     w17, w27, [x5], #80\n+    __ ldpw(r14, r11, Address(__ post(r16, -256)));    \/\/       ldp     w14, w11, [x16], #-256\n+    __ ldpsw(r12, r23, Address(__ post(r9, -240)));    \/\/       ldpsw   x12, x23, [x9], #-240\n+    __ stp(r23, r7, Address(__ post(r0, 32)));         \/\/       stp     x23, x7, [x0], #32\n+    __ ldp(r17, r8, Address(__ post(r26, 80)));        \/\/       ldp     x17, x8, [x26], #80\n@@ -544,4 +548,4 @@\n-    __ stnpw(r0, r26, Address(r15, 128));              \/\/       stnp    w0, w26, [x15, #128]\n-    __ ldnpw(r26, r6, Address(r8, -208));              \/\/       ldnp    w26, w6, [x8, #-208]\n-    __ stnp(r15, r10, Address(r25, -112));             \/\/       stnp    x15, x10, [x25, #-112]\n-    __ ldnp(r16, r1, Address(r19, -160));              \/\/       ldnp    x16, x1, [x19, #-160]\n+    __ stnpw(r11, r15, Address(r10, -176));            \/\/       stnp    w11, w15, [x10, #-176]\n+    __ ldnpw(r19, r16, Address(r4, 64));               \/\/       ldnp    w19, w16, [x4, #64]\n+    __ stnp(r30, r14, Address(r9, -240));              \/\/       stnp    x30, x14, [x9, #-240]\n+    __ ldnp(r29, r23, Address(r20, 32));               \/\/       ldnp    x29, x23, [x20, #32]\n@@ -550,22 +554,22 @@\n-    __ ld1(v27, __ T8B, Address(r30));                 \/\/       ld1     {v27.8B}, [x30]\n-    __ ld1(v25, v26, __ T16B, Address(__ post(r3, 32))); \/\/     ld1     {v25.16B, v26.16B}, [x3], 32\n-    __ ld1(v30, v31, v0, __ T1D, Address(__ post(r16, r10))); \/\/        ld1     {v30.1D, v31.1D, v0.1D}, [x16], x10\n-    __ ld1(v16, v17, v18, v19, __ T8H, Address(__ post(r19, 64))); \/\/   ld1     {v16.8H, v17.8H, v18.8H, v19.8H}, [x19], 64\n-    __ ld1r(v23, __ T8B, Address(r24));                \/\/       ld1r    {v23.8B}, [x24]\n-    __ ld1r(v8, __ T4S, Address(__ post(r10, 4)));     \/\/       ld1r    {v8.4S}, [x10], 4\n-    __ ld1r(v9, __ T1D, Address(__ post(r20, r23)));   \/\/       ld1r    {v9.1D}, [x20], x23\n-    __ ld2(v2, v3, __ T2D, Address(r3));               \/\/       ld2     {v2.2D, v3.2D}, [x3]\n-    __ ld2(v8, v9, __ T4H, Address(__ post(r30, 16))); \/\/       ld2     {v8.4H, v9.4H}, [x30], 16\n-    __ ld2r(v4, v5, __ T16B, Address(r26));            \/\/       ld2r    {v4.16B, v5.16B}, [x26]\n-    __ ld2r(v3, v4, __ T2S, Address(__ post(r17, 8))); \/\/       ld2r    {v3.2S, v4.2S}, [x17], 8\n-    __ ld2r(v29, v30, __ T2D, Address(__ post(r11, r16))); \/\/   ld2r    {v29.2D, v30.2D}, [x11], x16\n-    __ ld3(v1, v2, v3, __ T4S, Address(__ post(r0, r23))); \/\/   ld3     {v1.4S, v2.4S, v3.4S}, [x0], x23\n-    __ ld3(v0, v1, v2, __ T2S, Address(r21));          \/\/       ld3     {v0.2S, v1.2S, v2.2S}, [x21]\n-    __ ld3r(v5, v6, v7, __ T8H, Address(r7));          \/\/       ld3r    {v5.8H, v6.8H, v7.8H}, [x7]\n-    __ ld3r(v1, v2, v3, __ T4S, Address(__ post(r7, 12))); \/\/   ld3r    {v1.4S, v2.4S, v3.4S}, [x7], 12\n-    __ ld3r(v2, v3, v4, __ T1D, Address(__ post(r5, r15))); \/\/  ld3r    {v2.1D, v3.1D, v4.1D}, [x5], x15\n-    __ ld4(v27, v28, v29, v30, __ T8H, Address(__ post(r29, 64))); \/\/   ld4     {v27.8H, v28.8H, v29.8H, v30.8H}, [x29], 64\n-    __ ld4(v24, v25, v26, v27, __ T8B, Address(__ post(r4, r7))); \/\/    ld4     {v24.8B, v25.8B, v26.8B, v27.8B}, [x4], x7\n-    __ ld4r(v15, v16, v17, v18, __ T8B, Address(r23)); \/\/       ld4r    {v15.8B, v16.8B, v17.8B, v18.8B}, [x23]\n-    __ ld4r(v14, v15, v16, v17, __ T4H, Address(__ post(r21, 8))); \/\/   ld4r    {v14.4H, v15.4H, v16.4H, v17.4H}, [x21], 8\n-    __ ld4r(v20, v21, v22, v23, __ T2S, Address(__ post(r9, r25))); \/\/  ld4r    {v20.2S, v21.2S, v22.2S, v23.2S}, [x9], x25\n+    __ ld1(v5, __ T8B, Address(r27));                  \/\/       ld1     {v5.8B}, [x27]\n+    __ ld1(v10, v11, __ T16B, Address(__ post(r25, 32))); \/\/    ld1     {v10.16B, v11.16B}, [x25], 32\n+    __ ld1(v15, v16, v17, __ T1D, Address(__ post(r30, r19))); \/\/       ld1     {v15.1D, v16.1D, v17.1D}, [x30], x19\n+    __ ld1(v17, v18, v19, v20, __ T8H, Address(__ post(r16, 64))); \/\/   ld1     {v17.8H, v18.8H, v19.8H, v20.8H}, [x16], 64\n+    __ ld1r(v30, __ T8B, Address(r23));                \/\/       ld1r    {v30.8B}, [x23]\n+    __ ld1r(v17, __ T4S, Address(__ post(r8, 4)));     \/\/       ld1r    {v17.4S}, [x8], 4\n+    __ ld1r(v12, __ T1D, Address(__ post(r9, r3)));    \/\/       ld1r    {v12.1D}, [x9], x3\n+    __ ld2(v19, v20, __ T2D, Address(r2));             \/\/       ld2     {v19.2D, v20.2D}, [x2]\n+    __ ld2(v21, v22, __ T4H, Address(__ post(r8, 16))); \/\/      ld2     {v21.4H, v22.4H}, [x8], 16\n+    __ ld2r(v13, v14, __ T16B, Address(r4));           \/\/       ld2r    {v13.16B, v14.16B}, [x4]\n+    __ ld2r(v28, v29, __ T2S, Address(__ post(r3, 8))); \/\/      ld2r    {v28.2S, v29.2S}, [x3], 8\n+    __ ld2r(v29, v30, __ T2D, Address(__ post(r29, r0))); \/\/    ld2r    {v29.2D, v30.2D}, [x29], x0\n+    __ ld3(v7, v8, v9, __ T4S, Address(__ post(r1, r21))); \/\/   ld3     {v7.4S, v8.4S, v9.4S}, [x1], x21\n+    __ ld3(v17, v18, v19, __ T2S, Address(r0));        \/\/       ld3     {v17.2S, v18.2S, v19.2S}, [x0]\n+    __ ld3r(v26, v27, v28, __ T8H, Address(r5));       \/\/       ld3r    {v26.8H, v27.8H, v28.8H}, [x5]\n+    __ ld3r(v25, v26, v27, __ T4S, Address(__ post(r1, 12))); \/\/        ld3r    {v25.4S, v26.4S, v27.4S}, [x1], 12\n+    __ ld3r(v22, v23, v24, __ T1D, Address(__ post(r2, r29))); \/\/       ld3r    {v22.1D, v23.1D, v24.1D}, [x2], x29\n+    __ ld4(v13, v14, v15, v16, __ T8H, Address(__ post(r27, 64))); \/\/   ld4     {v13.8H, v14.8H, v15.8H, v16.8H}, [x27], 64\n+    __ ld4(v29, v30, v31, v0, __ T8B, Address(__ post(r24, r23))); \/\/   ld4     {v29.8B, v30.8B, v31.8B, v0.8B}, [x24], x23\n+    __ ld4r(v13, v14, v15, v16, __ T8B, Address(r15)); \/\/       ld4r    {v13.8B, v14.8B, v15.8B, v16.8B}, [x15]\n+    __ ld4r(v15, v16, v17, v18, __ T4H, Address(__ post(r14, 8))); \/\/   ld4r    {v15.4H, v16.4H, v17.4H, v18.4H}, [x14], 8\n+    __ ld4r(v27, v28, v29, v30, __ T2S, Address(__ post(r20, r23))); \/\/ ld4r    {v27.2S, v28.2S, v29.2S, v30.2S}, [x20], x23\n@@ -574,26 +578,26 @@\n-    __ addv(v23, __ T8B, v24);                         \/\/       addv    b23, v24.8B\n-    __ addv(v26, __ T16B, v27);                        \/\/       addv    b26, v27.16B\n-    __ addv(v5, __ T4H, v6);                           \/\/       addv    h5, v6.4H\n-    __ addv(v6, __ T8H, v7);                           \/\/       addv    h6, v7.8H\n-    __ addv(v15, __ T4S, v16);                         \/\/       addv    s15, v16.4S\n-    __ smaxv(v15, __ T8B, v16);                        \/\/       smaxv   b15, v16.8B\n-    __ smaxv(v25, __ T16B, v26);                       \/\/       smaxv   b25, v26.16B\n-    __ smaxv(v16, __ T4H, v17);                        \/\/       smaxv   h16, v17.4H\n-    __ smaxv(v27, __ T8H, v28);                        \/\/       smaxv   h27, v28.8H\n-    __ smaxv(v24, __ T4S, v25);                        \/\/       smaxv   s24, v25.4S\n-    __ fmaxv(v15, __ T4S, v16);                        \/\/       fmaxv   s15, v16.4S\n-    __ sminv(v25, __ T8B, v26);                        \/\/       sminv   b25, v26.8B\n-    __ uminv(v14, __ T8B, v15);                        \/\/       uminv   b14, v15.8B\n-    __ sminv(v10, __ T16B, v11);                       \/\/       sminv   b10, v11.16B\n-    __ uminv(v13, __ T16B, v14);                       \/\/       uminv   b13, v14.16B\n-    __ sminv(v14, __ T4H, v15);                        \/\/       sminv   h14, v15.4H\n-    __ uminv(v20, __ T4H, v21);                        \/\/       uminv   h20, v21.4H\n-    __ sminv(v1, __ T8H, v2);                          \/\/       sminv   h1, v2.8H\n-    __ uminv(v22, __ T8H, v23);                        \/\/       uminv   h22, v23.8H\n-    __ sminv(v30, __ T4S, v31);                        \/\/       sminv   s30, v31.4S\n-    __ uminv(v14, __ T4S, v15);                        \/\/       uminv   s14, v15.4S\n-    __ fminv(v2, __ T4S, v3);                          \/\/       fminv   s2, v3.4S\n-    __ fmaxp(v6, v7, __ S);                            \/\/       fmaxp   s6, v7.2S\n-    __ fmaxp(v3, v4, __ D);                            \/\/       fmaxp   d3, v4.2D\n-    __ fminp(v7, v8, __ S);                            \/\/       fminp   s7, v8.2S\n-    __ fminp(v24, v25, __ D);                          \/\/       fminp   d24, v25.2D\n+    __ addv(v24, __ T8B, v25);                         \/\/       addv    b24, v25.8B\n+    __ addv(v15, __ T16B, v16);                        \/\/       addv    b15, v16.16B\n+    __ addv(v25, __ T4H, v26);                         \/\/       addv    h25, v26.4H\n+    __ addv(v14, __ T8H, v15);                         \/\/       addv    h14, v15.8H\n+    __ addv(v10, __ T4S, v11);                         \/\/       addv    s10, v11.4S\n+    __ smaxv(v13, __ T8B, v14);                        \/\/       smaxv   b13, v14.8B\n+    __ smaxv(v14, __ T16B, v15);                       \/\/       smaxv   b14, v15.16B\n+    __ smaxv(v20, __ T4H, v21);                        \/\/       smaxv   h20, v21.4H\n+    __ smaxv(v1, __ T8H, v2);                          \/\/       smaxv   h1, v2.8H\n+    __ smaxv(v22, __ T4S, v23);                        \/\/       smaxv   s22, v23.4S\n+    __ fmaxv(v30, __ T4S, v31);                        \/\/       fmaxv   s30, v31.4S\n+    __ sminv(v14, __ T8B, v15);                        \/\/       sminv   b14, v15.8B\n+    __ uminv(v2, __ T8B, v3);                          \/\/       uminv   b2, v3.8B\n+    __ sminv(v6, __ T16B, v7);                         \/\/       sminv   b6, v7.16B\n+    __ uminv(v3, __ T16B, v4);                         \/\/       uminv   b3, v4.16B\n+    __ sminv(v7, __ T4H, v8);                          \/\/       sminv   h7, v8.4H\n+    __ uminv(v24, __ T4H, v25);                        \/\/       uminv   h24, v25.4H\n+    __ sminv(v0, __ T8H, v1);                          \/\/       sminv   h0, v1.8H\n+    __ uminv(v27, __ T8H, v28);                        \/\/       uminv   h27, v28.8H\n+    __ sminv(v29, __ T4S, v30);                        \/\/       sminv   s29, v30.4S\n+    __ uminv(v5, __ T4S, v6);                          \/\/       uminv   s5, v6.4S\n+    __ fminv(v5, __ T4S, v6);                          \/\/       fminv   s5, v6.4S\n+    __ fmaxp(v29, v30, __ S);                          \/\/       fmaxp   s29, v30.2S\n+    __ fmaxp(v11, v12, __ D);                          \/\/       fmaxp   d11, v12.2D\n+    __ fminp(v25, v26, __ S);                          \/\/       fminp   s25, v26.2S\n+    __ fminp(v0, v1, __ D);                            \/\/       fminp   d0, v1.2D\n@@ -602,11 +606,11 @@\n-    __ absr(v0, __ T8B, v1);                           \/\/       abs     v0.8B, v1.8B\n-    __ absr(v27, __ T16B, v28);                        \/\/       abs     v27.16B, v28.16B\n-    __ absr(v29, __ T4H, v30);                         \/\/       abs     v29.4H, v30.4H\n-    __ absr(v5, __ T8H, v6);                           \/\/       abs     v5.8H, v6.8H\n-    __ absr(v5, __ T2S, v6);                           \/\/       abs     v5.2S, v6.2S\n-    __ absr(v29, __ T4S, v30);                         \/\/       abs     v29.4S, v30.4S\n-    __ absr(v11, __ T2D, v12);                         \/\/       abs     v11.2D, v12.2D\n-    __ fabs(v25, __ T2S, v26);                         \/\/       fabs    v25.2S, v26.2S\n-    __ fabs(v0, __ T4S, v1);                           \/\/       fabs    v0.4S, v1.4S\n-    __ fabs(v30, __ T2D, v31);                         \/\/       fabs    v30.2D, v31.2D\n-    __ fneg(v0, __ T2S, v1);                           \/\/       fneg    v0.2S, v1.2S\n+    __ absr(v30, __ T8B, v31);                         \/\/       abs     v30.8B, v31.8B\n+    __ absr(v0, __ T16B, v1);                          \/\/       abs     v0.16B, v1.16B\n+    __ absr(v17, __ T4H, v18);                         \/\/       abs     v17.4H, v18.4H\n+    __ absr(v28, __ T8H, v29);                         \/\/       abs     v28.8H, v29.8H\n+    __ absr(v25, __ T2S, v26);                         \/\/       abs     v25.2S, v26.2S\n+    __ absr(v9, __ T4S, v10);                          \/\/       abs     v9.4S, v10.4S\n+    __ absr(v25, __ T2D, v26);                         \/\/       abs     v25.2D, v26.2D\n+    __ fabs(v12, __ T2S, v13);                         \/\/       fabs    v12.2S, v13.2S\n+    __ fabs(v15, __ T4S, v16);                         \/\/       fabs    v15.4S, v16.4S\n+    __ fabs(v11, __ T2D, v12);                         \/\/       fabs    v11.2D, v12.2D\n+    __ fneg(v10, __ T2S, v11);                         \/\/       fneg    v10.2S, v11.2S\n@@ -614,6 +618,6 @@\n-    __ fneg(v28, __ T2D, v29);                         \/\/       fneg    v28.2D, v29.2D\n-    __ fsqrt(v25, __ T2S, v26);                        \/\/       fsqrt   v25.2S, v26.2S\n-    __ fsqrt(v9, __ T4S, v10);                         \/\/       fsqrt   v9.4S, v10.4S\n-    __ fsqrt(v25, __ T2D, v26);                        \/\/       fsqrt   v25.2D, v26.2D\n-    __ notr(v12, __ T8B, v13);                         \/\/       not     v12.8B, v13.8B\n-    __ notr(v15, __ T16B, v16);                        \/\/       not     v15.16B, v16.16B\n+    __ fneg(v24, __ T2D, v25);                         \/\/       fneg    v24.2D, v25.2D\n+    __ fsqrt(v21, __ T2S, v22);                        \/\/       fsqrt   v21.2S, v22.2S\n+    __ fsqrt(v23, __ T4S, v24);                        \/\/       fsqrt   v23.4S, v24.4S\n+    __ fsqrt(v0, __ T2D, v1);                          \/\/       fsqrt   v0.2D, v1.2D\n+    __ notr(v16, __ T8B, v17);                         \/\/       not     v16.8B, v17.8B\n+    __ notr(v10, __ T16B, v11);                        \/\/       not     v10.16B, v11.16B\n@@ -622,20 +626,20 @@\n-    __ andr(v11, __ T8B, v12, v13);                    \/\/       and     v11.8B, v12.8B, v13.8B\n-    __ andr(v10, __ T16B, v11, v12);                   \/\/       and     v10.16B, v11.16B, v12.16B\n-    __ orr(v17, __ T8B, v18, v19);                     \/\/       orr     v17.8B, v18.8B, v19.8B\n-    __ orr(v24, __ T16B, v25, v26);                    \/\/       orr     v24.16B, v25.16B, v26.16B\n-    __ eor(v21, __ T8B, v22, v23);                     \/\/       eor     v21.8B, v22.8B, v23.8B\n-    __ eor(v23, __ T16B, v24, v25);                    \/\/       eor     v23.16B, v24.16B, v25.16B\n-    __ addv(v0, __ T8B, v1, v2);                       \/\/       add     v0.8B, v1.8B, v2.8B\n-    __ addv(v16, __ T16B, v17, v18);                   \/\/       add     v16.16B, v17.16B, v18.16B\n-    __ addv(v10, __ T4H, v11, v12);                    \/\/       add     v10.4H, v11.4H, v12.4H\n-    __ addv(v6, __ T8H, v7, v8);                       \/\/       add     v6.8H, v7.8H, v8.8H\n-    __ addv(v28, __ T2S, v29, v30);                    \/\/       add     v28.2S, v29.2S, v30.2S\n-    __ addv(v6, __ T4S, v7, v8);                       \/\/       add     v6.4S, v7.4S, v8.4S\n-    __ addv(v5, __ T2D, v6, v7);                       \/\/       add     v5.2D, v6.2D, v7.2D\n-    __ fadd(v5, __ T2S, v6, v7);                       \/\/       fadd    v5.2S, v6.2S, v7.2S\n-    __ fadd(v20, __ T4S, v21, v22);                    \/\/       fadd    v20.4S, v21.4S, v22.4S\n-    __ fadd(v17, __ T2D, v18, v19);                    \/\/       fadd    v17.2D, v18.2D, v19.2D\n-    __ subv(v15, __ T8B, v16, v17);                    \/\/       sub     v15.8B, v16.8B, v17.8B\n-    __ subv(v17, __ T16B, v18, v19);                   \/\/       sub     v17.16B, v18.16B, v19.16B\n-    __ subv(v29, __ T4H, v30, v31);                    \/\/       sub     v29.4H, v30.4H, v31.4H\n-    __ subv(v26, __ T8H, v27, v28);                    \/\/       sub     v26.8H, v27.8H, v28.8H\n+    __ andr(v6, __ T8B, v7, v8);                       \/\/       and     v6.8B, v7.8B, v8.8B\n+    __ andr(v28, __ T16B, v29, v30);                   \/\/       and     v28.16B, v29.16B, v30.16B\n+    __ orr(v6, __ T8B, v7, v8);                        \/\/       orr     v6.8B, v7.8B, v8.8B\n+    __ orr(v5, __ T16B, v6, v7);                       \/\/       orr     v5.16B, v6.16B, v7.16B\n+    __ eor(v5, __ T8B, v6, v7);                        \/\/       eor     v5.8B, v6.8B, v7.8B\n+    __ eor(v20, __ T16B, v21, v22);                    \/\/       eor     v20.16B, v21.16B, v22.16B\n+    __ addv(v17, __ T8B, v18, v19);                    \/\/       add     v17.8B, v18.8B, v19.8B\n+    __ addv(v15, __ T16B, v16, v17);                   \/\/       add     v15.16B, v16.16B, v17.16B\n+    __ addv(v17, __ T4H, v18, v19);                    \/\/       add     v17.4H, v18.4H, v19.4H\n+    __ addv(v29, __ T8H, v30, v31);                    \/\/       add     v29.8H, v30.8H, v31.8H\n+    __ addv(v26, __ T2S, v27, v28);                    \/\/       add     v26.2S, v27.2S, v28.2S\n+    __ addv(v28, __ T4S, v29, v30);                    \/\/       add     v28.4S, v29.4S, v30.4S\n+    __ addv(v1, __ T2D, v2, v3);                       \/\/       add     v1.2D, v2.2D, v3.2D\n+    __ fadd(v27, __ T2S, v28, v29);                    \/\/       fadd    v27.2S, v28.2S, v29.2S\n+    __ fadd(v0, __ T4S, v1, v2);                       \/\/       fadd    v0.4S, v1.4S, v2.4S\n+    __ fadd(v20, __ T2D, v21, v22);                    \/\/       fadd    v20.2D, v21.2D, v22.2D\n+    __ subv(v28, __ T8B, v29, v30);                    \/\/       sub     v28.8B, v29.8B, v30.8B\n+    __ subv(v15, __ T16B, v16, v17);                   \/\/       sub     v15.16B, v16.16B, v17.16B\n+    __ subv(v12, __ T4H, v13, v14);                    \/\/       sub     v12.4H, v13.4H, v14.4H\n+    __ subv(v10, __ T8H, v11, v12);                    \/\/       sub     v10.8H, v11.8H, v12.8H\n@@ -643,16 +647,16 @@\n-    __ subv(v1, __ T4S, v2, v3);                       \/\/       sub     v1.4S, v2.4S, v3.4S\n-    __ subv(v27, __ T2D, v28, v29);                    \/\/       sub     v27.2D, v28.2D, v29.2D\n-    __ fsub(v0, __ T2S, v1, v2);                       \/\/       fsub    v0.2S, v1.2S, v2.2S\n-    __ fsub(v20, __ T4S, v21, v22);                    \/\/       fsub    v20.4S, v21.4S, v22.4S\n-    __ fsub(v28, __ T2D, v29, v30);                    \/\/       fsub    v28.2D, v29.2D, v30.2D\n-    __ mulv(v15, __ T8B, v16, v17);                    \/\/       mul     v15.8B, v16.8B, v17.8B\n-    __ mulv(v12, __ T16B, v13, v14);                   \/\/       mul     v12.16B, v13.16B, v14.16B\n-    __ mulv(v10, __ T4H, v11, v12);                    \/\/       mul     v10.4H, v11.4H, v12.4H\n-    __ mulv(v28, __ T8H, v29, v30);                    \/\/       mul     v28.8H, v29.8H, v30.8H\n-    __ mulv(v28, __ T2S, v29, v30);                    \/\/       mul     v28.2S, v29.2S, v30.2S\n-    __ mulv(v19, __ T4S, v20, v21);                    \/\/       mul     v19.4S, v20.4S, v21.4S\n-    __ fabd(v22, __ T2S, v23, v24);                    \/\/       fabd    v22.2S, v23.2S, v24.2S\n-    __ fabd(v10, __ T4S, v11, v12);                    \/\/       fabd    v10.4S, v11.4S, v12.4S\n-    __ fabd(v4, __ T2D, v5, v6);                       \/\/       fabd    v4.2D, v5.2D, v6.2D\n-    __ fmul(v30, __ T2S, v31, v0);                     \/\/       fmul    v30.2S, v31.2S, v0.2S\n-    __ fmul(v20, __ T4S, v21, v22);                    \/\/       fmul    v20.4S, v21.4S, v22.4S\n+    __ subv(v28, __ T4S, v29, v30);                    \/\/       sub     v28.4S, v29.4S, v30.4S\n+    __ subv(v19, __ T2D, v20, v21);                    \/\/       sub     v19.2D, v20.2D, v21.2D\n+    __ fsub(v22, __ T2S, v23, v24);                    \/\/       fsub    v22.2S, v23.2S, v24.2S\n+    __ fsub(v10, __ T4S, v11, v12);                    \/\/       fsub    v10.4S, v11.4S, v12.4S\n+    __ fsub(v4, __ T2D, v5, v6);                       \/\/       fsub    v4.2D, v5.2D, v6.2D\n+    __ mulv(v30, __ T8B, v31, v0);                     \/\/       mul     v30.8B, v31.8B, v0.8B\n+    __ mulv(v20, __ T16B, v21, v22);                   \/\/       mul     v20.16B, v21.16B, v22.16B\n+    __ mulv(v8, __ T4H, v9, v10);                      \/\/       mul     v8.4H, v9.4H, v10.4H\n+    __ mulv(v30, __ T8H, v31, v0);                     \/\/       mul     v30.8H, v31.8H, v0.8H\n+    __ mulv(v17, __ T2S, v18, v19);                    \/\/       mul     v17.2S, v18.2S, v19.2S\n+    __ mulv(v10, __ T4S, v11, v12);                    \/\/       mul     v10.4S, v11.4S, v12.4S\n+    __ fabd(v27, __ T2S, v28, v29);                    \/\/       fabd    v27.2S, v28.2S, v29.2S\n+    __ fabd(v2, __ T4S, v3, v4);                       \/\/       fabd    v2.4S, v3.4S, v4.4S\n+    __ fabd(v24, __ T2D, v25, v26);                    \/\/       fabd    v24.2D, v25.2D, v26.2D\n+    __ fmul(v4, __ T2S, v5, v6);                       \/\/       fmul    v4.2S, v5.2S, v6.2S\n+    __ fmul(v3, __ T4S, v4, v5);                       \/\/       fmul    v3.4S, v4.4S, v5.4S\n@@ -660,1 +664,1 @@\n-    __ mlav(v30, __ T4H, v31, v0);                     \/\/       mla     v30.4H, v31.4H, v0.4H\n+    __ mlav(v22, __ T4H, v23, v24);                    \/\/       mla     v22.4H, v23.4H, v24.4H\n@@ -662,54 +666,54 @@\n-    __ mlav(v10, __ T2S, v11, v12);                    \/\/       mla     v10.2S, v11.2S, v12.2S\n-    __ mlav(v27, __ T4S, v28, v29);                    \/\/       mla     v27.4S, v28.4S, v29.4S\n-    __ fmla(v2, __ T2S, v3, v4);                       \/\/       fmla    v2.2S, v3.2S, v4.2S\n-    __ fmla(v24, __ T4S, v25, v26);                    \/\/       fmla    v24.4S, v25.4S, v26.4S\n-    __ fmla(v4, __ T2D, v5, v6);                       \/\/       fmla    v4.2D, v5.2D, v6.2D\n-    __ mlsv(v3, __ T4H, v4, v5);                       \/\/       mls     v3.4H, v4.4H, v5.4H\n-    __ mlsv(v8, __ T8H, v9, v10);                      \/\/       mls     v8.8H, v9.8H, v10.8H\n-    __ mlsv(v22, __ T2S, v23, v24);                    \/\/       mls     v22.2S, v23.2S, v24.2S\n-    __ mlsv(v17, __ T4S, v18, v19);                    \/\/       mls     v17.4S, v18.4S, v19.4S\n-    __ fmls(v13, __ T2S, v14, v15);                    \/\/       fmls    v13.2S, v14.2S, v15.2S\n-    __ fmls(v4, __ T4S, v5, v6);                       \/\/       fmls    v4.4S, v5.4S, v6.4S\n-    __ fmls(v28, __ T2D, v29, v30);                    \/\/       fmls    v28.2D, v29.2D, v30.2D\n-    __ fdiv(v23, __ T2S, v24, v25);                    \/\/       fdiv    v23.2S, v24.2S, v25.2S\n-    __ fdiv(v21, __ T4S, v22, v23);                    \/\/       fdiv    v21.4S, v22.4S, v23.4S\n-    __ fdiv(v25, __ T2D, v26, v27);                    \/\/       fdiv    v25.2D, v26.2D, v27.2D\n-    __ maxv(v24, __ T8B, v25, v26);                    \/\/       smax    v24.8B, v25.8B, v26.8B\n-    __ maxv(v3, __ T16B, v4, v5);                      \/\/       smax    v3.16B, v4.16B, v5.16B\n-    __ maxv(v23, __ T4H, v24, v25);                    \/\/       smax    v23.4H, v24.4H, v25.4H\n-    __ maxv(v26, __ T8H, v27, v28);                    \/\/       smax    v26.8H, v27.8H, v28.8H\n-    __ maxv(v23, __ T2S, v24, v25);                    \/\/       smax    v23.2S, v24.2S, v25.2S\n-    __ maxv(v14, __ T4S, v15, v16);                    \/\/       smax    v14.4S, v15.4S, v16.4S\n-    __ smaxp(v21, __ T8B, v22, v23);                   \/\/       smaxp   v21.8B, v22.8B, v23.8B\n-    __ smaxp(v3, __ T16B, v4, v5);                     \/\/       smaxp   v3.16B, v4.16B, v5.16B\n-    __ smaxp(v23, __ T4H, v24, v25);                   \/\/       smaxp   v23.4H, v24.4H, v25.4H\n-    __ smaxp(v8, __ T8H, v9, v10);                     \/\/       smaxp   v8.8H, v9.8H, v10.8H\n-    __ smaxp(v24, __ T2S, v25, v26);                   \/\/       smaxp   v24.2S, v25.2S, v26.2S\n-    __ smaxp(v19, __ T4S, v20, v21);                   \/\/       smaxp   v19.4S, v20.4S, v21.4S\n-    __ fmax(v15, __ T2S, v16, v17);                    \/\/       fmax    v15.2S, v16.2S, v17.2S\n-    __ fmax(v16, __ T4S, v17, v18);                    \/\/       fmax    v16.4S, v17.4S, v18.4S\n-    __ fmax(v2, __ T2D, v3, v4);                       \/\/       fmax    v2.2D, v3.2D, v4.2D\n-    __ minv(v1, __ T8B, v2, v3);                       \/\/       smin    v1.8B, v2.8B, v3.8B\n-    __ minv(v0, __ T16B, v1, v2);                      \/\/       smin    v0.16B, v1.16B, v2.16B\n-    __ minv(v24, __ T4H, v25, v26);                    \/\/       smin    v24.4H, v25.4H, v26.4H\n-    __ minv(v4, __ T8H, v5, v6);                       \/\/       smin    v4.8H, v5.8H, v6.8H\n-    __ minv(v3, __ T2S, v4, v5);                       \/\/       smin    v3.2S, v4.2S, v5.2S\n-    __ minv(v11, __ T4S, v12, v13);                    \/\/       smin    v11.4S, v12.4S, v13.4S\n-    __ sminp(v30, __ T8B, v31, v0);                    \/\/       sminp   v30.8B, v31.8B, v0.8B\n-    __ sminp(v27, __ T16B, v28, v29);                  \/\/       sminp   v27.16B, v28.16B, v29.16B\n-    __ sminp(v9, __ T4H, v10, v11);                    \/\/       sminp   v9.4H, v10.4H, v11.4H\n-    __ sminp(v25, __ T8H, v26, v27);                   \/\/       sminp   v25.8H, v26.8H, v27.8H\n-    __ sminp(v2, __ T2S, v3, v4);                      \/\/       sminp   v2.2S, v3.2S, v4.2S\n-    __ sminp(v12, __ T4S, v13, v14);                   \/\/       sminp   v12.4S, v13.4S, v14.4S\n-    __ fmin(v17, __ T2S, v18, v19);                    \/\/       fmin    v17.2S, v18.2S, v19.2S\n-    __ fmin(v30, __ T4S, v31, v0);                     \/\/       fmin    v30.4S, v31.4S, v0.4S\n-    __ fmin(v1, __ T2D, v2, v3);                       \/\/       fmin    v1.2D, v2.2D, v3.2D\n-    __ cmeq(v12, __ T8B, v13, v14);                    \/\/       cmeq    v12.8B, v13.8B, v14.8B\n-    __ cmeq(v28, __ T16B, v29, v30);                   \/\/       cmeq    v28.16B, v29.16B, v30.16B\n-    __ cmeq(v0, __ T4H, v1, v2);                       \/\/       cmeq    v0.4H, v1.4H, v2.4H\n-    __ cmeq(v17, __ T8H, v18, v19);                    \/\/       cmeq    v17.8H, v18.8H, v19.8H\n-    __ cmeq(v12, __ T2S, v13, v14);                    \/\/       cmeq    v12.2S, v13.2S, v14.2S\n-    __ cmeq(v17, __ T4S, v18, v19);                    \/\/       cmeq    v17.4S, v18.4S, v19.4S\n-    __ cmeq(v21, __ T2D, v22, v23);                    \/\/       cmeq    v21.2D, v22.2D, v23.2D\n-    __ fcmeq(v12, __ T2S, v13, v14);                   \/\/       fcmeq   v12.2S, v13.2S, v14.2S\n-    __ fcmeq(v27, __ T4S, v28, v29);                   \/\/       fcmeq   v27.4S, v28.4S, v29.4S\n+    __ mlav(v13, __ T2S, v14, v15);                    \/\/       mla     v13.2S, v14.2S, v15.2S\n+    __ mlav(v4, __ T4S, v5, v6);                       \/\/       mla     v4.4S, v5.4S, v6.4S\n+    __ fmla(v28, __ T2S, v29, v30);                    \/\/       fmla    v28.2S, v29.2S, v30.2S\n+    __ fmla(v23, __ T4S, v24, v25);                    \/\/       fmla    v23.4S, v24.4S, v25.4S\n+    __ fmla(v21, __ T2D, v22, v23);                    \/\/       fmla    v21.2D, v22.2D, v23.2D\n+    __ mlsv(v25, __ T4H, v26, v27);                    \/\/       mls     v25.4H, v26.4H, v27.4H\n+    __ mlsv(v24, __ T8H, v25, v26);                    \/\/       mls     v24.8H, v25.8H, v26.8H\n+    __ mlsv(v3, __ T2S, v4, v5);                       \/\/       mls     v3.2S, v4.2S, v5.2S\n+    __ mlsv(v23, __ T4S, v24, v25);                    \/\/       mls     v23.4S, v24.4S, v25.4S\n+    __ fmls(v26, __ T2S, v27, v28);                    \/\/       fmls    v26.2S, v27.2S, v28.2S\n+    __ fmls(v23, __ T4S, v24, v25);                    \/\/       fmls    v23.4S, v24.4S, v25.4S\n+    __ fmls(v14, __ T2D, v15, v16);                    \/\/       fmls    v14.2D, v15.2D, v16.2D\n+    __ fdiv(v21, __ T2S, v22, v23);                    \/\/       fdiv    v21.2S, v22.2S, v23.2S\n+    __ fdiv(v3, __ T4S, v4, v5);                       \/\/       fdiv    v3.4S, v4.4S, v5.4S\n+    __ fdiv(v23, __ T2D, v24, v25);                    \/\/       fdiv    v23.2D, v24.2D, v25.2D\n+    __ maxv(v8, __ T8B, v9, v10);                      \/\/       smax    v8.8B, v9.8B, v10.8B\n+    __ maxv(v24, __ T16B, v25, v26);                   \/\/       smax    v24.16B, v25.16B, v26.16B\n+    __ maxv(v19, __ T4H, v20, v21);                    \/\/       smax    v19.4H, v20.4H, v21.4H\n+    __ maxv(v15, __ T8H, v16, v17);                    \/\/       smax    v15.8H, v16.8H, v17.8H\n+    __ maxv(v16, __ T2S, v17, v18);                    \/\/       smax    v16.2S, v17.2S, v18.2S\n+    __ maxv(v2, __ T4S, v3, v4);                       \/\/       smax    v2.4S, v3.4S, v4.4S\n+    __ smaxp(v1, __ T8B, v2, v3);                      \/\/       smaxp   v1.8B, v2.8B, v3.8B\n+    __ smaxp(v0, __ T16B, v1, v2);                     \/\/       smaxp   v0.16B, v1.16B, v2.16B\n+    __ smaxp(v24, __ T4H, v25, v26);                   \/\/       smaxp   v24.4H, v25.4H, v26.4H\n+    __ smaxp(v4, __ T8H, v5, v6);                      \/\/       smaxp   v4.8H, v5.8H, v6.8H\n+    __ smaxp(v3, __ T2S, v4, v5);                      \/\/       smaxp   v3.2S, v4.2S, v5.2S\n+    __ smaxp(v11, __ T4S, v12, v13);                   \/\/       smaxp   v11.4S, v12.4S, v13.4S\n+    __ fmax(v30, __ T2S, v31, v0);                     \/\/       fmax    v30.2S, v31.2S, v0.2S\n+    __ fmax(v27, __ T4S, v28, v29);                    \/\/       fmax    v27.4S, v28.4S, v29.4S\n+    __ fmax(v9, __ T2D, v10, v11);                     \/\/       fmax    v9.2D, v10.2D, v11.2D\n+    __ minv(v25, __ T8B, v26, v27);                    \/\/       smin    v25.8B, v26.8B, v27.8B\n+    __ minv(v2, __ T16B, v3, v4);                      \/\/       smin    v2.16B, v3.16B, v4.16B\n+    __ minv(v12, __ T4H, v13, v14);                    \/\/       smin    v12.4H, v13.4H, v14.4H\n+    __ minv(v17, __ T8H, v18, v19);                    \/\/       smin    v17.8H, v18.8H, v19.8H\n+    __ minv(v30, __ T2S, v31, v0);                     \/\/       smin    v30.2S, v31.2S, v0.2S\n+    __ minv(v1, __ T4S, v2, v3);                       \/\/       smin    v1.4S, v2.4S, v3.4S\n+    __ sminp(v12, __ T8B, v13, v14);                   \/\/       sminp   v12.8B, v13.8B, v14.8B\n+    __ sminp(v28, __ T16B, v29, v30);                  \/\/       sminp   v28.16B, v29.16B, v30.16B\n+    __ sminp(v0, __ T4H, v1, v2);                      \/\/       sminp   v0.4H, v1.4H, v2.4H\n+    __ sminp(v17, __ T8H, v18, v19);                   \/\/       sminp   v17.8H, v18.8H, v19.8H\n+    __ sminp(v12, __ T2S, v13, v14);                   \/\/       sminp   v12.2S, v13.2S, v14.2S\n+    __ sminp(v17, __ T4S, v18, v19);                   \/\/       sminp   v17.4S, v18.4S, v19.4S\n+    __ fmin(v21, __ T2S, v22, v23);                    \/\/       fmin    v21.2S, v22.2S, v23.2S\n+    __ fmin(v12, __ T4S, v13, v14);                    \/\/       fmin    v12.4S, v13.4S, v14.4S\n+    __ fmin(v27, __ T2D, v28, v29);                    \/\/       fmin    v27.2D, v28.2D, v29.2D\n+    __ cmeq(v29, __ T8B, v30, v31);                    \/\/       cmeq    v29.8B, v30.8B, v31.8B\n+    __ cmeq(v30, __ T16B, v31, v0);                    \/\/       cmeq    v30.16B, v31.16B, v0.16B\n+    __ cmeq(v1, __ T4H, v2, v3);                       \/\/       cmeq    v1.4H, v2.4H, v3.4H\n+    __ cmeq(v25, __ T8H, v26, v27);                    \/\/       cmeq    v25.8H, v26.8H, v27.8H\n+    __ cmeq(v27, __ T2S, v28, v29);                    \/\/       cmeq    v27.2S, v28.2S, v29.2S\n+    __ cmeq(v4, __ T4S, v5, v6);                       \/\/       cmeq    v4.4S, v5.4S, v6.4S\n+    __ cmeq(v29, __ T2D, v30, v31);                    \/\/       cmeq    v29.2D, v30.2D, v31.2D\n+    __ fcmeq(v3, __ T2S, v4, v5);                      \/\/       fcmeq   v3.2S, v4.2S, v5.2S\n+    __ fcmeq(v6, __ T4S, v7, v8);                      \/\/       fcmeq   v6.4S, v7.4S, v8.4S\n@@ -717,11 +721,11 @@\n-    __ cmgt(v30, __ T8B, v31, v0);                     \/\/       cmgt    v30.8B, v31.8B, v0.8B\n-    __ cmgt(v1, __ T16B, v2, v3);                      \/\/       cmgt    v1.16B, v2.16B, v3.16B\n-    __ cmgt(v25, __ T4H, v26, v27);                    \/\/       cmgt    v25.4H, v26.4H, v27.4H\n-    __ cmgt(v27, __ T8H, v28, v29);                    \/\/       cmgt    v27.8H, v28.8H, v29.8H\n-    __ cmgt(v4, __ T2S, v5, v6);                       \/\/       cmgt    v4.2S, v5.2S, v6.2S\n-    __ cmgt(v29, __ T4S, v30, v31);                    \/\/       cmgt    v29.4S, v30.4S, v31.4S\n-    __ cmgt(v3, __ T2D, v4, v5);                       \/\/       cmgt    v3.2D, v4.2D, v5.2D\n-    __ cmhi(v6, __ T8B, v7, v8);                       \/\/       cmhi    v6.8B, v7.8B, v8.8B\n-    __ cmhi(v29, __ T16B, v30, v31);                   \/\/       cmhi    v29.16B, v30.16B, v31.16B\n-    __ cmhi(v25, __ T4H, v26, v27);                    \/\/       cmhi    v25.4H, v26.4H, v27.4H\n-    __ cmhi(v17, __ T8H, v18, v19);                    \/\/       cmhi    v17.8H, v18.8H, v19.8H\n+    __ cmgt(v25, __ T8B, v26, v27);                    \/\/       cmgt    v25.8B, v26.8B, v27.8B\n+    __ cmgt(v17, __ T16B, v18, v19);                   \/\/       cmgt    v17.16B, v18.16B, v19.16B\n+    __ cmgt(v8, __ T4H, v9, v10);                      \/\/       cmgt    v8.4H, v9.4H, v10.4H\n+    __ cmgt(v7, __ T8H, v8, v9);                       \/\/       cmgt    v7.8H, v8.8H, v9.8H\n+    __ cmgt(v12, __ T2S, v13, v14);                    \/\/       cmgt    v12.2S, v13.2S, v14.2S\n+    __ cmgt(v0, __ T4S, v1, v2);                       \/\/       cmgt    v0.4S, v1.4S, v2.4S\n+    __ cmgt(v19, __ T2D, v20, v21);                    \/\/       cmgt    v19.2D, v20.2D, v21.2D\n+    __ cmhi(v1, __ T8B, v2, v3);                       \/\/       cmhi    v1.8B, v2.8B, v3.8B\n+    __ cmhi(v23, __ T16B, v24, v25);                   \/\/       cmhi    v23.16B, v24.16B, v25.16B\n+    __ cmhi(v2, __ T4H, v3, v4);                       \/\/       cmhi    v2.4H, v3.4H, v4.4H\n+    __ cmhi(v0, __ T8H, v1, v2);                       \/\/       cmhi    v0.8H, v1.8H, v2.8H\n@@ -729,22 +733,30 @@\n-    __ cmhi(v7, __ T4S, v8, v9);                       \/\/       cmhi    v7.4S, v8.4S, v9.4S\n-    __ cmhi(v12, __ T2D, v13, v14);                    \/\/       cmhi    v12.2D, v13.2D, v14.2D\n-    __ cmhs(v0, __ T8B, v1, v2);                       \/\/       cmhs    v0.8B, v1.8B, v2.8B\n-    __ cmhs(v19, __ T16B, v20, v21);                   \/\/       cmhs    v19.16B, v20.16B, v21.16B\n-    __ cmhs(v1, __ T4H, v2, v3);                       \/\/       cmhs    v1.4H, v2.4H, v3.4H\n-    __ cmhs(v23, __ T8H, v24, v25);                    \/\/       cmhs    v23.8H, v24.8H, v25.8H\n-    __ cmhs(v2, __ T2S, v3, v4);                       \/\/       cmhs    v2.2S, v3.2S, v4.2S\n-    __ cmhs(v0, __ T4S, v1, v2);                       \/\/       cmhs    v0.4S, v1.4S, v2.4S\n-    __ cmhs(v8, __ T2D, v9, v10);                      \/\/       cmhs    v8.2D, v9.2D, v10.2D\n-    __ fcmgt(v23, __ T2S, v24, v25);                   \/\/       fcmgt   v23.2S, v24.2S, v25.2S\n-    __ fcmgt(v25, __ T4S, v26, v27);                   \/\/       fcmgt   v25.4S, v26.4S, v27.4S\n-    __ fcmgt(v15, __ T2D, v16, v17);                   \/\/       fcmgt   v15.2D, v16.2D, v17.2D\n-    __ cmge(v29, __ T8B, v30, v31);                    \/\/       cmge    v29.8B, v30.8B, v31.8B\n-    __ cmge(v3, __ T16B, v4, v5);                      \/\/       cmge    v3.16B, v4.16B, v5.16B\n-    __ cmge(v10, __ T4H, v11, v12);                    \/\/       cmge    v10.4H, v11.4H, v12.4H\n-    __ cmge(v22, __ T8H, v23, v24);                    \/\/       cmge    v22.8H, v23.8H, v24.8H\n-    __ cmge(v10, __ T2S, v11, v12);                    \/\/       cmge    v10.2S, v11.2S, v12.2S\n-    __ cmge(v4, __ T4S, v5, v6);                       \/\/       cmge    v4.4S, v5.4S, v6.4S\n-    __ cmge(v17, __ T2D, v18, v19);                    \/\/       cmge    v17.2D, v18.2D, v19.2D\n-    __ fcmge(v1, __ T2S, v2, v3);                      \/\/       fcmge   v1.2S, v2.2S, v3.2S\n-    __ fcmge(v11, __ T4S, v12, v13);                   \/\/       fcmge   v11.4S, v12.4S, v13.4S\n-    __ fcmge(v7, __ T2D, v8, v9);                      \/\/       fcmge   v7.2D, v8.2D, v9.2D\n+    __ cmhi(v23, __ T4S, v24, v25);                    \/\/       cmhi    v23.4S, v24.4S, v25.4S\n+    __ cmhi(v25, __ T2D, v26, v27);                    \/\/       cmhi    v25.2D, v26.2D, v27.2D\n+    __ cmhs(v15, __ T8B, v16, v17);                    \/\/       cmhs    v15.8B, v16.8B, v17.8B\n+    __ cmhs(v29, __ T16B, v30, v31);                   \/\/       cmhs    v29.16B, v30.16B, v31.16B\n+    __ cmhs(v3, __ T4H, v4, v5);                       \/\/       cmhs    v3.4H, v4.4H, v5.4H\n+    __ cmhs(v10, __ T8H, v11, v12);                    \/\/       cmhs    v10.8H, v11.8H, v12.8H\n+    __ cmhs(v22, __ T2S, v23, v24);                    \/\/       cmhs    v22.2S, v23.2S, v24.2S\n+    __ cmhs(v10, __ T4S, v11, v12);                    \/\/       cmhs    v10.4S, v11.4S, v12.4S\n+    __ cmhs(v4, __ T2D, v5, v6);                       \/\/       cmhs    v4.2D, v5.2D, v6.2D\n+    __ fcmgt(v17, __ T2S, v18, v19);                   \/\/       fcmgt   v17.2S, v18.2S, v19.2S\n+    __ fcmgt(v1, __ T4S, v2, v3);                      \/\/       fcmgt   v1.4S, v2.4S, v3.4S\n+    __ fcmgt(v11, __ T2D, v12, v13);                   \/\/       fcmgt   v11.2D, v12.2D, v13.2D\n+    __ cmge(v7, __ T8B, v8, v9);                       \/\/       cmge    v7.8B, v8.8B, v9.8B\n+    __ cmge(v10, __ T16B, v11, v12);                   \/\/       cmge    v10.16B, v11.16B, v12.16B\n+    __ cmge(v15, __ T4H, v16, v17);                    \/\/       cmge    v15.4H, v16.4H, v17.4H\n+    __ cmge(v16, __ T8H, v17, v18);                    \/\/       cmge    v16.8H, v17.8H, v18.8H\n+    __ cmge(v2, __ T2S, v3, v4);                       \/\/       cmge    v2.2S, v3.2S, v4.2S\n+    __ cmge(v9, __ T4S, v10, v11);                     \/\/       cmge    v9.4S, v10.4S, v11.4S\n+    __ cmge(v11, __ T2D, v12, v13);                    \/\/       cmge    v11.2D, v12.2D, v13.2D\n+    __ fcmge(v12, __ T2S, v13, v14);                   \/\/       fcmge   v12.2S, v13.2S, v14.2S\n+    __ fcmge(v14, __ T4S, v15, v16);                   \/\/       fcmge   v14.4S, v15.4S, v16.4S\n+    __ fcmge(v13, __ T2D, v14, v15);                   \/\/       fcmge   v13.2D, v14.2D, v15.2D\n+\n+\/\/ SVEComparisonWithZero\n+    __ sve_fcm(Assembler::EQ, p1, __ D, p6, z6, 0.0);  \/\/       fcmeq   p1.d, p6\/z, z6.d, #0.0\n+    __ sve_fcm(Assembler::GT, p8, __ S, p1, z4, 0.0);  \/\/       fcmgt   p8.s, p1\/z, z4.s, #0.0\n+    __ sve_fcm(Assembler::GE, p6, __ D, p4, z17, 0.0); \/\/       fcmge   p6.d, p4\/z, z17.d, #0.0\n+    __ sve_fcm(Assembler::LT, p9, __ D, p5, z10, 0.0); \/\/       fcmlt   p9.d, p5\/z, z10.d, #0.0\n+    __ sve_fcm(Assembler::LE, p6, __ D, p7, z25, 0.0); \/\/       fcmle   p6.d, p7\/z, z25.d, #0.0\n+    __ sve_fcm(Assembler::NE, p7, __ D, p0, z10, 0.0); \/\/       fcmne   p7.d, p0\/z, z10.d, #0.0\n@@ -776,0 +788,2 @@\n+    __ fmovs(v9, __ T2S, 0.5f);                        \/\/       fmov    v9.2s, 0.5\n+    __ fmovd(v14, __ T2D, 0.5f);                       \/\/       fmov    v14.2d, 0.5\n@@ -777,1 +791,3 @@\n-    __ fcvtzs(v0, __ T4S, v1);                         \/\/       fcvtzs  v0.4s, v1.4s\n+    __ fcvtzs(v0, __ T2S, v1);                         \/\/       fcvtzs  v0.2s, v1.2s\n+    __ fcvtas(v2, __ T4S, v3);                         \/\/       fcvtas  v2.4s, v3.4s\n+    __ fcvtms(v4, __ T2D, v5);                         \/\/       fcvtms  v4.2d, v5.2d\n@@ -948,0 +964,1 @@\n+    __ sve_ext(z17, z16, 63);                          \/\/       ext     z17.b, z17.b, z16.b, #63\n@@ -986,9 +1003,9 @@\n-    __ swp(Assembler::xword, r10, r15, r17);           \/\/       swp     x10, x15, [x17]\n-    __ ldadd(Assembler::xword, r2, r10, r12);          \/\/       ldadd   x2, x10, [x12]\n-    __ ldbic(Assembler::xword, r12, r15, r13);         \/\/       ldclr   x12, x15, [x13]\n-    __ ldeor(Assembler::xword, r2, r7, r20);           \/\/       ldeor   x2, x7, [x20]\n-    __ ldorr(Assembler::xword, r26, r16, r4);          \/\/       ldset   x26, x16, [x4]\n-    __ ldsmin(Assembler::xword, r2, r4, r12);          \/\/       ldsmin  x2, x4, [x12]\n-    __ ldsmax(Assembler::xword, r16, r21, r16);        \/\/       ldsmax  x16, x21, [x16]\n-    __ ldumin(Assembler::xword, r16, r11, r21);        \/\/       ldumin  x16, x11, [x21]\n-    __ ldumax(Assembler::xword, r23, r12, r26);        \/\/       ldumax  x23, x12, [x26]\n+    __ swp(Assembler::xword, r12, zr, r10);            \/\/       swp     x12, xzr, [x10]\n+    __ ldadd(Assembler::xword, r16, r7, r2);           \/\/       ldadd   x16, x7, [x2]\n+    __ ldbic(Assembler::xword, r3, r13, r19);          \/\/       ldclr   x3, x13, [x19]\n+    __ ldeor(Assembler::xword, r17, r16, r3);          \/\/       ldeor   x17, x16, [x3]\n+    __ ldorr(Assembler::xword, r1, r11, r30);          \/\/       ldset   x1, x11, [x30]\n+    __ ldsmin(Assembler::xword, r5, r8, r15);          \/\/       ldsmin  x5, x8, [x15]\n+    __ ldsmax(Assembler::xword, r29, r30, r0);         \/\/       ldsmax  x29, x30, [x0]\n+    __ ldumin(Assembler::xword, r20, r7, r20);         \/\/       ldumin  x20, x7, [x20]\n+    __ ldumax(Assembler::xword, r23, r28, r21);        \/\/       ldumax  x23, x28, [x21]\n@@ -997,9 +1014,9 @@\n-    __ swpa(Assembler::xword, r23, r28, r14);          \/\/       swpa    x23, x28, [x14]\n-    __ ldadda(Assembler::xword, r11, r24, r1);         \/\/       ldadda  x11, x24, [x1]\n-    __ ldbica(Assembler::xword, r12, zr, r10);         \/\/       ldclra  x12, xzr, [x10]\n-    __ ldeora(Assembler::xword, r16, r7, r2);          \/\/       ldeora  x16, x7, [x2]\n-    __ ldorra(Assembler::xword, r3, r13, r19);         \/\/       ldseta  x3, x13, [x19]\n-    __ ldsmina(Assembler::xword, r17, r16, r3);        \/\/       ldsmina x17, x16, [x3]\n-    __ ldsmaxa(Assembler::xword, r1, r11, r30);        \/\/       ldsmaxa x1, x11, [x30]\n-    __ ldumina(Assembler::xword, r5, r8, r15);         \/\/       ldumina x5, x8, [x15]\n-    __ ldumaxa(Assembler::xword, r29, r30, r0);        \/\/       ldumaxa x29, x30, [x0]\n+    __ swpa(Assembler::xword, r27, r25, r5);           \/\/       swpa    x27, x25, [x5]\n+    __ ldadda(Assembler::xword, r1, r23, r16);         \/\/       ldadda  x1, x23, [x16]\n+    __ ldbica(Assembler::xword, zr, r5, r12);          \/\/       ldclra  xzr, x5, [x12]\n+    __ ldeora(Assembler::xword, r9, r28, r15);         \/\/       ldeora  x9, x28, [x15]\n+    __ ldorra(Assembler::xword, r29, r22, sp);         \/\/       ldseta  x29, x22, [sp]\n+    __ ldsmina(Assembler::xword, r19, zr, r5);         \/\/       ldsmina x19, xzr, [x5]\n+    __ ldsmaxa(Assembler::xword, r14, r16, sp);        \/\/       ldsmaxa x14, x16, [sp]\n+    __ ldumina(Assembler::xword, r16, r27, r20);       \/\/       ldumina x16, x27, [x20]\n+    __ ldumaxa(Assembler::xword, r16, r12, r11);       \/\/       ldumaxa x16, x12, [x11]\n@@ -1008,9 +1025,9 @@\n-    __ swpal(Assembler::xword, r20, r7, r20);          \/\/       swpal   x20, x7, [x20]\n-    __ ldaddal(Assembler::xword, r23, r28, r21);       \/\/       ldaddal x23, x28, [x21]\n-    __ ldbical(Assembler::xword, r27, r25, r5);        \/\/       ldclral x27, x25, [x5]\n-    __ ldeoral(Assembler::xword, r1, r23, r16);        \/\/       ldeoral x1, x23, [x16]\n-    __ ldorral(Assembler::xword, zr, r5, r12);         \/\/       ldsetal xzr, x5, [x12]\n-    __ ldsminal(Assembler::xword, r9, r28, r15);       \/\/       ldsminal        x9, x28, [x15]\n-    __ ldsmaxal(Assembler::xword, r29, r22, sp);       \/\/       ldsmaxal        x29, x22, [sp]\n-    __ lduminal(Assembler::xword, r19, zr, r5);        \/\/       lduminal        x19, xzr, [x5]\n-    __ ldumaxal(Assembler::xword, r14, r16, sp);       \/\/       ldumaxal        x14, x16, [sp]\n+    __ swpal(Assembler::xword, r9, r6, r30);           \/\/       swpal   x9, x6, [x30]\n+    __ ldaddal(Assembler::xword, r17, r27, r28);       \/\/       ldaddal x17, x27, [x28]\n+    __ ldbical(Assembler::xword, r30, r7, r10);        \/\/       ldclral x30, x7, [x10]\n+    __ ldeoral(Assembler::xword, r20, r10, r4);        \/\/       ldeoral x20, x10, [x4]\n+    __ ldorral(Assembler::xword, r24, r17, r17);       \/\/       ldsetal x24, x17, [x17]\n+    __ ldsminal(Assembler::xword, r22, r3, r29);       \/\/       ldsminal        x22, x3, [x29]\n+    __ ldsmaxal(Assembler::xword, r15, r22, r19);      \/\/       ldsmaxal        x15, x22, [x19]\n+    __ lduminal(Assembler::xword, r19, r22, r2);       \/\/       lduminal        x19, x22, [x2]\n+    __ ldumaxal(Assembler::xword, r15, r6, r12);       \/\/       ldumaxal        x15, x6, [x12]\n@@ -1019,9 +1036,9 @@\n-    __ swpl(Assembler::xword, r16, r27, r20);          \/\/       swpl    x16, x27, [x20]\n-    __ ldaddl(Assembler::xword, r16, r12, r11);        \/\/       ldaddl  x16, x12, [x11]\n-    __ ldbicl(Assembler::xword, r9, r6, r30);          \/\/       ldclrl  x9, x6, [x30]\n-    __ ldeorl(Assembler::xword, r17, r27, r28);        \/\/       ldeorl  x17, x27, [x28]\n-    __ ldorrl(Assembler::xword, r30, r7, r10);         \/\/       ldsetl  x30, x7, [x10]\n-    __ ldsminl(Assembler::xword, r20, r10, r4);        \/\/       ldsminl x20, x10, [x4]\n-    __ ldsmaxl(Assembler::xword, r24, r17, r17);       \/\/       ldsmaxl x24, x17, [x17]\n-    __ lduminl(Assembler::xword, r22, r3, r29);        \/\/       lduminl x22, x3, [x29]\n-    __ ldumaxl(Assembler::xword, r15, r22, r19);       \/\/       ldumaxl x15, x22, [x19]\n+    __ swpl(Assembler::xword, r16, r11, r13);          \/\/       swpl    x16, x11, [x13]\n+    __ ldaddl(Assembler::xword, r23, r1, r30);         \/\/       ldaddl  x23, x1, [x30]\n+    __ ldbicl(Assembler::xword, r19, r5, r17);         \/\/       ldclrl  x19, x5, [x17]\n+    __ ldeorl(Assembler::xword, r2, r16, r22);         \/\/       ldeorl  x2, x16, [x22]\n+    __ ldorrl(Assembler::xword, r13, r10, r21);        \/\/       ldsetl  x13, x10, [x21]\n+    __ ldsminl(Assembler::xword, r29, r27, r12);       \/\/       ldsminl x29, x27, [x12]\n+    __ ldsmaxl(Assembler::xword, r27, r3, r1);         \/\/       ldsmaxl x27, x3, [x1]\n+    __ lduminl(Assembler::xword, zr, r24, r19);        \/\/       lduminl xzr, x24, [x19]\n+    __ ldumaxl(Assembler::xword, r17, r9, r28);        \/\/       ldumaxl x17, x9, [x28]\n@@ -1030,9 +1047,9 @@\n-    __ swp(Assembler::word, r19, r22, r2);             \/\/       swp     w19, w22, [x2]\n-    __ ldadd(Assembler::word, r15, r6, r12);           \/\/       ldadd   w15, w6, [x12]\n-    __ ldbic(Assembler::word, r16, r11, r13);          \/\/       ldclr   w16, w11, [x13]\n-    __ ldeor(Assembler::word, r23, r1, r30);           \/\/       ldeor   w23, w1, [x30]\n-    __ ldorr(Assembler::word, r19, r5, r17);           \/\/       ldset   w19, w5, [x17]\n-    __ ldsmin(Assembler::word, r2, r16, r22);          \/\/       ldsmin  w2, w16, [x22]\n-    __ ldsmax(Assembler::word, r13, r10, r21);         \/\/       ldsmax  w13, w10, [x21]\n-    __ ldumin(Assembler::word, r29, r27, r12);         \/\/       ldumin  w29, w27, [x12]\n-    __ ldumax(Assembler::word, r27, r3, r1);           \/\/       ldumax  w27, w3, [x1]\n+    __ swp(Assembler::word, r27, r15, r7);             \/\/       swp     w27, w15, [x7]\n+    __ ldadd(Assembler::word, r21, r23, sp);           \/\/       ldadd   w21, w23, [sp]\n+    __ ldbic(Assembler::word, r25, r2, sp);            \/\/       ldclr   w25, w2, [sp]\n+    __ ldeor(Assembler::word, r27, r16, r10);          \/\/       ldeor   w27, w16, [x10]\n+    __ ldorr(Assembler::word, r23, r19, r3);           \/\/       ldset   w23, w19, [x3]\n+    __ ldsmin(Assembler::word, r16, r0, r25);          \/\/       ldsmin  w16, w0, [x25]\n+    __ ldsmax(Assembler::word, r26, r23, r2);          \/\/       ldsmax  w26, w23, [x2]\n+    __ ldumin(Assembler::word, r16, r12, r4);          \/\/       ldumin  w16, w12, [x4]\n+    __ ldumax(Assembler::word, r28, r30, r29);         \/\/       ldumax  w28, w30, [x29]\n@@ -1041,9 +1058,9 @@\n-    __ swpa(Assembler::word, zr, r24, r19);            \/\/       swpa    wzr, w24, [x19]\n-    __ ldadda(Assembler::word, r17, r9, r28);          \/\/       ldadda  w17, w9, [x28]\n-    __ ldbica(Assembler::word, r27, r15, r7);          \/\/       ldclra  w27, w15, [x7]\n-    __ ldeora(Assembler::word, r21, r23, sp);          \/\/       ldeora  w21, w23, [sp]\n-    __ ldorra(Assembler::word, r25, r2, sp);           \/\/       ldseta  w25, w2, [sp]\n-    __ ldsmina(Assembler::word, r27, r16, r10);        \/\/       ldsmina w27, w16, [x10]\n-    __ ldsmaxa(Assembler::word, r23, r19, r3);         \/\/       ldsmaxa w23, w19, [x3]\n-    __ ldumina(Assembler::word, r16, r0, r25);         \/\/       ldumina w16, w0, [x25]\n-    __ ldumaxa(Assembler::word, r26, r23, r2);         \/\/       ldumaxa w26, w23, [x2]\n+    __ swpa(Assembler::word, r16, r27, r6);            \/\/       swpa    w16, w27, [x6]\n+    __ ldadda(Assembler::word, r9, r29, r15);          \/\/       ldadda  w9, w29, [x15]\n+    __ ldbica(Assembler::word, r7, r4, r7);            \/\/       ldclra  w7, w4, [x7]\n+    __ ldeora(Assembler::word, r15, r9, r23);          \/\/       ldeora  w15, w9, [x23]\n+    __ ldorra(Assembler::word, r8, r2, r28);           \/\/       ldseta  w8, w2, [x28]\n+    __ ldsmina(Assembler::word, r21, zr, r5);          \/\/       ldsmina w21, wzr, [x5]\n+    __ ldsmaxa(Assembler::word, r27, r0, r17);         \/\/       ldsmaxa w27, w0, [x17]\n+    __ ldumina(Assembler::word, r15, r4, r26);         \/\/       ldumina w15, w4, [x26]\n+    __ ldumaxa(Assembler::word, r8, r28, r22);         \/\/       ldumaxa w8, w28, [x22]\n@@ -1052,9 +1069,9 @@\n-    __ swpal(Assembler::word, r16, r12, r4);           \/\/       swpal   w16, w12, [x4]\n-    __ ldaddal(Assembler::word, r28, r30, r29);        \/\/       ldaddal w28, w30, [x29]\n-    __ ldbical(Assembler::word, r16, r27, r6);         \/\/       ldclral w16, w27, [x6]\n-    __ ldeoral(Assembler::word, r9, r29, r15);         \/\/       ldeoral w9, w29, [x15]\n-    __ ldorral(Assembler::word, r7, r4, r7);           \/\/       ldsetal w7, w4, [x7]\n-    __ ldsminal(Assembler::word, r15, r9, r23);        \/\/       ldsminal        w15, w9, [x23]\n-    __ ldsmaxal(Assembler::word, r8, r2, r28);         \/\/       ldsmaxal        w8, w2, [x28]\n-    __ lduminal(Assembler::word, r21, zr, r5);         \/\/       lduminal        w21, wzr, [x5]\n-    __ ldumaxal(Assembler::word, r27, r0, r17);        \/\/       ldumaxal        w27, w0, [x17]\n+    __ swpal(Assembler::word, r27, r27, r25);          \/\/       swpal   w27, w27, [x25]\n+    __ ldaddal(Assembler::word, r23, r0, r4);          \/\/       ldaddal w23, w0, [x4]\n+    __ ldbical(Assembler::word, r6, r16, r0);          \/\/       ldclral w6, w16, [x0]\n+    __ ldeoral(Assembler::word, r4, r15, r1);          \/\/       ldeoral w4, w15, [x1]\n+    __ ldorral(Assembler::word, r10, r7, r5);          \/\/       ldsetal w10, w7, [x5]\n+    __ ldsminal(Assembler::word, r10, r28, r7);        \/\/       ldsminal        w10, w28, [x7]\n+    __ ldsmaxal(Assembler::word, r20, r23, r21);       \/\/       ldsmaxal        w20, w23, [x21]\n+    __ lduminal(Assembler::word, r6, r11, r8);         \/\/       lduminal        w6, w11, [x8]\n+    __ ldumaxal(Assembler::word, r17, zr, r6);         \/\/       ldumaxal        w17, wzr, [x6]\n@@ -1063,9 +1080,9 @@\n-    __ swpl(Assembler::word, r15, r4, r26);            \/\/       swpl    w15, w4, [x26]\n-    __ ldaddl(Assembler::word, r8, r28, r22);          \/\/       ldaddl  w8, w28, [x22]\n-    __ ldbicl(Assembler::word, r27, r27, r25);         \/\/       ldclrl  w27, w27, [x25]\n-    __ ldeorl(Assembler::word, r23, r0, r4);           \/\/       ldeorl  w23, w0, [x4]\n-    __ ldorrl(Assembler::word, r6, r16, r0);           \/\/       ldsetl  w6, w16, [x0]\n-    __ ldsminl(Assembler::word, r4, r15, r1);          \/\/       ldsminl w4, w15, [x1]\n-    __ ldsmaxl(Assembler::word, r10, r7, r5);          \/\/       ldsmaxl w10, w7, [x5]\n-    __ lduminl(Assembler::word, r10, r28, r7);         \/\/       lduminl w10, w28, [x7]\n-    __ ldumaxl(Assembler::word, r20, r23, r21);        \/\/       ldumaxl w20, w23, [x21]\n+    __ swpl(Assembler::word, r17, r2, r12);            \/\/       swpl    w17, w2, [x12]\n+    __ ldaddl(Assembler::word, r30, r29, r3);          \/\/       ldaddl  w30, w29, [x3]\n+    __ ldbicl(Assembler::word, r27, r22, r29);         \/\/       ldclrl  w27, w22, [x29]\n+    __ ldeorl(Assembler::word, r14, r13, r28);         \/\/       ldeorl  w14, w13, [x28]\n+    __ ldorrl(Assembler::word, r17, r24, r5);          \/\/       ldsetl  w17, w24, [x5]\n+    __ ldsminl(Assembler::word, r2, r14, r10);         \/\/       ldsminl w2, w14, [x10]\n+    __ ldsmaxl(Assembler::word, r16, r11, r27);        \/\/       ldsmaxl w16, w11, [x27]\n+    __ lduminl(Assembler::word, r23, r12, r4);         \/\/       lduminl w23, w12, [x4]\n+    __ ldumaxl(Assembler::word, r22, r17, r4);         \/\/       ldumaxl w22, w17, [x4]\n@@ -1074,4 +1091,4 @@\n-    __ bcax(v5, __ T16B, v10, v8, v16);                \/\/       bcax            v5.16B, v10.16B, v8.16B, v16.16B\n-    __ eor3(v30, __ T16B, v6, v17, v2);                \/\/       eor3            v30.16B, v6.16B, v17.16B, v2.16B\n-    __ rax1(v11, __ T2D, v29, v28);                    \/\/       rax1            v11.2D, v29.2D, v28.2D\n-    __ xar(v2, __ T2D, v26, v22, 58);                  \/\/       xar             v2.2D, v26.2D, v22.2D, #58\n+    __ bcax(v1, __ T16B, v19, v16, v17);               \/\/       bcax            v1.16B, v19.16B, v16.16B, v17.16B\n+    __ eor3(v12, __ T16B, v14, v12, v2);               \/\/       eor3            v12.16B, v14.16B, v12.16B, v2.16B\n+    __ rax1(v16, __ T2D, v3, v20);                     \/\/       rax1            v16.2D, v3.2D, v20.2D\n+    __ xar(v23, __ T2D, v5, v6, 15);                   \/\/       xar             v23.2D, v5.2D, v6.2D, #15\n@@ -1080,4 +1097,4 @@\n-    __ sha512h(v14, __ T2D, v13, v27);                 \/\/       sha512h         q14, q13, v27.2D\n-    __ sha512h2(v16, __ T2D, v23, v5);                 \/\/       sha512h2                q16, q23, v5.2D\n-    __ sha512su0(v2, __ T2D, v13);                     \/\/       sha512su0               v2.2D, v13.2D\n-    __ sha512su1(v10, __ T2D, v15, v10);               \/\/       sha512su1               v10.2D, v15.2D, v10.2D\n+    __ sha512h(v17, __ T2D, v12, v27);                 \/\/       sha512h         q17, q12, v27.2D\n+    __ sha512h2(v16, __ T2D, v16, v6);                 \/\/       sha512h2                q16, q16, v6.2D\n+    __ sha512su0(v2, __ T2D, v28);                     \/\/       sha512su0               v2.2D, v28.2D\n+    __ sha512su1(v3, __ T2D, v4, v6);                  \/\/       sha512su1               v3.2D, v4.2D, v6.2D\n@@ -1086,5 +1103,5 @@\n-    __ sve_add(z26, __ S, 98u);                        \/\/       add     z26.s, z26.s, #0x62\n-    __ sve_sub(z3, __ S, 138u);                        \/\/       sub     z3.s, z3.s, #0x8a\n-    __ sve_and(z4, __ B, 131u);                        \/\/       and     z4.b, z4.b, #0x83\n-    __ sve_eor(z17, __ H, 16368u);                     \/\/       eor     z17.h, z17.h, #0x3ff0\n-    __ sve_orr(z2, __ S, 4164941887u);                 \/\/       orr     z2.s, z2.s, #0xf83ff83f\n+    __ sve_add(z17, __ S, 110u);                       \/\/       add     z17.s, z17.s, #0x6e\n+    __ sve_sub(z12, __ S, 67u);                        \/\/       sub     z12.s, z12.s, #0x43\n+    __ sve_and(z24, __ S, 63u);                        \/\/       and     z24.s, z24.s, #0x3f\n+    __ sve_eor(z10, __ D, 18374686479671656447u);      \/\/       eor     z10.d, z10.d, #0xff00000000007fff\n+    __ sve_orr(z30, __ H, 511u);                       \/\/       orr     z30.h, z30.h, #0x1ff\n@@ -1093,5 +1110,5 @@\n-    __ sve_add(z23, __ B, 51u);                        \/\/       add     z23.b, z23.b, #0x33\n-    __ sve_sub(z7, __ S, 104u);                        \/\/       sub     z7.s, z7.s, #0x68\n-    __ sve_and(z27, __ S, 7864320u);                   \/\/       and     z27.s, z27.s, #0x780000\n-    __ sve_eor(z2, __ D, 68719476224u);                \/\/       eor     z2.d, z2.d, #0xffffffe00\n-    __ sve_orr(z6, __ S, 1056980736u);                 \/\/       orr     z6.s, z6.s, #0x3f003f00\n+    __ sve_add(z0, __ B, 120u);                        \/\/       add     z0.b, z0.b, #0x78\n+    __ sve_sub(z17, __ D, 74u);                        \/\/       sub     z17.d, z17.d, #0x4a\n+    __ sve_and(z10, __ S, 4261413375u);                \/\/       and     z10.s, z10.s, #0xfe0001ff\n+    __ sve_eor(z27, __ B, 128u);                       \/\/       eor     z27.b, z27.b, #0x80\n+    __ sve_orr(z17, __ S, 253952u);                    \/\/       orr     z17.s, z17.s, #0x3e000\n@@ -1100,5 +1117,5 @@\n-    __ sve_add(z12, __ S, 67u);                        \/\/       add     z12.s, z12.s, #0x43\n-    __ sve_sub(z24, __ S, 154u);                       \/\/       sub     z24.s, z24.s, #0x9a\n-    __ sve_and(z0, __ H, 511u);                        \/\/       and     z0.h, z0.h, #0x1ff\n-    __ sve_eor(z19, __ D, 9241386433220968447u);       \/\/       eor     z19.d, z19.d, #0x803fffff803fffff\n-    __ sve_orr(z6, __ B, 128u);                        \/\/       orr     z6.b, z6.b, #0x80\n+    __ sve_add(z28, __ B, 4u);                         \/\/       add     z28.b, z28.b, #0x4\n+    __ sve_sub(z8, __ S, 162u);                        \/\/       sub     z8.s, z8.s, #0xa2\n+    __ sve_and(z22, __ B, 96u);                        \/\/       and     z22.b, z22.b, #0x60\n+    __ sve_eor(z22, __ H, 511u);                       \/\/       eor     z22.h, z22.h, #0x1ff\n+    __ sve_orr(z30, __ S, 4261413375u);                \/\/       orr     z30.s, z30.s, #0xfe0001ff\n@@ -1107,5 +1124,5 @@\n-    __ sve_add(z17, __ D, 74u);                        \/\/       add     z17.d, z17.d, #0x4a\n-    __ sve_sub(z10, __ S, 170u);                       \/\/       sub     z10.s, z10.s, #0xaa\n-    __ sve_and(z22, __ D, 17179852800u);               \/\/       and     z22.d, z22.d, #0x3ffffc000\n-    __ sve_eor(z15, __ S, 8388600u);                   \/\/       eor     z15.s, z15.s, #0x7ffff8\n-    __ sve_orr(z4, __ D, 8064u);                       \/\/       orr     z4.d, z4.d, #0x1f80\n+    __ sve_add(z11, __ B, 112u);                       \/\/       add     z11.b, z11.b, #0x70\n+    __ sve_sub(z8, __ S, 134u);                        \/\/       sub     z8.s, z8.s, #0x86\n+    __ sve_and(z25, __ H, 508u);                       \/\/       and     z25.h, z25.h, #0x1fc\n+    __ sve_eor(z17, __ H, 65283u);                     \/\/       eor     z17.h, z17.h, #0xff03\n+    __ sve_orr(z4, __ D, 18446744073172942847u);       \/\/       orr     z4.d, z4.d, #0xffffffffe003ffff\n@@ -1114,5 +1131,5 @@\n-    __ sve_add(z8, __ S, 162u);                        \/\/       add     z8.s, z8.s, #0xa2\n-    __ sve_sub(z22, __ B, 130u);                       \/\/       sub     z22.b, z22.b, #0x82\n-    __ sve_and(z9, __ S, 4292870159u);                 \/\/       and     z9.s, z9.s, #0xffe0000f\n-    __ sve_eor(z5, __ D, 1150687262887383032u);        \/\/       eor     z5.d, z5.d, #0xff80ff80ff80ff8\n-    __ sve_orr(z22, __ H, 32256u);                     \/\/       orr     z22.h, z22.h, #0x7e00\n+    __ sve_add(z26, __ H, 120u);                       \/\/       add     z26.h, z26.h, #0x78\n+    __ sve_sub(z2, __ H, 237u);                        \/\/       sub     z2.h, z2.h, #0xed\n+    __ sve_and(z3, __ B, 243u);                        \/\/       and     z3.b, z3.b, #0xf3\n+    __ sve_eor(z21, __ S, 25166208u);                  \/\/       eor     z21.s, z21.s, #0x1800180\n+    __ sve_orr(z17, __ S, 917504u);                    \/\/       orr     z17.s, z17.s, #0xe0000\n@@ -1121,5 +1138,5 @@\n-    __ sve_add(z8, __ S, 134u);                        \/\/       add     z8.s, z8.s, #0x86\n-    __ sve_sub(z25, __ H, 39u);                        \/\/       sub     z25.h, z25.h, #0x27\n-    __ sve_and(z4, __ S, 4186112u);                    \/\/       and     z4.s, z4.s, #0x3fe000\n-    __ sve_eor(z29, __ B, 131u);                       \/\/       eor     z29.b, z29.b, #0x83\n-    __ sve_orr(z29, __ D, 4611685469745315712u);       \/\/       orr     z29.d, z29.d, #0x3fffff803fffff80\n+    __ sve_add(z19, __ S, 148u);                       \/\/       add     z19.s, z19.s, #0x94\n+    __ sve_sub(z22, __ S, 244u);                       \/\/       sub     z22.s, z22.s, #0xf4\n+    __ sve_and(z20, __ S, 491520u);                    \/\/       and     z20.s, z20.s, #0x78000\n+    __ sve_eor(z17, __ D, 18302628885642084351u);      \/\/       eor     z17.d, z17.d, #0xfe000000007fffff\n+    __ sve_orr(z4, __ D, 18158513714670600195u);       \/\/       orr     z4.d, z4.d, #0xfc000003fc000003\n@@ -1128,53 +1145,52 @@\n-    __ sve_add(z2, __ B, z11, z28);                    \/\/       add     z2.b, z11.b, z28.b\n-    __ sve_sub(z7, __ S, z1, z26);                     \/\/       sub     z7.s, z1.s, z26.s\n-    __ sve_fadd(z17, __ D, z14, z8);                   \/\/       fadd    z17.d, z14.d, z8.d\n-    __ sve_fmul(z21, __ D, z24, z5);                   \/\/       fmul    z21.d, z24.d, z5.d\n-    __ sve_fsub(z21, __ D, z17, z22);                  \/\/       fsub    z21.d, z17.d, z22.d\n-    __ sve_abs(z29, __ B, p5, z19);                    \/\/       abs     z29.b, p5\/m, z19.b\n-    __ sve_add(z4, __ B, p4, z23);                     \/\/       add     z4.b, p4\/m, z4.b, z23.b\n-    __ sve_and(z19, __ D, p1, z23);                    \/\/       and     z19.d, p1\/m, z19.d, z23.d\n-    __ sve_asr(z19, __ H, p0, z8);                     \/\/       asr     z19.h, p0\/m, z19.h, z8.h\n-    __ sve_bic(z14, __ D, p6, z17);                    \/\/       bic     z14.d, p6\/m, z14.d, z17.d\n-    __ sve_clz(z21, __ B, p1, z30);                    \/\/       clz     z21.b, p1\/m, z30.b\n-    __ sve_cnt(z10, __ B, p5, z12);                    \/\/       cnt     z10.b, p5\/m, z12.b\n-    __ sve_eor(z9, __ S, p1, z24);                     \/\/       eor     z9.s, p1\/m, z9.s, z24.s\n-    __ sve_lsl(z4, __ H, p6, z6);                      \/\/       lsl     z4.h, p6\/m, z4.h, z6.h\n-    __ sve_lsr(z27, __ S, p6, z13);                    \/\/       lsr     z27.s, p6\/m, z27.s, z13.s\n-    __ sve_mul(z30, __ S, p5, z22);                    \/\/       mul     z30.s, p5\/m, z30.s, z22.s\n-    __ sve_neg(z30, __ H, p7, z9);                     \/\/       neg     z30.h, p7\/m, z9.h\n-    __ sve_not(z19, __ D, p1, z20);                    \/\/       not     z19.d, p1\/m, z20.d\n-    __ sve_orr(z9, __ H, p2, z13);                     \/\/       orr     z9.h, p2\/m, z9.h, z13.h\n-    __ sve_rbit(z19, __ H, p0, z24);                   \/\/       rbit    z19.h, p0\/m, z24.h\n-    __ sve_revb(z19, __ S, p3, z17);                   \/\/       revb    z19.s, p3\/m, z17.s\n-    __ sve_smax(z16, __ B, p1, z0);                    \/\/       smax    z16.b, p1\/m, z16.b, z0.b\n-    __ sve_smin(z11, __ H, p2, z15);                   \/\/       smin    z11.h, p2\/m, z11.h, z15.h\n-    __ sve_sub(z15, __ D, p1, z15);                    \/\/       sub     z15.d, p1\/m, z15.d, z15.d\n-    __ sve_fabs(z5, __ D, p0, z10);                    \/\/       fabs    z5.d, p0\/m, z10.d\n-    __ sve_fadd(z26, __ S, p0, z0);                    \/\/       fadd    z26.s, p0\/m, z26.s, z0.s\n-    __ sve_fdiv(z19, __ D, p7, z10);                   \/\/       fdiv    z19.d, p7\/m, z19.d, z10.d\n-    __ sve_fmax(z3, __ D, p5, z7);                     \/\/       fmax    z3.d, p5\/m, z3.d, z7.d\n-    __ sve_fmin(z28, __ S, p3, z21);                   \/\/       fmin    z28.s, p3\/m, z28.s, z21.s\n-    __ sve_fmul(z26, __ D, p3, z17);                   \/\/       fmul    z26.d, p3\/m, z26.d, z17.d\n-    __ sve_fneg(z17, __ D, p3, z2);                    \/\/       fneg    z17.d, p3\/m, z2.d\n-    __ sve_frintm(z16, __ S, p5, z20);                 \/\/       frintm  z16.s, p5\/m, z20.s\n-    __ sve_frintn(z19, __ D, p0, z1);                  \/\/       frintn  z19.d, p0\/m, z1.d\n-    __ sve_frintp(z17, __ D, p2, z16);                 \/\/       frintp  z17.d, p2\/m, z16.d\n-    __ sve_fsqrt(z21, __ S, p0, z4);                   \/\/       fsqrt   z21.s, p0\/m, z4.s\n-    __ sve_fsub(z23, __ S, p3, z6);                    \/\/       fsub    z23.s, p3\/m, z23.s, z6.s\n-    __ sve_fmad(z20, __ S, p3, z16, z29);              \/\/       fmad    z20.s, p3\/m, z16.s, z29.s\n-    __ sve_fmla(z3, __ S, p0, z22, z9);                \/\/       fmla    z3.s, p0\/m, z22.s, z9.s\n-    __ sve_fmls(z24, __ D, p7, z3, z19);               \/\/       fmls    z24.d, p7\/m, z3.d, z19.d\n-    __ sve_fmsb(z7, __ S, p6, z21, z13);               \/\/       fmsb    z7.s, p6\/m, z21.s, z13.s\n-    __ sve_fnmad(z7, __ D, p6, z5, z21);               \/\/       fnmad   z7.d, p6\/m, z5.d, z21.d\n-    __ sve_fnmsb(z17, __ D, p0, z3, z9);               \/\/       fnmsb   z17.d, p0\/m, z3.d, z9.d\n-    __ sve_fnmla(z11, __ D, p2, z11, z14);             \/\/       fnmla   z11.d, p2\/m, z11.d, z14.d\n-    __ sve_fnmls(z17, __ D, p2, z13, z24);             \/\/       fnmls   z17.d, p2\/m, z13.d, z24.d\n-    __ sve_mla(z30, __ H, p4, z8, z15);                \/\/       mla     z30.h, p4\/m, z8.h, z15.h\n-    __ sve_mls(z26, __ H, p5, z27, z22);               \/\/       mls     z26.h, p5\/m, z27.h, z22.h\n-    __ sve_and(z8, z5, z27);                           \/\/       and     z8.d, z5.d, z27.d\n-    __ sve_eor(z10, z0, z14);                          \/\/       eor     z10.d, z0.d, z14.d\n-    __ sve_orr(z21, z20, z0);                          \/\/       orr     z21.d, z20.d, z0.d\n-    __ sve_bic(z22, z25, z5);                          \/\/       bic     z22.d, z25.d, z5.d\n-    __ sve_uzp1(z29, __ B, z17, z17);                  \/\/       uzp1    z29.b, z17.b, z17.b\n-    __ sve_uzp2(z12, __ H, z14, z29);                  \/\/       uzp2    z12.h, z14.h, z29.h\n-    __ sve_histcnt(z0, __ S, p4, z2, z30);             \/\/       histcnt z0.s, p4\/z, z2.s, z30.s\n+    __ sve_add(z2, __ H, z8, z8);                      \/\/       add     z2.h, z8.h, z8.h\n+    __ sve_sub(z24, __ S, z17, z30);                   \/\/       sub     z24.s, z17.s, z30.s\n+    __ sve_fadd(z4, __ S, z30, z1);                    \/\/       fadd    z4.s, z30.s, z1.s\n+    __ sve_fmul(z19, __ S, z12, z0);                   \/\/       fmul    z19.s, z12.s, z0.s\n+    __ sve_fsub(z7, __ S, z24, z17);                   \/\/       fsub    z7.s, z24.s, z17.s\n+    __ sve_abs(z27, __ D, p1, z9);                     \/\/       abs     z27.d, p1\/m, z9.d\n+    __ sve_add(z23, __ D, p3, z16);                    \/\/       add     z23.d, p3\/m, z23.d, z16.d\n+    __ sve_and(z22, __ D, p5, z20);                    \/\/       and     z22.d, p5\/m, z22.d, z20.d\n+    __ sve_asr(z28, __ S, p2, z13);                    \/\/       asr     z28.s, p2\/m, z28.s, z13.s\n+    __ sve_bic(z7, __ H, p5, z28);                     \/\/       bic     z7.h, p5\/m, z7.h, z28.h\n+    __ sve_clz(z11, __ S, p3, z11);                    \/\/       clz     z11.s, p3\/m, z11.s\n+    __ sve_cnt(z1, __ S, p6, z8);                      \/\/       cnt     z1.s, p6\/m, z8.s\n+    __ sve_eor(z13, __ S, p4, z17);                    \/\/       eor     z13.s, p4\/m, z13.s, z17.s\n+    __ sve_lsl(z4, __ H, p0, z3);                      \/\/       lsl     z4.h, p0\/m, z4.h, z3.h\n+    __ sve_lsr(z7, __ S, p3, z14);                     \/\/       lsr     z7.s, p3\/m, z7.s, z14.s\n+    __ sve_mul(z4, __ B, p3, z29);                     \/\/       mul     z4.b, p3\/m, z4.b, z29.b\n+    __ sve_neg(z0, __ D, p2, z21);                     \/\/       neg     z0.d, p2\/m, z21.d\n+    __ sve_not(z3, __ S, p0, z9);                      \/\/       not     z3.s, p0\/m, z9.s\n+    __ sve_orr(z28, __ B, p2, z24);                    \/\/       orr     z28.b, p2\/m, z28.b, z24.b\n+    __ sve_rbit(z19, __ D, p1, z23);                   \/\/       rbit    z19.d, p1\/m, z23.d\n+    __ sve_revb(z13, __ D, p5, z10);                   \/\/       revb    z13.d, p5\/m, z10.d\n+    __ sve_smax(z12, __ S, p4, z30);                   \/\/       smax    z12.s, p4\/m, z12.s, z30.s\n+    __ sve_smin(z14, __ S, p0, z29);                   \/\/       smin    z14.s, p0\/m, z14.s, z29.s\n+    __ sve_sub(z21, __ S, p5, z7);                     \/\/       sub     z21.s, p5\/m, z21.s, z7.s\n+    __ sve_fabs(z2, __ D, p0, z26);                    \/\/       fabs    z2.d, p0\/m, z26.d\n+    __ sve_fadd(z9, __ D, p4, z17);                    \/\/       fadd    z9.d, p4\/m, z9.d, z17.d\n+    __ sve_fdiv(z0, __ D, p1, z2);                     \/\/       fdiv    z0.d, p1\/m, z0.d, z2.d\n+    __ sve_fmax(z14, __ D, p1, z11);                   \/\/       fmax    z14.d, p1\/m, z14.d, z11.d\n+    __ sve_fmin(z14, __ S, p4, z29);                   \/\/       fmin    z14.s, p4\/m, z14.s, z29.s\n+    __ sve_fmul(z3, __ S, p0, z22);                    \/\/       fmul    z3.s, p0\/m, z3.s, z22.s\n+    __ sve_fneg(z3, __ S, p6, z27);                    \/\/       fneg    z3.s, p6\/m, z27.s\n+    __ sve_frintm(z19, __ D, p5, z7);                  \/\/       frintm  z19.d, p5\/m, z7.d\n+    __ sve_frintn(z21, __ S, p3, z5);                  \/\/       frintn  z21.s, p3\/m, z5.s\n+    __ sve_frintp(z25, __ D, p1, z21);                 \/\/       frintp  z25.d, p1\/m, z21.d\n+    __ sve_fsqrt(z17, __ S, p0, z3);                   \/\/       fsqrt   z17.s, p0\/m, z3.s\n+    __ sve_fsub(z19, __ S, p3, z7);                    \/\/       fsub    z19.s, p3\/m, z19.s, z7.s\n+    __ sve_fmad(z14, __ S, p4, z17, z11);              \/\/       fmad    z14.s, p4\/m, z17.s, z11.s\n+    __ sve_fmla(z24, __ S, p4, z30, z17);              \/\/       fmla    z24.s, p4\/m, z30.s, z17.s\n+    __ sve_fmls(z15, __ D, p3, z26, z22);              \/\/       fmls    z15.d, p3\/m, z26.d, z22.d\n+    __ sve_fmsb(z22, __ D, p2, z8, z5);                \/\/       fmsb    z22.d, p2\/m, z8.d, z5.d\n+    __ sve_fnmad(z27, __ D, p2, z0, z14);              \/\/       fnmad   z27.d, p2\/m, z0.d, z14.d\n+    __ sve_fnmsb(z21, __ D, p5, z0, z3);               \/\/       fnmsb   z21.d, p5\/m, z0.d, z3.d\n+    __ sve_fnmla(z25, __ D, p1, z25, z29);             \/\/       fnmla   z25.d, p1\/m, z25.d, z29.d\n+    __ sve_fnmls(z17, __ D, p0, z12, z14);             \/\/       fnmls   z17.d, p0\/m, z12.d, z14.d\n+    __ sve_mla(z13, __ D, p0, z17, z2);                \/\/       mla     z13.d, p0\/m, z17.d, z2.d\n+    __ sve_mls(z20, __ H, p5, z21, z29);               \/\/       mls     z20.h, p5\/m, z21.h, z29.h\n+    __ sve_and(z8, z2, z0);                            \/\/       and     z8.d, z2.d, z0.d\n+    __ sve_eor(z23, z22, z0);                          \/\/       eor     z23.d, z22.d, z0.d\n+    __ sve_orr(z25, z26, z23);                         \/\/       orr     z25.d, z26.d, z23.d\n+    __ sve_bic(z21, z21, z1);                          \/\/       bic     z21.d, z21.d, z1.d\n+    __ sve_uzp1(z10, __ S, z19, z11);                  \/\/       uzp1    z10.s, z19.s, z11.s\n+    __ sve_uzp2(z23, __ D, z23, z8);                   \/\/       uzp2    z23.d, z23.d, z8.d\n@@ -1183,9 +1199,9 @@\n-    __ sve_andv(v22, __ H, p5, z29);                   \/\/       andv h22, p5, z29.h\n-    __ sve_orv(v8, __ H, p0, z0);                      \/\/       orv h8, p0, z0.h\n-    __ sve_eorv(v23, __ S, p5, z0);                    \/\/       eorv s23, p5, z0.s\n-    __ sve_smaxv(v25, __ H, p6, z23);                  \/\/       smaxv h25, p6, z23.h\n-    __ sve_sminv(v21, __ B, p5, z1);                   \/\/       sminv b21, p5, z1.b\n-    __ sve_fminv(v10, __ D, p5, z11);                  \/\/       fminv d10, p5, z11.d\n-    __ sve_fmaxv(v23, __ D, p6, z8);                   \/\/       fmaxv d23, p6, z8.d\n-    __ sve_fadda(v17, __ D, p5, z19);                  \/\/       fadda d17, p5, d17, z19.d\n-    __ sve_uaddv(v4, __ D, p5, z13);                   \/\/       uaddv d4, p5, z13.d\n+    __ sve_andv(v17, __ S, p5, z19);                   \/\/       andv s17, p5, z19.s\n+    __ sve_orv(v4, __ D, p5, z13);                     \/\/       orv d4, p5, z13.d\n+    __ sve_eorv(v22, __ D, p7, z30);                   \/\/       eorv d22, p7, z30.d\n+    __ sve_smaxv(v17, __ H, p4, z14);                  \/\/       smaxv h17, p4, z14.h\n+    __ sve_sminv(v12, __ B, p7, z20);                  \/\/       sminv b12, p7, z20.b\n+    __ sve_fminv(v1, __ S, p3, z13);                   \/\/       fminv s1, p3, z13.s\n+    __ sve_fmaxv(v7, __ D, p2, z11);                   \/\/       fmaxv d7, p2, z11.d\n+    __ sve_fadda(v4, __ S, p6, z15);                   \/\/       fadda s4, p6, s4, z15.s\n+    __ sve_uaddv(v3, __ S, p7, z0);                    \/\/       uaddv d3, p7, z0.s\n@@ -1210,7 +1226,7 @@\n-    0x14000000,     0x17ffffd7,     0x140003e1,     0x94000000,\n-    0x97ffffd4,     0x940003de,     0x3400000a,     0x34fffa2a,\n-    0x34007b6a,     0x35000008,     0x35fff9c8,     0x35007b08,\n-    0xb400000b,     0xb4fff96b,     0xb4007aab,     0xb500001d,\n-    0xb5fff91d,     0xb5007a5d,     0x10000013,     0x10fff8b3,\n-    0x100079f3,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36307976,     0x3758000c,     0x375ff7cc,     0x3758790c,\n+    0x14000000,     0x17ffffd7,     0x140003ef,     0x94000000,\n+    0x97ffffd4,     0x940003ec,     0x3400000a,     0x34fffa2a,\n+    0x34007d2a,     0x35000008,     0x35fff9c8,     0x35007cc8,\n+    0xb400000b,     0xb4fff96b,     0xb4007c6b,     0xb500001d,\n+    0xb5fff91d,     0xb5007c1d,     0x10000013,     0x10fff8b3,\n+    0x10007bb3,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36307b36,     0x3758000c,     0x375ff7cc,     0x37587acc,\n@@ -1221,13 +1237,13 @@\n-    0x540076e0,     0x54000001,     0x54fff541,     0x54007681,\n-    0x54000002,     0x54fff4e2,     0x54007622,     0x54000002,\n-    0x54fff482,     0x540075c2,     0x54000003,     0x54fff423,\n-    0x54007563,     0x54000003,     0x54fff3c3,     0x54007503,\n-    0x54000004,     0x54fff364,     0x540074a4,     0x54000005,\n-    0x54fff305,     0x54007445,     0x54000006,     0x54fff2a6,\n-    0x540073e6,     0x54000007,     0x54fff247,     0x54007387,\n-    0x54000008,     0x54fff1e8,     0x54007328,     0x54000009,\n-    0x54fff189,     0x540072c9,     0x5400000a,     0x54fff12a,\n-    0x5400726a,     0x5400000b,     0x54fff0cb,     0x5400720b,\n-    0x5400000c,     0x54fff06c,     0x540071ac,     0x5400000d,\n-    0x54fff00d,     0x5400714d,     0x5400000e,     0x54ffefae,\n-    0x540070ee,     0x5400000f,     0x54ffef4f,     0x5400708f,\n+    0x540078a0,     0x54000001,     0x54fff541,     0x54007841,\n+    0x54000002,     0x54fff4e2,     0x540077e2,     0x54000002,\n+    0x54fff482,     0x54007782,     0x54000003,     0x54fff423,\n+    0x54007723,     0x54000003,     0x54fff3c3,     0x540076c3,\n+    0x54000004,     0x54fff364,     0x54007664,     0x54000005,\n+    0x54fff305,     0x54007605,     0x54000006,     0x54fff2a6,\n+    0x540075a6,     0x54000007,     0x54fff247,     0x54007547,\n+    0x54000008,     0x54fff1e8,     0x540074e8,     0x54000009,\n+    0x54fff189,     0x54007489,     0x5400000a,     0x54fff12a,\n+    0x5400742a,     0x5400000b,     0x54fff0cb,     0x540073cb,\n+    0x5400000c,     0x54fff06c,     0x5400736c,     0x5400000d,\n+    0x54fff00d,     0x5400730d,     0x5400000e,     0x54ffefae,\n+    0x540072ae,     0x5400000f,     0x54ffef4f,     0x5400724f,\n@@ -1303,156 +1319,160 @@\n-    0x1e2601c7,     0x9e660107,     0x1e270234,     0x9e6703dc,\n-    0x1e222200,     0x1e702120,     0x1e202288,     0x1e6023a8,\n-    0x29266b01,     0x29462d85,     0x69463f75,     0xa90272c5,\n-    0xa97e467b,     0x29aa1f4d,     0x29fa54cd,     0x69c27b74,\n-    0xa9b81555,     0xa9fa12ee,     0x2884321d,     0x28cc477a,\n-    0x68f451c4,     0xa8b909d0,     0xa8f060f7,     0x281069e0,\n-    0x2866191a,     0xa8392b2f,     0xa8760670,     0x0c4073db,\n-    0x4cdfa079,     0x0cca6e1e,     0x4cdf2670,     0x0d40c317,\n-    0x4ddfc948,     0x0dd7ce89,     0x4c408c62,     0x0cdf87c8,\n-    0x4d60c344,     0x0dffca23,     0x4df0cd7d,     0x4cd74801,\n-    0x0c404aa0,     0x4d40e4e5,     0x4ddfe8e1,     0x0dcfeca2,\n-    0x4cdf07bb,     0x0cc70098,     0x0d60e2ef,     0x0dffe6ae,\n-    0x0df9e934,     0x0e31bb17,     0x4e31bb7a,     0x0e71b8c5,\n-    0x4e71b8e6,     0x4eb1ba0f,     0x0e30aa0f,     0x4e30ab59,\n-    0x0e70aa30,     0x4e70ab9b,     0x4eb0ab38,     0x6e30fa0f,\n-    0x0e31ab59,     0x2e31a9ee,     0x4e31a96a,     0x6e31a9cd,\n-    0x0e71a9ee,     0x2e71aab4,     0x4e71a841,     0x6e71aaf6,\n-    0x4eb1abfe,     0x6eb1a9ee,     0x6eb0f862,     0x7e30f8e6,\n-    0x7e70f883,     0x7eb0f907,     0x7ef0fb38,     0x0e20b820,\n-    0x4e20bb9b,     0x0e60bbdd,     0x4e60b8c5,     0x0ea0b8c5,\n-    0x4ea0bbdd,     0x4ee0b98b,     0x0ea0fb59,     0x4ea0f820,\n-    0x4ee0fbfe,     0x2ea0f820,     0x6ea0fa51,     0x6ee0fbbc,\n-    0x2ea1fb59,     0x6ea1f949,     0x6ee1fb59,     0x2e2059ac,\n-    0x6e205a0f,     0x0e2d1d8b,     0x4e2c1d6a,     0x0eb31e51,\n-    0x4eba1f38,     0x2e371ed5,     0x6e391f17,     0x0e228420,\n-    0x4e328630,     0x0e6c856a,     0x4e6884e6,     0x0ebe87bc,\n-    0x4ea884e6,     0x4ee784c5,     0x0e27d4c5,     0x4e36d6b4,\n-    0x4e73d651,     0x2e31860f,     0x6e338651,     0x2e7f87dd,\n-    0x6e7c877a,     0x2ebe87bc,     0x6ea38441,     0x6efd879b,\n-    0x0ea2d420,     0x4eb6d6b4,     0x4efed7bc,     0x0e319e0f,\n-    0x4e2e9dac,     0x0e6c9d6a,     0x4e7e9fbc,     0x0ebe9fbc,\n-    0x4eb59e93,     0x2eb8d6f6,     0x6eacd56a,     0x6ee6d4a4,\n-    0x2e20dffe,     0x6e36deb4,     0x6e6add28,     0x0e6097fe,\n-    0x4e739651,     0x0eac956a,     0x4ebd979b,     0x0e24cc62,\n-    0x4e3acf38,     0x4e66cca4,     0x2e659483,     0x6e6a9528,\n-    0x2eb896f6,     0x6eb39651,     0x0eafcdcd,     0x4ea6cca4,\n-    0x4efecfbc,     0x2e39ff17,     0x6e37fed5,     0x6e7bff59,\n-    0x0e3a6738,     0x4e256483,     0x0e796717,     0x4e7c677a,\n-    0x0eb96717,     0x4eb065ee,     0x0e37a6d5,     0x4e25a483,\n-    0x0e79a717,     0x4e6aa528,     0x0ebaa738,     0x4eb5a693,\n-    0x0e31f60f,     0x4e32f630,     0x4e64f462,     0x0e236c41,\n-    0x4e226c20,     0x0e7a6f38,     0x4e666ca4,     0x0ea56c83,\n-    0x4ead6d8b,     0x0e20affe,     0x4e3daf9b,     0x0e6bad49,\n-    0x4e7baf59,     0x0ea4ac62,     0x4eaeadac,     0x0eb3f651,\n-    0x4ea0f7fe,     0x4ee3f441,     0x2e2e8dac,     0x6e3e8fbc,\n-    0x2e628c20,     0x6e738e51,     0x2eae8dac,     0x6eb38e51,\n-    0x6ef78ed5,     0x0e2ee5ac,     0x4e3de79b,     0x4e7fe7dd,\n-    0x0e2037fe,     0x4e233441,     0x0e7b3759,     0x4e7d379b,\n-    0x0ea634a4,     0x4ebf37dd,     0x4ee53483,     0x2e2834e6,\n-    0x6e3f37dd,     0x2e7b3759,     0x6e733651,     0x2eaa3528,\n-    0x6ea93507,     0x6eee35ac,     0x2e223c20,     0x6e353e93,\n-    0x2e633c41,     0x6e793f17,     0x2ea43c62,     0x6ea23c20,\n-    0x6eea3d28,     0x2eb9e717,     0x6ebbe759,     0x6ef1e60f,\n-    0x0e3f3fdd,     0x4e253c83,     0x0e6c3d6a,     0x4e783ef6,\n-    0x0eac3d6a,     0x4ea63ca4,     0x4ef33e51,     0x2e23e441,\n-    0x6e2de58b,     0x6e69e507,     0xba5fd3e3,     0x3a5f03e5,\n-    0xfa411be4,     0x7a42cbe2,     0x93df03ff,     0xc820ffff,\n-    0x8822fc7f,     0xc8247cbf,     0x88267fff,     0x4e010fe0,\n-    0x5e040420,     0x4e081fe1,     0x4e0c1fe1,     0x4e0a1fe1,\n-    0x4e071fe1,     0x4e042c20,     0x4e062c20,     0x4e052c20,\n-    0x4e083c20,     0x0e0c3c20,     0x0e0a3c20,     0x0e073c20,\n-    0x9eae0020,     0x4cc0ac3f,     0x4ea1b820,     0x05a08020,\n-    0x05104fe0,     0x05505001,     0x05906fe2,     0x05d03005,\n-    0x05101fea,     0x05901feb,     0x04b0e3e0,     0x0470e7e1,\n-    0x042f9c20,     0x043f9c35,     0x047f9c20,     0x04ff9c20,\n-    0x04299420,     0x04319160,     0x0461943e,     0x04a19020,\n-    0x04038100,     0x040381a0,     0x040387e1,     0x04438be2,\n-    0x04c38fe3,     0x040181e0,     0x04018100,     0x04018621,\n-    0x04418b22,     0x04418822,     0x04818c23,     0x040081e0,\n-    0x04008120,     0x04008761,     0x04008621,     0x04408822,\n-    0x04808c23,     0x042053ff,     0x047f5401,     0x25208028,\n-    0x2538cfe0,     0x2578d001,     0x25b8efe2,     0x25f8f007,\n-    0x2538dfea,     0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,\n-    0xa4484be0,     0xa467afe0,     0xa4a8a7ea,     0xa547a814,\n-    0xa4084ffe,     0xa55c53e0,     0xa5e1540b,     0xe400fbf6,\n-    0xe408ffff,     0xe420e7e0,     0xe4484be0,     0xe460efe0,\n-    0xe547e400,     0xe4014be0,     0xe4a84fe0,     0xe5f15000,\n-    0x858043e0,     0x85a043ff,     0xe59f5d08,     0x0420e3e9,\n-    0x0460e3ea,     0x04a0e3eb,     0x04e0e3ec,     0x25104042,\n-    0x25104871,     0x25904861,     0x25904c92,     0x05344020,\n-    0x05744041,     0x05b44062,     0x05f44083,     0x252c8840,\n-    0x253c1420,     0x25681572,     0x25a21ce3,     0x25ea1e34,\n-    0x0522c020,     0x05e6c0a4,     0x2401a001,     0x2443a051,\n-    0x24858881,     0x24c78cd1,     0x24850891,     0x24c70cc1,\n-    0x250f9001,     0x25508051,     0x25802491,     0x25df28c1,\n-    0x25850c81,     0x251e10d1,     0x65816001,     0x65c36051,\n-    0x65854891,     0x65c74cc1,     0x05733820,     0x05b238a4,\n-    0x05f138e6,     0x0570396a,     0x65d0a001,     0x65d6a443,\n-    0x65d4a826,     0x6594ac26,     0x6554ac26,     0x6556ac26,\n-    0x6552ac26,     0x65cbac85,     0x65caac01,     0x65dea833,\n-    0x659ca509,     0x65d8a801,     0x65dcac01,     0x655cb241,\n-    0x0520a1e0,     0x0521a601,     0x052281e0,     0x05238601,\n-    0x04a14026,     0x042244a6,     0x046344a6,     0x04a444a6,\n-    0x04e544a7,     0x0568aca7,     0x05b23230,     0x853040af,\n-    0xc5b040af,     0xe57080af,     0xe5b080af,     0x25034440,\n-    0x254054c4,     0x25034640,     0x25415a05,     0x25834440,\n-    0x25c54489,     0x250b5d3a,     0x2550dc20,     0x2518e3e1,\n-    0x2518e021,     0x2518e0a1,     0x2518e121,     0x2518e1a1,\n-    0x2558e3e2,     0x2558e042,     0x2558e0c2,     0x2558e142,\n-    0x2598e3e3,     0x2598e063,     0x2598e0e3,     0x2598e163,\n-    0x25d8e3e4,     0x25d8e084,     0x25d8e104,     0x25d8e184,\n-    0x2518e407,     0x05214800,     0x05614800,     0x05a14800,\n-    0x05e14800,     0x05214c00,     0x05614c00,     0x05a14c00,\n-    0x05e14c00,     0x05304001,     0x05314001,     0x05a18610,\n-    0x05e18610,     0x45b0c210,     0x45f1c231,     0x1e601000,\n-    0x1e603000,     0x1e621000,     0x1e623000,     0x1e641000,\n-    0x1e643000,     0x1e661000,     0x1e663000,     0x1e681000,\n-    0x1e683000,     0x1e6a1000,     0x1e6a3000,     0x1e6c1000,\n-    0x1e6c3000,     0x1e6e1000,     0x1e6e3000,     0x1e701000,\n-    0x1e703000,     0x1e721000,     0x1e723000,     0x1e741000,\n-    0x1e743000,     0x1e761000,     0x1e763000,     0x1e781000,\n-    0x1e783000,     0x1e7a1000,     0x1e7a3000,     0x1e7c1000,\n-    0x1e7c3000,     0x1e7e1000,     0x1e7e3000,     0xf82a822f,\n-    0xf822018a,     0xf82c11af,     0xf8222287,     0xf83a3090,\n-    0xf8225184,     0xf8304215,     0xf83072ab,     0xf837634c,\n-    0xf8b781dc,     0xf8ab0038,     0xf8ac115f,     0xf8b02047,\n-    0xf8a3326d,     0xf8b15070,     0xf8a143cb,     0xf8a571e8,\n-    0xf8bd601e,     0xf8f48287,     0xf8f702bc,     0xf8fb10b9,\n-    0xf8e12217,     0xf8ff3185,     0xf8e951fc,     0xf8fd43f6,\n-    0xf8f370bf,     0xf8ee63f0,     0xf870829b,     0xf870016c,\n-    0xf86913c6,     0xf871239b,     0xf87e3147,     0xf874508a,\n-    0xf8784231,     0xf87673a3,     0xf86f6276,     0xb8338056,\n-    0xb82f0186,     0xb83011ab,     0xb83723c1,     0xb8333225,\n-    0xb82252d0,     0xb82d42aa,     0xb83d719b,     0xb83b6023,\n-    0xb8bf8278,     0xb8b10389,     0xb8bb10ef,     0xb8b523f7,\n-    0xb8b933e2,     0xb8bb5150,     0xb8b74073,     0xb8b07320,\n-    0xb8ba6057,     0xb8f0808c,     0xb8fc03be,     0xb8f010db,\n-    0xb8e921fd,     0xb8e730e4,     0xb8ef52e9,     0xb8e84382,\n-    0xb8f570bf,     0xb8fb6220,     0xb86f8344,     0xb86802dc,\n-    0xb87b133b,     0xb8772080,     0xb8663010,     0xb864502f,\n-    0xb86a40a7,     0xb86a70fc,     0xb87462b7,     0xce284145,\n-    0xce1108de,     0xce7c8fab,     0xce96eb42,     0xce7b81ae,\n-    0xce6586f0,     0xcec081a2,     0xce6a89ea,     0x25a0cc5a,\n-    0x25a1d143,     0x05800e44,     0x05406531,     0x05002d42,\n-    0x2520c677,     0x25a1cd07,     0x0580687b,     0x0543bb42,\n-    0x050044a6,     0x25a0c86c,     0x25a1d358,     0x05800500,\n-    0x05400ad3,     0x05000e06,     0x25e0c951,     0x25a1d54a,\n-    0x05839276,     0x0540ea6f,     0x0503c8a4,     0x25a0d448,\n-    0x2521d056,     0x058059c9,     0x05406d05,     0x05003cb6,\n-    0x25a0d0c8,     0x2561c4f9,     0x05809904,     0x05400e5d,\n-    0x0500cadd,     0x043c0162,     0x04ba0427,     0x65c801d1,\n-    0x65c50b15,     0x65d60635,     0x0416b67d,     0x040012e4,\n-    0x04da06f3,     0x04508113,     0x04db1a2e,     0x0419a7d5,\n-    0x041ab58a,     0x04990709,     0x045398c4,     0x049199bb,\n-    0x049016de,     0x0457bd3e,     0x04dea693,     0x045809a9,\n-    0x05678313,     0x05a48e33,     0x04080410,     0x044a09eb,\n-    0x04c105ef,     0x04dca145,     0x6580801a,     0x65cd9d53,\n-    0x65c694e3,     0x65878ebc,     0x65c28e3a,     0x04ddac51,\n-    0x6582b690,     0x65c0a033,     0x65c1aa11,     0x658da095,\n-    0x65818cd7,     0x65bd8e14,     0x65a902c3,     0x65f33c78,\n-    0x65adbaa7,     0x65f5d8a7,     0x65e9e071,     0x65ee496b,\n-    0x65f869b1,     0x044f511e,     0x0456777a,     0x043b30a8,\n-    0x04ae300a,     0x04603295,     0x04e53336,     0x05316a3d,\n-    0x057d6dcc,     0x45bed040,     0x045a37b6,     0x04582008,\n-    0x04993417,     0x04483af9,     0x040a3435,     0x65c7356a,\n-    0x65c63917,     0x65d83671,     0x04c135a4,\n+    0x1e2401c7,     0x9e640107,     0x1e300234,     0x9e7003dc,\n+    0x1e260050,     0x9e660209,     0x1e2703b4,     0x9e670024,\n+    0x1e382340,     0x1e6e22e0,     0x1e2022a8,     0x1e602188,\n+    0x2928630c,     0x29501616,     0x694e4db4,     0xa90619b1,\n+    0xa9760625,     0x29a652cd,     0x29ca6d5e,     0x69c2534d,\n+    0xa9bb5fa4,     0xa9f900d6,     0x288a6cb1,     0x28e02e0e,\n+    0x68e25d2c,     0xa8821c17,     0xa8c52351,     0x282a3d4b,\n+    0x28484093,     0xa831393e,     0xa8425e9d,     0x0c407365,\n+    0x4cdfa32a,     0x0cd36fcf,     0x4cdf2611,     0x0d40c2fe,\n+    0x4ddfc911,     0x0dc3cd2c,     0x4c408c53,     0x0cdf8515,\n+    0x4d60c08d,     0x0dffc87c,     0x4de0cfbd,     0x4cd54827,\n+    0x0c404811,     0x4d40e4ba,     0x4ddfe839,     0x0dddec56,\n+    0x4cdf076d,     0x0cd7031d,     0x0d60e1ed,     0x0dffe5cf,\n+    0x0df7ea9b,     0x0e31bb38,     0x4e31ba0f,     0x0e71bb59,\n+    0x4e71b9ee,     0x4eb1b96a,     0x0e30a9cd,     0x4e30a9ee,\n+    0x0e70aab4,     0x4e70a841,     0x4eb0aaf6,     0x6e30fbfe,\n+    0x0e31a9ee,     0x2e31a862,     0x4e31a8e6,     0x6e31a883,\n+    0x0e71a907,     0x2e71ab38,     0x4e71a820,     0x6e71ab9b,\n+    0x4eb1abdd,     0x6eb1a8c5,     0x6eb0f8c5,     0x7e30fbdd,\n+    0x7e70f98b,     0x7eb0fb59,     0x7ef0f820,     0x0e20bbfe,\n+    0x4e20b820,     0x0e60ba51,     0x4e60bbbc,     0x0ea0bb59,\n+    0x4ea0b949,     0x4ee0bb59,     0x0ea0f9ac,     0x4ea0fa0f,\n+    0x4ee0f98b,     0x2ea0f96a,     0x6ea0fa51,     0x6ee0fb38,\n+    0x2ea1fad5,     0x6ea1fb17,     0x6ee1f820,     0x2e205a30,\n+    0x6e20596a,     0x0e281ce6,     0x4e3e1fbc,     0x0ea81ce6,\n+    0x4ea71cc5,     0x2e271cc5,     0x6e361eb4,     0x0e338651,\n+    0x4e31860f,     0x0e738651,     0x4e7f87dd,     0x0ebc877a,\n+    0x4ebe87bc,     0x4ee38441,     0x0e3dd79b,     0x4e22d420,\n+    0x4e76d6b4,     0x2e3e87bc,     0x6e31860f,     0x2e6e85ac,\n+    0x6e6c856a,     0x2ebe87bc,     0x6ebe87bc,     0x6ef58693,\n+    0x0eb8d6f6,     0x4eacd56a,     0x4ee6d4a4,     0x0e209ffe,\n+    0x4e369eb4,     0x0e6a9d28,     0x4e609ffe,     0x0eb39e51,\n+    0x4eac9d6a,     0x2ebdd79b,     0x6ea4d462,     0x6efad738,\n+    0x2e26dca4,     0x6e25dc83,     0x6e6add28,     0x0e7896f6,\n+    0x4e739651,     0x0eaf95cd,     0x4ea694a4,     0x0e3ecfbc,\n+    0x4e39cf17,     0x4e77ced5,     0x2e7b9759,     0x6e7a9738,\n+    0x2ea59483,     0x6eb99717,     0x0ebccf7a,     0x4eb9cf17,\n+    0x4ef0cdee,     0x2e37fed5,     0x6e25fc83,     0x6e79ff17,\n+    0x0e2a6528,     0x4e3a6738,     0x0e756693,     0x4e71660f,\n+    0x0eb26630,     0x4ea46462,     0x0e23a441,     0x4e22a420,\n+    0x0e7aa738,     0x4e66a4a4,     0x0ea5a483,     0x4eada58b,\n+    0x0e20f7fe,     0x4e3df79b,     0x4e6bf549,     0x0e3b6f59,\n+    0x4e246c62,     0x0e6e6dac,     0x4e736e51,     0x0ea06ffe,\n+    0x4ea36c41,     0x0e2eadac,     0x4e3eafbc,     0x0e62ac20,\n+    0x4e73ae51,     0x0eaeadac,     0x4eb3ae51,     0x0eb7f6d5,\n+    0x4eaef5ac,     0x4efdf79b,     0x2e3f8fdd,     0x6e208ffe,\n+    0x2e638c41,     0x6e7b8f59,     0x2ebd8f9b,     0x6ea68ca4,\n+    0x6eff8fdd,     0x0e25e483,     0x4e28e4e6,     0x4e7fe7dd,\n+    0x0e3b3759,     0x4e333651,     0x0e6a3528,     0x4e693507,\n+    0x0eae35ac,     0x4ea23420,     0x4ef53693,     0x2e233441,\n+    0x6e393717,     0x2e643462,     0x6e623420,     0x2eaa3528,\n+    0x6eb93717,     0x6efb3759,     0x2e313e0f,     0x6e3f3fdd,\n+    0x2e653c83,     0x6e6c3d6a,     0x2eb83ef6,     0x6eac3d6a,\n+    0x6ee63ca4,     0x2eb3e651,     0x6ea3e441,     0x6eede58b,\n+    0x0e293d07,     0x4e2c3d6a,     0x0e713e0f,     0x4e723e30,\n+    0x0ea43c62,     0x4eab3d49,     0x4eed3d8b,     0x2e2ee5ac,\n+    0x6e30e5ee,     0x6e6fe5cd,     0x65d238c1,     0x65902498,\n+    0x65d03226,     0x65d13549,     0x65d13f36,     0x65d32147,\n+    0xba5fd3e3,     0x3a5f03e5,     0xfa411be4,     0x7a42cbe2,\n+    0x93df03ff,     0xc820ffff,     0x8822fc7f,     0xc8247cbf,\n+    0x88267fff,     0x4e010fe0,     0x5e040420,     0x4e081fe1,\n+    0x4e0c1fe1,     0x4e0a1fe1,     0x4e071fe1,     0x4e042c20,\n+    0x4e062c20,     0x4e052c20,     0x4e083c20,     0x0e0c3c20,\n+    0x0e0a3c20,     0x0e073c20,     0x9eae0020,     0x0f03f409,\n+    0x6f03f40e,     0x4cc0ac3f,     0x0ea1b820,     0x4e21c862,\n+    0x4e61b8a4,     0x05a08020,     0x05104fe0,     0x05505001,\n+    0x05906fe2,     0x05d03005,     0x05101fea,     0x05901feb,\n+    0x04b0e3e0,     0x0470e7e1,     0x042f9c20,     0x043f9c35,\n+    0x047f9c20,     0x04ff9c20,     0x04299420,     0x04319160,\n+    0x0461943e,     0x04a19020,     0x04038100,     0x040381a0,\n+    0x040387e1,     0x04438be2,     0x04c38fe3,     0x040181e0,\n+    0x04018100,     0x04018621,     0x04418b22,     0x04418822,\n+    0x04818c23,     0x040081e0,     0x04008120,     0x04008761,\n+    0x04008621,     0x04408822,     0x04808c23,     0x042053ff,\n+    0x047f5401,     0x25208028,     0x2538cfe0,     0x2578d001,\n+    0x25b8efe2,     0x25f8f007,     0x2538dfea,     0x25b8dfeb,\n+    0xa400a3e0,     0xa420a7e0,     0xa4484be0,     0xa467afe0,\n+    0xa4a8a7ea,     0xa547a814,     0xa4084ffe,     0xa55c53e0,\n+    0xa5e1540b,     0xe400fbf6,     0xe408ffff,     0xe420e7e0,\n+    0xe4484be0,     0xe460efe0,     0xe547e400,     0xe4014be0,\n+    0xe4a84fe0,     0xe5f15000,     0x858043e0,     0x85a043ff,\n+    0xe59f5d08,     0x0420e3e9,     0x0460e3ea,     0x04a0e3eb,\n+    0x04e0e3ec,     0x25104042,     0x25104871,     0x25904861,\n+    0x25904c92,     0x05344020,     0x05744041,     0x05b44062,\n+    0x05f44083,     0x252c8840,     0x253c1420,     0x25681572,\n+    0x25a21ce3,     0x25ea1e34,     0x0522c020,     0x05e6c0a4,\n+    0x2401a001,     0x2443a051,     0x24858881,     0x24c78cd1,\n+    0x24850891,     0x24c70cc1,     0x250f9001,     0x25508051,\n+    0x25802491,     0x25df28c1,     0x25850c81,     0x251e10d1,\n+    0x65816001,     0x65c36051,     0x65854891,     0x65c74cc1,\n+    0x05733820,     0x05b238a4,     0x05f138e6,     0x0570396a,\n+    0x65d0a001,     0x65d6a443,     0x65d4a826,     0x6594ac26,\n+    0x6554ac26,     0x6556ac26,     0x6552ac26,     0x65cbac85,\n+    0x65caac01,     0x65dea833,     0x659ca509,     0x65d8a801,\n+    0x65dcac01,     0x655cb241,     0x0520a1e0,     0x0521a601,\n+    0x052281e0,     0x05238601,     0x04a14026,     0x042244a6,\n+    0x046344a6,     0x04a444a6,     0x04e544a7,     0x0568aca7,\n+    0x05b23230,     0x853040af,     0xc5b040af,     0xe57080af,\n+    0xe5b080af,     0x25034440,     0x254054c4,     0x25034640,\n+    0x25415a05,     0x25834440,     0x25c54489,     0x250b5d3a,\n+    0x2550dc20,     0x2518e3e1,     0x2518e021,     0x2518e0a1,\n+    0x2518e121,     0x2518e1a1,     0x2558e3e2,     0x2558e042,\n+    0x2558e0c2,     0x2558e142,     0x2598e3e3,     0x2598e063,\n+    0x2598e0e3,     0x2598e163,     0x25d8e3e4,     0x25d8e084,\n+    0x25d8e104,     0x25d8e184,     0x2518e407,     0x05214800,\n+    0x05614800,     0x05a14800,     0x05e14800,     0x05214c00,\n+    0x05614c00,     0x05a14c00,     0x05e14c00,     0x05304001,\n+    0x05314001,     0x05a18610,     0x05e18610,     0x05271e11,\n+    0x45b0c210,     0x45f1c231,     0x1e601000,     0x1e603000,\n+    0x1e621000,     0x1e623000,     0x1e641000,     0x1e643000,\n+    0x1e661000,     0x1e663000,     0x1e681000,     0x1e683000,\n+    0x1e6a1000,     0x1e6a3000,     0x1e6c1000,     0x1e6c3000,\n+    0x1e6e1000,     0x1e6e3000,     0x1e701000,     0x1e703000,\n+    0x1e721000,     0x1e723000,     0x1e741000,     0x1e743000,\n+    0x1e761000,     0x1e763000,     0x1e781000,     0x1e783000,\n+    0x1e7a1000,     0x1e7a3000,     0x1e7c1000,     0x1e7c3000,\n+    0x1e7e1000,     0x1e7e3000,     0xf82c815f,     0xf8300047,\n+    0xf823126d,     0xf8312070,     0xf82133cb,     0xf82551e8,\n+    0xf83d401e,     0xf8347287,     0xf83762bc,     0xf8bb80b9,\n+    0xf8a10217,     0xf8bf1185,     0xf8a921fc,     0xf8bd33f6,\n+    0xf8b350bf,     0xf8ae43f0,     0xf8b0729b,     0xf8b0616c,\n+    0xf8e983c6,     0xf8f1039b,     0xf8fe1147,     0xf8f4208a,\n+    0xf8f83231,     0xf8f653a3,     0xf8ef4276,     0xf8f37056,\n+    0xf8ef6186,     0xf87081ab,     0xf87703c1,     0xf8731225,\n+    0xf86222d0,     0xf86d32aa,     0xf87d519b,     0xf87b4023,\n+    0xf87f7278,     0xf8716389,     0xb83b80ef,     0xb83503f7,\n+    0xb83913e2,     0xb83b2150,     0xb8373073,     0xb8305320,\n+    0xb83a4057,     0xb830708c,     0xb83c63be,     0xb8b080db,\n+    0xb8a901fd,     0xb8a710e4,     0xb8af22e9,     0xb8a83382,\n+    0xb8b550bf,     0xb8bb4220,     0xb8af7344,     0xb8a862dc,\n+    0xb8fb833b,     0xb8f70080,     0xb8e61010,     0xb8e4202f,\n+    0xb8ea30a7,     0xb8ea50fc,     0xb8f442b7,     0xb8e6710b,\n+    0xb8f160df,     0xb8718182,     0xb87e007d,     0xb87b13b6,\n+    0xb86e238d,     0xb87130b8,     0xb862514e,     0xb870436b,\n+    0xb877708c,     0xb8766091,     0xce304661,     0xce0c09cc,\n+    0xce748c70,     0xce863cb7,     0xce7b8191,     0xce668610,\n+    0xcec08382,     0xce668883,     0x25a0cdd1,     0x25a1c86c,\n+    0x058000b8,     0x054242ca,     0x0500051e,     0x2520cf00,\n+    0x25e1c951,     0x058039ea,     0x05400e1b,     0x05009891,\n+    0x2520c09c,     0x25a1d448,     0x05801e36,     0x05400516,\n+    0x050039fe,     0x2520ce0b,     0x25a1d0c8,     0x058074d9,\n+    0x05404531,     0x05031e84,     0x2560cf1a,     0x2561dda2,\n+    0x058026a3,     0x05404c35,     0x05007851,     0x25a0d293,\n+    0x25a1de96,     0x05808874,     0x05423bb1,     0x050030e4,\n+    0x04680102,     0x04be0638,     0x658103c4,     0x65800993,\n+    0x65910707,     0x04d6a53b,     0x04c00e17,     0x04da1696,\n+    0x049089bc,     0x045b1787,     0x0499ad6b,     0x049ab901,\n+    0x0499122d,     0x04538064,     0x04918dc7,     0x04100fa4,\n+    0x04d7aaa0,     0x049ea123,     0x04180b1c,     0x05e786f3,\n+    0x05e4954d,     0x048813cc,     0x048a03ae,     0x048114f5,\n+    0x04dca342,     0x65c09229,     0x65cd8440,     0x65c6856e,\n+    0x658793ae,     0x658282c3,     0x049dbb63,     0x65c2b4f3,\n+    0x6580acb5,     0x65c1a6b9,     0x658da071,     0x65818cf3,\n+    0x65ab922e,     0x65b113d8,     0x65f62f4f,     0x65e5a916,\n+    0x65eec81b,     0x65e3f415,     0x65fd4739,     0x65ee6191,\n+    0x04c2422d,     0x045d76b4,     0x04203048,     0x04a032d7,\n+    0x04773359,     0x04e132b5,     0x05ab6a6a,     0x05e86ef7,\n+    0x049a3671,     0x04d835a4,     0x04d93fd6,     0x044831d1,\n+    0x040a3e8c,     0x65872da1,     0x65c62967,     0x659839e4,\n+    0x04813c03,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":585,"deletions":565,"binary":false,"changes":1150,"status":"modified"}]}