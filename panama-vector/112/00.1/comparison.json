{"files":[{"patch":"@@ -2062,1 +2062,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -2077,1 +2077,1 @@\n-  if (bottom_type()->isa_vect() != NULL) {\n+  if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2183,0 +2183,3 @@\n+      } else if (dst_lo_rc == rc_predicate) {\n+        __ unspill_sve_predicate(as_PRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo),\n+                                 Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n@@ -2185,2 +2188,18 @@\n-        __ unspill(rscratch1, is64, src_offset);\n-        __ spill(rscratch1, is64, dst_offset);\n+        if (ideal_reg() == Op_RegVectMask) {\n+          __ spill_copy_sve_predicate_stack_to_stack(src_offset, dst_offset,\n+                                                     Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+        } else {\n+          __ unspill(rscratch1, is64, src_offset);\n+          __ spill(rscratch1, is64, dst_offset);\n+        }\n+      }\n+      break;\n+    case rc_predicate:\n+      if (dst_lo_rc == rc_predicate) {\n+        __ sve_mov(as_PRegister(Matcher::_regEncode[dst_lo]), as_PRegister(Matcher::_regEncode[src_lo]));\n+      } else if (dst_lo_rc == rc_stack) {\n+        __ spill_sve_predicate(as_PRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo),\n+                               Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+      } else {\n+        assert(false, \"bad src and dst rc_class combination.\");\n+        ShouldNotReachHere();\n@@ -2207,1 +2226,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2224,0 +2243,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# predicate spill size = %d\", vsize);\n@@ -2383,0 +2406,14 @@\n+    case Op_LoadVectorMask:\n+    case Op_StoreVectorMask:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+    case Op_MaskAll:\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (UseSVE == 0) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -2451,0 +2488,9 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ Only SVE supports masked operations.\n+  if (UseSVE == 0) {\n+    return false;\n+  }\n+  return match_rule_supported(opcode) &&\n+         masked_op_sve_supported(opcode, vlen, bt);\n+}\n+\n@@ -8867,0 +8913,11 @@\n+instruct castVVMask(pRegGov dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":62,"deletions":5,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -161,0 +162,6 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    \/\/ Currently we support all masked vector opcodes.\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -304,1 +311,1 @@\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+            \"sve_ldr $dst, $pTmp, $mem\\t# load vector partial\" %}\n@@ -324,1 +331,1 @@\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+            \"sve_str $src, $pTmp, $mem\\t# store vector partial\" %}\n@@ -337,0 +344,204 @@\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt), vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt), vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ maskAll\n+\n+instruct vmaskAll_immI(pRegGov dst, immI src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(pRegGov dst, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and\/or\/xor\n+\n+instruct vmask_and(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_and(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (OrVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (XorVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and_not\n+\n+instruct vmask_and_notI(pRegGov pd, pRegGov pn, pRegGov pm, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_and_notL(pRegGov pd, pRegGov pn, pRegGov pm, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -373,0 +584,34 @@\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmpeq(as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -377,1 +622,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -390,1 +635,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -403,1 +648,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -416,1 +661,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -429,1 +674,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -442,1 +687,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -453,0 +698,80 @@\n+\/\/ vector abs - predicated\n+\n+instruct vabsB_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVB dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsS_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVS dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVI dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVL dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -533,0 +858,80 @@\n+\/\/ vector add - predicated\n+\n+instruct vaddB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -578,0 +983,51 @@\n+\/\/ vector and - predicated\n+\n+instruct vand_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AndV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_and(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or - predicated\n+\n+instruct vor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (OrV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_orr(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor - predicated\n+\n+instruct vxor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (XorV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_eor(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -606,1 +1062,0 @@\n-\n@@ -637,1 +1092,0 @@\n-\n@@ -664,1 +1118,1 @@\n-\/\/ vector min\/max\n+\/\/ vector float div - predicated\n@@ -666,1 +1120,1 @@\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+instruct vfdivF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -668,1 +1122,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) pg));\n@@ -670,1 +1124,29 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vfdivD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min\/max\n+\n+instruct vmin(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MinV dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -678,1 +1160,1 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n+      assert(is_integral_type(bt), \"unsupported type\");\n@@ -698,1 +1180,1 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n+      assert(is_integral_type(bt), \"unsupported type\");\n@@ -706,0 +1188,42 @@\n+\/\/ vector min\/max - predicated\n+\n+instruct vmin_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MinV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmax_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MaxV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -734,0 +1258,28 @@\n+\/\/ vector fmla - predicated\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -942,1 +1494,0 @@\n-\n@@ -1019,0 +1570,80 @@\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1022,1 +1653,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -1034,1 +1666,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -1045,0 +1678,28 @@\n+\/\/ vector fneg - predicated\n+\n+instruct vnegF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1059,1 +1720,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -1062,4 +1723,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -1067,0 +1727,4 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+=======\n@@ -1069,0 +1733,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1070,2 +1735,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -1076,3 +1739,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -1080,5 +1741,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -1086,0 +1746,5 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+=======\n@@ -1092,0 +1757,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1096,1 +1762,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -1098,2 +1764,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -1101,5 +1766,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -1107,0 +1770,6 @@\n+<<<<<<< HEAD\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1113,0 +1782,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1119,1 +1789,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -1123,0 +1793,1 @@\n+  effect(KILL cr);\n@@ -1124,1 +1795,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -1126,1 +1797,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg), 0);\n@@ -1131,1 +1802,1 @@\n-instruct vloadmaskS(vReg dst, vReg src) %{\n+instruct vloadmaskS(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -1135,0 +1806,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1136,2 +1808,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskS $dst, $src\\t# vector load mask (sve) (B to H)\" %}\n@@ -1139,2 +1810,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ H, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1145,1 +1816,1 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n+instruct vloadmaskI(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -1150,0 +1821,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1151,3 +1823,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n+  format %{ \"vloadmaskI $dst, $src\\t# vector load mask (sve) (B to S)\" %}\n@@ -1155,3 +1825,3 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ S, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1162,1 +1832,1 @@\n-instruct vloadmaskL(vReg dst, vReg src) %{\n+instruct vloadmaskL(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -1167,0 +1837,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1168,4 +1839,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmaskL $dst, $src\\t# vector load mask (sve) (B to D)\" %}\n@@ -1173,4 +1841,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ D, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ D, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1183,1 +1851,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -1187,1 +1855,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremaskB $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -1189,2 +1857,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -1195,1 +1862,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremaskS(vReg dst, pRegGov src, vReg tmp, immI_2 size) %{\n@@ -1200,3 +1867,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+  format %{ \"vstoremaskS $dst, $src\\t# vector store mask (sve) (H to B)\" %}\n@@ -1204,0 +1869,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ H, as_PRegister($src$$reg), 1, false);\n@@ -1206,4 +1872,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -1214,1 +1877,1 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+instruct vstoremaskI(vReg dst, pRegGov src, vReg tmp, immI_4 size) %{\n@@ -1219,4 +1882,1 @@\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+  format %{ \"vstoremaskI $dst, $src\\t# vector store mask (sve) (S to B)\" %}\n@@ -1224,0 +1884,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($src$$reg), 1, false);\n@@ -1226,1 +1887,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -1229,2 +1890,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1235,1 +1894,1 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+instruct vstoremaskL(vReg dst, pRegGov src, vReg tmp, immI_8 size) %{\n@@ -1240,5 +1899,1 @@\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+  format %{ \"vstoremaskL $dst, $src\\t# vector store mask (sve) (D to B)\" %}\n@@ -1246,0 +1901,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($src$$reg), 1, false);\n@@ -1248,1 +1904,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -1253,2 +1909,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1261,2 +1915,3 @@\n-instruct vloadmask_loadV_byte(vReg dst, vmemA mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+instruct loadVMask_byte(pRegGov dst, vmemA mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1264,1 +1919,2 @@\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP tmp, KILL cr);\n@@ -1266,3 +1922,10 @@\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n-  ins_encode %{\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (B)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+=======\n@@ -1274,0 +1937,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1275,1 +1939,2 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+                 ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1280,2 +1945,3 @@\n-instruct vloadmask_loadV_non_byte(vReg dst, indirect mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+instruct loadVMask_non_byte(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1283,1 +1949,2 @@\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP tmp, KILL cr);\n@@ -1285,3 +1952,57 @@\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n-  ins_encode %{\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+                 ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadVMask_byte_partial(pRegGov dst, vmemA mem, vReg vtmp,\n+                              pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) == 1);\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"loadVMask $dst, $mem\\t# load vector mask partial (sve) (B)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadVMask_non_byte_partial(pRegGov dst, indirect mem, vReg vtmp,\n+                              pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"loadVMask $dst, $mem\\t# load vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+=======\n@@ -1293,0 +2014,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1294,1 +2016,1 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n@@ -1299,4 +2021,5 @@\n-instruct storeV_vstoremask_byte(vmemA mem, vReg src, vReg tmp, immI_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+instruct storeVMask_byte(vmemA mem, pRegGov src, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) == 1);\n+  match(Set mem (StoreVectorMask mem src));\n@@ -1304,0 +2027,2 @@\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (B)\" %}\n@@ -1305,2 +2030,7 @@\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+<<<<<<< HEAD\n+    \/\/ Convert the src predicate to vector. And store the vector elements\n+    \/\/ as boolean values.\n+    BasicType from_vect_bt = vector_element_basic_type(this, $src);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), __ elemType_to_regVariant(from_vect_bt),\n+               as_PRegister($src$$reg), 1, false);\n+=======\n@@ -1312,0 +2042,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1320,4 +2051,5 @@\n-instruct storeV_vstoremask_non_byte(indirect mem, vReg src, vReg tmp, immI_gt_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+instruct storeVMask_non_byte(indirect mem, pRegGov src, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1);\n+  match(Set mem (StoreVectorMask mem src));\n@@ -1325,0 +2057,2 @@\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (H\/S\/D)\" %}\n@@ -1326,2 +2060,7 @@\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+<<<<<<< HEAD\n+    \/\/ Convert the src predicate to vector. And store the vector elements\n+    \/\/ as boolean values.\n+    BasicType from_vect_bt = vector_element_basic_type(this, $src);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), __ elemType_to_regVariant(from_vect_bt),\n+               as_PRegister($src$$reg), 1, false);\n+=======\n@@ -1333,0 +2072,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1341,8 +2081,9 @@\n-\/\/ vector add reduction\n-\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+instruct storeVMask_byte_partial(vmemA mem, pRegGov src, vReg vtmp,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) == 1);\n+  match(Set mem (StoreVectorMask mem src));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeVMask $src, $mem\\t# store vector mask partial (sve) (B)\" %}\n+  ins_cost(6 * SVE_COST);\n@@ -1350,0 +2091,11 @@\n+<<<<<<< HEAD\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+=======\n@@ -1362,0 +2114,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1366,3 +2119,28 @@\n-instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+instruct storeVMask_non_byte_partial(indirect mem, pRegGov src, vReg vtmp,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1);\n+  match(Set mem (StoreVectorMask mem src));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeVMask $src, $mem\\t# store vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector add reduction\n+\n+instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n@@ -1370,1 +2148,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1372,1 +2150,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction (sve) (may extend)\" %}\n@@ -1374,0 +2152,6 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+=======\n@@ -1389,0 +2173,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1393,2 +2178,3 @@\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n@@ -1396,1 +2182,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1400,21 +2186,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1425,2 +2193,2 @@\n-\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n@@ -1438,16 +2206,2 @@\n-instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n-  ins_cost(SVE_COST);\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n@@ -1465,11 +2219,656 @@\n-instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n-  ins_cost(SVE_COST);\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVI src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVL src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n+  ins_cost(SVE_COST);\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n+  ins_cost(SVE_COST);\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+=======\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector add reduction - predicated\n+\n+instruct reduce_addI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked(vRegF src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_masked(vRegD src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked_partial(vRegF src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+=======\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_masked_partial(vRegD src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reduction\n+\n+instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reduction - predicated\n+\n+instruct reduce_andI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reduction\n+\n+instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reduction - predicated\n+\n+instruct reduce_orI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reduction\n+\n+instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reduction - predicated\n+\n+instruct reduce_eorI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1480,1 +2879,1 @@\n-\/\/ vector and reduction\n+\/\/ vector max reduction\n@@ -1482,5 +2881,24 @@\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1488,1 +2906,63 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction  partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF(vRegF dst, vRegF src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction (sve)\" %}\n@@ -1490,0 +2970,4 @@\n+<<<<<<< HEAD\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1502,0 +2986,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1506,1 +2991,1 @@\n-instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+instruct reduce_maxF_partial(vRegF dst, vRegF src1, vReg src2,\n@@ -1508,1 +2993,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1510,4 +2996,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction partial (sve)\" %}\n@@ -1515,0 +3001,5 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src2));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1530,0 +3021,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1534,2 +3026,3 @@\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+instruct reduce_maxD(vRegD dst, vRegD src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1537,4 +3030,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction (sve)\" %}\n@@ -1542,3 +3035,2 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1549,1 +3041,1 @@\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+instruct reduce_maxD_partial(vRegD dst, vRegD src1, vReg src2,\n@@ -1551,1 +3043,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1553,4 +3046,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction partial (sve)\" %}\n@@ -1558,0 +3051,5 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1564,0 +3062,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1568,1 +3067,1 @@\n-\/\/ vector or reduction\n+\/\/ vector max reduction - predicated\n@@ -1570,8 +3069,17 @@\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n-  ins_encode %{\n+instruct reduce_maxI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+=======\n@@ -1590,0 +3098,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1594,9 +3103,15 @@\n-instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n-  ins_encode %{\n+instruct reduce_maxL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+=======\n@@ -1618,0 +3133,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1622,7 +3138,10 @@\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+instruct reduce_maxI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated partial (sve)\" %}\n@@ -1630,3 +3149,8 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1637,5 +3161,6 @@\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n+instruct reduce_maxL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n@@ -1643,3 +3168,11 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n-  ins_encode %{\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1652,0 +3185,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1656,7 +3190,5 @@\n-\/\/ vector xor reduction\n-\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_maxF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n@@ -1664,1 +3196,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated (sve)\" %}\n@@ -1666,0 +3198,4 @@\n+<<<<<<< HEAD\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1678,0 +3214,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1682,6 +3219,5 @@\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_maxD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n@@ -1689,1 +3225,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated (sve)\" %}\n@@ -1691,0 +3227,4 @@\n+<<<<<<< HEAD\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1706,0 +3246,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1710,7 +3251,9 @@\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n+instruct reduce_maxF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated partial (sve)\" %}\n@@ -1718,3 +3261,6 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1725,8 +3271,9 @@\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+instruct reduce_maxD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated partial (sve)\" %}\n@@ -1734,0 +3281,8 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1740,0 +3295,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1744,0 +3300,1 @@\n+\/\/ vector min reduction\n@@ -1745,9 +3302,7 @@\n-\/\/ vector max reduction\n-\n-instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MaxReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1755,1 +3310,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction (sve)\" %}\n@@ -1757,0 +3312,6 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+=======\n@@ -1763,0 +3324,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1767,8 +3329,6 @@\n-instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MaxReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1776,1 +3336,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxI partial (sve)\" %}\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction (sve)\" %}\n@@ -1778,0 +3338,5 @@\n+<<<<<<< HEAD\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+=======\n@@ -1787,0 +3352,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1791,7 +3357,10 @@\n-instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MaxReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction partial (sve)\" %}\n@@ -1799,4 +3368,6 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1807,1 +3378,1 @@\n-instruct reduce_maxL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+instruct reduce_minL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1809,1 +3380,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1811,1 +3383,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1813,3 +3385,9 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n-  ins_encode %{\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction  partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1823,0 +3401,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1827,2 +3406,3 @@\n-instruct reduce_maxF(vRegF dst, vRegF src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+instruct reduce_minF(vRegF dst, vRegF src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1830,1 +3410,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1833,2 +3413,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction F\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction (sve)\" %}\n@@ -1836,3 +3415,2 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n-    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1843,1 +3421,1 @@\n-instruct reduce_maxF_partial(vRegF dst, vRegF src1, vReg src2,\n+instruct reduce_minF_partial(vRegF dst, vRegF src1, vReg src2,\n@@ -1845,1 +3423,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1847,1 +3426,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1850,1 +3429,1 @@\n-  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# reduce max S partial (sve)\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction partial (sve)\" %}\n@@ -1852,0 +3431,5 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src2));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1857,0 +3441,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1861,2 +3446,3 @@\n-instruct reduce_maxD(vRegD dst, vRegD src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+instruct reduce_minD(vRegD dst, vRegD src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1864,1 +3450,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1867,2 +3453,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction D\" %}\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction (sve)\" %}\n@@ -1870,3 +3455,2 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n-    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1877,1 +3461,1 @@\n-instruct reduce_maxD_partial(vRegD dst, vRegD src1, vReg src2,\n+instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n@@ -1879,1 +3463,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1881,1 +3466,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1884,1 +3469,1 @@\n-  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# reduce max D partial (sve)\" %}\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction partial (sve)\" %}\n@@ -1886,0 +3471,5 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -1891,0 +3481,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1895,1 +3486,1 @@\n-\/\/ vector min reduction\n+\/\/ vector min reduction - predicated\n@@ -1897,10 +3488,17 @@\n-instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minB\/S\/I (sve)\" %}\n-  ins_encode %{\n+instruct reduce_minI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+=======\n@@ -1913,0 +3511,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1917,11 +3516,15 @@\n-instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minI partial (sve)\" %}\n-  ins_encode %{\n+instruct reduce_minL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+=======\n@@ -1937,0 +3540,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1941,7 +3545,10 @@\n-instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+instruct reduce_minI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated partial (sve)\" %}\n@@ -1949,4 +3556,8 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1957,9 +3568,18 @@\n-instruct reduce_minL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n-  ins_encode %{\n+instruct reduce_minL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1973,0 +3593,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1977,8 +3598,7 @@\n-instruct reduce_minF(vRegF dst, vRegF src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n-  effect(TEMP_DEF dst);\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction F\" %}\n+instruct reduce_minF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated (sve)\" %}\n@@ -1986,2 +3606,1 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1993,8 +3612,7 @@\n-instruct reduce_minF_partial(vRegF dst, vRegF src1, vReg src2,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# reduce min S partial (sve)\" %}\n+instruct reduce_minD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated (sve)\" %}\n@@ -2002,0 +3620,4 @@\n+<<<<<<< HEAD\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -2007,0 +3629,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2011,8 +3634,9 @@\n-instruct reduce_minD(vRegD dst, vRegD src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n-  effect(TEMP_DEF dst);\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction D\" %}\n+instruct reduce_minF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated partial (sve)\" %}\n@@ -2020,3 +3644,6 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n-    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -2027,6 +3654,6 @@\n-instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n+instruct reduce_minD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n@@ -2034,1 +3661,2 @@\n-  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# reduce min D partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated partial (sve)\" %}\n@@ -2036,0 +3664,5 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+=======\n@@ -2038,0 +3671,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2039,1 +3673,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -2505,1 +4139,317 @@\n-instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con >= 16) {\n+      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ H,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift - predicated\n+\n+instruct vasrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2507,1 +4457,1 @@\n-  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2509,1 +4459,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (H)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2512,7 +4462,2 @@\n-    if (con >= 16) {\n-      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ H,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2523,1 +4468,1 @@\n-instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2525,1 +4470,1 @@\n-  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2527,1 +4472,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2530,2 +4475,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2536,1 +4481,1 @@\n-instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2538,1 +4483,1 @@\n-  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2540,1 +4485,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2543,2 +4488,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2549,6 +4494,5 @@\n-instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+instruct vlslB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVB (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n@@ -2556,1 +4500,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n@@ -2561,7 +4507,5 @@\n-instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+instruct vlslS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVS (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2569,1 +4513,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2574,6 +4520,5 @@\n-instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+instruct vlslI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVI (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2581,1 +4526,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2586,6 +4533,5 @@\n-instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+instruct vlslL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVL (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2593,1 +4539,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2601,1 +4549,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2613,1 +4562,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2624,0 +4574,28 @@\n+\/\/ vector sqrt - predicated\n+\n+instruct vsqrtF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsqrtD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2704,0 +4682,80 @@\n+\/\/ vector sub - predicated\n+\n+instruct vsubB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2706,2 +4764,3 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+instruct vmaskcast(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n@@ -2709,1 +4768,1 @@\n-  match(Set dst (VectorMaskCast dst));\n+  match(Set dst_src (VectorMaskCast dst_src));\n@@ -2711,1 +4770,1 @@\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n@@ -3301,1 +5360,1 @@\n-instruct vtest_alltrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -3303,1 +5362,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3306,1 +5366,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3308,1 +5368,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, 0\\n\\t\"\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n@@ -3311,0 +5371,4 @@\n+<<<<<<< HEAD\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+=======\n@@ -3316,0 +5380,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3321,1 +5386,1 @@\n-instruct vtest_anytrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n@@ -3323,1 +5388,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3326,1 +5392,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(KILL cr);\n@@ -3328,1 +5394,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, -1\\n\\t\"\n+  format %{ \"sve_ptest $src1\\n\\t\"\n@@ -3332,0 +5398,3 @@\n+<<<<<<< HEAD\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n+=======\n@@ -3336,0 +5405,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3341,1 +5411,1 @@\n-instruct vtest_alltrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_alltrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3343,1 +5413,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3346,1 +5417,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3350,0 +5421,7 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src1);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this, $src1));\n+    __ sve_eors(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+=======\n@@ -3357,0 +5435,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3362,1 +5441,1 @@\n-instruct vtest_anytrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_anytrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3364,1 +5443,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3367,1 +5447,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3371,0 +5451,7 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src1);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this, $src1));\n+    __ sve_ands(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+=======\n@@ -3378,0 +5465,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3616,1 +5704,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -3631,2 +5719,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -3643,1 +5730,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3649,1 +5736,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3651,2 +5738,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -3654,0 +5740,4 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+=======\n@@ -3658,0 +5748,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3662,1 +5753,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3668,1 +5759,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3670,3 +5761,72 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n@@ -3674,0 +5834,7 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+=======\n@@ -3678,0 +5845,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3692,1 +5860,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -3707,2 +5875,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -3717,1 +5884,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -3719,1 +5886,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3725,1 +5892,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3727,2 +5894,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -3730,0 +5896,4 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+=======\n@@ -3734,0 +5904,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3738,1 +5909,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3744,1 +5915,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3746,3 +5917,34 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n@@ -3750,0 +5952,4 @@\n+<<<<<<< HEAD\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+=======\n@@ -3754,0 +5960,22 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3759,0 +5987,19 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -3907,1 +6154,1 @@\n-instruct vstoremask_truecount(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_truecount(iRegINoSp dst, pRegGov src, immI esize, rFlagsReg cr) %{\n@@ -3911,2 +6158,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n@@ -3918,0 +6165,3 @@\n+<<<<<<< HEAD\n+    __ sve_cntp($dst$$Register, variant, ptrue, as_PRegister($src$$reg));\n+=======\n@@ -3920,0 +6170,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3924,1 +6175,1 @@\n-instruct vstoremask_firsttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_firsttrue(iRegINoSp dst, pRegGov src, immI esize, pReg ptmp, rFlagsReg cr) %{\n@@ -3929,1 +6180,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -3935,0 +6186,5 @@\n+<<<<<<< HEAD\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg),\n+                           vector_length(this, $src));\n+=======\n@@ -3937,0 +6193,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3941,1 +6198,1 @@\n-instruct vstoremask_lasttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_lasttrue(iRegINoSp dst, pRegGov src, immI esize, pReg ptmp, rFlagsReg cr) %{\n@@ -3946,1 +6203,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -3952,0 +6209,5 @@\n+<<<<<<< HEAD\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg),\n+                           vector_length(this, $src));\n+=======\n@@ -3954,0 +6216,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3958,1 +6221,2 @@\n-instruct vstoremask_truecount_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vstoremask_truecount_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n@@ -3963,1 +6227,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -3969,0 +6233,4 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, variant, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+=======\n@@ -3973,0 +6241,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3977,1 +6246,2 @@\n-instruct vstoremask_firsttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_firsttrue_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n@@ -3982,1 +6252,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -3988,0 +6258,6 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), variant, vector_length(this, $src));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), as_PRegister($pgtmp$$reg),\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+=======\n@@ -3992,0 +6268,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -3996,1 +6273,2 @@\n-instruct vstoremask_lasttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vstoremask_lasttrue_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n@@ -4007,0 +6285,8 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($src$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($ptmp$$reg), ptrue,\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+=======\n@@ -4011,0 +6297,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":2816,"deletions":529,"binary":false,"changes":3345,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -156,0 +157,6 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    \/\/ Currently we support all masked vector opcodes.\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -241,1 +248,1 @@\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+            \"sve_ldr $dst, $pTmp, $mem\\t# load vector partial\" %}\n@@ -261,1 +268,1 @@\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+            \"sve_str $src, $pTmp, $mem\\t# store vector partial\" %}\n@@ -272,1 +279,71 @@\n-%}dnl\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt), vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt), vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -274,0 +351,87 @@\n+dnl\n+dnl MASKALL_IMM($1,   $2  )\n+dnl MASKALL_IMM(type, size)\n+define(`MASKALL_IMM', `\n+instruct vmaskAll_imm$1(pRegGov dst, imm$1 src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    ifelse($1, `I', int, long) con = (ifelse($1, `I', int, long))$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl MASKALL($1,   $2  )\n+dnl MASKALL(type, size)\n+define(`MASKALL', `\n+instruct vmaskAll$1(pRegGov dst, ifelse($1, `I', iRegIorL2I, iRegL) src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ maskAll\n+MASKALL_IMM(I, B\/H\/S)\n+MASKALL(I, B\/H\/S)\n+MASKALL_IMM(L, D)\n+MASKALL(L, D)\n+\n+dnl\n+dnl MASK_LOGICAL_OP($1,        $2,      $3  )\n+dnl MASK_LOGICAL_OP(insn_name, op_name, insn)\n+define(`MASK_LOGICAL_OP', `\n+instruct vmask_$1(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd ($2 pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ $3(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and\/or\/xor\n+MASK_LOGICAL_OP(and, AndVMask, sve_and)\n+MASK_LOGICAL_OP(or, OrVMask, sve_orr)\n+MASK_LOGICAL_OP(xor, XorVMask, sve_eor)\n+\n+dnl\n+dnl MASK_LOGICAL_AND_NOT($1,   $2  )\n+dnl MASK_LOGICAL_AND_NOT(type, size)\n+define(`MASK_LOGICAL_AND_NOT', `\n+instruct vmask_and_not$1(pRegGov pd, pRegGov pn, pRegGov pm, imm$1_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and_not\n+MASK_LOGICAL_AND_NOT(I, B\/H\/S)\n+MASK_LOGICAL_AND_NOT(L, D)\n@@ -310,0 +474,34 @@\n+\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmpeq(as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -311,3 +509,3 @@\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE($1,        $2,      $3,           $4,   $5,          %6  )\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE(insn_name, op_name, element_type, size, min_vec_len, insn)\n-define(`UNARY_OP_TRUE_PREDICATE_ETYPE', `\n+dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_TRUE_PREDICATE', `\n@@ -316,1 +514,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == $3);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -319,1 +517,1 @@\n-  format %{ \"$6 $dst, $src\\t# vector (sve) ($4)\" %}\n+  format %{ \"$4 $dst, $src\\t# vector (sve) ($3)\" %}\n@@ -321,1 +519,1 @@\n-    __ $6(as_FloatRegister($dst$$reg), __ $4,\n+    __ $4(as_FloatRegister($dst$$reg), __ $3,\n@@ -329,10 +527,34 @@\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsB, AbsVB, T_BYTE,   B, 16, sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsS, AbsVS, T_SHORT,  H, 8,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsI, AbsVI, T_INT,    S, 4,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsL, AbsVL, T_LONG,   D, 2,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsF, AbsVF, T_FLOAT,  S, 4,  sve_fabs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsD, AbsVD, T_DOUBLE, D, 2,  sve_fabs)\n-dnl\n-dnl BINARY_OP_UNPREDICATED($1,        $2       $3,   $4           $5  )\n-dnl BINARY_OP_UNPREDICATED(insn_name, op_name, size, min_vec_len, insn)\n-define(`BINARY_OP_UNPREDICATED', `\n+UNARY_OP_TRUE_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_TRUE_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl UNARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src, $pg, $dst_src\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\/\/ vector abs - predicated\n+UNARY_OP_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl\n+dnl BINARY_OP_UNPREDICATE($1,        $2       $3,   $4           $5  )\n+dnl BINARY_OP_UNPREDICATE(insn_name, op_name, size, min_vec_len, insn)\n+define(`BINARY_OP_UNPREDICATE', `\n@@ -351,1 +573,18 @@\n-\n+dnl\n+dnl\n+dnl BINARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl BINARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`BINARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src1$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -353,9 +592,18 @@\n-BINARY_OP_UNPREDICATED(vaddB, AddVB, B, 16, sve_add)\n-BINARY_OP_UNPREDICATED(vaddS, AddVS, H, 8,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddI, AddVI, S, 4,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddL, AddVL, D, 2,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddF, AddVF, S, 4,  sve_fadd)\n-BINARY_OP_UNPREDICATED(vaddD, AddVD, D, 2,  sve_fadd)\n-dnl\n-dnl BINARY_OP_UNSIZED($1,        $2,      $3,          $4  )\n-dnl BINARY_OP_UNSIZED(insn_name, op_name, min_vec_len, insn)\n+BINARY_OP_UNPREDICATE(vaddB, AddVB, B, 16, sve_add)\n+BINARY_OP_UNPREDICATE(vaddS, AddVS, H, 8,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddI, AddVI, S, 4,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddL, AddVL, D, 2,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddF, AddVF, S, 4,  sve_fadd)\n+BINARY_OP_UNPREDICATE(vaddD, AddVD, D, 2,  sve_fadd)\n+\n+\/\/ vector add - predicated\n+BINARY_OP_PREDICATE(vaddB, AddVB, B, sve_add)\n+BINARY_OP_PREDICATE(vaddS, AddVS, H, sve_add)\n+BINARY_OP_PREDICATE(vaddI, AddVI, S, sve_add)\n+BINARY_OP_PREDICATE(vaddL, AddVL, D, sve_add)\n+BINARY_OP_PREDICATE(vaddF, AddVF, S, sve_fadd)\n+BINARY_OP_PREDICATE(vaddD, AddVD, D, sve_fadd)\n+\n+dnl\n+dnl BINARY_OP_UNSIZED($1,        $2,      $3  )\n+dnl BINARY_OP_UNSIZED(insn_name, op_name, insn)\n@@ -367,1 +615,1 @@\n-  format %{ \"$4  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"$3  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -369,1 +617,1 @@\n-    __ $4(as_FloatRegister($dst$$reg),\n+    __ $3(as_FloatRegister($dst$$reg),\n@@ -375,1 +623,1 @@\n-\n+dnl\n@@ -377,1 +625,1 @@\n-BINARY_OP_UNSIZED(vand, AndV, 16, sve_and)\n+BINARY_OP_UNSIZED(vand, AndV, sve_and)\n@@ -380,1 +628,1 @@\n-BINARY_OP_UNSIZED(vor, OrV, 16, sve_orr)\n+BINARY_OP_UNSIZED(vor, OrV, sve_orr)\n@@ -383,1 +631,28 @@\n-BINARY_OP_UNSIZED(vxor, XorV, 16, sve_eor)\n+BINARY_OP_UNSIZED(vxor, XorV, sve_eor)\n+\n+dnl BINARY_LOGIC_OP_PREDICATE($1,        $2,      $3  )\n+dnl BINARY_LOGIC_OP_PREDICATE(insn_name, op_name, insn)\n+define(`BINARY_LOGIC_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ $3(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector and - predicated\n+BINARY_LOGIC_OP_PREDICATE(vand, AndV, sve_and)\n+\n+\/\/ vector or - predicated\n+BINARY_LOGIC_OP_PREDICATE(vor, OrV, sve_orr)\n+\n+\/\/ vector xor - predicated\n+BINARY_LOGIC_OP_PREDICATE(vxor, XorV, sve_eor)\n@@ -409,1 +684,1 @@\n-\n+dnl\n@@ -450,1 +725,1 @@\n-\n+dnl\n@@ -455,1 +730,3 @@\n-\/\/ vector min\/max\n+\/\/ vector float div - predicated\n+BINARY_OP_PREDICATE(vfdivF, DivVF, S, sve_fdiv)\n+BINARY_OP_PREDICATE(vfdivD, DivVD, D, sve_fdiv)\n@@ -457,1 +734,5 @@\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX($1     , $2, $3   , $4  )\n+dnl VMINMAX(op_name, op, finsn, insn)\n+define(`VMINMAX', `\n+instruct v$1(vReg dst_src1, vReg src2) %{\n@@ -459,1 +740,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src1 ($2 dst_src1 src2));\n@@ -461,1 +742,1 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -466,1 +747,1 @@\n-      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n@@ -469,2 +750,2 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n@@ -475,1 +756,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max\n+VMINMAX(min, MinV, sve_fmin, sve_smin)\n+VMINMAX(max, MaxV, sve_fmax, sve_smax)\n@@ -477,1 +762,5 @@\n-instruct vmax(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX_PREDICATE($1     , $2, $3   , $4  )\n+dnl VMINMAX_PREDICATE(op_name, op, finsn, insn)\n+define(`VMINMAX_PREDICATE', `\n+instruct v$1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -479,1 +768,1 @@\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n@@ -481,1 +770,1 @@\n-  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n@@ -486,2 +775,2 @@\n-      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -489,3 +778,3 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -495,1 +784,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max - predicated\n+VMINMAX_PREDICATE(min, MinV, sve_fmin, sve_smin)\n+VMINMAX_PREDICATE(max, MaxV, sve_fmax, sve_smax)\n@@ -518,0 +811,21 @@\n+dnl\n+dnl VFMLA_PREDICATE($1,   $2  )\n+dnl VFMLA_PREDICATE(type, size)\n+define(`VFMLA_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmla$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fmla - predicated\n+VFMLA_PREDICATE(F, S)\n+VFMLA_PREDICATE(D, D)\n+\n@@ -648,1 +962,1 @@\n-\n+dnl\n@@ -654,2 +968,10 @@\n-BINARY_OP_UNPREDICATED(vmulF, MulVF, S, 4, sve_fmul)\n-BINARY_OP_UNPREDICATED(vmulD, MulVD, D, 2, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulF, MulVF, S, 4, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulD, MulVD, D, 2, sve_fmul)\n+\n+\/\/ vector mul - predicated\n+BINARY_OP_PREDICATE(vmulB, MulVB, B, sve_mul)\n+BINARY_OP_PREDICATE(vmulS, MulVS, H, sve_mul)\n+BINARY_OP_PREDICATE(vmulI, MulVI, S, sve_mul)\n+BINARY_OP_PREDICATE(vmulL, MulVL, D, sve_mul)\n+BINARY_OP_PREDICATE(vmulF, MulVF, S, sve_fmul)\n+BINARY_OP_PREDICATE(vmulD, MulVD, D, sve_fmul)\n@@ -657,18 +979,6 @@\n-dnl\n-dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4,            $5  )\n-dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, min_vec_bytes, insn)\n-define(`UNARY_OP_TRUE_PREDICATE', `\n-instruct $1(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst ($2 src));\n-  ins_cost(SVE_COST);\n-  format %{ \"$5 $dst, $src\\t# vector (sve) ($3)\" %}\n-  ins_encode %{\n-    __ $5(as_FloatRegister($dst$$reg), __ $3,\n-         ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, 16, sve_fneg)\n-UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, 16, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, sve_fneg)\n+\n+\/\/ vector fneg - predicated\n+UNARY_OP_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_PREDICATE(vnegD, NegVD, D, sve_fneg)\n@@ -691,1 +1001,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -694,4 +1004,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -699,0 +1008,4 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+=======\n@@ -701,0 +1014,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -702,2 +1016,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -708,3 +1020,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -712,5 +1022,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -718,0 +1027,5 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+=======\n@@ -724,0 +1038,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -728,1 +1043,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -730,2 +1045,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -733,5 +1047,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -739,0 +1051,6 @@\n+<<<<<<< HEAD\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+=======\n@@ -745,0 +1063,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -751,1 +1070,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -755,0 +1074,1 @@\n+  effect(KILL cr);\n@@ -756,1 +1076,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -758,1 +1078,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg), 0);\n@@ -763,1 +1083,1 @@\n-instruct vloadmaskS(vReg dst, vReg src) %{\n+instruct vloadmaskS(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -767,0 +1087,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -768,2 +1089,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskS $dst, $src\\t# vector load mask (sve) (B to H)\" %}\n@@ -771,2 +1091,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ H, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -777,1 +1097,1 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n+instruct vloadmaskI(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -782,0 +1102,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -783,3 +1104,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n+  format %{ \"vloadmaskI $dst, $src\\t# vector load mask (sve) (B to S)\" %}\n@@ -787,3 +1106,3 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ S, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -794,1 +1113,1 @@\n-instruct vloadmaskL(vReg dst, vReg src) %{\n+instruct vloadmaskL(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -799,0 +1118,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -800,4 +1120,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmaskL $dst, $src\\t# vector load mask (sve) (B to D)\" %}\n@@ -805,4 +1122,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ D, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ D, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -815,1 +1132,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -819,1 +1136,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremaskB $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -821,2 +1138,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -827,1 +1143,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremaskS(vReg dst, pRegGov src, vReg tmp, immI_2 size) %{\n@@ -832,3 +1148,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+  format %{ \"vstoremaskS $dst, $src\\t# vector store mask (sve) (H to B)\" %}\n@@ -836,0 +1150,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ H, as_PRegister($src$$reg), 1, false);\n@@ -838,4 +1153,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -846,1 +1158,1 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+instruct vstoremaskI(vReg dst, pRegGov src, vReg tmp, immI_4 size) %{\n@@ -851,4 +1163,1 @@\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+  format %{ \"vstoremaskI $dst, $src\\t# vector store mask (sve) (S to B)\" %}\n@@ -856,0 +1165,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($src$$reg), 1, false);\n@@ -858,1 +1168,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -861,2 +1171,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -867,1 +1175,1 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+instruct vstoremaskL(vReg dst, pRegGov src, vReg tmp, immI_8 size) %{\n@@ -872,5 +1180,1 @@\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+  format %{ \"vstoremaskL $dst, $src\\t# vector store mask (sve) (D to B)\" %}\n@@ -878,0 +1182,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($src$$reg), 1, false);\n@@ -880,1 +1185,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -885,2 +1190,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -891,6 +1194,6 @@\n-dnl\n-dnl VLOADMASK_LOADV($1,    $2  )\n-dnl VLOADMASK_LOADV(esize, cond)\n-define(`VLOADMASK_LOADV', `\n-instruct vloadmask_loadV_$1(vReg dst, ifelse($1, `byte', vmemA, indirect) mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+dnl LOADVMASK($1,    $2  )\n+dnl LOADVMASK(esize, cond)\n+define(`LOADVMASK', `\n+instruct loadVMask_$1(pRegGov dst, ifelse($1, `byte', vmemA, indirect) mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -898,1 +1201,2 @@\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP tmp, KILL cr);\n@@ -900,3 +1204,10 @@\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n-  ins_encode %{\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) ($3)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+=======\n@@ -908,0 +1219,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -909,1 +1221,2 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+                 ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -915,9 +1228,10 @@\n-`ifelse($1, `byte', vmemA, indirect) mem, vReg src, vReg tmp, ifelse($1, `byte', immI_1, immI_gt_1) esize')\n-dnl\n-dnl STOREV_VSTOREMASK($1,  )\n-dnl STOREV_VSTOREMASK(esize)\n-define(`STOREV_VSTOREMASK', `\n-instruct storeV_vstoremask_$1(ARGLIST($1)) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+`ifelse($1, `byte', vmemA, indirect) mem, pRegGov src, vReg tmp')\n+dnl\n+dnl STOREVMASK($1,    $2  )\n+dnl STOREVMASK(esize, cond)\n+define(`STOREVMASK', `\n+instruct storeVMask_$1(ARGLIST($1)) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) $2);\n+  match(Set mem (StoreVectorMask mem src));\n@@ -925,0 +1239,2 @@\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) ($3)\" %}\n@@ -926,2 +1242,7 @@\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+<<<<<<< HEAD\n+    \/\/ Convert the src predicate to vector. And store the vector elements\n+    \/\/ as boolean values.\n+    BasicType from_vect_bt = vector_element_basic_type(this, $src);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), __ elemType_to_regVariant(from_vect_bt),\n+               as_PRegister($src$$reg), 1, false);\n+=======\n@@ -933,0 +1254,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -942,0 +1264,52 @@\n+dnl LOADVMASK_PARTIAL($1,    $2  )\n+dnl LOADVMASK_PARTIAL(esize, cond)\n+define(`LOADVMASK_PARTIAL', `\n+instruct loadVMask_$1_partial(pRegGov dst, ifelse($1, `byte', vmemA, indirect) mem, vReg vtmp,\n+                              pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) $2);\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"loadVMask $dst, $mem\\t# load vector mask partial (sve) ($3)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl STOREVMASK_PARTIAL($1,    $2  )\n+dnl STOREVMASK_PARTIAL(esize, cond)\n+define(`STOREVMASK_PARTIAL', `\n+instruct storeVMask_$1_partial(ifelse($1, `byte', vmemA, indirect) mem, pRegGov src, vReg vtmp,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) $2);\n+  match(Set mem (StoreVectorMask mem src));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeVMask $src, $mem\\t# store vector mask partial (sve) ($3)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -943,11 +1317,21 @@\n-VLOADMASK_LOADV(byte, == 1)\n-VLOADMASK_LOADV(non_byte, > 1)\n-STOREV_VSTOREMASK(byte)\n-STOREV_VSTOREMASK(non_byte)\n-\n-\/\/ vector add reduction\n-\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+LOADVMASK(byte, == 1, B)\n+LOADVMASK(non_byte, > 1, H\/S\/D)\n+LOADVMASK_PARTIAL(byte, == 1, B)\n+LOADVMASK_PARTIAL(non_byte, > 1, H\/S\/D)\n+STOREVMASK(byte, == 1, B)\n+STOREVMASK(non_byte, > 1, H\/S\/D)\n+STOREVMASK_PARTIAL(byte, == 1, B)\n+STOREVMASK_PARTIAL(non_byte, > 1, H\/S\/D)\n+dnl\n+dnl REDUCE_I($1,        $2     )\n+dnl REDUCE_I(insn_name, op_name)\n+define(`REDUCE_I', `\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -955,2 +1339,8 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n-  ins_encode %{\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+=======\n@@ -969,0 +1359,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -971,3 +1362,29 @@\n-%}\n-\n-instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl\n+dnl REDUCE_L($1,        $2    )\n+dnl REDUCE_L(insn_name, op_name)\n+define(`REDUCE_L', `\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PARTIAL', `\n+instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -975,2 +1392,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -978,2 +1400,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve) (may extend)\" %}\n@@ -983,0 +1405,6 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1010,0 +1438,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1012,3 +1441,6 @@\n-%}\n-\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PARTIAL', `\n+instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1016,2 +1448,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -1019,3 +1456,9 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n-  ins_encode %{\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1028,0 +1471,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1030,2 +1474,1 @@\n-%}\n-\n+%}')dnl\n@@ -1036,3 +1479,4 @@\n-instruct $1($3 src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1($3 src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1052,3 +1496,4 @@\n-instruct $1($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1_partial($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1057,1 +1502,1 @@\n-  format %{ \"sve_reduce_add$2 $src1_dst, $src1_dst, $src2\\t# add$2 reduction partial (sve) ($4)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $src1_dst, $src2\\t# $1 reduction partial (sve) ($4)\" %}\n@@ -1067,0 +1512,15 @@\n+<<<<<<< HEAD\n+dnl\n+dnl REDUCE_I_PREDICATE($1,        $2     )\n+dnl REDUCE_I_PREDICATE(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+=======\n@@ -1192,0 +1652,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1193,2 +1654,8 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n-  ins_encode %{\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+=======\n@@ -1210,0 +1677,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1212,7 +1680,14 @@\n-%}\n-\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE($1,        $2    )\n+dnl REDUCE_L_PREDICATE(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1220,1 +1695,1 @@\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n@@ -1222,3 +1697,3 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1227,7 +1702,14 @@\n-%}\n-\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PREDICATE_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1235,0 +1717,4 @@\n+<<<<<<< HEAD\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve) (may extend)\" %}\n+=======\n@@ -1252,26 +1738,1 @@\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n@@ -1279,1 +1740,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP vtmp);\n@@ -1281,1 +1742,2 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1283,0 +1745,36 @@\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1298,0 +1796,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1300,7 +1799,9 @@\n-%}\n-\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE', `\n+instruct reduce_$1_masked($3 src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n@@ -1308,1 +1809,1 @@\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated (sve)\" %}\n@@ -1310,3 +1811,2 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1315,8 +1815,10 @@\n-%}\n-\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE_PARTIAL', `\n+instruct reduce_$1_masked_partial($3 src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n@@ -1324,1 +1826,1 @@\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated partial (sve)\" %}\n@@ -1326,0 +1828,7 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+=======\n@@ -1332,0 +1841,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1334,1 +1844,58 @@\n-%}\n+%}')dnl\n+dnl\n+\n+\/\/ vector add reduction\n+REDUCE_I(add, AddReductionVI)\n+REDUCE_L(add, AddReductionVL)\n+REDUCE_ADDF(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PARTIAL(addD, AddReductionVD, vRegD, D)\n+\n+\/\/ vector add reduction - predicated\n+REDUCE_I_PREDICATE(add, AddReductionVI)\n+REDUCE_L_PREDICATE(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PREDICATE_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PREDICATE_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addD, AddReductionVD, vRegD, D)\n+\n+\/\/ vector and reduction\n+REDUCE_I(and, AndReductionV)\n+REDUCE_L(and, AndReductionV)\n+REDUCE_I_PARTIAL(and, AndReductionV)\n+REDUCE_L_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector and reduction - predicated\n+REDUCE_I_PREDICATE(and, AndReductionV)\n+REDUCE_L_PREDICATE(and, AndReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(and, AndReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector or reduction\n+REDUCE_I(or, OrReductionV)\n+REDUCE_L(or, OrReductionV)\n+REDUCE_I_PARTIAL(or, OrReductionV)\n+REDUCE_L_PARTIAL(or, OrReductionV)\n+\n+\/\/ vector or reduction - predicated\n+REDUCE_I_PREDICATE(or, OrReductionV)\n+REDUCE_L_PREDICATE(or, OrReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(or, OrReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(or, OrReductionV)\n+\n+\/\/ vector xor reduction\n+REDUCE_I(eor, XorReductionV)\n+REDUCE_L(eor, XorReductionV)\n+REDUCE_I_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PARTIAL(eor, XorReductionV)\n+\n+\/\/ vector xor reduction - predicated\n+REDUCE_I_PREDICATE(eor, XorReductionV)\n+REDUCE_L_PREDICATE(eor, XorReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(eor, XorReductionV)\n@@ -1337,2 +1904,2 @@\n-dnl REDUCE_MAXMIN_I($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I(min_max, op_mame, cmp)\n+dnl REDUCE_MAXMIN_I($1,        $2     )\n+dnl REDUCE_MAXMIN_I(insn_name, op_name)\n@@ -1340,5 +1907,5 @@\n-instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1346,1 +1913,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1348,2 +1915,8 @@\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1B\/S\/I (sve)\" %}\n-  ins_encode %{\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+=======\n@@ -1356,0 +1929,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1360,2 +1934,2 @@\n-dnl REDUCE_MAXMIN_L($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L(min_max, op_name, cmp)\n+dnl REDUCE_MAXMIN_L($1,        $2     )\n+dnl REDUCE_MAXMIN_L(insn_name, op_name)\n@@ -1363,2 +1937,3 @@\n-instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1367,1 +1942,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1369,1 +1944,1 @@\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n@@ -1371,4 +1946,3 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1379,2 +1953,2 @@\n-dnl REDUCE_MAXMIN_I_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_mame, cmp)\n+dnl REDUCE_MAXMIN_I_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_name)\n@@ -1384,4 +1958,4 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1390,2 +1964,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1I partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve)\" %}\n@@ -1395,0 +1969,6 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1402,0 +1982,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1406,2 +1987,2 @@\n-dnl REDUCE_MAXMIN_L_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name, cmp)\n+dnl REDUCE_MAXMIN_L_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name)\n@@ -1411,1 +1992,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1415,0 +1997,43 @@\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction  partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_I_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1416,1 +2041,22 @@\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve)\" %}\n@@ -1418,0 +2064,33 @@\n+    BasicType bt = vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+=======\n@@ -1425,0 +2104,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1433,1 +2113,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1438,2 +2119,1 @@\n-  format %{ \"sve_f$1v $dst, $src2 # vector (sve) ($4)\\n\\t\"\n-            \"f$1s $dst, $dst, $src1\\t# $1 reduction $2\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction (sve)\" %}\n@@ -1441,2 +2121,1 @@\n-    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src2$$reg));\n@@ -1448,1 +2127,0 @@\n-dnl\n@@ -1454,1 +2132,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1459,1 +2138,38 @@\n-  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# reduce $1 $4 partial (sve)\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4, vector_length(this, $src2));\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE', `\n+instruct reduce_$1$2_masked($5 dst, $5 src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE_PARTIAL', `\n+instruct reduce_$1$2_masked_partial($5 dst, $5 src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated partial (sve)\" %}\n@@ -1461,0 +2177,5 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4, vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+=======\n@@ -1463,0 +2184,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -1464,1 +2186,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1469,5 +2191,4 @@\n-\n-REDUCE_MAXMIN_I(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV, GT)\n+REDUCE_MAXMIN_I(max, MaxReductionV)\n+REDUCE_MAXMIN_L(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV)\n@@ -1480,0 +2201,10 @@\n+\/\/ vector max reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_FMINMAX_PREDICATE(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(max, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, D, T_DOUBLE, D, vRegD)\n+\n@@ -1481,4 +2212,4 @@\n-REDUCE_MAXMIN_I(min, MinReductionV, LT)\n-REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV, LT)\n+REDUCE_MAXMIN_I(min, MinReductionV)\n+REDUCE_MAXMIN_L(min, MinReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV)\n@@ -1490,0 +2221,10 @@\n+\/\/ vector min reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_FMINMAX_PREDICATE(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(min, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, D, T_DOUBLE, D, vRegD)\n+\n@@ -1667,0 +2408,43 @@\n+\/\/ vector shift - predicated\n+BINARY_OP_PREDICATE(vasrB, RShiftVB,  B, sve_asr)\n+BINARY_OP_PREDICATE(vasrS, RShiftVS,  H, sve_asr)\n+BINARY_OP_PREDICATE(vasrI, RShiftVI,  S, sve_asr)\n+BINARY_OP_PREDICATE(vasrL, RShiftVL,  D, sve_asr)\n+BINARY_OP_PREDICATE(vlslB, LShiftVB,  B, sve_lsl)\n+BINARY_OP_PREDICATE(vlslS, LShiftVS,  H, sve_lsl)\n+BINARY_OP_PREDICATE(vlslI, LShiftVI,  S, sve_lsl)\n+BINARY_OP_PREDICATE(vlslL, LShiftVL,  D, sve_lsl)\n+BINARY_OP_PREDICATE(vlsrB, URShiftVB, B, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrS, URShiftVS, H, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrI, URShiftVI, S, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrL, URShiftVL, D, sve_lsr)\n+dnl\n+dnl VSHIFT_IMM_PREDICATED($1,        $2,      $3,       $4,   $5,   $6  )\n+dnl VSHIFT_IMM_PREDICATED(insn_name, op_name, op_name2, type, size, insn)\n+define(`VSHIFT_IMM_PREDICATED', `\n+instruct $1_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 (Binary dst_src ($3 shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$6 $dst_src, $pg, $dst_src, $shift\\t# vector (sve) ($4)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con ifelse(index(`$1', `vlsl'), 0, `>=', `>') 0 && con < $5, \"invalid shift immediate\");\n+    __ $6(as_FloatRegister($dst_src$$reg), __ $4, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+VSHIFT_IMM_PREDICATED(vasrB, RShiftVB,  RShiftCntV, B, 8,  sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrS, RShiftVS,  RShiftCntV, H, 16, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrI, RShiftVI,  RShiftCntV, S, 32, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrL, RShiftVL,  RShiftCntV, D, 64, sve_asr)\n+VSHIFT_IMM_PREDICATED(vlsrB, URShiftVB, RShiftCntV, B, 8,  sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrS, URShiftVS, RShiftCntV, H, 16, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrI, URShiftVI, RShiftCntV, S, 32, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrL, URShiftVL, RShiftCntV, D, 64, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlslB, LShiftVB,  LShiftCntV, B, 8,  sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslS, LShiftVS,  LShiftCntV, H, 16, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslI, LShiftVI,  LShiftCntV, S, 32, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslL, LShiftVL,  LShiftCntV, D, 64, sve_lsl)\n+\n@@ -1668,2 +2452,6 @@\n-UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, 16, sve_fsqrt)\n-UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, 16, sve_fsqrt)\n+UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n+\n+\/\/ vector sqrt - predicated\n+UNARY_OP_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n@@ -1672,6 +2460,14 @@\n-BINARY_OP_UNPREDICATED(vsubB, SubVB, B, 16, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubS, SubVS, H, 8, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubI, SubVI, S, 4, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubL, SubVL, D, 2, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubF, SubVF, S, 4, sve_fsub)\n-BINARY_OP_UNPREDICATED(vsubD, SubVD, D, 2, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubB, SubVB, B, 16, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubS, SubVS, H, 8, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubI, SubVI, S, 4, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubL, SubVL, D, 2, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubF, SubVF, S, 4, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubD, SubVD, D, 2, sve_fsub)\n+\n+\/\/ vector sub - predicated\n+BINARY_OP_PREDICATE(vsubB, SubVB, B, sve_sub)\n+BINARY_OP_PREDICATE(vsubS, SubVS, H, sve_sub)\n+BINARY_OP_PREDICATE(vsubI, SubVI, S, sve_sub)\n+BINARY_OP_PREDICATE(vsubL, SubVL, D, sve_sub)\n+BINARY_OP_PREDICATE(vsubF, SubVF, S, sve_fsub)\n+BINARY_OP_PREDICATE(vsubD, SubVD, D, sve_fsub)\n@@ -1681,2 +2477,3 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+instruct vmaskcast(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n@@ -1684,1 +2481,1 @@\n-  match(Set dst (VectorMaskCast dst));\n+  match(Set dst_src (VectorMaskCast dst_src));\n@@ -1686,1 +2483,1 @@\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n@@ -2061,5 +2858,2 @@\n-dnl\n-dnl VTEST($1,      $2,   $3,  $4  )\n-dnl VTEST(op_name, pred, imm, cond)\n-define(`VTEST', `\n-instruct vtest_$1`'(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -2067,2 +2861,3 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::$2);\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n@@ -2070,1 +2865,19 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n+            \"csetw $dst, EQ\\t# VectorTest (sve) - alltrue\" %}\n+  ins_encode %{\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n+%{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n@@ -2072,2 +2885,2 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, $3\\n\\t\"\n-            \"csetw $dst, $4\\t# VectorTest (sve) - $1\" %}\n+  format %{ \"sve_ptest $src1\\n\\t\"\n+            \"csetw $dst, NE\\t# VectorTest (sve) - anytrue\" %}\n@@ -2076,0 +2889,4 @@\n+<<<<<<< HEAD\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::NE);\n+=======\n@@ -2081,0 +2898,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2083,4 +2901,1 @@\n-%}')dnl\n-dnl\n-VTEST(alltrue, overflow, 0, EQ)\n-VTEST(anytrue, ne,      -1, NE)\n+%}\n@@ -2089,2 +2904,2 @@\n-dnl VTEST_PARTIAL($1,      $2,   $3,  $4  )\n-dnl VTEST_PARTIAL(op_name, pred, imm, cond)\n+dnl VTEST_PARTIAL($1,      $2,   $3,   $4  )\n+dnl VTEST_PARTIAL(op_name, pred, inst, cond)\n@@ -2092,1 +2907,1 @@\n-instruct vtest_$1_partial`'(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_$1_partial`'(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -2094,1 +2909,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -2097,1 +2913,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2101,0 +2917,7 @@\n+<<<<<<< HEAD\n+    BasicType bt = vector_element_basic_type(this, $src1);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, vector_length(this, $src1));\n+    __ $3(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+=======\n@@ -2108,0 +2931,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2113,2 +2937,2 @@\n-VTEST_PARTIAL(alltrue, overflow, 0, EQ)\n-VTEST_PARTIAL(anytrue, ne,      -1, NE)\n+VTEST_PARTIAL(alltrue, overflow, sve_eors, EQ)\n+VTEST_PARTIAL(anytrue, ne,       sve_ands, NE)\n@@ -2333,1 +3157,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -2348,2 +3172,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -2360,1 +3183,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2366,1 +3189,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2368,2 +3191,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -2371,0 +3193,4 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+=======\n@@ -2375,0 +3201,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2379,1 +3206,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2385,1 +3212,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2387,3 +3214,72 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n@@ -2391,0 +3287,7 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+=======\n@@ -2395,0 +3298,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2409,1 +3313,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -2424,2 +3328,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -2434,1 +3337,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -2436,1 +3339,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2442,1 +3345,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2444,2 +3347,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -2447,0 +3349,4 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+=======\n@@ -2451,0 +3357,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2455,1 +3362,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2461,1 +3368,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2463,3 +3370,54 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg), as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+<<<<<<< HEAD\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n@@ -2467,0 +3425,5 @@\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+=======\n@@ -2471,0 +3434,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2476,0 +3440,19 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2566,1 +3549,1 @@\n-instruct vstoremask_$1(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_$1(iRegINoSp dst, pRegGov src, immI esize, ifelse($1, `truecount', `rFlagsReg cr', `pReg ptmp, rFlagsReg cr')) %{\n@@ -2570,2 +3553,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost($3 * SVE_COST);\n+  effect(ifelse($1, `truecount', `KILL cr', `TEMP ptmp, KILL cr'));\n+  ins_cost($3);\n@@ -2576,0 +3559,8 @@\n+<<<<<<< HEAD\n+    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);dnl\n+ifelse(`$1', `truecount', `\n+    __ sve_cntp($dst$$Register, variant, ptrue, as_PRegister($src$$reg));', `\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg),\n+                           vector_length(this, $src));')\n+=======\n@@ -2579,0 +3570,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2584,3 +3576,3 @@\n-VSTOREMASK_REDUCTION(truecount, VectorMaskTrueCount, 2)\n-VSTOREMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 3)\n-VSTOREMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 4)\n+VSTOREMASK_REDUCTION(truecount, VectorMaskTrueCount, SVE_COST)\n+VSTOREMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 2 * SVE_COST)\n+VSTOREMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 3 * SVE_COST)\n@@ -2591,1 +3583,2 @@\n-instruct vstoremask_$1_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n+instruct vstoremask_$1_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n@@ -2602,0 +3595,14 @@\n+<<<<<<< HEAD\n+    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), variant, vector_length(this, $src));dnl\n+ifelse($1, `truecount', `\n+    __ sve_cntp($dst$$Register, variant, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));',\n+       $1, `firsttrue', `\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), as_PRegister($pgtmp$$reg),\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);', `\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($src$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($ptmp$$reg), ptrue,\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);')\n+=======\n@@ -2606,0 +3613,1 @@\n+>>>>>>> 3b21593199cbb11cb4019ef53c831fd087a0ba50\n@@ -2610,2 +3618,2 @@\n-VSTOREMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 3)\n-VSTOREMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 4)\n+VSTOREMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 2)\n+VSTOREMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 3)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":1404,"deletions":396,"binary":false,"changes":1800,"status":"modified"},{"patch":"@@ -2049,0 +2049,76 @@\n+\/\/ Return the number of dwords pushed\n+int MacroAssembler::push_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  const int num_of_regs = PRegisterImpl::number_of_saved_registers;\n+  unsigned char regs[num_of_regs];\n+  int count = 0;\n+  for (int reg = 0; reg < num_of_regs; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_push_bytes = align_up(sve_predicate_size_in_slots *\n+                                  VMRegImpl::stack_slot_size * count, 16);\n+  sub(stack, stack, total_push_bytes);\n+  for (int i = 0; i < count; i++) {\n+    sve_str(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  return total_push_bytes \/ 8;\n+}\n+\n+\/\/ Return the number of dwords poped\n+int MacroAssembler::pop_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  const int num_of_regs = PRegisterImpl::number_of_saved_registers;\n+  unsigned char regs[num_of_regs];\n+  int count = 0;\n+  for (int reg = 0; reg < num_of_regs; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_pop_bytes = align_up(sve_predicate_size_in_slots *\n+                                 VMRegImpl::stack_slot_size * count, 16);\n+  for (int i = count - 1; i >= 0; i--) {\n+    sve_ldr(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  add(stack, stack, total_pop_bytes);\n+  return total_pop_bytes \/ 8;\n+}\n+\n@@ -2507,1 +2583,1 @@\n-                                    int sve_vector_size_in_bytes) {\n+                                    int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n@@ -2524,0 +2600,6 @@\n+  if (save_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    sub(sp, sp, total_predicate_in_bytes);\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      sve_str(as_PRegister(i), Address(sp, i));\n+    }\n+  }\n@@ -2527,1 +2609,7 @@\n-                                   int sve_vector_size_in_bytes) {\n+                                   int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n+  if (restore_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    for (int i = PRegisterImpl::number_of_saved_registers - 1; i >= 0; i--) {\n+      sve_ldr(as_PRegister(i), Address(sp, i));\n+    }\n+    add(sp, sp, total_predicate_in_bytes);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":90,"deletions":2,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -458,0 +458,3 @@\n+  int push_p(unsigned int bitset, Register stack);\n+  int pop_p(unsigned int bitset, Register stack);\n+\n@@ -469,0 +472,3 @@\n+  void push_p(PRegSet regs, Register stack) { if (regs.bits()) push_p(regs.bits(), stack); }\n+  void pop_p(PRegSet regs, Register stack) { if (regs.bits()) pop_p(regs.bits(), stack); }\n+\n@@ -868,1 +874,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                      int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -870,1 +876,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                     int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -1342,0 +1348,1 @@\n+\n@@ -1345,0 +1352,4 @@\n+  void spill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_str(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1355,0 +1366,1 @@\n+\n@@ -1358,0 +1370,4 @@\n+  void unspill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_ldr(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1380,0 +1396,6 @@\n+  void spill_copy_sve_predicate_stack_to_stack(int src_offset, int dst_offset,\n+                                               int sve_predicate_reg_size_in_bytes) {\n+    sve_ldr(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, src_offset));\n+    sve_str(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, dst_offset));\n+    reinitialize_ptrue();\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1826,0 +1826,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3428,0 +3428,2 @@\n+  case Op_LoadVectorGatherMasked:\n+  case Op_StoreVectorScatterMasked:\n@@ -3430,0 +3432,2 @@\n+  case Op_LoadVectorMask:\n+  case Op_StoreVectorMask:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -319,0 +319,1 @@\n+  bool inline_vector_mem_masked_operation(bool is_store);\n@@ -332,4 +333,5 @@\n-    VecMaskUseLoad,\n-    VecMaskUseStore,\n-    VecMaskUseAll,\n-    VecMaskNotUsed\n+    VecMaskUseLoad  = 1 << 0,\n+    VecMaskUseStore = 1 << 1,\n+    VecMaskUseAll   = VecMaskUseLoad | VecMaskUseStore,\n+    VecMaskUsePred  = 1 << 2,\n+    VecMaskNotUsed  = 1 << 3\n@@ -339,1 +341,1 @@\n-  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args = false);\n+  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, VectorMaskUseType mask_use_type, bool has_scalar_args = false);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -437,0 +437,18 @@\n+const int Matcher::scalable_predicate_reg_slots() {\n+  assert(Matcher::has_predicated_vectors() && Matcher::supports_scalable_vector(),\n+        \"scalable predicate vector should be supported\");\n+  int vector_reg_bit_size = Matcher::scalable_vector_reg_size(T_BYTE) << LogBitsPerByte;\n+  \/\/ We assume each predicate register is one-eighth of the size of\n+  \/\/ scalable vector register, one mask bit per vector byte.\n+  int predicate_reg_bit_size = vector_reg_bit_size >> 3;\n+  \/\/ Compute number of slots which is required when scalable predicate\n+  \/\/ register is spilled. E.g. if scalable vector register is 640 bits,\n+  \/\/ predicate register is 80 bits, which is 2.5 * slots.\n+  \/\/ We will round up the slot number to power of 2, which is required\n+  \/\/ by find_first_set().\n+  int slots = predicate_reg_bit_size & (BitsPerInt - 1)\n+              ? (predicate_reg_bit_size >> LogBitsPerInt) + 1\n+              : predicate_reg_bit_size >> LogBitsPerInt;\n+  return round_up_power_of_2(slots);\n+}\n+\n@@ -545,0 +563,2 @@\n+  } else {\n+    *idealreg2spillmask[Op_RegVectMask] = RegMask::Empty;\n@@ -617,0 +637,12 @@\n+    \/\/ Exclude last input arg stack slots to avoid spilling vector register there,\n+    \/\/ otherwise RegVectMask spills could stomp over stack slots in caller frame.\n+    for (; (in >= init_in) && (k < scalable_predicate_reg_slots()); k++) {\n+      scalable_stack_mask.Remove(in);\n+      in = OptoReg::add(in, -1);\n+    }\n+\n+    \/\/ For RegVectMask\n+    scalable_stack_mask.clear_to_sets(scalable_predicate_reg_slots());\n+    assert(scalable_stack_mask.is_AllStack(), \"should be infinite stack\");\n+    idealreg2spillmask[Op_RegVectMask]->OR(scalable_stack_mask);\n+\n@@ -2276,0 +2308,15 @@\n+  if (n->is_predicated_vector()) {\n+    \/\/ Restructure into binary trees for Matching.\n+    if (n->req() == 4) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, n->in(3));\n+      n->del_req(3);\n+    } else if (n->req() == 5) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, new BinaryNode(n->in(3), n->in(4)));\n+      n->del_req(4);\n+      n->del_req(3);\n+    }\n+    return;\n+  }\n+\n@@ -2415,0 +2462,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -2421,0 +2469,9 @@\n+    case Op_StoreVectorScatterMasked: {\n+      Node* pair = new BinaryNode(n->in(MemNode::ValueIn+1), n->in(MemNode::ValueIn+2));\n+      n->set_req(MemNode::ValueIn+1, pair);\n+      n->del_req(MemNode::ValueIn+2);\n+      pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+      n->set_req(MemNode::ValueIn, pair);\n+      n->del_req(MemNode::ValueIn+1);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -332,0 +332,2 @@\n+  static const bool match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt);\n+\n@@ -348,0 +350,2 @@\n+  \/\/ Actual max scalable predicate register length.\n+  static const int scalable_predicate_reg_slots();\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,40 +62,78 @@\n-bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args) {\n-    bool is_supported = true;\n-    \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n-    \/\/ since in this case shift needs to be broadcasted.\n-    if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n-         (has_scalar_args &&\n-           !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n-      is_supported = false;\n-    }\n-\n-    int lshiftopc, rshiftopc;\n-    switch(elem_bt) {\n-      case T_BYTE:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftB;\n-        break;\n-      case T_SHORT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftS;\n-        break;\n-      case T_INT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftI;\n-        break;\n-      case T_LONG:\n-        lshiftopc = Op_LShiftL;\n-        rshiftopc = Op_URShiftL;\n-        break;\n-      default:\n-        assert(false, \"Unexpected type\");\n-    }\n-    int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n-    int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n-    if (!is_supported &&\n-        arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n-      is_supported = true;\n-    }\n-    return is_supported;\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt,\n+                                                 VectorMaskUseType mask_use_type, bool has_scalar_args) {\n+  bool is_supported = true;\n+\n+  \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n+  \/\/ since in this case shift needs to be broadcasted.\n+  if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n+       (has_scalar_args &&\n+         !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+    is_supported = false;\n+  }\n+\n+  if (is_supported) {\n+    \/\/ Check whether mask unboxing is supported.\n+    if ((mask_use_type & VecMaskUseLoad) != 0) {\n+      if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** Rejected vector mask loading (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[Op_VectorLoadMask], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+\n+    if ((mask_use_type & VecMaskUsePred) != 0) {\n+      if (!Matcher::has_predicated_vectors() ||\n+          !Matcher::match_rule_supported_vector_masked(opc, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[opc], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+  }\n+\n+  int lshiftopc, rshiftopc;\n+  switch(elem_bt) {\n+    case T_BYTE:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftB;\n+      break;\n+    case T_SHORT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftS;\n+      break;\n+    case T_INT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftI;\n+      break;\n+    case T_LONG:\n+      lshiftopc = Op_LShiftL;\n+      rshiftopc = Op_URShiftL;\n+      break;\n+    default:\n+      assert(false, \"Unexpected type\");\n+  }\n+  int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n+  int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n+  if (!is_supported &&\n+      arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n+    is_supported = true;\n+  }\n+  return is_supported;\n@@ -118,1 +156,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -133,1 +171,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -158,1 +196,1 @@\n-    if(!arch_supports_vector_rotate(sopc, num_elem, type, has_scalar_args)) {\n+    if(!arch_supports_vector_rotate(sopc, num_elem, type, mask_use_type, has_scalar_args)) {\n@@ -216,1 +254,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseLoad) {\n+  if ((mask_use_type & VecMaskUseLoad) != 0) {\n@@ -229,1 +267,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseStore) {\n+  if ((mask_use_type & VecMaskUseStore) != 0) {\n@@ -241,6 +279,12 @@\n-  return true;\n-}\n-\n-static bool is_vector_mask(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n-}\n+  if ((mask_use_type & VecMaskUsePred) != 0) {\n+    if (!Matcher::has_predicated_vectors() ||\n+        !Matcher::match_rule_supported_vector_masked(sopc, num_elem, type)) {\n+    #ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                      NodeClassNames[sopc], type2name(type), num_elem);\n+      }\n+    #endif\n+      return false;\n+    }\n+  }\n@@ -248,2 +292,1 @@\n-static bool is_vector_shuffle(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+  return true;\n@@ -262,4 +305,4 @@\n-\/\/ <VM>\n-\/\/ VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/            VM vm,\n-\/\/            Function<VM, VM> defaultImpl) {\n+\/\/ <V, M>\n+\/\/ V unaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/           int length, V v, M m,\n+\/\/           UnaryOperation<V, M> defaultImpl)\n@@ -268,4 +311,4 @@\n-\/\/ <VM>\n-\/\/ VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/             VM vm1, VM vm2,\n-\/\/             BiFunction<VM, VM, VM> defaultImpl) {\n+\/\/ <V, M>\n+\/\/ V binaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/            int length, V v1, V v2, M m,\n+\/\/            BinaryOperation<V, M> defaultImpl)\n@@ -274,4 +317,4 @@\n-\/\/ <VM>\n-\/\/ VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/              VM vm1, VM vm2, VM vm3,\n-\/\/              TernaryOperation<VM> defaultImpl) {\n+\/\/ <V, M>\n+\/\/ V ternaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/             int length, V v1, V v2, V v3, M m,\n+\/\/             TernaryOperation<V, M> defaultImpl)\n@@ -282,2 +325,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -291,2 +335,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -296,0 +340,1 @@\n+\n@@ -309,0 +354,28 @@\n+\n+  \/\/ \"argument(n + 5)\" should be the mask object. We assume it is \"null\" when no mask\n+  \/\/ is used to control this operation.\n+  const Type* vmask_type = gvn().type(argument(n + 5));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -331,0 +404,4 @@\n+  if (is_vector_mask(vbox_klass)) {\n+    assert(!is_masked_op, \"mask operations do not need mask to control\");\n+  }\n+\n@@ -353,3 +430,4 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if ((sopc != 0) &&\n-      !arch_supports_vector(sopc, num_elem, elem_bt, is_vector_mask(vbox_klass) ? VecMaskUseAll : VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  VectorMaskUseType mask_use_type = is_vector_mask(vbox_klass) ? VecMaskUseAll\n+                                      : is_masked_op ? VecMaskUseLoad : VecMaskNotUsed;\n+  if ((sopc != 0) && !arch_supports_vector(sopc, num_elem, elem_bt, mask_use_type)) {\n@@ -357,1 +435,1 @@\n-      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d is_masked_op=%d\",\n@@ -359,1 +437,1 @@\n-                    is_vector_mask(vbox_klass) ? 1 : 0);\n+                    is_vector_mask(vbox_klass) ? 1 : 0, is_masked_op ? 1 : 0);\n@@ -364,0 +442,10 @@\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && sopc != 0 && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=0 is_masked_op=1\",\n+                    n, sopc, num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -367,1 +455,1 @@\n-      opd3 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      opd3 = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n@@ -371,1 +459,1 @@\n-                        NodeClassNames[argument(6)->Opcode()]);\n+                        NodeClassNames[argument(7)->Opcode()]);\n@@ -378,1 +466,1 @@\n-      opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      opd2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n@@ -382,1 +470,1 @@\n-                        NodeClassNames[argument(5)->Opcode()]);\n+                        NodeClassNames[argument(6)->Opcode()]);\n@@ -389,1 +477,1 @@\n-      opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+      opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -393,1 +481,1 @@\n-                        NodeClassNames[argument(4)->Opcode()]);\n+                        NodeClassNames[argument(5)->Opcode()]);\n@@ -402,0 +490,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(n + 5), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(n + 5)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -420,1 +523,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, vt, is_vector_mask(vbox_klass));\n@@ -424,1 +527,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, opd3, vt);\n@@ -430,0 +533,11 @@\n+\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n+\n@@ -515,1 +629,2 @@\n-    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vt));\n+    const TypeVect* vmask_type = TypeVect::makemask(elem_bt, num_elem);\n+    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vmask_type));\n@@ -698,1 +813,0 @@\n-\n@@ -725,1 +839,1 @@\n-  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n+  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt), is_vector_mask(vbox_klass));\n@@ -762,1 +876,1 @@\n-\/\/               StoreVectorOperation<C, V> defaultImpl) {\n+\/\/               StoreVectorOperation<C, V> defaultImpl)\n@@ -926,2 +1040,1 @@\n-        const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n-        vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n+        vload = gvn().transform(new VectorLoadMaskNode(vload, TypeVect::makemask(elem_bt, num_elem)));\n@@ -946,6 +1059,14 @@\n-\/\/   <C, V extends Vector<?>, W extends IntVector, E, S extends VectorSpecies<E>>\n-\/\/   void loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n-\/\/                    Object base, long offset, \/\/ Unsafe addressing\n-\/\/                    W index_vector,\n-\/\/                    C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n-\/\/                    LoadVectorOperationWithMap<C, V, E, S> defaultImpl)\n+\/\/    <C, V, E, S extends VectorSpecies<E>,\n+\/\/     M extends VectorMask<E>>\n+\/\/    V loadMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/                 int length, Object base, long offset, M m\n+\/\/                 C container, int index, S s,  \/\/ Arguments for default implementation\n+\/\/                 LoadVectorMaskedOperation<C, V, E, S, M> defaultImpl) {\n+\/\/\n+\/\/    <C, V extends Vector<?>,\n+\/\/     M extends VectorMask<?>>\n+\/\/    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                     int length, Object base, long offset,\n+\/\/                     V v, M m,\n+\/\/                     C container, int index,  \/\/ Arguments for default implementation\n+\/\/                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n@@ -953,4 +1074,228 @@\n-\/\/    <C, V extends Vector<?>, W extends IntVector>\n-\/\/    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-\/\/                      Object base, long offset,    \/\/ Unsafe addressing\n-\/\/                      W index_vector, V v,\n+bool LibraryCallKit::inline_vector_mem_masked_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+\n+  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+\n+  \/\/ Save state and restore on bailout\n+  uint old_sp = sp();\n+  SafePointNode* old_map = clone_map();\n+\n+  Node* addr = make_unsafe_address(base, offset, elem_bt, true);\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+\n+  \/\/ Now handle special case where load\/store happens from\/to byte array but element type is not byte.\n+  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  \/\/ If there is no consistency between array and vector element types, it must be special byte array case\n+  if (arr_type != NULL && !using_byte_array && !elem_consistent_with_arr(elem_bt, arr_type)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s atype=%s\",\n+                    is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                    num_elem, type2name(elem_bt), type2name(arr_type->elem()->array_element_basic_type()));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  int mem_num_elem = using_byte_array ? num_elem * type2aelembytes(elem_bt) : num_elem;\n+  BasicType mem_elem_bt = using_byte_array ? T_BYTE : elem_bt;\n+  bool use_predicate = arch_supports_vector(is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked,\n+                                            mem_num_elem, mem_elem_bt,\n+                                            (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred));\n+  \/\/ Masked vector store operation needs the architecture predicate feature. We need to check\n+  \/\/ whether the predicated vector operation is supported by backend.\n+  if (is_store && !use_predicate) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=storeMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ This only happens for masked vector load. If predicate is not supported, then check whether\n+  \/\/ the normal vector load and blend operations are supported by backend.\n+  if (!use_predicate && (!arch_supports_vector(Op_LoadVector, mem_num_elem, mem_elem_bt, VecMaskNotUsed) ||\n+      !arch_supports_vector(Op_VectorBlend, mem_num_elem, mem_elem_bt, VecMaskUseLoad))) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=loadMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Since we are using byte array, we need to double check that the vector reinterpret operation\n+  \/\/ with byte type is supported by backend.\n+  if (using_byte_array) {\n+    if (!arch_supports_vector(Op_VectorReinterpret, mem_num_elem, T_BYTE, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s using_byte_array=1\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Since it needs to unbox the mask, we need to double check that the related load operations\n+  \/\/ for mask are supported by backend.\n+  if (!arch_supports_vector(Op_LoadVector, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(!is_vector_mask(vbox_klass) && is_vector_mask(mbox_klass), \"Invalid class type\");\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* mask = unbox_vector(is_store ? argument(8) : argument(7), mbox_type, elem_bt, num_elem);\n+  if (mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_store ? NodeClassNames[argument(8)->Opcode()]\n+                             : NodeClassNames[argument(7)->Opcode()]);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false; \/\/ operand unboxing failed\n+    }\n+    set_all_memory(reset_memory());\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the incoming vector to byte vector.\n+      const TypeVect* to_vect_type = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      val = gvn().transform(new VectorReinterpretNode(val, val->bottom_type()->is_vect(), to_vect_type));\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+    Node* vstore = gvn().transform(new StoreVectorMaskedNode(control(), memory(addr), addr, val, addr_type, mask));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    Node* vload = NULL;\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+\n+    if (use_predicate) {\n+      \/\/ Generate masked load vector node if predicate feature is supported.\n+      const TypeVect* vt = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      vload = gvn().transform(new LoadVectorMaskedNode(control(), memory(addr), addr, addr_type, vt, mask));\n+    } else {\n+      \/\/ Use the vector blend to implement the masked load vector. The biased elements are zeros.\n+      Node* zero = gvn().transform(gvn().zerocon(mem_elem_bt));\n+      zero = gvn().transform(VectorNode::scalar2vector(zero, mem_num_elem, Type::get_const_basic_type(mem_elem_bt)));\n+      vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, mem_num_elem, mem_elem_bt));\n+      vload = gvn().transform(new VectorBlendNode(zero, vload, mask));\n+    }\n+\n+    if (using_byte_array) {\n+      const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+      vload = gvn().transform(new VectorReinterpretNode(vload, vload->bottom_type()->is_vect(), to_vect_type));\n+    }\n+\n+    Node* box = box_vector(vload, vbox_type, elem_bt, num_elem);\n+    set_result(box);\n+  }\n+\n+  old_map->destruct(&_gvn);\n+\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/   <C, V extends Vector<?>, W extends Vector<Integer>, E,\n+\/\/    S extends VectorSpecies<E>, M extends VectorMask<E>>\n+\/\/   V loadWithMap(Class<?> vectorClass, Class<M> maskClass, Class<E> E, int length,\n+\/\/                 Class<?> vectorIndexClass,\n+\/\/                 Object base, long offset, \/\/ Unsafe addressing\n+\/\/                 W index_vector, M m,\n+\/\/                 C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+\/\/                 LoadVectorOperationWithMap<C, V, E, S, M> defaultImpl)\n+\/\/\n+\/\/    <C, V extends Vector<?>, W extends Vector<Integer>, M extends VectorMask<?>>\n+\/\/    void storeWithMap(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                      int length, Class<?> vectorIndexClass, Object base, long offset,    \/\/ Unsafe addressing\n+\/\/                      W index_vector, V v, M m,\n@@ -958,1 +1303,1 @@\n-\/\/                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+\/\/                      StoreVectorOperationWithMap<C, V, M> defaultImpl)\n@@ -962,3 +1307,4 @@\n-  const TypeInstPtr* elem_klass       = gvn().type(argument(1))->isa_instptr();\n-  const TypeInt*     vlen             = gvn().type(argument(2))->isa_int();\n-  const TypeInstPtr* vector_idx_klass = gvn().type(argument(3))->isa_instptr();\n+  const TypeInstPtr* mask_klass       = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass       = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen             = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* vector_idx_klass = gvn().type(argument(4))->isa_instptr();\n@@ -971,2 +1317,2 @@\n-                    NodeClassNames[argument(1)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -984,0 +1330,1 @@\n+\n@@ -991,0 +1338,1 @@\n+\n@@ -994,5 +1342,43 @@\n-  if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=no\",\n-                    is_scatter, is_scatter ? \"scatter\" : \"gather\",\n-                    num_elem, type2name(elem_bt));\n+  const Type* vmask_type = gvn().type(is_scatter ? argument(10) : argument(9));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(1)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Check whether the predicated gather\/scatter node is supported by architecture.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n+                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=1\",\n+                      is_scatter, is_scatter ? \"scatterMasked\" : \"gatherMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+  } else {\n+    \/\/ Check whether the normal gather\/scatter node is supported for non-masked operation.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=0\",\n+                      is_scatter, is_scatter ? \"scatter\" : \"gather\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -1000,1 +1386,0 @@\n-    return false; \/\/ not supported\n@@ -1006,1 +1391,1 @@\n-        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int ismask=no\",\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int is_masked_op=%d\",\n@@ -1008,1 +1393,1 @@\n-                      num_elem);\n+                      num_elem, is_masked_op ? 1 : 0);\n@@ -1011,1 +1396,1 @@\n-    }\n+  }\n@@ -1013,2 +1398,2 @@\n-  Node* base = argument(4);\n-  Node* offset = ConvL2X(argument(5));\n+  Node* base = argument(5);\n+  Node* offset = ConvL2X(argument(6));\n@@ -1036,0 +1421,1 @@\n+\n@@ -1038,2 +1424,0 @@\n-\n-\n@@ -1048,2 +1432,1 @@\n-\n-  Node* index_vect = unbox_vector(argument(7), vbox_idx_type, T_INT, num_elem);\n+  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n@@ -1055,0 +1438,18 @@\n+\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(is_scatter ? argument(10) : argument(9), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_scatter ? NodeClassNames[argument(10)->Opcode()]\n+                               : NodeClassNames[argument(9)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n@@ -1057,1 +1458,1 @@\n-    Node* val = unbox_vector(argument(8), vbox_type, elem_bt, num_elem);\n+    Node* val = unbox_vector(argument(9), vbox_type, elem_bt, num_elem);\n@@ -1065,1 +1466,6 @@\n-    Node* vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    Node* vstore = NULL;\n+    if (mask != NULL) {\n+      vstore = gvn().transform(new StoreVectorScatterMaskedNode(control(), memory(addr), addr, addr_type, val, index_vect, mask));\n+    } else {\n+      vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    }\n@@ -1068,2 +1474,6 @@\n-    Node* vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n-\n+    Node* vload = NULL;\n+    if (mask != NULL) {\n+      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+    } else {\n+      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+    }\n@@ -1080,5 +1490,5 @@\n-\/\/ <V extends Vector<?,?>>\n-\/\/ long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/                       V v,\n-\/\/                       Function<V,Long> defaultImpl)\n-\n+\/\/ <V, M>\n+\/\/ long reductionCoerced(int oprId, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                       Class<?> elementType, int length, V v, M m,\n+\/\/                       ReductionOperation<V, M> defaultImpl)\n+\/\/\n@@ -1088,2 +1498,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1097,2 +1508,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1115,0 +1526,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(6));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1117,1 +1554,0 @@\n-\n@@ -1121,2 +1557,12 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, is_masked_op ? VecMaskUseLoad : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=%d\",\n+                    sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n@@ -1124,1 +1570,1 @@\n-      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s ismask=no\",\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=1\",\n@@ -1133,1 +1579,1 @@\n-  Node* opd = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* opd = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1138,0 +1584,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(6), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(6)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1139,1 +1600,16 @@\n-  Node* rn = gvn().transform(ReductionNode::make(opc, NULL, init, opd, elem_bt));\n+  Node* value = NULL;\n+  if (mask == NULL) {\n+    assert(!is_masked_op, \"Masked op needs the mask value never null\");\n+    value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+  } else {\n+    if (use_predicate) {\n+      value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+      value->add_req(mask);\n+      value->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      Node* reduce_identity = gvn().transform(VectorNode::scalar2vector(init, num_elem, Type::get_const_basic_type(elem_bt)));\n+      value = gvn().transform(new VectorBlendNode(reduce_identity, opd, mask));\n+      value = ReductionNode::make(opc, NULL, init, value, elem_bt);\n+    }\n+  }\n+  value = gvn().transform(value);\n@@ -1146,1 +1622,1 @@\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      bits = gvn().transform(new ConvI2LNode(value));\n@@ -1150,2 +1626,2 @@\n-      rn   = gvn().transform(new MoveF2INode(rn));\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      value = gvn().transform(new MoveF2INode(value));\n+      bits  = gvn().transform(new ConvI2LNode(value));\n@@ -1155,1 +1631,1 @@\n-      bits = gvn().transform(new MoveD2LNode(rn));\n+      bits = gvn().transform(new MoveD2LNode(value));\n@@ -1159,1 +1635,1 @@\n-      bits = rn; \/\/ no conversion needed\n+      bits = value; \/\/ no conversion needed\n@@ -1171,1 +1647,1 @@\n-\/\/                                BiFunction<V, V, Boolean> defaultImpl) {\n+\/\/                                BiFunction<V, V, Boolean> defaultImpl)\n@@ -1235,1 +1711,1 @@\n-\/\/         VectorBlendOp<V,M> defaultImpl) { ...\n+\/\/         VectorBlendOp<V,M> defaultImpl)\n@@ -1307,2 +1783,2 @@\n-\/\/            V v1, V v2,\n-\/\/            VectorCompareOp<V,M> defaultImpl) { ...\n+\/\/            V v1, V v2, M m,\n+\/\/            VectorCompareOp<V,M> defaultImpl)\n@@ -1377,0 +1853,19 @@\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  Node* mask = is_masked_op ? unbox_vector(argument(7), mbox_type, elem_bt, num_elem) : NULL;\n+  if (is_masked_op && mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: mask = null arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  bool use_predicate = is_masked_op && arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_AndV, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -1383,2 +1878,14 @@\n-  const TypeVect* vt = TypeVect::make(mask_bt, num_elem);\n-  Node* operation = gvn().transform(new VectorMaskCmpNode(pred, v1, v2, pred_node, vt));\n+  const TypeVect* vmask_type = TypeVect::makemask(mask_bt, num_elem);\n+  Node* operation = new VectorMaskCmpNode(pred, v1, v2, pred_node, vmask_type);\n+\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = VectorNode::make(Op_AndV, operation, mask, vmask_type);\n+    }\n+  }\n+\n+  operation = gvn().transform(operation);\n@@ -1393,5 +1900,7 @@\n-\/\/ <V extends Vector, Sh extends Shuffle>\n-\/\/  V rearrangeOp(Class<V> vectorClass, Class<Sh> shuffleClass, Class< ? > elementType, int vlen,\n-\/\/    V v1, Sh sh,\n-\/\/    VectorSwizzleOp<V, Sh, S, E> defaultImpl) { ...\n-\n+\/\/ <V extends Vector<E>,\n+\/\/     Sh extends VectorShuffle<E>,\n+\/\/     M extends VectorMask<E>,\n+\/\/     E>\n+\/\/ V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+\/\/               V v1, Sh sh, M m,\n+\/\/               VectorRearrangeOp<V,Sh,M,E> defaultImpl)\n@@ -1401,2 +1910,3 @@\n-  const TypeInstPtr* elem_klass    = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen          = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass    = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass    = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen          = gvn().type(argument(4))->isa_int();\n@@ -1404,1 +1914,1 @@\n-  if (vector_klass == NULL || shuffle_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (vector_klass == NULL  || shuffle_klass == NULL ||  elem_klass == NULL || vlen == NULL) {\n@@ -1407,2 +1917,4 @@\n-  if (shuffle_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n-    elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (shuffle_klass->const_oop() == NULL ||\n+      vector_klass->const_oop()  == NULL ||\n+      elem_klass->const_oop()    == NULL ||\n+      !vlen->is_con()) {\n@@ -1413,2 +1925,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1418,1 +1930,2 @@\n-  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(shuffle_klass)) {\n+  if (!is_klass_initialized(vector_klass)  ||\n+      !is_klass_initialized(shuffle_klass)) {\n@@ -1442,1 +1955,7 @@\n-  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed)) {\n+\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  bool use_predicate = is_masked_op;\n+  if (is_masked_op &&\n+      (mask_klass == NULL ||\n+       mask_klass->const_oop() == NULL ||\n+       !is_klass_initialized(mask_klass))) {\n@@ -1444,2 +1963,15 @@\n-      tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n-                    num_elem, type2name(elem_bt));\n+      tty->print_cr(\"  ** mask_klass argument not initialized\");\n+    }\n+  }\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, checkFlags)) {\n+    use_predicate = false;\n+    if(!is_masked_op ||\n+       (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed) ||\n+        !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)     ||\n+        !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -1447,1 +1979,0 @@\n-    return false; \/\/ not supported\n@@ -1455,2 +1986,2 @@\n-  Node* v1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n-  Node* shuffle = unbox_vector(argument(5), shbox_type, shuffle_bt, num_elem);\n+  Node* v1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* shuffle = unbox_vector(argument(6), shbox_type, shuffle_bt, num_elem);\n@@ -1462,1 +1993,28 @@\n-  Node* rearrange = gvn().transform(new VectorRearrangeNode(v1, shuffle));\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=3 op=shuffle\/rearrange vlen=%d etype=%s ismask=useload is_masked_op=1\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* rearrange = new VectorRearrangeNode(v1, shuffle);\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      rearrange->add_req(mask);\n+      rearrange->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      const TypeVect* vt = v1->bottom_type()->is_vect();\n+      rearrange = gvn().transform(rearrange);\n+      Node* zero = gvn().makecon(TypeInt::ZERO);\n+      Node* zerovec = gvn().transform(VectorNode::scalar2vector(zero, num_elem, Type::get_const_basic_type(elem_bt)));\n+      rearrange = new VectorBlendNode(zerovec, rearrange, mask);\n+    }\n+  }\n+  rearrange = gvn().transform(rearrange);\n@@ -1528,4 +2086,5 @@\n-\/\/  <V extends Vector<?,?>>\n-\/\/  V broadcastInt(int opr, Class<V> vectorClass, Class<?> elementType, int vlen,\n-\/\/                 V v, int i,\n-\/\/                 VectorBroadcastIntOp<V> defaultImpl) {\n+\/\/  <V extends Vector<?>, M>\n+\/\/  V broadcastInt(int opr, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                 Class<?> elementType, int length,\n+\/\/                 V v, int n, M m,\n+\/\/                 VectorBroadcastIntOp<V> defaultImpl)\n@@ -1536,2 +2095,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1547,2 +2107,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1558,0 +2118,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(7));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1565,1 +2151,1 @@\n-  BasicType elem_bt = elem_type->basic_type();\n+\n@@ -1567,0 +2153,1 @@\n+  BasicType elem_bt = elem_type->basic_type();\n@@ -1568,0 +2155,1 @@\n+\n@@ -1570,0 +2158,1 @@\n+\n@@ -1576,0 +2165,1 @@\n+\n@@ -1583,1 +2173,2 @@\n-  Node* cnt  = argument(5);\n+\n+  Node* cnt  = argument(6);\n@@ -1592,4 +2183,15 @@\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s ismask=no\",\n-                    sopc, num_elem, type2name(elem_bt));\n+\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  bool use_predicate = is_masked_op;\n+\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, checkFlags, has_scalar_args)) {\n+    use_predicate = false;\n+    if (!is_masked_op ||\n+        (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) ||\n+         !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad))) {\n+\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s is_masked_op=%d\",\n+                      sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+      }\n+      return false; \/\/ not supported\n@@ -1597,2 +2199,2 @@\n-    return false; \/\/ not supported\n-  Node* opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+\n+  Node* opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1613,0 +2215,1 @@\n+\n@@ -1617,1 +2220,23 @@\n-  Node* operation = gvn().transform(VectorNode::make(opc, opd1, opd2, num_elem, elem_bt));\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\", NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* operation = VectorNode::make(opc, opd1, opd2, num_elem, elem_bt);\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n@@ -1632,1 +2257,1 @@\n-\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl)\n@@ -1741,2 +2366,2 @@\n-  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from);\n-  const TypeVect* dst_type = TypeVect::make(elem_bt_to,   num_elem_to);\n+  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from, is_mask);\n+  const TypeVect* dst_type = TypeVect::make(elem_bt_to, num_elem_to, is_mask);\n@@ -1822,1 +2447,1 @@\n-\/\/           VecInsertOp<V> defaultImpl) {\n+\/\/           VecInsertOp<V> defaultImpl)\n@@ -1915,1 +2540,1 @@\n-\/\/               VecExtractOp<V> defaultImpl) {\n+\/\/               VecExtractOp<V> defaultImpl)\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":816,"deletions":191,"binary":false,"changes":1007,"status":"modified"},{"patch":"@@ -445,0 +445,25 @@\n+VectorNode* VectorNode::make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n+  guarantee(vopc > 0, \"vopc must be > 0\");\n+  const TypeVect* vmask_type = TypeVect::makemask(bt, vlen);\n+  switch (vopc) {\n+    case Op_AndV:\n+      if (Matcher::match_rule_supported_vector(Op_AndVMask, vlen, bt)) {\n+        return new AndVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new AndVNode(n1, n2, vmask_type);\n+    case Op_OrV:\n+      if (Matcher::match_rule_supported_vector(Op_OrVMask, vlen, bt)) {\n+        return new OrVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new OrVNode(n1, n2, vmask_type);\n+    case Op_XorV:\n+      if (Matcher::match_rule_supported_vector(Op_XorVMask, vlen, bt)) {\n+        return new XorVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new XorVNode(n1, n2, vmask_type);\n+    default:\n+      fatal(\"Unsupported mask vector creation for '%s'\", NodeClassNames[vopc]);\n+      return NULL;\n+  }\n+}\n+\n@@ -446,1 +471,1 @@\n-VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt) {\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask) {\n@@ -449,0 +474,5 @@\n+\n+  if (is_mask) {\n+    return make_mask_node(vopc, n1, n2, vt->length(), vt->element_basic_type());\n+  }\n+\n@@ -555,1 +585,1 @@\n-VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t) {\n+VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask) {\n@@ -557,2 +587,7 @@\n-  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen)\n-                                          : TypeVect::make(bt, vlen);\n+  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen, is_mask)\n+                                          : TypeVect::make(bt, vlen, is_mask);\n+\n+  if (is_mask && Matcher::match_rule_supported_vector(Op_MaskAll, vlen, bt)) {\n+    return new MaskAllNode(s, vt);\n+  }\n+\n@@ -752,0 +787,17 @@\n+Node* StoreVectorNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ StoreVector (VectorStoreMask src)  ==>  (StoreVectorMask src).\n+  Node* value = in(MemNode::ValueIn);\n+  if (value->Opcode() == Op_VectorStoreMask) {\n+    assert(vect_type()->element_basic_type() == T_BOOLEAN, \"Invalid basic type to store mask\");\n+    const TypeVect* type = value->in(1)->bottom_type()->is_vect();\n+    if (Matcher::match_rule_supported_vector(Op_StoreVectorMask, type->length(), type->element_basic_type())) {\n+      const TypeVect* mem_type = TypeVect::make(T_BOOLEAN, type->length());\n+      return new StoreVectorMaskNode(in(MemNode::Control),\n+                                     in(MemNode::Memory),\n+                                     in(MemNode::Address),\n+                                     adr_type(), value->in(1), mem_type);\n+    }\n+  }\n+  return StoreNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -1007,0 +1059,15 @@\n+Node* VectorLoadMaskNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ (VectorLoadMask (LoadVector mem))  ==> (LoadVectorMask mem)\n+  LoadVectorNode* load = this->in(1)->isa_LoadVector();\n+  BasicType out_bt = vect_type()->element_basic_type();\n+  if (load != NULL &&\n+      Matcher::match_rule_supported_vector(Op_LoadVectorMask, length(), out_bt)) {\n+    const TypeVect* mem_type = TypeVect::make(T_BOOLEAN, length());\n+    return new LoadVectorMaskNode(load->in(MemNode::Control),\n+                                  load->in(MemNode::Memory),\n+                                  load->in(MemNode::Address),\n+                                  load->adr_type(), vect_type(), mem_type);\n+  }\n+  return NULL;\n+}\n+\n@@ -1108,0 +1175,1 @@\n+          return gvn.makecon(TypeInt::make(max_jbyte));\n@@ -1109,0 +1177,1 @@\n+          return gvn.makecon(TypeInt::make(max_jshort));\n@@ -1123,0 +1192,1 @@\n+          return gvn.makecon(TypeInt::make(min_jbyte));\n@@ -1124,0 +1194,1 @@\n+          return gvn.makecon(TypeInt::make(min_jshort));\n@@ -1316,0 +1387,1 @@\n+          const TypeVect* vmask_type = TypeVect::makemask(out_vt->element_basic_type(), out_vt->length());\n@@ -1321,1 +1393,1 @@\n-            return new VectorMaskCastNode(value, out_vt);\n+            return new VectorMaskCastNode(value, vmask_type);\n@@ -1325,1 +1397,1 @@\n-          return new VectorLoadMaskNode(value, out_vt);\n+          return new VectorLoadMaskNode(value, vmask_type);\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":78,"deletions":6,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -69,1 +69,6 @@\n-  virtual uint ideal_reg() const { return Matcher::vector_ideal_reg(vect_type()->length_in_bytes()); }\n+  virtual uint ideal_reg() const {\n+    if (vect_type()->isa_vectmask()) {\n+      return Op_RegVectMask;\n+    }\n+    return Matcher::vector_ideal_reg(vect_type()->length_in_bytes());\n+  }\n@@ -71,1 +76,1 @@\n-  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t);\n+  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask = false);\n@@ -74,1 +79,1 @@\n-  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt);\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask = false);\n@@ -77,0 +82,1 @@\n+  static VectorNode* make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt);\n@@ -776,0 +782,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -801,2 +808,2 @@\n-                                                     idx == MemNode::ValueIn ||\n-                                                     idx == MemNode::ValueIn + 1; }\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1; }\n@@ -811,1 +818,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -831,1 +838,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -845,0 +852,39 @@\n+\/\/-------------------------------LoadVectorGatherMaskedNode---------------------------------\n+\/\/ Load Vector from memory via index map under the influence of a predicate register(mask).\n+class LoadVectorGatherMaskedNode : public LoadVectorNode {\n+ public:\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+    : LoadVectorNode(c, mem, adr, at, vt) {\n+    init_class_id(Class_LoadVector);\n+    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+    add_req(indices);\n+    add_req(mask);\n+    assert(req() == MemNode::ValueIn + 2, \"match_edge expects that last input is in MemNode::ValueIn+1\");\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                   idx == MemNode::ValueIn ||\n+                                                   idx == MemNode::ValueIn + 1; }\n+};\n+\n+\/\/------------------------------StoreVectorScatterMaskedNode--------------------------------\n+\/\/ Store Vector into memory via index map under the influence of a predicate register(mask).\n+class StoreVectorScatterMaskedNode : public StoreVectorNode {\n+  public:\n+   StoreVectorScatterMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* indices, Node* mask)\n+     : StoreVectorNode(c, mem, adr, at, val) {\n+     init_class_id(Class_StoreVector);\n+     assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+     assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+     add_req(indices);\n+     add_req(mask);\n+     assert(req() == MemNode::ValueIn + 3, \"match_edge expects that last input is in MemNode::ValueIn+2\");\n+   }\n+   virtual int Opcode() const;\n+   virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1 ||\n+                                                    idx == MemNode::ValueIn + 2; }\n+};\n@@ -859,1 +905,0 @@\n-\n@@ -916,0 +961,72 @@\n+class LoadVectorMaskNode : public LoadVectorNode {\n+ private:\n+  \/**\n+   * The type of the accessed memory, whose basic element type is T_BOOLEAN for mask vector.\n+   * It is different with the basic element type of the node, which can be T_BYTE, T_SHORT,\n+   * T_INT, T_LONG, T_FLOAT or T_DOUBLE.\n+   **\/\n+  const TypeVect* _mem_type;\n+\n+ public:\n+  LoadVectorMaskNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, const TypeVect* mt)\n+   : LoadVectorNode(c, mem, adr, at, vt), _mem_type(mt) {\n+    assert(_mem_type->element_basic_type() == T_BOOLEAN, \"Memory type must be T_BOOLEAN\");\n+    init_class_id(Class_LoadVector);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual int memory_size() const { return _mem_type->length_in_bytes(); }\n+  virtual int store_Opcode() const { return Op_StoreVectorMask; }\n+  virtual uint ideal_reg() const  { return vect_type()->ideal_reg(); }\n+  virtual uint size_of() const { return sizeof(LoadVectorMaskNode); }\n+};\n+\n+class StoreVectorMaskNode : public StoreVectorNode {\n+ private:\n+  \/**\n+   * The type of the accessed memory, whose basic element type is T_BOOLEAN for mask vector.\n+   * It is different with the basic element type of the src value, which can be T_BYTE, T_SHORT,\n+   * T_INT, T_LONG, T_FLOAT or T_DOUBLE.\n+   **\/\n+  const TypeVect* _mem_type;\n+\n+ public:\n+  StoreVectorMaskNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* src, const TypeVect* mt)\n+   : StoreVectorNode(c, mem, adr, at, src), _mem_type(mt) {\n+    assert(_mem_type->element_basic_type() == T_BOOLEAN, \"Memory type must be T_BOOLEAN\");\n+    init_class_id(Class_StoreVector);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual int memory_size() const { return _mem_type->length_in_bytes(); }\n+  virtual uint size_of() const { return sizeof(StoreVectorMaskNode); }\n+};\n+\n+\/\/-------------------------- Vector mask broadcast -----------------------------------\n+class MaskAllNode : public VectorNode {\n+ public:\n+  MaskAllNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical and --------------------------------\n+class AndVMaskNode : public VectorNode {\n+ public:\n+  AndVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical or ---------------------------------\n+class OrVMaskNode : public VectorNode {\n+ public:\n+  OrVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical xor --------------------------------\n+class XorVMaskNode : public VectorNode {\n+ public:\n+  XorVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -1290,0 +1407,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":126,"deletions":8,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -428,0 +428,43 @@\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                               int length, ByteBuffer bb, int offset, M m, S s,\n+                               VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, E, S, M> defaultImpl) {\n+        try {\n+            return loadFromByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    bb, offset, m,\n+                    s,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope, Class<? extends V> vmClass,\n+                                     Class<M> maskClass, Class<E> e, int length,\n+                                     ByteBuffer bb, int offset, M m,\n+                                     S s,\n+                                     VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, E, S, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            return VectorSupport.loadMasked(vmClass, maskClass, e, length,\n+                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset), m,\n+                    bb, offset, s,\n+                    defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n+\n@@ -473,0 +516,42 @@\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                                   int length, V v, M m,\n+                                   ByteBuffer bb, int offset,\n+                                   VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            storeIntoByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    v, m,\n+                    bb, offset,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope,\n+                                         Class<? extends V> vmClass, Class<M> maskClass,\n+                                         Class<E> e, int length, V v, M m,\n+                                         ByteBuffer bb, int offset,\n+                                         VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            VectorSupport.storeMasked(vmClass, maskClass, e, length,\n+                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset),\n+                    v, m,\n+                    bb, offset,\n+                    defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/X-ScopedMemoryAccess.java.template","additions":85,"deletions":0,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -219,4 +219,4 @@\n-    <V extends Vector<?>>\n-    long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int length,\n-                          V v,\n-                          Function<V,Long> defaultImpl) {\n+    <V, M>\n+    long reductionCoerced(int oprId, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+                          Class<?> elementType, int length, V v, M m,\n+                          ReductionOperation<V, M> defaultImpl) {\n@@ -224,1 +224,5 @@\n-        return defaultImpl.apply(v);\n+        return defaultImpl.apply(v, m);\n+    }\n+\n+    public interface ReductionOperation<V, M> {\n+        long apply(V v, M mask);\n@@ -227,0 +231,1 @@\n+\n@@ -263,4 +268,4 @@\n-    <VM>\n-    VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-               VM vm,\n-               Function<VM, VM> defaultImpl) {\n+    <V, M>\n+    V unaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+              Class<?> elementType, int length, V v, M m,\n+              UnaryOperation<V, M> defaultImpl) {\n@@ -268,1 +273,5 @@\n-        return defaultImpl.apply(vm);\n+        return defaultImpl.apply(v, m);\n+    }\n+\n+    public interface UnaryOperation<V, M> {\n+        V apply(V v, M mask);\n@@ -275,4 +284,4 @@\n-    <VM>\n-    VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                VM vm1, VM vm2,\n-                BiFunction<VM, VM, VM> defaultImpl) {\n+    <V, M>\n+    V binaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+               Class<?> elementType, int length, V v1, V v2, M m,\n+               BinaryOperation<V, M> defaultImpl) {\n@@ -280,1 +289,1 @@\n-        return defaultImpl.apply(vm1, vm2);\n+        return defaultImpl.apply(v1, v2, m);\n@@ -283,4 +292,2 @@\n-    \/* ============================================================================ *\/\n-\n-    public interface TernaryOperation<V> {\n-        V apply(V v1, V v2, V v3);\n+    public interface BinaryOperation<V, M> {\n+        V apply(V v1, V v2, M mask);\n@@ -289,0 +296,2 @@\n+    \/* ============================================================================ *\/\n+\n@@ -291,4 +300,4 @@\n-    <VM>\n-    VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                 VM vm1, VM vm2, VM vm3,\n-                 TernaryOperation<VM> defaultImpl) {\n+    <V, M>\n+    V ternaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+                Class<?> elementType, int length, V v1, V v2, V v3, M m,\n+                TernaryOperation<V, M> defaultImpl) {\n@@ -296,1 +305,5 @@\n-        return defaultImpl.apply(vm1, vm2, vm3);\n+        return defaultImpl.apply(v1, v2, v3, m);\n+    }\n+\n+    public interface TernaryOperation<V, M> {\n+        V apply(V v1, V v2, V v3, M mask);\n@@ -320,2 +333,2 @@\n-    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>> {\n-        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s);\n+    public interface LoadVectorMaskedOperation<C, V, E, S extends VectorSpecies<E>, M extends VectorMask<E>> {\n+        V load(C container, int index, S s, M m);\n@@ -326,2 +339,23 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>, E, S extends VectorSpecies<E>>\n-    V loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n+    <C, V, E, S extends VectorSpecies<E>,\n+     M extends VectorMask<E>>\n+    V loadMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+                 int length, Object base, long offset, M m,\n+                 C container, int index, S s,  \/\/ Arguments for default implementation\n+                 LoadVectorMaskedOperation<C, V, E, S, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.load(container, index, s, m);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>,\n+                                                M extends VectorMask<E>> {\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>, W extends Vector<Integer>, E,\n+     S extends VectorSpecies<E>, M extends VectorMask<E>>\n+    V loadWithMap(Class<?> vectorClass, Class<M> maskClass, Class<E> E, int length,\n+                  Class<?> vectorIndexClass,\n@@ -329,1 +363,1 @@\n-                  W index_vector,\n+                  W index_vector, M m,\n@@ -331,1 +365,1 @@\n-                  LoadVectorOperationWithMap<C, V, E, S> defaultImpl) {\n+                  LoadVectorOperationWithMap<C, V, E, S, M> defaultImpl) {\n@@ -333,1 +367,1 @@\n-        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s);\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s, m);\n@@ -354,0 +388,17 @@\n+    public interface StoreVectorMaskedOperation<C, V extends Vector<?>, M extends VectorMask<?>> {\n+        void store(C container, int index, V v, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>,\n+     M extends VectorMask<?>>\n+    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+                     int length, Object base, long offset,   \/\/ Unsafe addressing\n+                     V v, M m,\n+                     C container, int index,      \/\/ Arguments for default implementation\n+                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v, m);\n+    }\n+\n@@ -356,2 +407,2 @@\n-    public interface StoreVectorOperationWithMap<C, V extends Vector<?>> {\n-        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM);\n+    public interface StoreVectorOperationWithMap<C, V extends Vector<?>, M extends VectorMask<?>> {\n+        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM, M m);\n@@ -362,4 +413,4 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>>\n-    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-                      Object base, long offset,    \/\/ Unsafe addressing\n-                      W index_vector, V v,\n+    <C, V extends Vector<?>, W extends Vector<Integer>, M extends VectorMask<?>>\n+    void storeWithMap(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+                      int length, Class<?> vectorIndexClass, Object base, long offset,    \/\/ Unsafe addressing\n+                      W index_vector, V v, M m,\n@@ -367,1 +418,1 @@\n-                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+                      StoreVectorOperationWithMap<C, V, M> defaultImpl) {\n@@ -369,1 +420,1 @@\n-        defaultImpl.storeWithMap(container, index, v, indexMap, indexM);\n+        defaultImpl.storeWithMap(container, index, v, indexMap, indexM, m);\n@@ -387,1 +438,1 @@\n-        M apply(int cond, V v1, V v2);\n+        M apply(int cond, V v1, V v2, M m);\n@@ -395,1 +446,1 @@\n-              V v1, V v2,\n+              V v1, V v2, M m,\n@@ -398,1 +449,1 @@\n-        return defaultImpl.apply(cond, v1, v2);\n+        return defaultImpl.apply(cond, v1, v2, m);\n@@ -402,1 +453,0 @@\n-\n@@ -405,0 +455,1 @@\n+            M  extends VectorMask<E>,\n@@ -406,1 +457,1 @@\n-        V apply(V v1, Sh shuffle);\n+        V apply(V v1, Sh shuffle, M mask);\n@@ -413,0 +464,1 @@\n+            M  extends VectorMask<E>,\n@@ -414,3 +466,3 @@\n-    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<?> elementType, int vlen,\n-                  V v1, Sh sh,\n-                  VectorRearrangeOp<V,Sh, E> defaultImpl) {\n+    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+                  V v1, Sh sh, M m,\n+                  VectorRearrangeOp<V,Sh,M,E> defaultImpl) {\n@@ -418,1 +470,1 @@\n-        return defaultImpl.apply(v1, sh);\n+        return defaultImpl.apply(v1, sh, m);\n@@ -443,2 +495,2 @@\n-    public interface VectorBroadcastIntOp<V extends Vector<?>> {\n-        V apply(V v, int n);\n+    public interface VectorBroadcastIntOp<V extends Vector<?>, M> {\n+        V apply(V v, int n, M m);\n@@ -449,4 +501,5 @@\n-    <V extends Vector<?>>\n-    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<?> elementType, int length,\n-                   V v, int n,\n-                   VectorBroadcastIntOp<V> defaultImpl) {\n+    <V extends Vector<?>, M>\n+    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+                   Class<?> elementType, int length,\n+                   V v, int n, M m,\n+                   VectorBroadcastIntOp<V, M> defaultImpl) {\n@@ -454,1 +507,1 @@\n-        return defaultImpl.apply(v, n);\n+        return defaultImpl.apply(v, n, m);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":108,"deletions":55,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    byte rOp(byte v, FBinOp f);\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    byte rOpTemplate(byte v, VectorMask<Byte> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        byte[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Byte>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (byte) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (byte) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ByteVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ByteVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ByteVector, VectorMask<Byte>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ByteVector.class);\n+\n+    private static UnaryOperation<ByteVector, VectorMask<Byte>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Byte> eqz = that.eq((byte)0);\n+                VectorMask<Byte> eqz = that.eq((byte) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ByteVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ByteVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Byte> m) {\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v, VectorMask<Byte> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Byte> eqz = that.eq((byte)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Byte> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (byte) 0);\n+                that = that.blend((byte) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Byte> eqz = that.eq((byte)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,4 +725,44 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ByteVector, VectorMask<Byte>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ByteVector.class);\n+\n+    private static BinaryOperation<ByteVector, VectorMask<Byte>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (byte) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        byte e1 = (byte) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract ByteVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ByteVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Byte>> maskClass,\n+                          int e, VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ByteVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ByteVector, VectorMask<Byte>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ByteVector, VectorMask<Byte>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ByteVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ByteVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v1,\n+                                          Vector<Byte> v2,\n+                                          VectorMask<Byte> m) {\n+        ByteVector that = (ByteVector) v1;\n+        ByteVector tother = (ByteVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ByteVector, VectorMask<Byte>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ByteVector.class);\n+\n+    private static TernaryOperation<ByteVector, VectorMask<Byte>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ByteSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Byte> v, M m) {\n+        ByteVector that = (ByteVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, byte.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Byte> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Byte> compare(VectorOperators.Comparison op,\n-                                  Vector<Byte> v,\n-                                  VectorMask<Byte> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, byte.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, byte.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Byte>>\n+    <S extends VectorShuffle<Byte>, M extends VectorMask<Byte>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Byte> m) {\n-        ByteVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, byte.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((byte)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, byte.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Byte>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ByteVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ByteVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((byte)1, (i, a, b) -> (byte)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (byte) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (byte) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((byte)-1, (i, a, b) -> (byte)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ByteVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ByteVector.class);\n+    ImplCache<Associative, ReductionOperation<ByteVector, VectorMask<Byte>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ByteVector.class);\n+\n+    private static ReductionOperation<ByteVector, VectorMask<Byte>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((byte)1, m, (i, a, b) -> (byte)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (byte) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (byte) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((byte)-1, m, (i, a, b) -> (byte)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ByteVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2924,1 +3092,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3102,3 +3270,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3342,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3344,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3332,1 +3497,0 @@\n-            \/\/ FIXME: optimize\n@@ -3335,1 +3499,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -3454,1 +3618,0 @@\n-            \/\/ FIXME: optimize\n@@ -3457,3 +3620,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3490,1 +3651,0 @@\n-            \/\/ FIXME: optimize\n@@ -3496,3 +3656,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3536,0 +3694,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3552,0 +3728,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n+\n@@ -3570,0 +3763,19 @@\n+    abstract\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get(o + i * 1));\n+            });\n+    }\n+\n@@ -3586,0 +3798,18 @@\n+    abstract\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get(o + i * 1));\n+                });\n+    }\n+\n@@ -3605,0 +3835,36 @@\n+    abstract\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+\n@@ -3622,0 +3888,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put(o + i * 1, e));\n+            });\n+    }\n+\n@@ -3636,0 +3921,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+                });\n+    }\n+\n+\n@@ -3962,1 +4266,1 @@\n-                                      AbstractMask<Byte> m,\n+                                      VectorMask<Byte> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":494,"deletions":190,"binary":false,"changes":684,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    double rOp(double v, FBinOp f);\n+    double rOp(double v, VectorMask<Double> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    double rOpTemplate(double v, VectorMask<Double> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        double[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Double>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (double) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<DoubleVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, DoubleVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          VectorMask<Double> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<DoubleVector, VectorMask<Double>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, DoubleVector.class);\n+\n+    private static UnaryOperation<DoubleVector, VectorMask<Double>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<DoubleVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, DoubleVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v, VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<DoubleVector, VectorMask<Double>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, DoubleVector.class);\n+\n+    private static BinaryOperation<DoubleVector, VectorMask<Double>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        double e1 = (double) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<DoubleVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, DoubleVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v1,\n+                                          Vector<Double> v2,\n+                                          VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v1;\n+        DoubleVector tother = (DoubleVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<DoubleVector, VectorMask<Double>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, DoubleVector.class);\n+\n+    private static TernaryOperation<DoubleVector, VectorMask<Double>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1650,2 +1747,0 @@\n-        Objects.requireNonNull(v);\n-        DoubleSpecies vsp = vspecies();\n@@ -1657,2 +1752,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1668,0 +1763,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Double> v, M m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, double.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Double> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1681,12 +1798,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Double> compare(VectorOperators.Comparison op,\n-                                  Vector<Double> v,\n-                                  VectorMask<Double> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1751,1 +1856,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2002,3 +2107,3 @@\n-            getClass(), shuffletype, double.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, double.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2021,1 +2126,1 @@\n-    <S extends VectorShuffle<Double>>\n+    <S extends VectorShuffle<Double>, M extends VectorMask<Double>>\n@@ -2023,0 +2128,1 @@\n+                                           Class<M> masktype,\n@@ -2024,9 +2130,3 @@\n-                                           VectorMask<Double> m) {\n-        DoubleVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, double.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2038,1 +2138,7 @@\n-        return broadcast((double)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, double.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2061,3 +2167,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2069,3 +2175,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2312,0 +2418,1 @@\n+                               Class<? extends VectorMask<Double>> maskClass,\n@@ -2313,2 +2420,10 @@\n-        DoubleVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            DoubleVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2329,14 +2444,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((double)0, (i, a, b) -> (double)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((double)1, (i, a, b) -> (double)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (double) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (double) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2344,0 +2448,1 @@\n+\n@@ -2345,2 +2450,16 @@\n-    ImplCache<Associative,Function<DoubleVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, DoubleVector.class);\n+    ImplCache<Associative, ReductionOperation<DoubleVector, VectorMask<Double>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, DoubleVector.class);\n+\n+    private static ReductionOperation<DoubleVector, VectorMask<Double>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((double)0, m, (i, a, b) -> (double)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((double)1, m, (i, a, b) -> (double)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (double) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (double) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2552,3 +2671,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2616,2 +2733,1 @@\n-            DoubleVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2693,3 +2809,3 @@\n-            vectorType, double.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2697,1 +2813,1 @@\n-            (double[] c, int idx, int[] iMap, int idy, DoubleSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2699,1 +2815,1 @@\n-        }\n+    }\n@@ -2747,2 +2863,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2843,3 +2958,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2917,1 +3030,0 @@\n-            \/\/ FIXME: optimize\n@@ -2920,1 +3032,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2983,1 +3095,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2986,1 +3098,1 @@\n-            this,\n+            this, null,\n@@ -2988,1 +3100,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3035,6 +3147,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3070,1 +3177,0 @@\n-            \/\/ FIXME: optimize\n@@ -3073,3 +3179,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3106,1 +3210,0 @@\n-            \/\/ FIXME: optimize\n@@ -3112,3 +3215,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3152,0 +3253,69 @@\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends DoubleVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return DoubleVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Double species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3172,0 +3342,19 @@\n+    abstract\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+            });\n+    }\n+\n@@ -3188,0 +3377,18 @@\n+    abstract\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+                });\n+    }\n+\n@@ -3207,0 +3414,71 @@\n+    abstract\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(double[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Double species is S_MAX_BIT. and the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3224,0 +3502,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putDouble(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3238,0 +3535,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3555,1 +3871,1 @@\n-                                      AbstractMask<Double> m,\n+                                      VectorMask<Double> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":510,"deletions":194,"binary":false,"changes":704,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    float rOp(float v, FBinOp f);\n+    float rOp(float v, VectorMask<Float> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    float rOpTemplate(float v, VectorMask<Float> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        float[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Float>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (float) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<FloatVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, FloatVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          VectorMask<Float> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<FloatVector, VectorMask<Float>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, FloatVector.class);\n+\n+    private static UnaryOperation<FloatVector, VectorMask<Float>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<FloatVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, FloatVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v, VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<FloatVector, VectorMask<Float>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, FloatVector.class);\n+\n+    private static BinaryOperation<FloatVector, VectorMask<Float>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        float e1 = (float) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<FloatVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, FloatVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v1,\n+                                          Vector<Float> v2,\n+                                          VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v1;\n+        FloatVector tother = (FloatVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<FloatVector, VectorMask<Float>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, FloatVector.class);\n+\n+    private static TernaryOperation<FloatVector, VectorMask<Float>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1662,2 +1759,0 @@\n-        Objects.requireNonNull(v);\n-        FloatSpecies vsp = vspecies();\n@@ -1669,2 +1764,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1680,0 +1775,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Float> v, M m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, float.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Float> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1693,12 +1810,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Float> compare(VectorOperators.Comparison op,\n-                                  Vector<Float> v,\n-                                  VectorMask<Float> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1763,1 +1868,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2014,3 +2119,3 @@\n-            getClass(), shuffletype, float.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, float.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2033,1 +2138,1 @@\n-    <S extends VectorShuffle<Float>>\n+    <S extends VectorShuffle<Float>, M extends VectorMask<Float>>\n@@ -2035,0 +2140,1 @@\n+                                           Class<M> masktype,\n@@ -2036,9 +2142,3 @@\n-                                           VectorMask<Float> m) {\n-        FloatVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, float.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2050,1 +2150,7 @@\n-        return broadcast((float)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, float.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2073,3 +2179,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2081,3 +2187,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2332,0 +2438,1 @@\n+                               Class<? extends VectorMask<Float>> maskClass,\n@@ -2333,2 +2440,10 @@\n-        FloatVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            FloatVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2349,14 +2464,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((float)0, (i, a, b) -> (float)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((float)1, (i, a, b) -> (float)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (float) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (float) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2364,0 +2468,1 @@\n+\n@@ -2365,2 +2470,16 @@\n-    ImplCache<Associative,Function<FloatVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, FloatVector.class);\n+    ImplCache<Associative, ReductionOperation<FloatVector, VectorMask<Float>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, FloatVector.class);\n+\n+    private static ReductionOperation<FloatVector, VectorMask<Float>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((float)0, m, (i, a, b) -> (float)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((float)1, m, (i, a, b) -> (float)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (float) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (float) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2576,3 +2695,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2640,2 +2757,1 @@\n-            FloatVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2699,3 +2815,3 @@\n-            vectorType, float.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2703,1 +2819,1 @@\n-            (float[] c, int idx, int[] iMap, int idy, FloatSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2705,1 +2821,1 @@\n-        }\n+    }\n@@ -2753,2 +2869,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2849,3 +2964,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2923,1 +3036,0 @@\n-            \/\/ FIXME: optimize\n@@ -2926,1 +3038,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2970,1 +3082,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2973,1 +3085,1 @@\n-            this,\n+            this, null,\n@@ -2975,1 +3087,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3022,6 +3134,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3057,1 +3164,0 @@\n-            \/\/ FIXME: optimize\n@@ -3060,3 +3166,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3093,1 +3197,0 @@\n-            \/\/ FIXME: optimize\n@@ -3099,3 +3202,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3139,0 +3240,51 @@\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends FloatVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3159,0 +3311,19 @@\n+    abstract\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+            });\n+    }\n+\n@@ -3175,0 +3346,18 @@\n+    abstract\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+                });\n+    }\n+\n@@ -3194,0 +3383,52 @@\n+    abstract\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(float[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3211,0 +3452,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putFloat(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3225,0 +3485,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3542,1 +3821,1 @@\n-                                      AbstractMask<Float> m,\n+                                      VectorMask<Float> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":473,"deletions":194,"binary":false,"changes":667,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    int rOp(int v, FBinOp f);\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    int rOpTemplate(int v, VectorMask<Integer> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        int[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Integer>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (int) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (int) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<IntVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, IntVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<IntVector, VectorMask<Integer>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, IntVector.class);\n+\n+    private static UnaryOperation<IntVector, VectorMask<Integer>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Integer> eqz = that.eq((int)0);\n+                VectorMask<Integer> eqz = that.eq((int) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<IntVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, IntVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Integer> m) {\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v, VectorMask<Integer> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Integer> eqz = that.eq((int)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Integer> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (int) 0);\n+                that = that.blend((int) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Integer> eqz = that.eq((int)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,4 +725,44 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<IntVector, VectorMask<Integer>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, IntVector.class);\n+\n+    private static BinaryOperation<IntVector, VectorMask<Integer>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (int) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        int e1 = (int) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract IntVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final IntVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Integer>> maskClass,\n+                          int e, VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<IntVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<IntVector, VectorMask<Integer>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<IntVector, VectorMask<Integer>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -880,6 +1000,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n@@ -887,3 +1004,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<IntVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, IntVector.class);\n@@ -897,2 +1011,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -902,2 +1016,37 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v1,\n+                                          Vector<Integer> v2,\n+                                          VectorMask<Integer> m) {\n+        IntVector that = (IntVector) v1;\n+        IntVector tother = (IntVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<IntVector, VectorMask<Integer>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, IntVector.class);\n+\n+    private static TernaryOperation<IntVector, VectorMask<Integer>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -960,1 +1109,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1018,1 +1167,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1075,1 +1224,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1747,2 +1896,0 @@\n-        Objects.requireNonNull(v);\n-        IntSpecies vsp = vspecies();\n@@ -1754,2 +1901,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1765,0 +1912,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Integer> v, M m) {\n+        IntVector that = (IntVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, int.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Integer> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1782,12 +1951,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Integer> compare(VectorOperators.Comparison op,\n-                                  Vector<Integer> v,\n-                                  VectorMask<Integer> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1852,1 +2009,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2103,3 +2260,3 @@\n-            getClass(), shuffletype, int.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, int.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2122,1 +2279,1 @@\n-    <S extends VectorShuffle<Integer>>\n+    <S extends VectorShuffle<Integer>, M extends VectorMask<Integer>>\n@@ -2124,0 +2281,1 @@\n+                                           Class<M> masktype,\n@@ -2125,9 +2283,3 @@\n-                                           VectorMask<Integer> m) {\n-        IntVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, int.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2139,1 +2291,7 @@\n-        return broadcast((int)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, int.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2162,3 +2320,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2170,3 +2328,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2435,0 +2593,1 @@\n+                               Class<? extends VectorMask<Integer>> maskClass,\n@@ -2436,2 +2595,10 @@\n-        IntVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            IntVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2452,20 +2619,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((int)1, (i, a, b) -> (int)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (int) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (int) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((int)-1, (i, a, b) -> (int)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2473,0 +2623,1 @@\n+\n@@ -2474,2 +2625,22 @@\n-    ImplCache<Associative,Function<IntVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, IntVector.class);\n+    ImplCache<Associative, ReductionOperation<IntVector, VectorMask<Integer>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, IntVector.class);\n+\n+    private static ReductionOperation<IntVector, VectorMask<Integer>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((int)1, m, (i, a, b) -> (int)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (int) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (int) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((int)-1, m, (i, a, b) -> (int)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2694,3 +2865,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2758,2 +2927,1 @@\n-            IntVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2817,3 +2985,3 @@\n-            vectorType, int.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2821,1 +2989,1 @@\n-            (int[] c, int idx, int[] iMap, int idy, IntSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2823,1 +2991,1 @@\n-        }\n+    }\n@@ -2871,2 +3039,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2967,3 +3134,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3041,1 +3206,0 @@\n-            \/\/ FIXME: optimize\n@@ -3044,1 +3208,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3088,1 +3252,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3091,1 +3255,1 @@\n-            this,\n+            this, null,\n@@ -3093,1 +3257,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3140,6 +3304,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3175,1 +3334,0 @@\n-            \/\/ FIXME: optimize\n@@ -3178,3 +3336,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3211,1 +3367,0 @@\n-            \/\/ FIXME: optimize\n@@ -3217,3 +3372,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3257,0 +3410,51 @@\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends IntVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3277,0 +3481,19 @@\n+    abstract\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getInt(o + i * 4));\n+            });\n+    }\n+\n@@ -3293,0 +3516,18 @@\n+    abstract\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getInt(o + i * 4));\n+                });\n+    }\n+\n@@ -3312,0 +3553,52 @@\n+    abstract\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(int[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3329,0 +3622,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putInt(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3343,0 +3655,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3660,1 +3991,1 @@\n-                                      AbstractMask<Integer> m,\n+                                      VectorMask<Integer> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":534,"deletions":203,"binary":false,"changes":737,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    long rOp(long v, FBinOp f);\n+    long rOp(long v, VectorMask<Long> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    long rOpTemplate(long v, VectorMask<Long> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        long[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Long>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -510,1 +533,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -513,1 +536,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -518,10 +541,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (long) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (long) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n@@ -529,3 +545,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<LongVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, LongVector.class);\n@@ -536,2 +549,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -539,2 +552,36 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<LongVector, VectorMask<Long>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, LongVector.class);\n+\n+    private static UnaryOperation<LongVector, VectorMask<Long>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) Math.abs(a));\n+            default: return null;\n+        }\n@@ -560,0 +607,1 @@\n+\n@@ -578,1 +626,1 @@\n-                VectorMask<Long> eqz = that.eq((long)0);\n+                VectorMask<Long> eqz = that.eq((long) 0);\n@@ -584,0 +632,1 @@\n+\n@@ -586,34 +635,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n@@ -621,3 +639,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<LongVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, LongVector.class);\n@@ -629,2 +644,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -633,1 +648,6 @@\n-                                  VectorMask<Long> m) {\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v, VectorMask<Long> m) {\n@@ -635,4 +655,10 @@\n-        if (op == DIV) {\n-            VectorMask<Long> eqz = that.eq((long)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Long> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (long) 0);\n+                that = that.blend((long) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n@@ -640,3 +666,61 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Long> eqz = that.eq((long)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<LongVector, VectorMask<Long>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, LongVector.class);\n+\n+    private static BinaryOperation<LongVector, VectorMask<Long>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n@@ -644,1 +728,1 @@\n-        return blend(lanewise(op, v), m);\n+\n@@ -708,1 +792,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (long) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -726,16 +816,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n@@ -743,0 +820,22 @@\n+\n+    \/*package-private*\/\n+    abstract LongVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final LongVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Long>> maskClass,\n+                          int e, VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n+    }\n+\n@@ -744,1 +843,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<LongVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<LongVector, VectorMask<Long>>> BIN_INT_IMPL\n@@ -747,0 +846,16 @@\n+    private static VectorBroadcastIntOp<LongVector, VectorMask<Long>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -798,6 +913,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n@@ -805,3 +917,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<LongVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, LongVector.class);\n@@ -815,2 +924,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -820,2 +929,37 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v1,\n+                                          Vector<Long> v2,\n+                                          VectorMask<Long> m) {\n+        LongVector that = (LongVector) v1;\n+        LongVector tother = (LongVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<LongVector, VectorMask<Long>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, LongVector.class);\n+\n+    private static TernaryOperation<LongVector, VectorMask<Long>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -878,1 +1022,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -936,1 +1080,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -993,1 +1137,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1665,2 +1809,0 @@\n-        Objects.requireNonNull(v);\n-        LongSpecies vsp = vspecies();\n@@ -1672,2 +1814,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1683,0 +1825,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Long> v, M m) {\n+        LongVector that = (LongVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, long.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Long> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1700,12 +1864,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Long> compare(VectorOperators.Comparison op,\n-                                  Vector<Long> v,\n-                                  VectorMask<Long> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1770,1 +1922,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -1974,3 +2126,3 @@\n-            getClass(), shuffletype, long.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, long.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -1993,1 +2145,1 @@\n-    <S extends VectorShuffle<Long>>\n+    <S extends VectorShuffle<Long>, M extends VectorMask<Long>>\n@@ -1995,0 +2147,1 @@\n+                                           Class<M> masktype,\n@@ -1996,9 +2149,3 @@\n-                                           VectorMask<Long> m) {\n-        LongVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, long.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2010,1 +2157,7 @@\n-        return broadcast((long)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, long.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2033,3 +2186,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2041,3 +2194,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2306,0 +2459,1 @@\n+                               Class<? extends VectorMask<Long>> maskClass,\n@@ -2307,2 +2461,10 @@\n-        LongVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            LongVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2323,20 +2485,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((long)1, (i, a, b) -> (long)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (long) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (long) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((long)-1, (i, a, b) -> (long)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2344,0 +2489,1 @@\n+\n@@ -2345,2 +2491,22 @@\n-    ImplCache<Associative,Function<LongVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, LongVector.class);\n+    ImplCache<Associative, ReductionOperation<LongVector, VectorMask<Long>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, LongVector.class);\n+\n+    private static ReductionOperation<LongVector, VectorMask<Long>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((long)1, m, (i, a, b) -> (long)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (long) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (long) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((long)-1, m, (i, a, b) -> (long)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2560,3 +2726,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2624,2 +2788,1 @@\n-            LongVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2701,3 +2864,3 @@\n-            vectorType, long.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2705,1 +2868,1 @@\n-            (long[] c, int idx, int[] iMap, int idy, LongSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2707,1 +2870,1 @@\n-        }\n+    }\n@@ -2755,2 +2918,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2851,3 +3013,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2925,1 +3085,0 @@\n-            \/\/ FIXME: optimize\n@@ -2928,1 +3087,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2991,1 +3150,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2994,1 +3153,1 @@\n-            this,\n+            this, null,\n@@ -2996,1 +3155,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3043,6 +3202,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3078,1 +3232,0 @@\n-            \/\/ FIXME: optimize\n@@ -3081,3 +3234,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3114,1 +3265,0 @@\n-            \/\/ FIXME: optimize\n@@ -3120,3 +3270,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3160,0 +3308,69 @@\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends LongVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return LongVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Long species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3180,0 +3397,19 @@\n+    abstract\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getLong(o + i * 8));\n+            });\n+    }\n+\n@@ -3196,0 +3432,18 @@\n+    abstract\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getLong(o + i * 8));\n+                });\n+    }\n+\n@@ -3215,0 +3469,71 @@\n+    abstract\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(long[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Long species is S_MAX_BIT. and the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3232,0 +3557,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putLong(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3246,0 +3590,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3554,1 +3917,1 @@\n-                                      AbstractMask<Long> m,\n+                                      VectorMask<Long> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":563,"deletions":200,"binary":false,"changes":763,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    short rOp(short v, FBinOp f);\n+    short rOp(short v, VectorMask<Short> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    short rOpTemplate(short v, VectorMask<Short> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        short[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Short>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (short) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (short) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ShortVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ShortVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ShortVector, VectorMask<Short>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ShortVector.class);\n+\n+    private static UnaryOperation<ShortVector, VectorMask<Short>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Short> eqz = that.eq((short)0);\n+                VectorMask<Short> eqz = that.eq((short) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ShortVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ShortVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Short> m) {\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v, VectorMask<Short> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Short> eqz = that.eq((short)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Short> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n+                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Short> eqz = that.eq((short)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,4 +725,44 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ShortVector, VectorMask<Short>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ShortVector.class);\n+\n+    private static BinaryOperation<ShortVector, VectorMask<Short>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (short) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        short e1 = (short) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,24 @@\n-            opc, getClass(), short.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract ShortVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ShortVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Short>> maskClass,\n+                          int e, VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n@@ -825,0 +928,1 @@\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ShortVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ShortVector, VectorMask<Short>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ShortVector, VectorMask<Short>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ShortVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ShortVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v1,\n+                                          Vector<Short> v2,\n+                                          VectorMask<Short> m) {\n+        ShortVector that = (ShortVector) v1;\n+        ShortVector tother = (ShortVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ShortVector, VectorMask<Short>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ShortVector.class);\n+\n+    private static TernaryOperation<ShortVector, VectorMask<Short>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ShortSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Short> v, M m) {\n+        ShortVector that = (ShortVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, short.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Short> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Short> compare(VectorOperators.Comparison op,\n-                                  Vector<Short> v,\n-                                  VectorMask<Short> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, short.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, short.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Short>>\n+    <S extends VectorShuffle<Short>, M extends VectorMask<Short>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Short> m) {\n-        ShortVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, short.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((short)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, short.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Short>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ShortVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ShortVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((short)1, (i, a, b) -> (short)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (short) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (short) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((short)-1, (i, a, b) -> (short)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ShortVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ShortVector.class);\n+    ImplCache<Associative, ReductionOperation<ShortVector, VectorMask<Short>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ShortVector.class);\n+\n+    private static ReductionOperation<ShortVector, VectorMask<Short>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((short)1, m, (i, a, b) -> (short)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (short) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (short) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((short)-1, m, (i, a, b) -> (short)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2916,2 +3084,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3102,3 +3269,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3341,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3343,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3324,1 +3488,0 @@\n-            \/\/ FIXME: optimize\n@@ -3327,1 +3490,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -3441,1 +3604,0 @@\n-            \/\/ FIXME: optimize\n@@ -3444,3 +3606,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3477,1 +3637,0 @@\n-            \/\/ FIXME: optimize\n@@ -3483,3 +3642,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3523,0 +3680,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3538,0 +3713,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n+\n@@ -3557,0 +3749,19 @@\n+    abstract\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getShort(o + i * 2));\n+            });\n+    }\n+\n@@ -3573,0 +3784,18 @@\n+    abstract\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getShort(o + i * 2));\n+                });\n+    }\n+\n@@ -3592,0 +3821,19 @@\n+    abstract\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+\n@@ -3609,0 +3857,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putShort(o + i * 2, e));\n+            });\n+    }\n+\n@@ -3623,0 +3890,36 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+                });\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+\n@@ -3957,1 +4260,1 @@\n-                                      AbstractMask<Short> m,\n+                                      VectorMask<Short> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":494,"deletions":191,"binary":false,"changes":685,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -180,0 +179,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -223,0 +225,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -272,0 +277,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -287,1 +295,16 @@\n-    $type$ rOp($type$ v, FBinOp f);\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    $type$ rOpTemplate($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        $type$[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<$Boxtype$>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -575,1 +598,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -578,1 +601,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -584,44 +607,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.abs(a));\n-#if[FP]\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log1p(a));\n-#end[FP]\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n@@ -629,3 +611,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<$abstractvectortype$>> UN_IMPL\n-        = new ImplCache<>(Unary.class, $Type$Vector.class);\n@@ -636,2 +615,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -639,2 +618,72 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+#if[BITWISE]\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+#end[BITWISE]\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, $Type$Vector.class);\n+\n+    private static UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.abs(a));\n+#if[FP]\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log1p(a));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -660,0 +709,1 @@\n+\n@@ -687,1 +737,1 @@\n-                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$) 0);\n@@ -694,0 +744,1 @@\n+\n@@ -696,44 +747,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.min(a, b));\n-#if[BITWISE]\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-#end[BITWISE]\n-#if[FP]\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n@@ -741,3 +751,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<$abstractvectortype$>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, $Type$Vector.class);\n@@ -749,2 +756,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -753,2 +760,6 @@\n-                                  VectorMask<$Boxtype$> m) {\n-#if[BITWISE]\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n@@ -756,4 +767,34 @@\n-        if (op == DIV) {\n-            VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL {#if[!FP]? | VO_SHIFT})) {\n+            if (op == FIRST_NONZERO) {\n+#if[FP]\n+                return blend(lanewise(op, v), m);\n+#else[FP]\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<$Boxbitstype$> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n+                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+#end[FP]\n+            }\n+#if[BITWISE]\n+#if[!FP]\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+#end[!FP]\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -761,3 +802,1 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+#end[BITWISE]\n@@ -765,0 +804,43 @@\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, $Type$Vector.class);\n+\n+    private static BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.min(a, b));\n+#if[BITWISE]\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n@@ -766,1 +848,12 @@\n-        return blend(lanewise(op, v), m);\n+#if[FP]\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.hypot(a, b));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -768,0 +861,1 @@\n+\n@@ -832,1 +926,9 @@\n-        return blend(lanewise(op, e), m);\n+#if[BITWISE]\n+        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = ($type$) ~e;\n+        }\n+#end[BITWISE]\n+        return lanewise(op, broadcast(e), m);\n@@ -851,1 +953,1 @@\n-        if ((long)e1 != e\n+        if ((long)e1 != e\n@@ -854,1 +956,3 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n@@ -856,1 +960,0 @@\n-            ) {\n@@ -876,1 +979,11 @@\n-        return blend(lanewise(op, e), m);\n+        $type$ e1 = ($type$) e;\n+#if[BITWISE]\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n+#end[BITWISE]\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -895,16 +1008,24 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract $abstractvectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final $abstractvectortype$\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                          int e, VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n@@ -912,0 +1033,1 @@\n+\n@@ -913,1 +1035,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>>> BIN_INT_IMPL\n@@ -916,0 +1038,16 @@\n+    private static VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -975,10 +1113,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-#if[FP]\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n@@ -986,3 +1117,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<$abstractvectortype$>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, $Type$Vector.class);\n@@ -996,2 +1124,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -1001,2 +1129,43 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v1,\n+                                          Vector<$Boxtype$> v2,\n+                                          VectorMask<$Boxtype$> m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v1;\n+        $abstractvectortype$ tother = ($abstractvectortype$) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+#if[BITWISE]\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+#end[BITWISE]\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, $Type$Vector.class);\n+\n+    private static TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+#if[FP]\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -1059,1 +1228,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1117,1 +1286,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1174,1 +1343,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -2019,2 +2188,0 @@\n-        Objects.requireNonNull(v);\n-        $Type$Species vsp = vspecies();\n@@ -2026,2 +2193,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -2037,0 +2204,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<$Boxtype$> v, M m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, $type$.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<$Boxtype$> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -2056,12 +2245,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<$Boxtype$> compare(VectorOperators.Comparison op,\n-                                  Vector<$Boxtype$> v,\n-                                  VectorMask<$Boxtype$> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -2126,1 +2303,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2381,3 +2558,3 @@\n-            getClass(), shuffletype, $type$.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, $type$.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2400,1 +2577,1 @@\n-    <S extends VectorShuffle<$Boxtype$>>\n+    <S extends VectorShuffle<$Boxtype$>, M extends VectorMask<$Boxtype$>>\n@@ -2402,0 +2579,1 @@\n+                                           Class<M> masktype,\n@@ -2403,9 +2581,3 @@\n-                                           VectorMask<$Boxtype$> m) {\n-        $abstractvectortype$ unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2417,1 +2589,7 @@\n-        return broadcast(($type$)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, $type$.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2440,3 +2618,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2448,3 +2626,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2842,0 +3020,1 @@\n+                               Class<? extends VectorMask<$Boxtype$>> maskClass,\n@@ -2843,2 +3022,10 @@\n-        $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n@@ -2859,12 +3046,19 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp(($type$)1, (i, a, b) -> ($type$)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> ($type$) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> ($type$) Math.max(a, b)));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n+    }\n+\n+    private static final\n+    ImplCache<Associative, ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, $Type$Vector.class);\n+\n+    private static ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp(($type$)1, m, (i, a, b) -> ($type$)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> ($type$) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> ($type$) Math.max(a, b)));\n@@ -2872,6 +3066,6 @@\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp(($type$)-1, (i, a, b) -> ($type$)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a ^ b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp(($type$)-1, m, (i, a, b) -> ($type$)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a ^ b)));\n@@ -2879,2 +3073,2 @@\n-              default: return null;\n-              }})));\n+            default: return null;\n+        }\n@@ -2882,3 +3076,0 @@\n-    private static final\n-    ImplCache<Associative,Function<$abstractvectortype$,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, $Type$Vector.class);\n@@ -3178,3 +3369,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -3242,2 +3431,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -3336,3 +3524,3 @@\n-            vectorType, $type$.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -3340,1 +3528,1 @@\n-            ($type$[] c, int idx, int[] iMap, int idy, $Type$Species s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -3342,1 +3530,1 @@\n-        }\n+    }\n@@ -3402,2 +3590,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -3465,2 +3652,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3626,1 +3812,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3817,3 +4003,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3891,1 +4075,0 @@\n-            \/\/ FIXME: optimize\n@@ -3894,1 +4077,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3976,1 +4159,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3979,1 +4162,1 @@\n-            this,\n+            this, null,\n@@ -3981,1 +4164,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -4042,6 +4225,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -4115,1 +4293,0 @@\n-            \/\/ FIXME: optimize\n@@ -4118,1 +4295,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -4278,1 +4455,0 @@\n-            \/\/ FIXME: optimize\n@@ -4281,1 +4457,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -4401,1 +4577,0 @@\n-            \/\/ FIXME: optimize\n@@ -4404,3 +4579,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -4437,1 +4610,0 @@\n-            \/\/ FIXME: optimize\n@@ -4443,3 +4615,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -4483,0 +4653,78 @@\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+#if[!byteOrShort]\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+          return $abstractvectortype$.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of $Type$ species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#end[!byteOrShort]\n+\n@@ -4498,0 +4746,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n@@ -4515,0 +4780,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n@@ -4535,0 +4817,19 @@\n+    abstract\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+            });\n+    }\n+\n@@ -4551,0 +4852,18 @@\n+    abstract\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+                });\n+    }\n+\n@@ -4570,0 +4889,99 @@\n+    abstract\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+#if[!byteOrShort]\n+    abstract\n+    void intoArray0($type$[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of $Type$ species is S_MAX_BIT. and the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+#end[byte]\n+\n@@ -4587,0 +5005,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            });\n+    }\n+\n@@ -4601,0 +5038,38 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+                });\n+    }\n+\n+#if[short]\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+#end[short]\n+\n@@ -4976,1 +5451,1 @@\n-                                      AbstractMask<$Boxtype$> m,\n+                                      VectorMask<$Boxtype$> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":736,"deletions":261,"binary":false,"changes":997,"status":"modified"}]}