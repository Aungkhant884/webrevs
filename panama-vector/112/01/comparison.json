{"files":[{"patch":"@@ -2062,1 +2062,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -2077,1 +2077,1 @@\n-  if (bottom_type()->isa_vect() != NULL) {\n+  if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2183,0 +2183,3 @@\n+      } else if (dst_lo_rc == rc_predicate) {\n+        __ unspill_sve_predicate(as_PRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo),\n+                                 Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n@@ -2185,2 +2188,18 @@\n-        __ unspill(rscratch1, is64, src_offset);\n-        __ spill(rscratch1, is64, dst_offset);\n+        if (ideal_reg() == Op_RegVectMask) {\n+          __ spill_copy_sve_predicate_stack_to_stack(src_offset, dst_offset,\n+                                                     Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+        } else {\n+          __ unspill(rscratch1, is64, src_offset);\n+          __ spill(rscratch1, is64, dst_offset);\n+        }\n+      }\n+      break;\n+    case rc_predicate:\n+      if (dst_lo_rc == rc_predicate) {\n+        __ sve_mov(as_PRegister(Matcher::_regEncode[dst_lo]), as_PRegister(Matcher::_regEncode[src_lo]));\n+      } else if (dst_lo_rc == rc_stack) {\n+        __ spill_sve_predicate(as_PRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo),\n+                               Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+      } else {\n+        assert(false, \"bad src and dst rc_class combination.\");\n+        ShouldNotReachHere();\n@@ -2207,1 +2226,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2224,0 +2243,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# predicate spill size = %d\", vsize);\n@@ -2383,0 +2406,14 @@\n+    case Op_LoadVectorMask:\n+    case Op_StoreVectorMask:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+    case Op_MaskAll:\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (UseSVE == 0) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -2451,0 +2488,9 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ Only SVE supports masked operations.\n+  if (UseSVE == 0) {\n+    return false;\n+  }\n+  return match_rule_supported(opcode) &&\n+         masked_op_sve_supported(opcode, vlen, bt);\n+}\n+\n@@ -8867,0 +8913,11 @@\n+instruct castVVMask(pRegGov dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":62,"deletions":5,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -161,0 +162,6 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    \/\/ Currently we support all masked vector opcodes.\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -304,1 +311,1 @@\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+            \"sve_ldr $dst, $pTmp, $mem\\t# load vector partial\" %}\n@@ -324,1 +331,1 @@\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+            \"sve_str $src, $pTmp, $mem\\t# store vector partial\" %}\n@@ -337,0 +344,206 @@\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ maskAll\n+\n+instruct vmaskAll_immI(pRegGov dst, immI src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(pRegGov dst, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and\/or\/xor\n+\n+instruct vmask_and(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_and(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (OrVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (XorVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and_not\n+\n+instruct vmask_and_notI(pRegGov pd, pRegGov pn, pRegGov pm, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_and_notL(pRegGov pd, pRegGov pn, pRegGov pm, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -373,0 +586,34 @@\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmpeq(as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -377,1 +624,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -390,1 +637,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -403,1 +650,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -416,1 +663,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -429,1 +676,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -442,1 +689,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -453,0 +700,80 @@\n+\/\/ vector abs - predicated\n+\n+instruct vabsB_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVB dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsS_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVS dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVI dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVL dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -533,0 +860,80 @@\n+\/\/ vector add - predicated\n+\n+instruct vaddB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -578,1 +985,1 @@\n-\/\/ vector not\n+\/\/ vector and - predicated\n@@ -580,1 +987,1 @@\n-instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n+instruct vand_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -582,3 +989,1 @@\n-  match(Set dst (XorV src (ReplicateB m1)));\n-  match(Set dst (XorV src (ReplicateS m1)));\n-  match(Set dst (XorV src (ReplicateI m1)));\n+  match(Set dst_src1 (AndV (Binary dst_src1 src2) pg));\n@@ -586,1 +991,1 @@\n-  format %{ \"sve_not $dst, $src\\t# vector (sve) B\/H\/S\" %}\n+  format %{ \"sve_and $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -588,2 +993,5 @@\n-    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n-               ptrue, as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_and(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -594,1 +1002,3 @@\n-instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n+\/\/ vector or - predicated\n+\n+instruct vor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -596,1 +1006,1 @@\n-  match(Set dst (XorV src (ReplicateL m1)));\n+  match(Set dst_src1 (OrV (Binary dst_src1 src2) pg));\n@@ -598,1 +1008,1 @@\n-  format %{ \"sve_not $dst, $src\\t# vector (sve) D\" %}\n+  format %{ \"sve_orr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -600,2 +1010,5 @@\n-    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n-               ptrue, as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_orr(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -606,0 +1019,1 @@\n+\/\/ vector xor - predicated\n@@ -607,3 +1021,1 @@\n-\/\/ vector and_not\n-\n-instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n+instruct vxor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -611,2 +1023,47 @@\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n+  match(Set dst_src1 (XorV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_eor(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector not\n+\n+instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_not $dst, $src\\t# vector (sve) B\/H\/S\" %}\n+  ins_encode %{\n+    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n+               ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (XorV src (ReplicateL m1)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_not $dst, $src\\t# vector (sve) D\" %}\n+  ins_encode %{\n+    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n+               ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and_not\n+\n+instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n@@ -637,1 +1094,0 @@\n-\n@@ -664,0 +1120,28 @@\n+\/\/ vector float div - predicated\n+\n+instruct vfdivF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vfdivD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -678,1 +1162,1 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n+      assert(is_integral_type(bt), \"unsupported type\");\n@@ -698,1 +1182,1 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n+      assert(is_integral_type(bt), \"unsupported type\");\n@@ -706,0 +1190,42 @@\n+\/\/ vector min\/max - predicated\n+\n+instruct vmin_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MinV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmax_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MaxV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -734,0 +1260,28 @@\n+\/\/ vector fmla - predicated\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -942,1 +1496,0 @@\n-\n@@ -1019,0 +1572,80 @@\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1022,1 +1655,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -1034,1 +1668,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -1045,0 +1680,28 @@\n+\/\/ vector fneg - predicated\n+\n+instruct vnegF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1059,1 +1722,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -1062,4 +1725,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -1068,1 +1730,1 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n@@ -1070,2 +1732,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -1076,3 +1736,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -1080,5 +1738,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -1086,6 +1743,3 @@\n-    Assembler::SIMD_RegVariant size =\n-      __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n@@ -1096,1 +1750,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -1098,2 +1752,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -1101,5 +1754,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -1107,6 +1758,4 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src3$$reg),\n-                   as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n@@ -1119,1 +1768,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -1123,0 +1772,1 @@\n+  effect(KILL cr);\n@@ -1124,1 +1774,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -1126,1 +1776,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg), 0);\n@@ -1131,1 +1781,1 @@\n-instruct vloadmaskS(vReg dst, vReg src) %{\n+instruct vloadmaskS(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -1135,0 +1785,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1136,2 +1787,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskS $dst, $src\\t# vector load mask (sve) (B to H)\" %}\n@@ -1139,2 +1789,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ H, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1145,1 +1795,1 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n+instruct vloadmaskI(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -1150,0 +1800,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1151,3 +1802,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n+  format %{ \"vloadmaskI $dst, $src\\t# vector load mask (sve) (B to S)\" %}\n@@ -1155,3 +1804,3 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ S, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1162,1 +1811,1 @@\n-instruct vloadmaskL(vReg dst, vReg src) %{\n+instruct vloadmaskL(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -1167,0 +1816,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1168,4 +1818,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmaskL $dst, $src\\t# vector load mask (sve) (B to D)\" %}\n@@ -1173,4 +1820,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ D, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ D, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1183,1 +1830,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -1187,1 +1834,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremaskB $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -1189,2 +1836,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -1195,1 +1841,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremaskS(vReg dst, pRegGov src, vReg tmp, immI_2 size) %{\n@@ -1200,3 +1846,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+  format %{ \"vstoremaskS $dst, $src\\t# vector store mask (sve) (H to B)\" %}\n@@ -1204,0 +1848,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ H, as_PRegister($src$$reg), 1, false);\n@@ -1206,4 +1851,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -1214,1 +1856,1 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+instruct vstoremaskI(vReg dst, pRegGov src, vReg tmp, immI_4 size) %{\n@@ -1219,4 +1861,1 @@\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+  format %{ \"vstoremaskI $dst, $src\\t# vector store mask (sve) (S to B)\" %}\n@@ -1224,0 +1863,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($src$$reg), 1, false);\n@@ -1226,1 +1866,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -1229,2 +1869,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1235,1 +1873,1 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+instruct vstoremaskL(vReg dst, pRegGov src, vReg tmp, immI_8 size) %{\n@@ -1240,5 +1878,1 @@\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+  format %{ \"vstoremaskL $dst, $src\\t# vector store mask (sve) (D to B)\" %}\n@@ -1246,0 +1880,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($src$$reg), 1, false);\n@@ -1248,1 +1883,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -1253,2 +1888,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1261,2 +1894,3 @@\n-instruct vloadmask_loadV_byte(vReg dst, vmemA mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+instruct loadVMask_byte(pRegGov dst, vmemA mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1264,1 +1898,2 @@\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP tmp, KILL cr);\n@@ -1266,2 +1901,2 @@\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (B)\" %}\n@@ -1269,1 +1904,2 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -1271,3 +1907,2 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n@@ -1275,1 +1910,2 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+                 ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1280,2 +1916,3 @@\n-instruct vloadmask_loadV_non_byte(vReg dst, indirect mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+instruct loadVMask_non_byte(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1283,1 +1920,2 @@\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP tmp, KILL cr);\n@@ -1285,2 +1923,2 @@\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (H\/S\/D)\" %}\n@@ -1288,1 +1926,2 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -1290,3 +1929,49 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+                 ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadVMask_byte_partial(pRegGov dst, vmemA mem, vReg vtmp,\n+                              pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) == 1);\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"loadVMask $dst, $mem\\t# load vector mask partial (sve) (B)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadVMask_non_byte_partial(pRegGov dst, indirect mem, vReg vtmp,\n+                              pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"loadVMask $dst, $mem\\t# load vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n@@ -1294,1 +1979,1 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n@@ -1299,4 +1984,5 @@\n-instruct storeV_vstoremask_byte(vmemA mem, vReg src, vReg tmp, immI_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+instruct storeVMask_byte(vmemA mem, pRegGov src, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) == 1);\n+  match(Set mem (StoreVectorMask mem src));\n@@ -1304,0 +1990,2 @@\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (B)\" %}\n@@ -1305,2 +1993,0 @@\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n@@ -1308,0 +1994,2 @@\n+    \/\/ Convert the src predicate to vector. And store the vector elements\n+    \/\/ as boolean values.\n@@ -1309,4 +1997,2 @@\n-    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), __ elemType_to_regVariant(from_vect_bt),\n+               as_PRegister($src$$reg), 1, false);\n@@ -1320,4 +2006,5 @@\n-instruct storeV_vstoremask_non_byte(indirect mem, vReg src, vReg tmp, immI_gt_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+instruct storeVMask_non_byte(indirect mem, pRegGov src, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1);\n+  match(Set mem (StoreVectorMask mem src));\n@@ -1325,0 +2012,2 @@\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (H\/S\/D)\" %}\n@@ -1326,2 +2015,0 @@\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n@@ -1329,0 +2016,2 @@\n+    \/\/ Convert the src predicate to vector. And store the vector elements\n+    \/\/ as boolean values.\n@@ -1330,4 +2019,2 @@\n-    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), __ elemType_to_regVariant(from_vect_bt),\n+               as_PRegister($src$$reg), 1, false);\n@@ -1341,1 +2028,22 @@\n-\/\/ vector add reduction\n+instruct storeVMask_byte_partial(vmemA mem, pRegGov src, vReg vtmp,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) == 1);\n+  match(Set mem (StoreVectorMask mem src));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeVMask $src, $mem\\t# store vector mask partial (sve) (B)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1343,6 +2051,9 @@\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+instruct storeVMask_non_byte_partial(indirect mem, pRegGov src, vReg vtmp,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1);\n+  match(Set mem (StoreVectorMask mem src));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeVMask $src, $mem\\t# store vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_cost(6 * SVE_COST);\n@@ -1350,12 +2061,68 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector add reduction\n+\n+instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVI src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF(vRegF src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD(vRegD src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n@@ -1368,1 +2135,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n@@ -1371,1 +2139,1 @@\n-  ins_cost(SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -1378,11 +2146,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1393,2 +2153,4 @@\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n@@ -1396,1 +2158,17 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n@@ -1398,1 +2176,2 @@\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n@@ -1400,3 +2179,4 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1407,5 +2187,4 @@\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n@@ -1413,1 +2192,2 @@\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n@@ -1417,1 +2197,1 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n@@ -1419,2 +2199,0 @@\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n@@ -1425,0 +2203,1 @@\n+\/\/ vector add reduction - predicated\n@@ -1426,3 +2205,5 @@\n-instruct reduce_addF(vRegF src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n+instruct reduce_addI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1430,1 +2211,31 @@\n-  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked(vRegF src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated (sve)\" %}\n@@ -1433,1 +2244,1 @@\n-         ptrue, as_FloatRegister($src2$$reg));\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1438,3 +2249,4 @@\n-instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n+instruct reduce_addD_masked(vRegD src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n@@ -1442,0 +2254,54 @@\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked_partial(vRegF src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n@@ -1443,1 +2309,2 @@\n-  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated partial (sve)\" %}\n@@ -1447,0 +2314,2 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -1453,3 +2322,5 @@\n-instruct reduce_addD(vRegD src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n+instruct reduce_addD_masked_partial(vRegD src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n@@ -1457,1 +2328,520 @@\n-  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reduction\n+\n+instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reduction - predicated\n+\n+instruct reduce_andI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reduction\n+\n+instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reduction - predicated\n+\n+instruct reduce_orI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reduction\n+\n+instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reduction - predicated\n+\n+instruct reduce_eorI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector max reduction\n+\n+instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction partial (sve)\" %}\n@@ -1459,2 +2849,7 @@\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1465,6 +2860,9 @@\n-instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n-  ins_cost(SVE_COST);\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n+instruct reduce_maxL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction  partial (sve)\" %}\n@@ -1474,2 +2872,3 @@\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1480,4 +2879,3 @@\n-\/\/ vector and reduction\n-\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+instruct reduce_maxF(vRegF dst, vRegF src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1485,4 +2883,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction (sve)\" %}\n@@ -1490,12 +2888,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1506,1 +2894,1 @@\n-instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+instruct reduce_maxF_partial(vRegF dst, vRegF src1, vReg src2,\n@@ -1508,1 +2896,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1510,4 +2899,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction partial (sve)\" %}\n@@ -1515,3 +2904,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -1519,11 +2906,2 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1534,2 +2912,3 @@\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+instruct reduce_maxD(vRegD dst, vRegD src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1537,4 +2916,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction (sve)\" %}\n@@ -1542,3 +2921,2 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1549,1 +2927,1 @@\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+instruct reduce_maxD_partial(vRegD dst, vRegD src1, vReg src2,\n@@ -1551,1 +2929,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1553,4 +2932,4 @@\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  match(Set dst (MaxReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction partial (sve)\" %}\n@@ -1560,4 +2939,2 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1568,1 +2945,1 @@\n-\/\/ vector or reduction\n+\/\/ vector max reduction - predicated\n@@ -1570,5 +2947,8 @@\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_maxI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1576,1 +2956,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated (sve)\" %}\n@@ -1579,11 +2959,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1594,6 +2966,7 @@\n-instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_maxL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1601,1 +2974,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated (sve)\" %}\n@@ -1603,15 +2976,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1622,7 +2983,10 @@\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+instruct reduce_maxI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated partial (sve)\" %}\n@@ -1630,3 +2994,9 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1637,5 +3007,6 @@\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n+instruct reduce_maxL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n@@ -1643,2 +3014,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated partial (sve)\" %}\n@@ -1648,4 +3019,5 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1656,7 +3028,5 @@\n-\/\/ vector xor reduction\n-\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_maxF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n@@ -1664,1 +3034,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated (sve)\" %}\n@@ -1666,12 +3036,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1682,6 +3042,5 @@\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_maxD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n@@ -1689,1 +3048,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated (sve)\" %}\n@@ -1691,15 +3050,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1710,7 +3056,9 @@\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n+instruct reduce_maxF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated partial (sve)\" %}\n@@ -1718,3 +3066,7 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1725,8 +3077,9 @@\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+instruct reduce_maxD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated partial (sve)\" %}\n@@ -1736,4 +3089,5 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1744,0 +3098,1 @@\n+\/\/ vector min reduction\n@@ -1745,9 +3100,7 @@\n-\/\/ vector max reduction\n-\n-instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MaxReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1755,1 +3108,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction (sve)\" %}\n@@ -1758,5 +3111,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1767,8 +3118,6 @@\n-instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MaxReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1776,1 +3125,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxI partial (sve)\" %}\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction (sve)\" %}\n@@ -1778,9 +3127,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1791,7 +3134,10 @@\n-instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MaxReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction partial (sve)\" %}\n@@ -1799,4 +3145,7 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1807,1 +3156,1 @@\n-instruct reduce_maxL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+instruct reduce_minL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1809,1 +3158,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1811,1 +3161,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1813,2 +3163,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction  partial (sve)\" %}\n@@ -1818,5 +3168,3 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1827,2 +3175,3 @@\n-instruct reduce_maxF(vRegF dst, vRegF src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+instruct reduce_minF(vRegF dst, vRegF src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1830,1 +3179,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1833,2 +3182,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction F\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction (sve)\" %}\n@@ -1836,3 +3184,2 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n-    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1843,1 +3190,1 @@\n-instruct reduce_maxF_partial(vRegF dst, vRegF src1, vReg src2,\n+instruct reduce_minF_partial(vRegF dst, vRegF src1, vReg src2,\n@@ -1845,1 +3192,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1847,1 +3195,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1850,1 +3198,1 @@\n-  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# reduce max S partial (sve)\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction partial (sve)\" %}\n@@ -1854,3 +3202,2 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1861,2 +3208,3 @@\n-instruct reduce_maxD(vRegD dst, vRegD src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+instruct reduce_minD(vRegD dst, vRegD src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1864,1 +3212,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1867,2 +3215,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction D\" %}\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction (sve)\" %}\n@@ -1870,3 +3217,2 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n-    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1877,1 +3223,1 @@\n-instruct reduce_maxD_partial(vRegD dst, vRegD src1, vReg src2,\n+instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n@@ -1879,1 +3225,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1881,1 +3228,1 @@\n-  match(Set dst (MaxReductionV src1 src2));\n+  match(Set dst (MinReductionV src1 src2));\n@@ -1884,1 +3231,1 @@\n-  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# reduce max D partial (sve)\" %}\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction partial (sve)\" %}\n@@ -1888,3 +3235,2 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -1895,1 +3241,1 @@\n-\/\/ vector min reduction\n+\/\/ vector min reduction - predicated\n@@ -1897,7 +3243,8 @@\n-instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_minI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1905,1 +3252,1 @@\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated (sve)\" %}\n@@ -1908,5 +3255,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1917,8 +3262,7 @@\n-instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_minL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1926,1 +3270,1 @@\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minI partial (sve)\" %}\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated (sve)\" %}\n@@ -1928,9 +3272,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1941,7 +3279,10 @@\n-instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+instruct reduce_minI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated partial (sve)\" %}\n@@ -1949,4 +3290,9 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1957,5 +3303,6 @@\n-instruct reduce_minL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MinReductionV src1 src2));\n+instruct reduce_minL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n@@ -1963,2 +3310,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated partial (sve)\" %}\n@@ -1968,5 +3315,5 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1977,8 +3324,7 @@\n-instruct reduce_minF(vRegF dst, vRegF src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n-  effect(TEMP_DEF dst);\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction F\" %}\n+instruct reduce_minF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated (sve)\" %}\n@@ -1986,2 +3332,1 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1993,8 +3338,7 @@\n-instruct reduce_minF_partial(vRegF dst, vRegF src1, vReg src2,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# reduce min S partial (sve)\" %}\n+instruct reduce_minD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated (sve)\" %}\n@@ -2002,5 +3346,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -2011,8 +3352,9 @@\n-instruct reduce_minD(vRegD dst, vRegD src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n-  effect(TEMP_DEF dst);\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction D\" %}\n+instruct reduce_minF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated partial (sve)\" %}\n@@ -2020,3 +3362,7 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n-    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n@@ -2027,6 +3373,6 @@\n-instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n+instruct reduce_minD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n@@ -2034,1 +3380,2 @@\n-  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# reduce min D partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated partial (sve)\" %}\n@@ -2038,0 +3385,2 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -2039,1 +3388,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -2540,1 +3889,312 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift - predicated\n+\n+instruct vasrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2543,2 +4203,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2549,6 +4209,5 @@\n-instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+instruct vlslB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVB (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n@@ -2556,1 +4215,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n@@ -2561,7 +4222,5 @@\n-instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+instruct vlslS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVS (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2569,1 +4228,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2574,6 +4235,5 @@\n-instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+instruct vlslI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVI (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2581,1 +4241,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2586,6 +4248,5 @@\n-instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+instruct vlslL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVL (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2593,1 +4254,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2601,1 +4264,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2613,1 +4277,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2624,0 +4289,28 @@\n+\/\/ vector sqrt - predicated\n+\n+instruct vsqrtF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsqrtD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2704,0 +4397,80 @@\n+\/\/ vector sub - predicated\n+\n+instruct vsubB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2706,2 +4479,3 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+instruct vmaskcast(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n@@ -2709,1 +4483,1 @@\n-  match(Set dst (VectorMaskCast dst));\n+  match(Set dst_src (VectorMaskCast dst_src));\n@@ -2711,1 +4485,1 @@\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n@@ -3301,1 +5075,1 @@\n-instruct vtest_alltrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -3303,1 +5077,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3306,1 +5081,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3308,1 +5083,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, 0\\n\\t\"\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n@@ -3311,5 +5086,2 @@\n-    \/\/ \"src2\" is not used for sve.\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src1$$reg), 0);\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3321,1 +5093,1 @@\n-instruct vtest_anytrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n@@ -3323,1 +5095,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3326,1 +5099,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(KILL cr);\n@@ -3328,1 +5101,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, -1\\n\\t\"\n+  format %{ \"sve_ptest $src1\\n\\t\"\n@@ -3332,4 +5105,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src1$$reg), -1);\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n@@ -3341,1 +5111,1 @@\n-instruct vtest_alltrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_alltrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3343,1 +5113,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3346,1 +5117,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3350,1 +5121,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -3353,1 +5123,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -3355,2 +5125,2 @@\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, as_PRegister($pTmp$$reg),\n-                 as_FloatRegister($src1$$reg), 0);\n+    __ sve_eors(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3362,1 +5132,1 @@\n-instruct vtest_anytrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_anytrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3364,1 +5134,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3367,1 +5138,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3371,1 +5142,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -3374,1 +5144,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -3376,2 +5146,2 @@\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, as_PRegister($pTmp$$reg),\n-                 as_FloatRegister($src1$$reg), -1);\n+    __ sve_ands(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3616,1 +5386,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -3631,2 +5401,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -3643,1 +5412,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3649,1 +5418,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3651,2 +5420,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -3654,3 +5422,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -3662,1 +5429,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3668,1 +5435,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3670,3 +5437,55 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n@@ -3674,1 +5493,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -3676,0 +5495,21 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -3677,1 +5517,1 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -3692,1 +5532,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -3707,2 +5547,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -3717,1 +5556,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -3719,1 +5558,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3725,1 +5564,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3727,2 +5566,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -3730,1 +5568,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -3732,1 +5570,1 @@\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3738,1 +5576,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3744,1 +5582,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3746,3 +5584,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n@@ -3750,1 +5586,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n@@ -3753,1 +5589,56 @@\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3759,0 +5650,20 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -3907,1 +5818,1 @@\n-instruct vstoremask_truecount(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_truecount(iRegINoSp dst, pRegGov src, immI esize, rFlagsReg cr) %{\n@@ -3911,2 +5822,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n@@ -3918,2 +5829,1 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, variant, ptrue, as_PRegister($src$$reg));\n@@ -3924,1 +5834,1 @@\n-instruct vstoremask_firsttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_firsttrue(iRegINoSp dst, pRegGov src, immI esize, pReg ptmp, rFlagsReg cr) %{\n@@ -3929,1 +5839,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -3935,2 +5845,3 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg),\n+                           Matcher::vector_length(this, $src));\n@@ -3941,1 +5852,1 @@\n-instruct vstoremask_lasttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_lasttrue(iRegINoSp dst, pRegGov src, immI esize, pReg ptmp, rFlagsReg cr) %{\n@@ -3946,1 +5857,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -3952,2 +5863,3 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg),\n+                           Matcher::vector_length(this, $src));\n@@ -3958,1 +5870,2 @@\n-instruct vstoremask_truecount_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vstoremask_truecount_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n@@ -3963,1 +5876,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -3969,4 +5882,3 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg),\n+                          variant, Matcher::vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, variant, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n@@ -3977,1 +5889,2 @@\n-instruct vstoremask_firsttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_firsttrue_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n@@ -3982,1 +5895,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -3988,4 +5901,5 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($pgtmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg),\n+                          variant, Matcher::vector_length(this, $src));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), as_PRegister($pgtmp$$reg),\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n@@ -3996,1 +5910,2 @@\n-instruct vstoremask_lasttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vstoremask_lasttrue_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n@@ -4007,4 +5922,7 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg),\n+                          variant, Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($src$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($ptmp$$reg), ptrue,\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":2648,"deletions":730,"binary":false,"changes":3378,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -156,0 +157,6 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    \/\/ Currently we support all masked vector opcodes.\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -241,1 +248,1 @@\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+            \"sve_ldr $dst, $pTmp, $mem\\t# load vector partial\" %}\n@@ -261,1 +268,1 @@\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+            \"sve_str $src, $pTmp, $mem\\t# store vector partial\" %}\n@@ -272,1 +279,120 @@\n-%}dnl\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($ptmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+dnl\n+dnl MASKALL_IMM($1,   $2  )\n+dnl MASKALL_IMM(type, size)\n+define(`MASKALL_IMM', `\n+instruct vmaskAll_imm$1(pRegGov dst, imm$1 src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    ifelse($1, `I', int, long) con = (ifelse($1, `I', int, long))$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl MASKALL($1,   $2  )\n+dnl MASKALL(type, size)\n+define(`MASKALL', `\n+instruct vmaskAll$1(pRegGov dst, ifelse($1, `I', iRegIorL2I, iRegL) src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ maskAll\n+MASKALL_IMM(I, B\/H\/S)\n+MASKALL(I, B\/H\/S)\n+MASKALL_IMM(L, D)\n+MASKALL(L, D)\n@@ -274,0 +400,40 @@\n+dnl\n+dnl MASK_LOGICAL_OP($1,        $2,      $3  )\n+dnl MASK_LOGICAL_OP(insn_name, op_name, insn)\n+define(`MASK_LOGICAL_OP', `\n+instruct vmask_$1(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd ($2 pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ $3(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and\/or\/xor\n+MASK_LOGICAL_OP(and, AndVMask, sve_and)\n+MASK_LOGICAL_OP(or, OrVMask, sve_orr)\n+MASK_LOGICAL_OP(xor, XorVMask, sve_eor)\n+\n+dnl\n+dnl MASK_LOGICAL_AND_NOT($1,   $2  )\n+dnl MASK_LOGICAL_AND_NOT(type, size)\n+define(`MASK_LOGICAL_AND_NOT', `\n+instruct vmask_and_not$1(pRegGov pd, pRegGov pn, pRegGov pm, imm$1_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and_not\n+MASK_LOGICAL_AND_NOT(I, B\/H\/S)\n+MASK_LOGICAL_AND_NOT(L, D)\n@@ -310,0 +476,34 @@\n+\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmpeq(as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -311,3 +511,3 @@\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE($1,        $2,      $3,           $4,   $5,          %6  )\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE(insn_name, op_name, element_type, size, min_vec_len, insn)\n-define(`UNARY_OP_TRUE_PREDICATE_ETYPE', `\n+dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_TRUE_PREDICATE', `\n@@ -316,1 +516,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == $3);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -319,1 +519,1 @@\n-  format %{ \"$6 $dst, $src\\t# vector (sve) ($4)\" %}\n+  format %{ \"$4 $dst, $src\\t# vector (sve) ($3)\" %}\n@@ -321,1 +521,1 @@\n-    __ $6(as_FloatRegister($dst$$reg), __ $4,\n+    __ $4(as_FloatRegister($dst$$reg), __ $3,\n@@ -329,10 +529,34 @@\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsB, AbsVB, T_BYTE,   B, 16, sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsS, AbsVS, T_SHORT,  H, 8,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsI, AbsVI, T_INT,    S, 4,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsL, AbsVL, T_LONG,   D, 2,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsF, AbsVF, T_FLOAT,  S, 4,  sve_fabs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsD, AbsVD, T_DOUBLE, D, 2,  sve_fabs)\n-dnl\n-dnl BINARY_OP_UNPREDICATED($1,        $2       $3,   $4           $5  )\n-dnl BINARY_OP_UNPREDICATED(insn_name, op_name, size, min_vec_len, insn)\n-define(`BINARY_OP_UNPREDICATED', `\n+UNARY_OP_TRUE_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_TRUE_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl UNARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src, $pg, $dst_src\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\/\/ vector abs - predicated\n+UNARY_OP_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl\n+dnl BINARY_OP_UNPREDICATE($1,        $2       $3,   $4           $5  )\n+dnl BINARY_OP_UNPREDICATE(insn_name, op_name, size, min_vec_len, insn)\n+define(`BINARY_OP_UNPREDICATE', `\n@@ -351,1 +575,18 @@\n-\n+dnl\n+dnl\n+dnl BINARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl BINARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`BINARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src1$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -353,9 +594,18 @@\n-BINARY_OP_UNPREDICATED(vaddB, AddVB, B, 16, sve_add)\n-BINARY_OP_UNPREDICATED(vaddS, AddVS, H, 8,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddI, AddVI, S, 4,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddL, AddVL, D, 2,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddF, AddVF, S, 4,  sve_fadd)\n-BINARY_OP_UNPREDICATED(vaddD, AddVD, D, 2,  sve_fadd)\n-dnl\n-dnl BINARY_OP_UNSIZED($1,        $2,      $3,          $4  )\n-dnl BINARY_OP_UNSIZED(insn_name, op_name, min_vec_len, insn)\n+BINARY_OP_UNPREDICATE(vaddB, AddVB, B, 16, sve_add)\n+BINARY_OP_UNPREDICATE(vaddS, AddVS, H, 8,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddI, AddVI, S, 4,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddL, AddVL, D, 2,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddF, AddVF, S, 4,  sve_fadd)\n+BINARY_OP_UNPREDICATE(vaddD, AddVD, D, 2,  sve_fadd)\n+\n+\/\/ vector add - predicated\n+BINARY_OP_PREDICATE(vaddB, AddVB, B, sve_add)\n+BINARY_OP_PREDICATE(vaddS, AddVS, H, sve_add)\n+BINARY_OP_PREDICATE(vaddI, AddVI, S, sve_add)\n+BINARY_OP_PREDICATE(vaddL, AddVL, D, sve_add)\n+BINARY_OP_PREDICATE(vaddF, AddVF, S, sve_fadd)\n+BINARY_OP_PREDICATE(vaddD, AddVD, D, sve_fadd)\n+\n+dnl\n+dnl BINARY_OP_UNSIZED($1,        $2,      $3  )\n+dnl BINARY_OP_UNSIZED(insn_name, op_name, insn)\n@@ -367,1 +617,1 @@\n-  format %{ \"$4  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"$3  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -369,1 +619,1 @@\n-    __ $4(as_FloatRegister($dst$$reg),\n+    __ $3(as_FloatRegister($dst$$reg),\n@@ -375,1 +625,1 @@\n-\n+dnl\n@@ -377,1 +627,1 @@\n-BINARY_OP_UNSIZED(vand, AndV, 16, sve_and)\n+BINARY_OP_UNSIZED(vand, AndV, sve_and)\n@@ -380,1 +630,1 @@\n-BINARY_OP_UNSIZED(vor, OrV, 16, sve_orr)\n+BINARY_OP_UNSIZED(vor, OrV, sve_orr)\n@@ -383,1 +633,28 @@\n-BINARY_OP_UNSIZED(vxor, XorV, 16, sve_eor)\n+BINARY_OP_UNSIZED(vxor, XorV, sve_eor)\n+\n+dnl BINARY_LOGIC_OP_PREDICATE($1,        $2,      $3  )\n+dnl BINARY_LOGIC_OP_PREDICATE(insn_name, op_name, insn)\n+define(`BINARY_LOGIC_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ $3(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector and - predicated\n+BINARY_LOGIC_OP_PREDICATE(vand, AndV, sve_and)\n+\n+\/\/ vector or - predicated\n+BINARY_LOGIC_OP_PREDICATE(vor, OrV, sve_orr)\n+\n+\/\/ vector xor - predicated\n+BINARY_LOGIC_OP_PREDICATE(vxor, XorV, sve_eor)\n@@ -409,1 +686,1 @@\n-\n+dnl\n@@ -450,1 +727,1 @@\n-\n+dnl\n@@ -455,1 +732,3 @@\n-\/\/ vector min\/max\n+\/\/ vector float div - predicated\n+BINARY_OP_PREDICATE(vfdivF, DivVF, S, sve_fdiv)\n+BINARY_OP_PREDICATE(vfdivD, DivVD, D, sve_fdiv)\n@@ -457,1 +736,5 @@\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX($1     , $2, $3   , $4  )\n+dnl VMINMAX(op_name, op, finsn, insn)\n+define(`VMINMAX', `\n+instruct v$1(vReg dst_src1, vReg src2) %{\n@@ -459,1 +742,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src1 ($2 dst_src1 src2));\n@@ -461,1 +744,1 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -466,1 +749,1 @@\n-      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n@@ -469,2 +752,2 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n@@ -475,1 +758,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max\n+VMINMAX(min, MinV, sve_fmin, sve_smin)\n+VMINMAX(max, MaxV, sve_fmax, sve_smax)\n@@ -477,1 +764,5 @@\n-instruct vmax(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX_PREDICATE($1     , $2, $3   , $4  )\n+dnl VMINMAX_PREDICATE(op_name, op, finsn, insn)\n+define(`VMINMAX_PREDICATE', `\n+instruct v$1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -479,1 +770,1 @@\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n@@ -481,1 +772,1 @@\n-  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n@@ -486,2 +777,2 @@\n-      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -489,3 +780,3 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -495,1 +786,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max - predicated\n+VMINMAX_PREDICATE(min, MinV, sve_fmin, sve_smin)\n+VMINMAX_PREDICATE(max, MaxV, sve_fmax, sve_smax)\n@@ -518,0 +813,21 @@\n+dnl\n+dnl VFMLA_PREDICATE($1,   $2  )\n+dnl VFMLA_PREDICATE(type, size)\n+define(`VFMLA_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmla$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fmla - predicated\n+VFMLA_PREDICATE(F, S)\n+VFMLA_PREDICATE(D, D)\n+\n@@ -648,1 +964,1 @@\n-\n+dnl\n@@ -654,2 +970,10 @@\n-BINARY_OP_UNPREDICATED(vmulF, MulVF, S, 4, sve_fmul)\n-BINARY_OP_UNPREDICATED(vmulD, MulVD, D, 2, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulF, MulVF, S, 4, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulD, MulVD, D, 2, sve_fmul)\n+\n+\/\/ vector mul - predicated\n+BINARY_OP_PREDICATE(vmulB, MulVB, B, sve_mul)\n+BINARY_OP_PREDICATE(vmulS, MulVS, H, sve_mul)\n+BINARY_OP_PREDICATE(vmulI, MulVI, S, sve_mul)\n+BINARY_OP_PREDICATE(vmulL, MulVL, D, sve_mul)\n+BINARY_OP_PREDICATE(vmulF, MulVF, S, sve_fmul)\n+BINARY_OP_PREDICATE(vmulD, MulVD, D, sve_fmul)\n@@ -657,16 +981,0 @@\n-dnl\n-dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4,            $5  )\n-dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, min_vec_bytes, insn)\n-define(`UNARY_OP_TRUE_PREDICATE', `\n-instruct $1(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst ($2 src));\n-  ins_cost(SVE_COST);\n-  format %{ \"$5 $dst, $src\\t# vector (sve) ($3)\" %}\n-  ins_encode %{\n-    __ $5(as_FloatRegister($dst$$reg), __ $3,\n-         ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n@@ -674,2 +982,6 @@\n-UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, 16, sve_fneg)\n-UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, 16, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, sve_fneg)\n+\n+\/\/ vector fneg - predicated\n+UNARY_OP_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_PREDICATE(vnegD, NegVD, D, sve_fneg)\n@@ -691,1 +1003,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -694,4 +1006,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -700,1 +1011,1 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n@@ -702,2 +1013,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -708,3 +1017,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -712,5 +1019,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -718,6 +1024,3 @@\n-    Assembler::SIMD_RegVariant size =\n-      __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n@@ -728,1 +1031,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -730,2 +1033,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -733,5 +1035,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -739,6 +1039,4 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src3$$reg),\n-                   as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n@@ -751,1 +1049,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -755,0 +1053,1 @@\n+  effect(KILL cr);\n@@ -756,1 +1055,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -758,1 +1057,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg), 0);\n@@ -763,1 +1062,1 @@\n-instruct vloadmaskS(vReg dst, vReg src) %{\n+instruct vloadmaskS(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -767,0 +1066,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -768,2 +1068,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskS $dst, $src\\t# vector load mask (sve) (B to H)\" %}\n@@ -771,2 +1070,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ H, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -777,1 +1076,1 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n+instruct vloadmaskI(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -782,0 +1081,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -783,3 +1083,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n+  format %{ \"vloadmaskI $dst, $src\\t# vector load mask (sve) (B to S)\" %}\n@@ -787,3 +1085,3 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ S, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -794,1 +1092,1 @@\n-instruct vloadmaskL(vReg dst, vReg src) %{\n+instruct vloadmaskL(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n@@ -799,0 +1097,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -800,4 +1099,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmaskL $dst, $src\\t# vector load mask (sve) (B to D)\" %}\n@@ -805,4 +1101,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ S, as_FloatRegister($tmp$$reg));\n+    __ sve_uunpklo(as_FloatRegister($tmp$$reg), __ D, as_FloatRegister($tmp$$reg));\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ D, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -815,1 +1111,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -819,1 +1115,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremaskB $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -821,2 +1117,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -827,1 +1122,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremaskS(vReg dst, pRegGov src, vReg tmp, immI_2 size) %{\n@@ -832,3 +1127,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+  format %{ \"vstoremaskS $dst, $src\\t# vector store mask (sve) (H to B)\" %}\n@@ -836,0 +1129,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ H, as_PRegister($src$$reg), 1, false);\n@@ -838,4 +1132,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -846,1 +1137,1 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+instruct vstoremaskI(vReg dst, pRegGov src, vReg tmp, immI_4 size) %{\n@@ -851,4 +1142,1 @@\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+  format %{ \"vstoremaskI $dst, $src\\t# vector store mask (sve) (S to B)\" %}\n@@ -856,0 +1144,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($src$$reg), 1, false);\n@@ -858,1 +1147,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -861,2 +1150,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -867,1 +1154,1 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+instruct vstoremaskL(vReg dst, pRegGov src, vReg tmp, immI_8 size) %{\n@@ -872,5 +1159,1 @@\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+  format %{ \"vstoremaskL $dst, $src\\t# vector store mask (sve) (D to B)\" %}\n@@ -878,0 +1161,1 @@\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($src$$reg), 1, false);\n@@ -880,1 +1164,1 @@\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n@@ -885,2 +1169,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -891,6 +1173,6 @@\n-dnl\n-dnl VLOADMASK_LOADV($1,    $2  )\n-dnl VLOADMASK_LOADV(esize, cond)\n-define(`VLOADMASK_LOADV', `\n-instruct vloadmask_loadV_$1(vReg dst, ifelse($1, `byte', vmemA, indirect) mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+dnl LOADVMASK($1,    $2  )\n+dnl LOADVMASK(esize, cond)\n+define(`LOADVMASK', `\n+instruct loadVMask_$1(pRegGov dst, ifelse($1, `byte', vmemA, indirect) mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -898,1 +1180,2 @@\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP tmp, KILL cr);\n@@ -900,2 +1183,2 @@\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) ($3)\" %}\n@@ -903,1 +1186,2 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -905,3 +1189,2 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n@@ -909,1 +1192,2 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmpne(as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+                 ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -915,9 +1199,10 @@\n-`ifelse($1, `byte', vmemA, indirect) mem, vReg src, vReg tmp, ifelse($1, `byte', immI_1, immI_gt_1) esize')\n-dnl\n-dnl STOREV_VSTOREMASK($1,  )\n-dnl STOREV_VSTOREMASK(esize)\n-define(`STOREV_VSTOREMASK', `\n-instruct storeV_vstoremask_$1(ARGLIST($1)) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+`ifelse($1, `byte', vmemA, indirect) mem, pRegGov src, vReg tmp')\n+dnl\n+dnl STOREVMASK($1,    $2  )\n+dnl STOREVMASK(esize, cond)\n+define(`STOREVMASK', `\n+instruct storeVMask_$1(ARGLIST($1)) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) $2);\n+  match(Set mem (StoreVectorMask mem src));\n@@ -925,0 +1210,2 @@\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) ($3)\" %}\n@@ -926,2 +1213,0 @@\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n@@ -929,0 +1214,2 @@\n+    \/\/ Convert the src predicate to vector. And store the vector elements\n+    \/\/ as boolean values.\n@@ -930,4 +1217,2 @@\n-    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), __ elemType_to_regVariant(from_vect_bt),\n+               as_PRegister($src$$reg), 1, false);\n@@ -942,0 +1227,52 @@\n+dnl LOADVMASK_PARTIAL($1,    $2  )\n+dnl LOADVMASK_PARTIAL(esize, cond)\n+define(`LOADVMASK_PARTIAL', `\n+instruct loadVMask_$1_partial(pRegGov dst, ifelse($1, `byte', vmemA, indirect) mem, vReg vtmp,\n+                              pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) $2);\n+  match(Set dst (LoadVectorMask mem));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"loadVMask $dst, $mem\\t# load vector mask partial (sve) ($3)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmpne(as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl STOREVMASK_PARTIAL($1,    $2  )\n+dnl STOREVMASK_PARTIAL(esize, cond)\n+define(`STOREVMASK_PARTIAL', `\n+instruct storeVMask_$1_partial(ifelse($1, `byte', vmemA, indirect) mem, pRegGov src, vReg vtmp,\n+                               pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->vect_type()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) $2);\n+  match(Set mem (StoreVectorMask mem src));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeVMask $src, $mem\\t# store vector mask partial (sve) ($3)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -943,11 +1280,21 @@\n-VLOADMASK_LOADV(byte, == 1)\n-VLOADMASK_LOADV(non_byte, > 1)\n-STOREV_VSTOREMASK(byte)\n-STOREV_VSTOREMASK(non_byte)\n-\n-\/\/ vector add reduction\n-\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+LOADVMASK(byte, == 1, B)\n+LOADVMASK(non_byte, > 1, H\/S\/D)\n+LOADVMASK_PARTIAL(byte, == 1, B)\n+LOADVMASK_PARTIAL(non_byte, > 1, H\/S\/D)\n+STOREVMASK(byte, == 1, B)\n+STOREVMASK(non_byte, > 1, H\/S\/D)\n+STOREVMASK_PARTIAL(byte, == 1, B)\n+STOREVMASK_PARTIAL(non_byte, > 1, H\/S\/D)\n+dnl\n+dnl REDUCE_I($1,        $2     )\n+dnl REDUCE_I(insn_name, op_name)\n+define(`REDUCE_I', `\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -955,1 +1302,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve) (may extend)\" %}\n@@ -958,11 +1305,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -971,3 +1310,29 @@\n-%}\n-\n-instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl\n+dnl REDUCE_L($1,        $2    )\n+dnl REDUCE_L(insn_name, op_name)\n+define(`REDUCE_L', `\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PARTIAL', `\n+instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -975,2 +1340,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -978,2 +1348,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve) (may extend)\" %}\n@@ -985,25 +1355,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1012,3 +1360,6 @@\n-%}\n-\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PARTIAL', `\n+instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1016,2 +1367,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -1019,2 +1375,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction partial (sve)\" %}\n@@ -1024,4 +1380,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1030,2 +1385,1 @@\n-%}\n-\n+%}')dnl\n@@ -1036,3 +1390,4 @@\n-instruct $1($3 src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1($3 src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1052,3 +1407,4 @@\n-instruct $1($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1_partial($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1057,1 +1413,1 @@\n-  format %{ \"sve_reduce_add$2 $src1_dst, $src1_dst, $src2\\t# add$2 reduction partial (sve) ($4)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $src1_dst, $src2\\t# $1 reduction partial (sve) ($4)\" %}\n@@ -1067,37 +1423,13 @@\n-REDUCE_ADDF(reduce_addF, F, vRegF, S)\n-REDUCE_ADDF_PARTIAL(reduce_addF_partial, F, vRegF, S)\n-REDUCE_ADDF(reduce_addD, D, vRegD, D)\n-REDUCE_ADDF_PARTIAL(reduce_addD_partial, D, vRegD, D)\n-\n-\/\/ vector and reduction\n-\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+dnl\n+dnl REDUCE_I_PREDICATE($1,        $2     )\n+dnl REDUCE_I_PREDICATE(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1105,1 +1437,1 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve) (may extend)\" %}\n@@ -1108,29 +1440,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1139,28 +1445,14 @@\n-%}\n-\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ vector or reduction\n-\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE($1,        $2    )\n+dnl REDUCE_L_PREDICATE(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1168,1 +1460,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n@@ -1170,12 +1462,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1184,7 +1467,14 @@\n-%}\n-\n-instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PREDICATE_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1192,2 +1482,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve) (may extend)\" %}\n@@ -1199,11 +1489,5 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1212,22 +1496,14 @@\n-%}\n-\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1235,2 +1511,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n@@ -1240,58 +1516,5 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ vector xor reduction\n-\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1299,8 +1522,10 @@\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE', `\n+instruct reduce_$1_masked($3 src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n@@ -1308,1 +1533,1 @@\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated (sve)\" %}\n@@ -1310,3 +1535,2 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1315,8 +1539,10 @@\n-%}\n-\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE_PARTIAL', `\n+instruct reduce_$1_masked_partial($3 src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n@@ -1324,1 +1550,1 @@\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated partial (sve)\" %}\n@@ -1326,1 +1552,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n@@ -1328,4 +1554,4 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1334,1 +1560,58 @@\n-%}\n+%}')dnl\n+dnl\n+\n+\/\/ vector add reduction\n+REDUCE_I(add, AddReductionVI)\n+REDUCE_L(add, AddReductionVL)\n+REDUCE_ADDF(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PARTIAL(addD, AddReductionVD, vRegD, D)\n+\n+\/\/ vector add reduction - predicated\n+REDUCE_I_PREDICATE(add, AddReductionVI)\n+REDUCE_L_PREDICATE(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PREDICATE_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PREDICATE_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addD, AddReductionVD, vRegD, D)\n+\n+\/\/ vector and reduction\n+REDUCE_I(and, AndReductionV)\n+REDUCE_L(and, AndReductionV)\n+REDUCE_I_PARTIAL(and, AndReductionV)\n+REDUCE_L_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector and reduction - predicated\n+REDUCE_I_PREDICATE(and, AndReductionV)\n+REDUCE_L_PREDICATE(and, AndReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(and, AndReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector or reduction\n+REDUCE_I(or, OrReductionV)\n+REDUCE_L(or, OrReductionV)\n+REDUCE_I_PARTIAL(or, OrReductionV)\n+REDUCE_L_PARTIAL(or, OrReductionV)\n+\n+\/\/ vector or reduction - predicated\n+REDUCE_I_PREDICATE(or, OrReductionV)\n+REDUCE_L_PREDICATE(or, OrReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(or, OrReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(or, OrReductionV)\n+\n+\/\/ vector xor reduction\n+REDUCE_I(eor, XorReductionV)\n+REDUCE_L(eor, XorReductionV)\n+REDUCE_I_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PARTIAL(eor, XorReductionV)\n+\n+\/\/ vector xor reduction - predicated\n+REDUCE_I_PREDICATE(eor, XorReductionV)\n+REDUCE_L_PREDICATE(eor, XorReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(eor, XorReductionV)\n@@ -1337,2 +1620,2 @@\n-dnl REDUCE_MAXMIN_I($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I(min_max, op_mame, cmp)\n+dnl REDUCE_MAXMIN_I($1,        $2     )\n+dnl REDUCE_MAXMIN_I(insn_name, op_name)\n@@ -1340,5 +1623,5 @@\n-instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1346,1 +1629,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1348,1 +1631,1 @@\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1B\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve)\" %}\n@@ -1351,5 +1634,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1360,2 +1641,2 @@\n-dnl REDUCE_MAXMIN_L($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L(min_max, op_name, cmp)\n+dnl REDUCE_MAXMIN_L($1,        $2     )\n+dnl REDUCE_MAXMIN_L(insn_name, op_name)\n@@ -1363,2 +1644,3 @@\n-instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1367,1 +1649,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1369,1 +1651,1 @@\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n@@ -1371,4 +1653,3 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1379,2 +1660,2 @@\n-dnl REDUCE_MAXMIN_I_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_mame, cmp)\n+dnl REDUCE_MAXMIN_I_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_name)\n@@ -1384,4 +1665,4 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1390,2 +1671,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1I partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve)\" %}\n@@ -1397,5 +1678,3 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1406,2 +1685,2 @@\n-dnl REDUCE_MAXMIN_L_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name, cmp)\n+dnl REDUCE_MAXMIN_L_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name)\n@@ -1411,1 +1690,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1415,0 +1695,44 @@\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction  partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_I_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1416,1 +1740,48 @@\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n@@ -1420,5 +1791,5 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1433,1 +1804,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1438,2 +1810,1 @@\n-  format %{ \"sve_f$1v $dst, $src2 # vector (sve) ($4)\\n\\t\"\n-            \"f$1s $dst, $dst, $src1\\t# $1 reduction $2\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction (sve)\" %}\n@@ -1441,2 +1812,1 @@\n-    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src2$$reg));\n@@ -1448,1 +1818,0 @@\n-dnl\n@@ -1454,1 +1823,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1459,1 +1829,39 @@\n-  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# reduce $1 $4 partial (sve)\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE', `\n+instruct reduce_$1$2_masked($5 dst, $5 src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE_PARTIAL', `\n+instruct reduce_$1$2_masked_partial($5 dst, $5 src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated partial (sve)\" %}\n@@ -1463,0 +1871,2 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -1464,1 +1874,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1469,1 +1879,0 @@\n-\n@@ -1471,4 +1880,4 @@\n-REDUCE_MAXMIN_I(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV, GT)\n+REDUCE_MAXMIN_I(max, MaxReductionV)\n+REDUCE_MAXMIN_L(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV)\n@@ -1480,0 +1889,10 @@\n+\/\/ vector max reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_FMINMAX_PREDICATE(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(max, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, D, T_DOUBLE, D, vRegD)\n+\n@@ -1481,4 +1900,4 @@\n-REDUCE_MAXMIN_I(min, MinReductionV, LT)\n-REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV, LT)\n+REDUCE_MAXMIN_I(min, MinReductionV)\n+REDUCE_MAXMIN_L(min, MinReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV)\n@@ -1490,0 +1909,10 @@\n+\/\/ vector min reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_FMINMAX_PREDICATE(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(min, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, D, T_DOUBLE, D, vRegD)\n+\n@@ -1667,0 +2096,43 @@\n+\/\/ vector shift - predicated\n+BINARY_OP_PREDICATE(vasrB, RShiftVB,  B, sve_asr)\n+BINARY_OP_PREDICATE(vasrS, RShiftVS,  H, sve_asr)\n+BINARY_OP_PREDICATE(vasrI, RShiftVI,  S, sve_asr)\n+BINARY_OP_PREDICATE(vasrL, RShiftVL,  D, sve_asr)\n+BINARY_OP_PREDICATE(vlslB, LShiftVB,  B, sve_lsl)\n+BINARY_OP_PREDICATE(vlslS, LShiftVS,  H, sve_lsl)\n+BINARY_OP_PREDICATE(vlslI, LShiftVI,  S, sve_lsl)\n+BINARY_OP_PREDICATE(vlslL, LShiftVL,  D, sve_lsl)\n+BINARY_OP_PREDICATE(vlsrB, URShiftVB, B, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrS, URShiftVS, H, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrI, URShiftVI, S, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrL, URShiftVL, D, sve_lsr)\n+dnl\n+dnl VSHIFT_IMM_PREDICATED($1,        $2,      $3,       $4,   $5,   $6  )\n+dnl VSHIFT_IMM_PREDICATED(insn_name, op_name, op_name2, type, size, insn)\n+define(`VSHIFT_IMM_PREDICATED', `\n+instruct $1_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 (Binary dst_src ($3 shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$6 $dst_src, $pg, $dst_src, $shift\\t# vector (sve) ($4)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con ifelse(index(`$1', `vlsl'), 0, `>=', `>') 0 && con < $5, \"invalid shift immediate\");\n+    __ $6(as_FloatRegister($dst_src$$reg), __ $4, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+VSHIFT_IMM_PREDICATED(vasrB, RShiftVB,  RShiftCntV, B, 8,  sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrS, RShiftVS,  RShiftCntV, H, 16, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrI, RShiftVI,  RShiftCntV, S, 32, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrL, RShiftVL,  RShiftCntV, D, 64, sve_asr)\n+VSHIFT_IMM_PREDICATED(vlsrB, URShiftVB, RShiftCntV, B, 8,  sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrS, URShiftVS, RShiftCntV, H, 16, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrI, URShiftVI, RShiftCntV, S, 32, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrL, URShiftVL, RShiftCntV, D, 64, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlslB, LShiftVB,  LShiftCntV, B, 8,  sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslS, LShiftVS,  LShiftCntV, H, 16, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslI, LShiftVI,  LShiftCntV, S, 32, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslL, LShiftVL,  LShiftCntV, D, 64, sve_lsl)\n+\n@@ -1668,2 +2140,6 @@\n-UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, 16, sve_fsqrt)\n-UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, 16, sve_fsqrt)\n+UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n+\n+\/\/ vector sqrt - predicated\n+UNARY_OP_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n@@ -1672,6 +2148,14 @@\n-BINARY_OP_UNPREDICATED(vsubB, SubVB, B, 16, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubS, SubVS, H, 8, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubI, SubVI, S, 4, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubL, SubVL, D, 2, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubF, SubVF, S, 4, sve_fsub)\n-BINARY_OP_UNPREDICATED(vsubD, SubVD, D, 2, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubB, SubVB, B, 16, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubS, SubVS, H, 8, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubI, SubVI, S, 4, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubL, SubVL, D, 2, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubF, SubVF, S, 4, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubD, SubVD, D, 2, sve_fsub)\n+\n+\/\/ vector sub - predicated\n+BINARY_OP_PREDICATE(vsubB, SubVB, B, sve_sub)\n+BINARY_OP_PREDICATE(vsubS, SubVS, H, sve_sub)\n+BINARY_OP_PREDICATE(vsubI, SubVI, S, sve_sub)\n+BINARY_OP_PREDICATE(vsubL, SubVL, D, sve_sub)\n+BINARY_OP_PREDICATE(vsubF, SubVF, S, sve_fsub)\n+BINARY_OP_PREDICATE(vsubD, SubVD, D, sve_fsub)\n@@ -1681,2 +2165,3 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+instruct vmaskcast(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n@@ -1684,1 +2169,1 @@\n-  match(Set dst (VectorMaskCast dst));\n+  match(Set dst_src (VectorMaskCast dst_src));\n@@ -1686,1 +2171,1 @@\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n@@ -2061,5 +2546,2 @@\n-dnl\n-dnl VTEST($1,      $2,   $3,  $4  )\n-dnl VTEST(op_name, pred, imm, cond)\n-define(`VTEST', `\n-instruct vtest_$1`'(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -2067,2 +2549,3 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::$2);\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n@@ -2070,1 +2553,19 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n+            \"csetw $dst, EQ\\t# VectorTest (sve) - alltrue\" %}\n+  ins_encode %{\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n+%{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n@@ -2072,2 +2573,2 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, $3\\n\\t\"\n-            \"csetw $dst, $4\\t# VectorTest (sve) - $1\" %}\n+  format %{ \"sve_ptest $src1\\n\\t\"\n+            \"csetw $dst, NE\\t# VectorTest (sve) - anytrue\" %}\n@@ -2076,5 +2577,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src1$$reg), $3);\n-    __ csetw(as_Register($dst$$reg), Assembler::$4);\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::NE);\n@@ -2083,4 +2581,1 @@\n-%}')dnl\n-dnl\n-VTEST(alltrue, overflow, 0, EQ)\n-VTEST(anytrue, ne,      -1, NE)\n+%}\n@@ -2089,2 +2584,2 @@\n-dnl VTEST_PARTIAL($1,      $2,   $3,  $4  )\n-dnl VTEST_PARTIAL(op_name, pred, imm, cond)\n+dnl VTEST_PARTIAL($1,      $2,   $3,   $4  )\n+dnl VTEST_PARTIAL(op_name, pred, inst, cond)\n@@ -2092,1 +2587,1 @@\n-instruct vtest_$1_partial`'(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_$1_partial`'(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -2094,1 +2589,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -2097,1 +2593,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2101,1 +2597,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -2104,1 +2599,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -2106,2 +2601,2 @@\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, as_PRegister($pTmp$$reg),\n-                 as_FloatRegister($src1$$reg), $3);\n+    __ $3(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -2113,2 +2608,2 @@\n-VTEST_PARTIAL(alltrue, overflow, 0, EQ)\n-VTEST_PARTIAL(anytrue, ne,      -1, NE)\n+VTEST_PARTIAL(alltrue, overflow, sve_eors, EQ)\n+VTEST_PARTIAL(anytrue, ne,       sve_ands, NE)\n@@ -2333,1 +2828,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -2348,2 +2843,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -2360,1 +2854,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2366,1 +2860,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2368,2 +2862,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -2371,3 +2864,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -2379,1 +2871,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2385,1 +2877,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2387,3 +2879,55 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n@@ -2391,1 +2935,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -2393,0 +2937,21 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -2394,1 +2959,1 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -2409,1 +2974,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -2424,2 +2989,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -2434,1 +2998,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -2436,1 +3000,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2442,1 +3006,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2444,2 +3008,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -2447,1 +3010,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -2449,1 +3012,1 @@\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -2455,1 +3018,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2461,1 +3024,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2463,3 +3026,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n@@ -2467,1 +3028,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n@@ -2470,1 +3031,56 @@\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -2476,0 +3092,20 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2566,1 +3202,1 @@\n-instruct vstoremask_$1(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vstoremask_$1(iRegINoSp dst, pRegGov src, immI esize, ifelse($1, `truecount', `rFlagsReg cr', `pReg ptmp, rFlagsReg cr')) %{\n@@ -2570,2 +3206,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost($3 * SVE_COST);\n+  effect(ifelse($1, `truecount', `KILL cr', `TEMP ptmp, KILL cr'));\n+  ins_cost($3);\n@@ -2576,3 +3212,6 @@\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n+    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);dnl\n+ifelse(`$1', `truecount', `\n+    __ sve_cntp($dst$$Register, variant, ptrue, as_PRegister($src$$reg));', `\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg),\n+                           Matcher::vector_length(this, $src));')\n@@ -2584,3 +3223,3 @@\n-VSTOREMASK_REDUCTION(truecount, VectorMaskTrueCount, 2)\n-VSTOREMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 3)\n-VSTOREMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 4)\n+VSTOREMASK_REDUCTION(truecount, VectorMaskTrueCount, SVE_COST)\n+VSTOREMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 2 * SVE_COST)\n+VSTOREMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 3 * SVE_COST)\n@@ -2591,1 +3230,2 @@\n-instruct vstoremask_$1_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n+instruct vstoremask_$1_partial(iRegINoSp dst, pRegGov src, immI esize,\n+                               pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n@@ -2602,4 +3242,13 @@\n-    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg),\n+                          variant, Matcher::vector_length(this, $src));dnl\n+ifelse($1, `truecount', `\n+    __ sve_cntp($dst$$Register, variant, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));',\n+       $1, `firsttrue', `\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($src$$reg), as_PRegister($pgtmp$$reg),\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);', `\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($src$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant,\n+                           as_PRegister($ptmp$$reg), ptrue,\n+                           as_PRegister($ptmp$$reg), MaxVectorSize \/ size);')\n@@ -2610,2 +3259,2 @@\n-VSTOREMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 3)\n-VSTOREMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 4)\n+VSTOREMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 2)\n+VSTOREMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 3)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":1310,"deletions":661,"binary":false,"changes":1971,"status":"modified"},{"patch":"@@ -2967,0 +2967,26 @@\n+  void sve_shift_imm_encoding(SIMD_RegVariant T, int shift, bool isSHR,\n+                              int& tszh, int& tszl_imm) {\n+    \/* The encodings for the tszh:tszl:imm3 fields\n+     * for shift right is calculated as:\n+     *   0001 xxx       B, shift = 16  - UInt(tszh:tszl:imm3)\n+     *   001x xxx       H, shift = 32  - UInt(tszh:tszl:imm3)\n+     *   01xx xxx       S, shift = 64  - UInt(tszh:tszl:imm3)\n+     *   1xxx xxx       D, shift = 128 - UInt(tszh:tszl:imm3)\n+     * for shift left is calculated as:\n+     *   0001 xxx       B, shift = UInt(tszh:tszl:imm3) - 8\n+     *   001x xxx       H, shift = UInt(tszh:tszl:imm3) - 16\n+     *   01xx xxx       S, shift = UInt(tszh:tszl:imm3) - 32\n+     *   1xxx xxx       D, shift = UInt(tszh:tszl:imm3) - 64\n+     *\/\n+    assert(T != Q, \"Invalid register variant\");\n+    if (isSHR) {\n+      assert(((1 << (T + 3)) >= shift) && (shift > 0) , \"Invalid shift value\");\n+    } else {\n+      assert(((1 << (T + 3)) > shift) && (shift >= 0) , \"Invalid shift value\");\n+    }\n+    int cVal = (1 << ((T + 3) + (isSHR ? 1 : 0)));\n+    int encodedShift = isSHR ? cVal - shift : cVal + shift;\n+    tszh = encodedShift >> 5;\n+    tszl_imm = encodedShift & 0x1f;\n+  }\n+\n@@ -2978,0 +3004,1 @@\n+  INSN(sve_and,  0b00000100, 0b011010000); \/\/ vector and\n@@ -2980,1 +3007,1 @@\n-  INSN(sve_cnt,  0b00000100, 0b011010101)  \/\/ count non-zero bits\n+  INSN(sve_cnt,  0b00000100, 0b011010101); \/\/ count non-zero bits\n@@ -2982,0 +3009,1 @@\n+  INSN(sve_eor,  0b00000100, 0b011001000); \/\/ vector eor\n@@ -2988,0 +3016,1 @@\n+  INSN(sve_orr,  0b00000100, 0b011000000); \/\/ vector or\n@@ -3030,1 +3059,1 @@\n-  INSN(sve_fmla,  0b01100101, 1, 0b000); \/\/ floating-point fused multiply-add: Zda = Zda + Zn * Zm\n+  INSN(sve_fmla,  0b01100101, 1, 0b000); \/\/ floating-point fused multiply-add, writing addend: Zda = Zda + Zn * Zm\n@@ -3034,0 +3063,1 @@\n+  INSN(sve_fmad,  0b01100101, 1, 0b100); \/\/ floating-point fused multiply-add, writing multiplicand: Zda = Zm + Zda * Zn\n@@ -3055,22 +3085,2 @@\n-    \/* The encodings for the tszh:tszl:imm3 fields (bits 23:22 20:19 18:16)     \\\n-     * for shift right is calculated as:                                        \\\n-     *   0001 xxx       B, shift = 16  - UInt(tszh:tszl:imm3)                   \\\n-     *   001x xxx       H, shift = 32  - UInt(tszh:tszl:imm3)                   \\\n-     *   01xx xxx       S, shift = 64  - UInt(tszh:tszl:imm3)                   \\\n-     *   1xxx xxx       D, shift = 128 - UInt(tszh:tszl:imm3)                   \\\n-     * for shift left is calculated as:                                         \\\n-     *   0001 xxx       B, shift = UInt(tszh:tszl:imm3) - 8                     \\\n-     *   001x xxx       H, shift = UInt(tszh:tszl:imm3) - 16                    \\\n-     *   01xx xxx       S, shift = UInt(tszh:tszl:imm3) - 32                    \\\n-     *   1xxx xxx       D, shift = UInt(tszh:tszl:imm3) - 64                    \\\n-     *\/                                                                         \\\n-    assert(T != Q, \"Invalid register variant\");                                 \\\n-    if (isSHR) {                                                                \\\n-      assert(((1 << (T + 3)) >= shift) && (shift > 0) , \"Invalid shift value\"); \\\n-    } else {                                                                    \\\n-      assert(((1 << (T + 3)) > shift) && (shift >= 0) , \"Invalid shift value\"); \\\n-    }                                                                           \\\n-    int cVal = (1 << ((T + 3) + (isSHR ? 1 : 0)));                              \\\n-    int encodedShift = isSHR ? cVal - shift : cVal + shift;                     \\\n-    int tszh = encodedShift >> 5;                                               \\\n-    int tszl_imm = encodedShift & 0x1f;                                         \\\n+    int tszh, tszl_imm;                                                         \\\n+    sve_shift_imm_encoding(T, shift, isSHR, tszh, tszl_imm);                    \\\n@@ -3087,0 +3097,15 @@\n+\/\/ SVE bitwise shift by immediate (predicated)\n+#define INSN(NAME, opc, isSHR)                                                  \\\n+  void NAME(FloatRegister Zdn, SIMD_RegVariant T, PRegister Pg, int shift) {    \\\n+    starti;                                                                     \\\n+    int tszh, tszl_imm;                                                         \\\n+    sve_shift_imm_encoding(T, shift, isSHR, tszh, tszl_imm);                    \\\n+    f(0b00000100, 31, 24), f(tszh, 23, 22), f(0b00, 21, 20), f(opc, 19, 16);    \\\n+    f(0b100, 15, 13), pgrf(Pg, 10), f(tszl_imm, 9, 5), rf(Zdn, 0);              \\\n+  }\n+\n+  INSN(sve_asr, 0b0000, \/* isSHR = *\/ true);\n+  INSN(sve_lsl, 0b0011, \/* isSHR = *\/ false);\n+  INSN(sve_lsr, 0b0001, \/* isSHR = *\/ true);\n+#undef INSN\n+\n@@ -3194,0 +3219,18 @@\n+\/\/ SVE predicate logical operations\n+#define INSN(NAME, op1, op2, op3) \\\n+  void NAME(PRegister Pd, PRegister Pg, PRegister Pn, PRegister Pm) { \\\n+    starti;                                                           \\\n+    f(0b00100101, 31, 24), f(op1, 23, 22), f(0b00, 21, 20);           \\\n+    prf(Pm, 16), f(0b01, 15, 14), prf(Pg, 10), f(op2, 9);             \\\n+    prf(Pn, 5), f(op3, 4), prf(Pd, 0);                                \\\n+  }\n+\n+  INSN(sve_and,  0b00, 0b0, 0b0);\n+  INSN(sve_ands, 0b01, 0b0, 0b0);\n+  INSN(sve_eor,  0b00, 0b1, 0b0);\n+  INSN(sve_eors, 0b01, 0b1, 0b0);\n+  INSN(sve_orr,  0b10, 0b0, 0b0);\n+  INSN(sve_orrs, 0b11, 0b0, 0b0);\n+  INSN(sve_bic,  0b00, 0b0, 0b1);\n+#undef INSN\n+\n@@ -3219,0 +3262,7 @@\n+  \/\/ SVE predicate test\n+  void sve_ptest(PRegister Pg, PRegister Pn) {\n+    starti;\n+    f(0b001001010101000011, 31, 14), prf(Pg, 10), f(0, 9), prf(Pn, 5), f(0, 4, 0);\n+  }\n+\n+  \/\/ SVE predicate initialize\n@@ -3225,0 +3275,28 @@\n+  \/\/ SVE predicate zero\n+  void sve_pfalse(PRegister pd) {\n+    starti;\n+    f(0b00100101, 31, 24), f(0b00, 23, 22), f(0b011000111001, 21, 10);\n+    f(0b000000, 9, 4), prf(pd, 0);\n+  }\n+\n+\/\/ SVE load\/store predicate register\n+#define INSN(NAME, op1)                                                  \\\n+  void NAME(PRegister Pt, const Address &a)  {                           \\\n+    starti;                                                              \\\n+    assert(a.index() == noreg, \"invalid address variant\");               \\\n+    f(op1, 31, 29), f(0b0010110, 28, 22), sf(a.offset() >> 3, 21, 16),   \\\n+    f(0b000, 15, 13), f(a.offset() & 0x7, 12, 10), srf(a.base(), 5),     \\\n+    f(0, 4), prf(Pt, 0);                                                 \\\n+  }\n+\n+  INSN(sve_ldr, 0b100); \/\/ LDR (predicate)\n+  INSN(sve_str, 0b111); \/\/ STR (predicate)\n+#undef INSN\n+\n+  \/\/ SVE move predicate register\n+  void sve_mov(PRegister Pd, PRegister Pn) {\n+    starti;\n+    f(0b001001011000, 31, 20), prf(Pn, 16), f(0b01, 15, 14), prf(Pn, 10);\n+    f(0, 9), prf(Pn, 5), f(0, 4), prf(Pd, 0);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":102,"deletions":24,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -979,4 +979,10 @@\n-                                            PRegister pg, PRegister pn, int length) {\n-  assert(pg->is_governing(), \"This register has to be a governing predicate register\");\n-  \/\/ The conditional flags will be clobbered by this function\n-  sve_cmpne(pn, size, pg, src, 0);\n+                                            PRegister pgtmp, PRegister ptmp) {\n+  assert(pgtmp->is_governing(), \"This register has to be a governing predicate register\");\n+  \/\/ The condition flags will be clobbered by this function\n+  sve_cmpne(ptmp, size, pgtmp, src, 0);\n+  sve_vmask_reduction(opc, dst, size, ptmp, pgtmp, ptmp, MaxVectorSize);\n+}\n+\n+void C2_MacroAssembler::sve_vmask_reduction(int opc, Register dst, SIMD_RegVariant size, PRegister src,\n+                                            PRegister pgtmp, PRegister ptmp, int length) {\n+  assert(pgtmp->is_governing(), \"This register has to be a governing predicate register\");\n@@ -985,1 +991,1 @@\n-      sve_cntp(dst, size, ptrue, pn);\n+      sve_cntp(dst, size, pgtmp, src);\n@@ -988,2 +994,2 @@\n-      sve_brkb(pn, pg, pn, false);\n-      sve_cntp(dst, size, ptrue, pn);\n+      sve_brkb(ptmp, pgtmp, src, false);\n+      sve_cntp(dst, size, pgtmp, ptmp);\n@@ -992,3 +998,3 @@\n-      sve_rev(pn, size, pn);\n-      sve_brkb(pn, ptrue, pn, false);\n-      sve_cntp(dst, size, ptrue, pn);\n+      sve_rev(ptmp, size, src);\n+      sve_brkb(ptmp, ptrue, ptmp, false);\n+      sve_cntp(dst, size, ptrue, ptmp);\n@@ -1003,0 +1009,99 @@\n+\n+void C2_MacroAssembler::sve_reduce_integral(int opc, Register dst, BasicType bt, Register src1,\n+                                            FloatRegister src2, PRegister pg, FloatRegister tmp) {\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported element type\");\n+  assert(pg->is_governing(), \"This register has to be a governing predicate register\");\n+  assert_different_registers(src1, dst);\n+  \/\/ Register \"dst\" and \"tmp\" are to be clobbered, and \"src1\" and \"src2\" should be preserved.\n+  Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+  switch (opc) {\n+    case Op_AddReductionVI: {\n+      sve_uaddv(tmp, size, pg, src2);\n+      smov(dst, tmp, size, 0);\n+      if (bt == T_BYTE) {\n+        addw(dst, src1, dst, ext::sxtb);\n+      } else if (bt == T_SHORT) {\n+        addw(dst, src1, dst, ext::sxth);\n+      } else {\n+        addw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_AddReductionVL: {\n+      sve_uaddv(tmp, size, pg, src2);\n+      umov(dst, tmp, size, 0);\n+      add(dst, dst, src1);\n+      break;\n+    }\n+    case Op_AndReductionV: {\n+      sve_andv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        andr(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        andw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_OrReductionV: {\n+      sve_orv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        orr(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        orrw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_XorReductionV: {\n+      sve_eorv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        eor(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        eorw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_MaxReductionV: {\n+      sve_smaxv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        cmp(dst, src1);\n+        csel(dst, dst, src1, Assembler::GT);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        cmpw(dst, src1);\n+        cselw(dst, dst, src1, Assembler::GT);\n+      }\n+      break;\n+    }\n+    case Op_MinReductionV: {\n+      sve_sminv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        cmp(dst, src1);\n+        csel(dst, dst, src1, Assembler::LT);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        cmpw(dst, src1);\n+        cselw(dst, dst, src1, Assembler::LT);\n+      }\n+      break;\n+    }\n+    default:\n+      assert(false, \"unsupported\");\n+      ShouldNotReachHere();\n+  }\n+\n+  if (opc == Op_AndReductionV || opc == Op_OrReductionV || opc == Op_XorReductionV) {\n+    if (bt == T_BYTE) {\n+      sxtb(dst, dst);\n+    } else if (bt == T_SHORT) {\n+      sxth(dst, dst);\n+    }\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":115,"deletions":10,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -65,1 +65,7 @@\n-                           PRegister pg, PRegister pn, int length = MaxVectorSize);\n+                           PRegister pgtmp, PRegister ptmp);\n+\n+  void sve_vmask_reduction(int opc, Register dst, SIMD_RegVariant size, PRegister src,\n+                           PRegister pgtmp, PRegister ptmp, int length);\n+\n+  void sve_reduce_integral(int opc, Register dst, BasicType bt, Register src1,\n+                           FloatRegister src2, PRegister pg, FloatRegister tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -318,0 +318,1 @@\n+  PRegSet               _p_regs;\n@@ -331,0 +332,2 @@\n+        } else if (vm_reg->is_PRegister()) {\n+          _p_regs += PRegSet::of(vm_reg->as_PRegister());\n@@ -344,1 +347,2 @@\n-      _fp_regs() {\n+      _fp_regs(),\n+      _p_regs() {\n@@ -352,0 +356,1 @@\n+    __ push_p(_p_regs, sp);\n@@ -356,0 +361,1 @@\n+    __ pop_p(_p_regs, sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2049,0 +2049,76 @@\n+\/\/ Return the number of dwords pushed\n+int MacroAssembler::push_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  const int num_of_regs = PRegisterImpl::number_of_saved_registers;\n+  unsigned char regs[num_of_regs];\n+  int count = 0;\n+  for (int reg = 0; reg < num_of_regs; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_push_bytes = align_up(sve_predicate_size_in_slots *\n+                                  VMRegImpl::stack_slot_size * count, 16);\n+  sub(stack, stack, total_push_bytes);\n+  for (int i = 0; i < count; i++) {\n+    sve_str(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  return total_push_bytes \/ 8;\n+}\n+\n+\/\/ Return the number of dwords poped\n+int MacroAssembler::pop_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  const int num_of_regs = PRegisterImpl::number_of_saved_registers;\n+  unsigned char regs[num_of_regs];\n+  int count = 0;\n+  for (int reg = 0; reg < num_of_regs; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_pop_bytes = align_up(sve_predicate_size_in_slots *\n+                                 VMRegImpl::stack_slot_size * count, 16);\n+  for (int i = count - 1; i >= 0; i--) {\n+    sve_ldr(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  add(stack, stack, total_pop_bytes);\n+  return total_pop_bytes \/ 8;\n+}\n+\n@@ -2507,1 +2583,1 @@\n-                                    int sve_vector_size_in_bytes) {\n+                                    int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n@@ -2524,0 +2600,6 @@\n+  if (save_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    sub(sp, sp, total_predicate_in_bytes);\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      sve_str(as_PRegister(i), Address(sp, i));\n+    }\n+  }\n@@ -2527,1 +2609,7 @@\n-                                   int sve_vector_size_in_bytes) {\n+                                   int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n+  if (restore_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    for (int i = PRegisterImpl::number_of_saved_registers - 1; i >= 0; i--) {\n+      sve_ldr(as_PRegister(i), Address(sp, i));\n+    }\n+    add(sp, sp, total_predicate_in_bytes);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":90,"deletions":2,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -458,0 +458,3 @@\n+  int push_p(unsigned int bitset, Register stack);\n+  int pop_p(unsigned int bitset, Register stack);\n+\n@@ -469,0 +472,3 @@\n+  void push_p(PRegSet regs, Register stack) { if (regs.bits()) push_p(regs.bits(), stack); }\n+  void pop_p(PRegSet regs, Register stack) { if (regs.bits()) pop_p(regs.bits(), stack); }\n+\n@@ -868,1 +874,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                      int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -870,1 +876,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                     int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -1342,0 +1348,1 @@\n+\n@@ -1345,0 +1352,4 @@\n+  void spill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_str(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1355,0 +1366,1 @@\n+\n@@ -1358,0 +1370,4 @@\n+  void unspill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_ldr(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1380,0 +1396,6 @@\n+  void spill_copy_sve_predicate_stack_to_stack(int src_offset, int dst_offset,\n+                                               int sve_predicate_reg_size_in_bytes) {\n+    sve_ldr(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, src_offset));\n+    sve_str(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, dst_offset));\n+    reinitialize_ptrue();\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -37,1 +37,2 @@\n-  = ConcreteRegisterImpl::max_fpr + PRegisterImpl::number_of_registers;\n+  = ConcreteRegisterImpl::max_fpr +\n+    PRegisterImpl::number_of_registers * PRegisterImpl::max_slots_per_register;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -246,0 +246,4 @@\n+    \/\/ AArch64 has 8 governing predicate registers, but p7 is used as an\n+    \/\/ all-1s register so the predicates to save are from p0 to p6 if we\n+    \/\/ don't have non-governing predicate registers support.\n+    number_of_saved_registers = number_of_governing_registers - 1,\n@@ -380,0 +384,1 @@\n+typedef AbstractRegSet<PRegister> PRegSet;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -104,1 +104,4 @@\n-  int v0_offset_in_bytes(void)   { return 0; }\n+  int v0_offset_in_bytes();\n+\n+  \/\/ Total stack size in bytes for saving sve predicate registers.\n+  int total_sve_predicate_in_bytes();\n@@ -142,1 +145,1 @@\n-  int r0_offset = (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n+  int r0_offset = v0_offset_in_bytes() + (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n@@ -146,0 +149,20 @@\n+int RegisterSaver::v0_offset_in_bytes() {\n+  \/\/ The floating point registers are located above the predicate registers if\n+  \/\/ they are present in the stack frame pushed by save_live_registers(). So the\n+  \/\/ offset depends on the saved total predicate vectors in the stack frame.\n+  return (total_sve_predicate_in_bytes() \/ VMRegImpl::stack_slot_size) * BytesPerInt;\n+}\n+\n+int RegisterSaver::total_sve_predicate_in_bytes() {\n+#if COMPILER2\n+  if (_save_vectors && Matcher::supports_scalable_vector()) {\n+    \/\/ The number of total predicate bytes is unlikely to be a multiple\n+    \/\/ of 16 bytes so we manually align it up.\n+    return align_up(Matcher::scalable_predicate_reg_slots() *\n+                    VMRegImpl::stack_slot_size *\n+                    PRegisterImpl::number_of_saved_registers, 16);\n+  }\n+#endif\n+  return 0;\n+}\n+\n@@ -150,0 +173,3 @@\n+  int sve_predicate_size_in_slots = 0;\n+  int total_predicate_in_bytes = total_sve_predicate_in_bytes();\n+  int total_predicate_in_slots = total_predicate_in_bytes \/ VMRegImpl::stack_slot_size;\n@@ -153,2 +179,5 @@\n-  sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n-  sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+  if (use_sve) {\n+    sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n+    sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n@@ -159,1 +188,0 @@\n-    int vect_words = 0;\n@@ -167,3 +195,4 @@\n-    vect_words = FloatRegisterImpl::number_of_registers * extra_save_slots_per_register \/\n-                 VMRegImpl::slots_per_word;\n-    additional_frame_words += vect_words;\n+    int extra_vector_bytes = extra_save_slots_per_register *\n+                             VMRegImpl::stack_slot_size *\n+                             FloatRegisterImpl::number_of_registers;\n+    additional_frame_words += ((extra_vector_bytes + total_predicate_in_bytes) \/ wordSize);\n@@ -187,1 +216,1 @@\n-  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes);\n+  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes, total_predicate_in_bytes);\n@@ -204,2 +233,1 @@\n-      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots),\n-                                r->as_VMReg());\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots), r->as_VMReg());\n@@ -213,1 +241,1 @@\n-      sp_offset = use_sve ? (sve_vector_size_in_slots * i) :\n+      sp_offset = use_sve ? (total_predicate_in_slots + sve_vector_size_in_slots * i) :\n@@ -218,2 +246,9 @@\n-    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n-                              r->as_VMReg());\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+  }\n+\n+  if (_save_vectors && use_sve) {\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      PRegister r = as_PRegister(i);\n+      int sp_offset = sve_predicate_size_in_slots * i;\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+    }\n@@ -228,1 +263,1 @@\n-                   Matcher::scalable_vector_reg_size(T_BYTE));\n+                   Matcher::scalable_vector_reg_size(T_BYTE), total_sve_predicate_in_bytes());\n@@ -241,0 +276,2 @@\n+\/\/ The SVE supported min vector size is 8 bytes and we need to save\n+\/\/ predicate registers when the vector size is 8 bytes as well.\n@@ -242,1 +279,1 @@\n-  return size > 8;\n+  return size > 8 || (UseSVE > 0 && size >= 8);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":53,"deletions":16,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -42,2 +42,1 @@\n-  assert( is_Register(), \"must be\");\n-  \/\/ Yuk\n+  assert(is_Register(), \"must be\");\n@@ -48,2 +47,1 @@\n-  assert( is_FloatRegister() && is_even(value()), \"must be\" );\n-  \/\/ Yuk\n+  assert(is_FloatRegister() && is_even(value()), \"must be\");\n@@ -55,1 +53,1 @@\n-  assert( is_PRegister(), \"must be\" );\n+  assert(is_PRegister(), \"must be\");\n","filename":"src\/hotspot\/cpu\/aarch64\/vmreg_aarch64.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -986,0 +986,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2180,0 +2180,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1539,0 +1539,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1826,0 +1826,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -272,0 +272,1 @@\n+  if( strcmp(opType,\"LoadVectorGatherMasked\")==0 )  return Form::idealV;\n@@ -273,0 +274,1 @@\n+  if( strcmp(opType,\"LoadVectorMask\")==0 )  return Form::idealV;\n@@ -290,0 +292,1 @@\n+  if( strcmp(opType,\"StoreVectorScatterMasked\")==0 )  return Form::idealV;\n@@ -291,0 +294,1 @@\n+  if( strcmp(opType,\"StoreVectorMask\")==0 )  return Form::idealV;\n","filename":"src\/hotspot\/share\/adlc\/forms.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2284,0 +2284,1 @@\n+  if (strcmp(name, \"RegVectMask\") == 0) size = globalAD->get_preproc_def(\"AARCH64\") ? 1 : 2;\n@@ -3517,1 +3518,3 @@\n-    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"StoreVector\", \"LoadVector\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorGatherMasked\", \"StoreVectorScatterMasked\",\n+    \"LoadVectorMask\", \"StoreVectorMask\",\n@@ -3821,1 +3824,1 @@\n-\/\/-------------------------- has_commutative_op -------------------------------\n+\/\/-------------------------- count_commutative_op -------------------------------\n@@ -3827,1 +3830,0 @@\n-    \"AddVB\",\"AddVS\",\"AddVI\",\"AddVL\",\"AddVF\",\"AddVD\",\n@@ -3829,1 +3831,0 @@\n-    \"AndV\",\n@@ -3831,1 +3832,0 @@\n-    \"MaxV\", \"MinV\",\n@@ -3833,1 +3833,0 @@\n-    \"MulVB\",\"MulVS\",\"MulVI\",\"MulVL\",\"MulVF\",\"MulVD\",\n@@ -3835,3 +3834,1 @@\n-    \"OrV\",\n-    \"XorI\",\"XorL\",\n-    \"XorV\"\n+    \"XorI\",\"XorL\"\n@@ -3839,1 +3836,0 @@\n-  int cnt = sizeof(commut_op_list)\/sizeof(char*);\n@@ -3841,1 +3837,8 @@\n-  if( _lChild && _rChild && (_lChild->_lChild || _rChild->_lChild) ) {\n+  static const char *commut_vector_op_list[] = {\n+    \"AddVB\", \"AddVS\", \"AddVI\", \"AddVL\", \"AddVF\", \"AddVD\",\n+    \"MulVB\", \"MulVS\", \"MulVI\", \"MulVL\", \"MulVF\", \"MulVD\",\n+    \"AndV\", \"OrV\", \"XorV\",\n+    \"MaxV\", \"MinV\"\n+  };\n+\n+  if (_lChild && _rChild && (_lChild->_lChild || _rChild->_lChild)) {\n@@ -3844,1 +3847,1 @@\n-    if( _rChild->_lChild == NULL && _rChild->_rChild == NULL ) {\n+    if (_rChild->_lChild == NULL && _rChild->_rChild == NULL) {\n@@ -3847,3 +3850,3 @@\n-      if ( form ) {\n-        OperandForm  *oper = form->is_operand();\n-        if( oper && oper->interface_type(globals) == Form::constant_interface )\n+      if (form) {\n+        OperandForm *oper = form->is_operand();\n+        if (oper && oper->interface_type(globals) == Form::constant_interface)\n@@ -3853,5 +3856,19 @@\n-    if( !is_const ) {\n-      for( int i=0; i<cnt; i++ ) {\n-        if( strcmp(_opType, commut_op_list[i]) == 0 ) {\n-          count++;\n-          _commutative_id = count; \/\/ id should be > 0\n+\n+    if (!is_const) {\n+      int scalar_cnt = sizeof(commut_op_list)\/sizeof(char*);\n+      int vector_cnt = sizeof(commut_vector_op_list)\/sizeof(char*);\n+      bool matched = false;\n+\n+      \/\/ Check the commutative vector op first. It's noncommutative if\n+      \/\/ the current node is a masked vector op, since a mask value\n+      \/\/ is added to the original vector node's input list and the original\n+      \/\/ first two inputs are packed into one BinaryNode. So don't swap\n+      \/\/ if one of the operands is a BinaryNode.\n+      for (int i = 0; i < vector_cnt; i++) {\n+        if (strcmp(_opType, commut_vector_op_list[i]) == 0) {\n+          if (strcmp(_lChild->_opType, \"Binary\") != 0 &&\n+              strcmp(_rChild->_opType, \"Binary\") != 0) {\n+            count++;\n+            _commutative_id = count; \/\/ id should be > 0\n+          }\n+          matched = true;\n@@ -3861,0 +3878,12 @@\n+\n+      \/\/ Then check the scalar op if the current op is not in\n+      \/\/ the commut_vector_op_list.\n+      if (!matched) {\n+        for (int i = 0; i < scalar_cnt; i++) {\n+          if (strcmp(_opType, commut_op_list[i]) == 0) {\n+            count++;\n+            _commutative_id = count; \/\/ id should be > 0\n+            break;\n+          }\n+        }\n+      }\n@@ -3863,1 +3892,1 @@\n-  if( _lChild )\n+  if (_lChild)\n@@ -3865,1 +3894,1 @@\n-  if( _rChild )\n+  if (_rChild)\n@@ -4091,0 +4120,1 @@\n+        strcmp(opType,\"MaskAll\")==0 ||\n@@ -4202,1 +4232,1 @@\n-    \"LoadVectorGather\", \"StoreVectorScatter\",\n+    \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorGatherMasked\", \"StoreVectorScatterMasked\",\n@@ -4209,0 +4239,3 @@\n+    \"LoadVectorMask\", \"StoreVectorMask\",\n+    \/\/ Next are vector mask ops.\n+    \"MaskAll\", \"AndVMask\", \"OrVMask\", \"XorVMask\", \"VectorMaskCast\",\n@@ -4211,2 +4244,1 @@\n-    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\",\n-    \"VectorMaskCast\"\n+    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\"\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":57,"deletions":25,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -812,1 +812,2 @@\n-   do_signature(vector_unary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/util\/function\/Function;)Ljava\/lang\/Object;\") \\\n+   do_signature(vector_unary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"              \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$UnaryOperation;)Ljava\/lang\/Object;\")                              \\\n@@ -816,2 +817,2 @@\n-   do_signature(vector_binary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                              \\\n-                                       \"Ljava\/util\/function\/BiFunction;)Ljava\/lang\/Object;\")                                                   \\\n+   do_signature(vector_binary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"             \\\n+                                       \"Ljava\/lang\/Object;Ljdk\/internal\/vm\/vector\/VectorSupport$BinaryOperation;)Ljava\/lang\/Object;\")          \\\n@@ -821,1 +822,1 @@\n-   do_signature(vector_ternary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                             \\\n+   do_signature(vector_ternary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;Ljava\/lang\/Object;\" \\\n@@ -845,0 +846,6 @@\n+  do_intrinsic(_VectorLoadMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_load_masked_op_name, vector_load_masked_op_sig, F_S)          \\\n+   do_signature(vector_load_masked_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\" \\\n+                                            \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                          \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$LoadVectorMaskedOperation;)Ljava\/lang\/Object;\")             \\\n+   do_name(vector_load_masked_op_name,     \"loadMasked\")                                                                                       \\\n+                                                                                                                                               \\\n@@ -850,2 +857,9 @@\n-  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S) \\\n-   do_signature(vector_reduction_coerced_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljava\/util\/function\/Function;)J\") \\\n+  do_intrinsic(_VectorStoreMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_store_masked_op_name, vector_store_masked_op_sig, F_S)       \\\n+   do_signature(vector_store_masked_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$Vector;\"  \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;I\"                            \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorMaskedOperation;)V\")                            \\\n+   do_name(vector_store_masked_op_name,     \"storeMasked\")                                                                                     \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S)\\\n+   do_signature(vector_reduction_coerced_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"     \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$ReductionOperation;)J\")                                  \\\n@@ -867,0 +881,1 @@\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                       \\\n@@ -871,1 +886,1 @@\n-   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                  \\\n+   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                 \\\n@@ -873,0 +888,1 @@\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                     \\\n@@ -889,2 +905,2 @@\n-   do_signature(vector_broadcast_int_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                              \\\n-                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                    \\\n+   do_signature(vector_broadcast_int_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                             \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;ILjava\/lang\/Object;\"                                  \\\n@@ -903,1 +919,1 @@\n-    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                    \\\n+    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                   \\\n@@ -906,1 +922,1 @@\n-                                     \"Ljava\/lang\/Object;I[II\"                                                                                  \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;I[II\"                                 \\\n@@ -913,1 +929,1 @@\n-    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                   \\\n+    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                  \\\n@@ -916,1 +932,1 @@\n-                                      \"Ljava\/lang\/Object;I[II\"                                                                                 \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;I[II\"                                \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":29,"deletions":13,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -681,0 +681,1 @@\n+  case vmIntrinsics::_VectorLoadMaskedOp:\n@@ -682,0 +683,1 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  if( _is_predicate ) tty->print(\"Predicate \");\n@@ -641,1 +642,2 @@\n-      } else if (lrg.num_regs() == 1) {\n+      } else if ((lrg.num_regs() == 1 && !lrg.is_scalable()) ||\n+                 (lrg.is_scalable() && lrg.scalable_reg_slots() == 1)) {\n@@ -657,1 +659,1 @@\n-          \/\/ We have to use pair [lo,lo+1] even for wide vectors because\n+          \/\/ We have to use pair [lo,lo+1] even for wide vectors\/vmasks because\n@@ -827,0 +829,14 @@\n+\n+        if (ireg == Op_RegVectMask) {\n+          assert(Matcher::has_predicated_vectors(), \"predicated vector should be supported\");\n+          lrg._is_predicate = 1;\n+          if (Matcher::supports_scalable_vector()) {\n+            lrg._is_scalable = 1;\n+            \/\/ For scalable predicate, when it is allocated in physical register,\n+            \/\/ num_regs is RegMask::SlotsPerRegVectMask for reg mask,\n+            \/\/ which may not be the actual physical register size.\n+            \/\/ If it is allocated in stack, we need to get the actual\n+            \/\/ physical length of scalable predicate register.\n+            lrg.set_scalable_reg_slots(Matcher::scalable_predicate_reg_slots());\n+          }\n+        }\n@@ -828,1 +844,1 @@\n-               ireg == Op_RegD || ireg == Op_RegL  || ireg == Op_RegVectMask,\n+               ireg == Op_RegD || ireg == Op_RegL || ireg == Op_RegVectMask,\n@@ -922,0 +938,2 @@\n+          assert(Matcher::has_predicated_vectors(), \"sanity\");\n+          assert(RegMask::num_registers(Op_RegVectMask) == RegMask::SlotsPerRegVectMask, \"sanity\");\n@@ -1374,0 +1392,5 @@\n+    } else if (lrg._is_predicate) {\n+      assert(num_regs == RegMask::SlotsPerRegVectMask, \"scalable predicate register\");\n+      num_regs = lrg.scalable_reg_slots();\n+      mask.clear_to_sets(num_regs);\n+      return mask.find_first_set(lrg, num_regs);\n@@ -1420,1 +1443,1 @@\n-  if (lrg._is_vector || lrg.num_regs() == 2) {\n+  if (lrg._is_vector || lrg.num_regs() == 2 || lrg.is_scalable()) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":27,"deletions":4,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -166,2 +166,2 @@\n-      \/\/ Should only be a vector for now, but it could also be a RegVectMask in future.\n-      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA), \"unexpected scalable reg\");\n+      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA) ||\n+             _is_predicate && (_num_regs == RegMask::SlotsPerRegVectMask), \"unexpected scalable reg\");\n@@ -198,0 +198,1 @@\n+         _is_predicate:1,       \/\/ True if in mask\/predicate registers\n","filename":"src\/hotspot\/share\/opto\/chaitin.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -416,0 +416,1 @@\n+macro(LoadVectorGatherMasked)\n@@ -418,0 +419,2 @@\n+macro(StoreVectorScatterMasked)\n+macro(LoadVectorMask)\n@@ -419,0 +422,1 @@\n+macro(StoreVectorMask)\n@@ -476,0 +480,4 @@\n+macro(MaskAll)\n+macro(AndVMask)\n+macro(OrVMask)\n+macro(XorVMask)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3428,0 +3428,2 @@\n+  case Op_LoadVectorGatherMasked:\n+  case Op_StoreVectorScatterMasked:\n@@ -3430,0 +3432,2 @@\n+  case Op_LoadVectorMask:\n+  case Op_StoreVectorMask:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -705,1 +705,1 @@\n-        case Op_StoreVectorScatter:\n+        case Op_StoreVectorMask:\n@@ -707,0 +707,2 @@\n+        case Op_StoreVectorScatter:\n+        case Op_StoreVectorScatterMasked:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -650,0 +650,2 @@\n+  case vmIntrinsics::_VectorLoadMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store*\/false);\n@@ -652,0 +654,2 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store=*\/true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -319,0 +319,1 @@\n+  bool inline_vector_mem_masked_operation(bool is_store);\n@@ -332,4 +333,5 @@\n-    VecMaskUseLoad,\n-    VecMaskUseStore,\n-    VecMaskUseAll,\n-    VecMaskNotUsed\n+    VecMaskUseLoad  = 1 << 0,\n+    VecMaskUseStore = 1 << 1,\n+    VecMaskUseAll   = VecMaskUseLoad | VecMaskUseStore,\n+    VecMaskUsePred  = 1 << 2,\n+    VecMaskNotUsed  = 1 << 3\n@@ -339,1 +341,1 @@\n-  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args = false);\n+  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, VectorMaskUseType mask_use_type, bool has_scalar_args = false);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -437,0 +437,18 @@\n+const int Matcher::scalable_predicate_reg_slots() {\n+  assert(Matcher::has_predicated_vectors() && Matcher::supports_scalable_vector(),\n+        \"scalable predicate vector should be supported\");\n+  int vector_reg_bit_size = Matcher::scalable_vector_reg_size(T_BYTE) << LogBitsPerByte;\n+  \/\/ We assume each predicate register is one-eighth of the size of\n+  \/\/ scalable vector register, one mask bit per vector byte.\n+  int predicate_reg_bit_size = vector_reg_bit_size >> 3;\n+  \/\/ Compute number of slots which is required when scalable predicate\n+  \/\/ register is spilled. E.g. if scalable vector register is 640 bits,\n+  \/\/ predicate register is 80 bits, which is 2.5 * slots.\n+  \/\/ We will round up the slot number to power of 2, which is required\n+  \/\/ by find_first_set().\n+  int slots = predicate_reg_bit_size & (BitsPerInt - 1)\n+              ? (predicate_reg_bit_size >> LogBitsPerInt) + 1\n+              : predicate_reg_bit_size >> LogBitsPerInt;\n+  return round_up_power_of_2(slots);\n+}\n+\n@@ -545,0 +563,2 @@\n+  } else {\n+    *idealreg2spillmask[Op_RegVectMask] = RegMask::Empty;\n@@ -617,0 +637,12 @@\n+    \/\/ Exclude last input arg stack slots to avoid spilling vector register there,\n+    \/\/ otherwise RegVectMask spills could stomp over stack slots in caller frame.\n+    for (; (in >= init_in) && (k < scalable_predicate_reg_slots()); k++) {\n+      scalable_stack_mask.Remove(in);\n+      in = OptoReg::add(in, -1);\n+    }\n+\n+    \/\/ For RegVectMask\n+    scalable_stack_mask.clear_to_sets(scalable_predicate_reg_slots());\n+    assert(scalable_stack_mask.is_AllStack(), \"should be infinite stack\");\n+    idealreg2spillmask[Op_RegVectMask]->OR(scalable_stack_mask);\n+\n@@ -2276,0 +2308,15 @@\n+  if (n->is_predicated_vector()) {\n+    \/\/ Restructure into binary trees for Matching.\n+    if (n->req() == 4) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, n->in(3));\n+      n->del_req(3);\n+    } else if (n->req() == 5) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, new BinaryNode(n->in(3), n->in(4)));\n+      n->del_req(4);\n+      n->del_req(3);\n+    }\n+    return;\n+  }\n+\n@@ -2415,0 +2462,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -2421,0 +2469,9 @@\n+    case Op_StoreVectorScatterMasked: {\n+      Node* pair = new BinaryNode(n->in(MemNode::ValueIn+1), n->in(MemNode::ValueIn+2));\n+      n->set_req(MemNode::ValueIn+1, pair);\n+      n->del_req(MemNode::ValueIn+2);\n+      pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+      n->set_req(MemNode::ValueIn, pair);\n+      n->del_req(MemNode::ValueIn+1);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -332,0 +332,2 @@\n+  static const bool match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt);\n+\n@@ -348,0 +350,2 @@\n+  \/\/ Actual max scalable predicate register length.\n+  static const int scalable_predicate_reg_slots();\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1137,1 +1137,1 @@\n-      if (store_Opcode() == Op_StoreVector) {\n+      if (st->is_StoreVector()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -779,1 +779,2 @@\n-    Flag_for_post_loop_opts_igvn     = 1 << 15,\n+    Flag_is_predicated_vector        = 1 << 15,\n+    Flag_for_post_loop_opts_igvn     = 1 << 16,\n@@ -988,0 +989,2 @@\n+  bool is_predicated_vector() const { return (_flags & Flag_is_predicated_vector) != 0; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -312,1 +312,1 @@\n-      assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      assert(val->ideal_reg() == Op_VecA || val->ideal_reg() == Op_RegVectMask, \"scalable register\");\n@@ -316,1 +316,1 @@\n-        n_regs = RegMask::SlotsPerVecA;\n+        n_regs = lrgs(val_idx)._is_predicate ? RegMask::SlotsPerRegVectMask : RegMask::SlotsPerVecA;\n@@ -321,2 +321,1 @@\n-      if (lrgs(val_idx).is_scalable()) {\n-        assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      if (lrgs(val_idx).is_scalable() && val->ideal_reg() == Op_VecA) {\n","filename":"src\/hotspot\/share\/opto\/postaloc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -240,1 +240,1 @@\n-  if (lrg.is_scalable()) {\n+  if (lrg.is_scalable() && lrg._is_vector) {\n","filename":"src\/hotspot\/share\/opto\/regmask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -996,1 +996,0 @@\n-\n@@ -2348,1 +2347,4 @@\n-const TypeVect* TypeVect::make(const Type *elem, uint length) {\n+const TypeVect* TypeVect::make(const Type *elem, uint length, bool is_mask) {\n+  if (is_mask) {\n+    return makemask(elem, length);\n+  }\n@@ -2374,1 +2376,5 @@\n-  if (Matcher::has_predicated_vectors()) {\n+  if (Matcher::has_predicated_vectors() &&\n+      \/\/ TODO: remove this condition once the backend is supported.\n+      \/\/ Workround to make tests pass on AVX-512\/SVE when predicate is not supported.\n+      \/\/ Could be removed once the backend is supported.\n+      Matcher::match_rule_supported_vector_masked(Op_StoreVectorMasked, MaxVectorSize, T_BOOLEAN)) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -799,1 +799,1 @@\n-  static const TypeVect *make(const BasicType elem_bt, uint length) {\n+  static const TypeVect *make(const BasicType elem_bt, uint length, bool is_mask = false) {\n@@ -801,1 +801,1 @@\n-    return make(get_const_basic_type(elem_bt), length);\n+    return make(get_const_basic_type(elem_bt), length, is_mask);\n@@ -804,1 +804,1 @@\n-  static const TypeVect *make(const Type* elem, uint length);\n+  static const TypeVect *make(const Type* elem, uint length, bool is_mask = false);\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -457,1 +457,1 @@\n-      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::makemask(masktype, num_elem)));\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,40 +62,78 @@\n-bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args) {\n-    bool is_supported = true;\n-    \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n-    \/\/ since in this case shift needs to be broadcasted.\n-    if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n-         (has_scalar_args &&\n-           !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n-      is_supported = false;\n-    }\n-\n-    int lshiftopc, rshiftopc;\n-    switch(elem_bt) {\n-      case T_BYTE:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftB;\n-        break;\n-      case T_SHORT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftS;\n-        break;\n-      case T_INT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftI;\n-        break;\n-      case T_LONG:\n-        lshiftopc = Op_LShiftL;\n-        rshiftopc = Op_URShiftL;\n-        break;\n-      default:\n-        assert(false, \"Unexpected type\");\n-    }\n-    int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n-    int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n-    if (!is_supported &&\n-        arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n-      is_supported = true;\n-    }\n-    return is_supported;\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt,\n+                                                 VectorMaskUseType mask_use_type, bool has_scalar_args) {\n+  bool is_supported = true;\n+\n+  \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n+  \/\/ since in this case shift needs to be broadcasted.\n+  if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n+       (has_scalar_args &&\n+         !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+    is_supported = false;\n+  }\n+\n+  if (is_supported) {\n+    \/\/ Check whether mask unboxing is supported.\n+    if ((mask_use_type & VecMaskUseLoad) != 0) {\n+      if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** Rejected vector mask loading (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[Op_VectorLoadMask], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+\n+    if ((mask_use_type & VecMaskUsePred) != 0) {\n+      if (!Matcher::has_predicated_vectors() ||\n+          !Matcher::match_rule_supported_vector_masked(opc, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[opc], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+  }\n+\n+  int lshiftopc, rshiftopc;\n+  switch(elem_bt) {\n+    case T_BYTE:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftB;\n+      break;\n+    case T_SHORT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftS;\n+      break;\n+    case T_INT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftI;\n+      break;\n+    case T_LONG:\n+      lshiftopc = Op_LShiftL;\n+      rshiftopc = Op_URShiftL;\n+      break;\n+    default:\n+      assert(false, \"Unexpected type\");\n+  }\n+  int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n+  int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n+  if (!is_supported &&\n+      arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n+    is_supported = true;\n+  }\n+  return is_supported;\n@@ -118,1 +156,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -133,1 +171,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -158,1 +196,1 @@\n-    if(!arch_supports_vector_rotate(sopc, num_elem, type, has_scalar_args)) {\n+    if(!arch_supports_vector_rotate(sopc, num_elem, type, mask_use_type, has_scalar_args)) {\n@@ -216,1 +254,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseLoad) {\n+  if ((mask_use_type & VecMaskUseLoad) != 0) {\n@@ -229,1 +267,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseStore) {\n+  if ((mask_use_type & VecMaskUseStore) != 0) {\n@@ -241,6 +279,12 @@\n-  return true;\n-}\n-\n-static bool is_vector_mask(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n-}\n+  if ((mask_use_type & VecMaskUsePred) != 0) {\n+    if (!Matcher::has_predicated_vectors() ||\n+        !Matcher::match_rule_supported_vector_masked(sopc, num_elem, type)) {\n+    #ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                      NodeClassNames[sopc], type2name(type), num_elem);\n+      }\n+    #endif\n+      return false;\n+    }\n+  }\n@@ -248,2 +292,1 @@\n-static bool is_vector_shuffle(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+  return true;\n@@ -262,4 +305,4 @@\n-\/\/ <VM>\n-\/\/ VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/            VM vm,\n-\/\/            Function<VM, VM> defaultImpl) {\n+\/\/ <V, M>\n+\/\/ V unaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/           int length, V v, M m,\n+\/\/           UnaryOperation<V, M> defaultImpl)\n@@ -268,4 +311,4 @@\n-\/\/ <VM>\n-\/\/ VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/             VM vm1, VM vm2,\n-\/\/             BiFunction<VM, VM, VM> defaultImpl) {\n+\/\/ <V, M>\n+\/\/ V binaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/            int length, V v1, V v2, M m,\n+\/\/            BinaryOperation<V, M> defaultImpl)\n@@ -274,4 +317,4 @@\n-\/\/ <VM>\n-\/\/ VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/              VM vm1, VM vm2, VM vm3,\n-\/\/              TernaryOperation<VM> defaultImpl) {\n+\/\/ <V, M>\n+\/\/ V ternaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/             int length, V v1, V v2, V v3, M m,\n+\/\/             TernaryOperation<V, M> defaultImpl)\n@@ -282,2 +325,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -291,2 +335,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -296,0 +340,1 @@\n+\n@@ -309,0 +354,28 @@\n+\n+  \/\/ \"argument(n + 5)\" should be the mask object. We assume it is \"null\" when no mask\n+  \/\/ is used to control this operation.\n+  const Type* vmask_type = gvn().type(argument(n + 5));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -331,0 +404,4 @@\n+  if (is_vector_mask(vbox_klass)) {\n+    assert(!is_masked_op, \"mask operations do not need mask to control\");\n+  }\n+\n@@ -353,3 +430,4 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if ((sopc != 0) &&\n-      !arch_supports_vector(sopc, num_elem, elem_bt, is_vector_mask(vbox_klass) ? VecMaskUseAll : VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  VectorMaskUseType mask_use_type = is_vector_mask(vbox_klass) ? VecMaskUseAll\n+                                      : is_masked_op ? VecMaskUseLoad : VecMaskNotUsed;\n+  if ((sopc != 0) && !arch_supports_vector(sopc, num_elem, elem_bt, mask_use_type)) {\n@@ -357,1 +435,1 @@\n-      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d is_masked_op=%d\",\n@@ -359,1 +437,1 @@\n-                    is_vector_mask(vbox_klass) ? 1 : 0);\n+                    is_vector_mask(vbox_klass) ? 1 : 0, is_masked_op ? 1 : 0);\n@@ -364,0 +442,10 @@\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && sopc != 0 && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=0 is_masked_op=1\",\n+                    n, sopc, num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -367,1 +455,1 @@\n-      opd3 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      opd3 = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n@@ -371,1 +459,1 @@\n-                        NodeClassNames[argument(6)->Opcode()]);\n+                        NodeClassNames[argument(7)->Opcode()]);\n@@ -378,1 +466,1 @@\n-      opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      opd2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n@@ -382,1 +470,1 @@\n-                        NodeClassNames[argument(5)->Opcode()]);\n+                        NodeClassNames[argument(6)->Opcode()]);\n@@ -389,1 +477,1 @@\n-      opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+      opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -393,1 +481,1 @@\n-                        NodeClassNames[argument(4)->Opcode()]);\n+                        NodeClassNames[argument(5)->Opcode()]);\n@@ -402,0 +490,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(n + 5), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(n + 5)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -420,1 +523,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, vt, is_vector_mask(vbox_klass));\n@@ -424,1 +527,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, opd3, vt);\n@@ -430,0 +533,11 @@\n+\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n+\n@@ -515,1 +629,2 @@\n-    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vt));\n+    const TypeVect* vmask_type = TypeVect::makemask(elem_bt, num_elem);\n+    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vmask_type));\n@@ -698,1 +813,0 @@\n-\n@@ -725,1 +839,1 @@\n-  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n+  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt), is_vector_mask(vbox_klass));\n@@ -762,1 +876,1 @@\n-\/\/               StoreVectorOperation<C, V> defaultImpl) {\n+\/\/               StoreVectorOperation<C, V> defaultImpl)\n@@ -926,2 +1040,1 @@\n-        const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n-        vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n+        vload = gvn().transform(new VectorLoadMaskNode(vload, TypeVect::makemask(elem_bt, num_elem)));\n@@ -946,6 +1059,14 @@\n-\/\/   <C, V extends Vector<?>, W extends IntVector, E, S extends VectorSpecies<E>>\n-\/\/   void loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n-\/\/                    Object base, long offset, \/\/ Unsafe addressing\n-\/\/                    W index_vector,\n-\/\/                    C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n-\/\/                    LoadVectorOperationWithMap<C, V, E, S> defaultImpl)\n+\/\/    <C, V, E, S extends VectorSpecies<E>,\n+\/\/     M extends VectorMask<E>>\n+\/\/    V loadMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/                 int length, Object base, long offset, M m\n+\/\/                 C container, int index, S s,  \/\/ Arguments for default implementation\n+\/\/                 LoadVectorMaskedOperation<C, V, E, S, M> defaultImpl) {\n+\/\/\n+\/\/    <C, V extends Vector<?>,\n+\/\/     M extends VectorMask<?>>\n+\/\/    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                     int length, Object base, long offset,\n+\/\/                     V v, M m,\n+\/\/                     C container, int index,  \/\/ Arguments for default implementation\n+\/\/                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n@@ -953,4 +1074,228 @@\n-\/\/    <C, V extends Vector<?>, W extends IntVector>\n-\/\/    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-\/\/                      Object base, long offset,    \/\/ Unsafe addressing\n-\/\/                      W index_vector, V v,\n+bool LibraryCallKit::inline_vector_mem_masked_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+\n+  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+\n+  \/\/ Save state and restore on bailout\n+  uint old_sp = sp();\n+  SafePointNode* old_map = clone_map();\n+\n+  Node* addr = make_unsafe_address(base, offset, elem_bt, true);\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+\n+  \/\/ Now handle special case where load\/store happens from\/to byte array but element type is not byte.\n+  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  \/\/ If there is no consistency between array and vector element types, it must be special byte array case\n+  if (arr_type != NULL && !using_byte_array && !elem_consistent_with_arr(elem_bt, arr_type)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s atype=%s\",\n+                    is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                    num_elem, type2name(elem_bt), type2name(arr_type->elem()->array_element_basic_type()));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  int mem_num_elem = using_byte_array ? num_elem * type2aelembytes(elem_bt) : num_elem;\n+  BasicType mem_elem_bt = using_byte_array ? T_BYTE : elem_bt;\n+  bool use_predicate = arch_supports_vector(is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked,\n+                                            mem_num_elem, mem_elem_bt,\n+                                            (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred));\n+  \/\/ Masked vector store operation needs the architecture predicate feature. We need to check\n+  \/\/ whether the predicated vector operation is supported by backend.\n+  if (is_store && !use_predicate) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=storeMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ This only happens for masked vector load. If predicate is not supported, then check whether\n+  \/\/ the normal vector load and blend operations are supported by backend.\n+  if (!use_predicate && (!arch_supports_vector(Op_LoadVector, mem_num_elem, mem_elem_bt, VecMaskNotUsed) ||\n+      !arch_supports_vector(Op_VectorBlend, mem_num_elem, mem_elem_bt, VecMaskUseLoad))) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=loadMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Since we are using byte array, we need to double check that the vector reinterpret operation\n+  \/\/ with byte type is supported by backend.\n+  if (using_byte_array) {\n+    if (!arch_supports_vector(Op_VectorReinterpret, mem_num_elem, T_BYTE, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s using_byte_array=1\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Since it needs to unbox the mask, we need to double check that the related load operations\n+  \/\/ for mask are supported by backend.\n+  if (!arch_supports_vector(Op_LoadVector, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(!is_vector_mask(vbox_klass) && is_vector_mask(mbox_klass), \"Invalid class type\");\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* mask = unbox_vector(is_store ? argument(8) : argument(7), mbox_type, elem_bt, num_elem);\n+  if (mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_store ? NodeClassNames[argument(8)->Opcode()]\n+                             : NodeClassNames[argument(7)->Opcode()]);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false; \/\/ operand unboxing failed\n+    }\n+    set_all_memory(reset_memory());\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the incoming vector to byte vector.\n+      const TypeVect* to_vect_type = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      val = gvn().transform(new VectorReinterpretNode(val, val->bottom_type()->is_vect(), to_vect_type));\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+    Node* vstore = gvn().transform(new StoreVectorMaskedNode(control(), memory(addr), addr, val, addr_type, mask));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    Node* vload = NULL;\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+\n+    if (use_predicate) {\n+      \/\/ Generate masked load vector node if predicate feature is supported.\n+      const TypeVect* vt = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      vload = gvn().transform(new LoadVectorMaskedNode(control(), memory(addr), addr, addr_type, vt, mask));\n+    } else {\n+      \/\/ Use the vector blend to implement the masked load vector. The biased elements are zeros.\n+      Node* zero = gvn().transform(gvn().zerocon(mem_elem_bt));\n+      zero = gvn().transform(VectorNode::scalar2vector(zero, mem_num_elem, Type::get_const_basic_type(mem_elem_bt)));\n+      vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, mem_num_elem, mem_elem_bt));\n+      vload = gvn().transform(new VectorBlendNode(zero, vload, mask));\n+    }\n+\n+    if (using_byte_array) {\n+      const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+      vload = gvn().transform(new VectorReinterpretNode(vload, vload->bottom_type()->is_vect(), to_vect_type));\n+    }\n+\n+    Node* box = box_vector(vload, vbox_type, elem_bt, num_elem);\n+    set_result(box);\n+  }\n+\n+  old_map->destruct(&_gvn);\n+\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/   <C, V extends Vector<?>, W extends Vector<Integer>, E,\n+\/\/    S extends VectorSpecies<E>, M extends VectorMask<E>>\n+\/\/   V loadWithMap(Class<?> vectorClass, Class<M> maskClass, Class<E> E, int length,\n+\/\/                 Class<?> vectorIndexClass,\n+\/\/                 Object base, long offset, \/\/ Unsafe addressing\n+\/\/                 W index_vector, M m,\n+\/\/                 C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+\/\/                 LoadVectorOperationWithMap<C, V, E, S, M> defaultImpl)\n+\/\/\n+\/\/    <C, V extends Vector<?>, W extends Vector<Integer>, M extends VectorMask<?>>\n+\/\/    void storeWithMap(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                      int length, Class<?> vectorIndexClass, Object base, long offset,    \/\/ Unsafe addressing\n+\/\/                      W index_vector, V v, M m,\n@@ -958,1 +1303,1 @@\n-\/\/                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+\/\/                      StoreVectorOperationWithMap<C, V, M> defaultImpl)\n@@ -962,3 +1307,4 @@\n-  const TypeInstPtr* elem_klass       = gvn().type(argument(1))->isa_instptr();\n-  const TypeInt*     vlen             = gvn().type(argument(2))->isa_int();\n-  const TypeInstPtr* vector_idx_klass = gvn().type(argument(3))->isa_instptr();\n+  const TypeInstPtr* mask_klass       = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass       = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen             = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* vector_idx_klass = gvn().type(argument(4))->isa_instptr();\n@@ -971,1 +1317,0 @@\n-                    NodeClassNames[argument(1)->Opcode()],\n@@ -973,1 +1318,2 @@\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -984,0 +1330,1 @@\n+\n@@ -991,0 +1338,1 @@\n+\n@@ -994,5 +1342,43 @@\n-  if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=no\",\n-                    is_scatter, is_scatter ? \"scatter\" : \"gather\",\n-                    num_elem, type2name(elem_bt));\n+  const Type* vmask_type = gvn().type(is_scatter ? argument(10) : argument(9));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(1)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Check whether the predicated gather\/scatter node is supported by architecture.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n+                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=1\",\n+                      is_scatter, is_scatter ? \"scatterMasked\" : \"gatherMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+  } else {\n+    \/\/ Check whether the normal gather\/scatter node is supported for non-masked operation.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=0\",\n+                      is_scatter, is_scatter ? \"scatter\" : \"gather\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -1000,1 +1386,0 @@\n-    return false; \/\/ not supported\n@@ -1006,1 +1391,1 @@\n-        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int ismask=no\",\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int is_masked_op=%d\",\n@@ -1008,1 +1393,1 @@\n-                      num_elem);\n+                      num_elem, is_masked_op ? 1 : 0);\n@@ -1011,1 +1396,1 @@\n-    }\n+  }\n@@ -1013,2 +1398,2 @@\n-  Node* base = argument(4);\n-  Node* offset = ConvL2X(argument(5));\n+  Node* base = argument(5);\n+  Node* offset = ConvL2X(argument(6));\n@@ -1036,0 +1421,1 @@\n+\n@@ -1038,1 +1424,0 @@\n-\n@@ -1040,1 +1425,0 @@\n-\n@@ -1048,2 +1432,1 @@\n-\n-  Node* index_vect = unbox_vector(argument(7), vbox_idx_type, T_INT, num_elem);\n+  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n@@ -1055,0 +1438,18 @@\n+\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(is_scatter ? argument(10) : argument(9), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_scatter ? NodeClassNames[argument(10)->Opcode()]\n+                               : NodeClassNames[argument(9)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n@@ -1057,1 +1458,1 @@\n-    Node* val = unbox_vector(argument(8), vbox_type, elem_bt, num_elem);\n+    Node* val = unbox_vector(argument(9), vbox_type, elem_bt, num_elem);\n@@ -1065,1 +1466,6 @@\n-    Node* vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    Node* vstore = NULL;\n+    if (mask != NULL) {\n+      vstore = gvn().transform(new StoreVectorScatterMaskedNode(control(), memory(addr), addr, addr_type, val, index_vect, mask));\n+    } else {\n+      vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    }\n@@ -1068,2 +1474,6 @@\n-    Node* vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n-\n+    Node* vload = NULL;\n+    if (mask != NULL) {\n+      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+    } else {\n+      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+    }\n@@ -1080,5 +1490,5 @@\n-\/\/ <V extends Vector<?,?>>\n-\/\/ long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/                       V v,\n-\/\/                       Function<V,Long> defaultImpl)\n-\n+\/\/ <V, M>\n+\/\/ long reductionCoerced(int oprId, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                       Class<?> elementType, int length, V v, M m,\n+\/\/                       ReductionOperation<V, M> defaultImpl)\n+\/\/\n@@ -1088,2 +1498,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1097,2 +1508,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1115,0 +1526,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(6));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1117,1 +1554,0 @@\n-\n@@ -1121,2 +1557,12 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, is_masked_op ? VecMaskUseLoad : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=%d\",\n+                    sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n@@ -1124,1 +1570,1 @@\n-      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s ismask=no\",\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=1\",\n@@ -1133,1 +1579,1 @@\n-  Node* opd = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* opd = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1138,0 +1584,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(6), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(6)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1139,1 +1600,16 @@\n-  Node* rn = gvn().transform(ReductionNode::make(opc, NULL, init, opd, elem_bt));\n+  Node* value = NULL;\n+  if (mask == NULL) {\n+    assert(!is_masked_op, \"Masked op needs the mask value never null\");\n+    value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+  } else {\n+    if (use_predicate) {\n+      value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+      value->add_req(mask);\n+      value->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      Node* reduce_identity = gvn().transform(VectorNode::scalar2vector(init, num_elem, Type::get_const_basic_type(elem_bt)));\n+      value = gvn().transform(new VectorBlendNode(reduce_identity, opd, mask));\n+      value = ReductionNode::make(opc, NULL, init, value, elem_bt);\n+    }\n+  }\n+  value = gvn().transform(value);\n@@ -1146,1 +1622,1 @@\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      bits = gvn().transform(new ConvI2LNode(value));\n@@ -1150,2 +1626,2 @@\n-      rn   = gvn().transform(new MoveF2INode(rn));\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      value = gvn().transform(new MoveF2INode(value));\n+      bits  = gvn().transform(new ConvI2LNode(value));\n@@ -1155,1 +1631,1 @@\n-      bits = gvn().transform(new MoveD2LNode(rn));\n+      bits = gvn().transform(new MoveD2LNode(value));\n@@ -1159,1 +1635,1 @@\n-      bits = rn; \/\/ no conversion needed\n+      bits = value; \/\/ no conversion needed\n@@ -1171,1 +1647,1 @@\n-\/\/                                BiFunction<V, V, Boolean> defaultImpl) {\n+\/\/                                BiFunction<V, V, Boolean> defaultImpl)\n@@ -1235,1 +1711,1 @@\n-\/\/         VectorBlendOp<V,M> defaultImpl) { ...\n+\/\/         VectorBlendOp<V,M> defaultImpl)\n@@ -1307,2 +1783,2 @@\n-\/\/            V v1, V v2,\n-\/\/            VectorCompareOp<V,M> defaultImpl) { ...\n+\/\/            V v1, V v2, M m,\n+\/\/            VectorCompareOp<V,M> defaultImpl)\n@@ -1377,0 +1853,19 @@\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  Node* mask = is_masked_op ? unbox_vector(argument(7), mbox_type, elem_bt, num_elem) : NULL;\n+  if (is_masked_op && mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: mask = null arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  bool use_predicate = is_masked_op && arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_AndV, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -1383,2 +1878,14 @@\n-  const TypeVect* vt = TypeVect::make(mask_bt, num_elem);\n-  Node* operation = gvn().transform(new VectorMaskCmpNode(pred, v1, v2, pred_node, vt));\n+  const TypeVect* vmask_type = TypeVect::makemask(mask_bt, num_elem);\n+  Node* operation = new VectorMaskCmpNode(pred, v1, v2, pred_node, vmask_type);\n+\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = VectorNode::make(Op_AndV, operation, mask, vmask_type);\n+    }\n+  }\n+\n+  operation = gvn().transform(operation);\n@@ -1393,5 +1900,7 @@\n-\/\/ <V extends Vector, Sh extends Shuffle>\n-\/\/  V rearrangeOp(Class<V> vectorClass, Class<Sh> shuffleClass, Class< ? > elementType, int vlen,\n-\/\/    V v1, Sh sh,\n-\/\/    VectorSwizzleOp<V, Sh, S, E> defaultImpl) { ...\n-\n+\/\/ <V extends Vector<E>,\n+\/\/     Sh extends VectorShuffle<E>,\n+\/\/     M extends VectorMask<E>,\n+\/\/     E>\n+\/\/ V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+\/\/               V v1, Sh sh, M m,\n+\/\/               VectorRearrangeOp<V,Sh,M,E> defaultImpl)\n@@ -1401,2 +1910,3 @@\n-  const TypeInstPtr* elem_klass    = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen          = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass    = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass    = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen          = gvn().type(argument(4))->isa_int();\n@@ -1404,1 +1914,1 @@\n-  if (vector_klass == NULL || shuffle_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (vector_klass == NULL  || shuffle_klass == NULL ||  elem_klass == NULL || vlen == NULL) {\n@@ -1407,2 +1917,4 @@\n-  if (shuffle_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n-    elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (shuffle_klass->const_oop() == NULL ||\n+      vector_klass->const_oop()  == NULL ||\n+      elem_klass->const_oop()    == NULL ||\n+      !vlen->is_con()) {\n@@ -1413,2 +1925,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1418,1 +1930,2 @@\n-  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(shuffle_klass)) {\n+  if (!is_klass_initialized(vector_klass)  ||\n+      !is_klass_initialized(shuffle_klass)) {\n@@ -1442,1 +1955,7 @@\n-  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed)) {\n+\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  bool use_predicate = is_masked_op;\n+  if (is_masked_op &&\n+      (mask_klass == NULL ||\n+       mask_klass->const_oop() == NULL ||\n+       !is_klass_initialized(mask_klass))) {\n@@ -1444,2 +1963,15 @@\n-      tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n-                    num_elem, type2name(elem_bt));\n+      tty->print_cr(\"  ** mask_klass argument not initialized\");\n+    }\n+  }\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, checkFlags)) {\n+    use_predicate = false;\n+    if(!is_masked_op ||\n+       (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed) ||\n+        !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)     ||\n+        !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -1447,1 +1979,0 @@\n-    return false; \/\/ not supported\n@@ -1455,2 +1986,2 @@\n-  Node* v1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n-  Node* shuffle = unbox_vector(argument(5), shbox_type, shuffle_bt, num_elem);\n+  Node* v1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* shuffle = unbox_vector(argument(6), shbox_type, shuffle_bt, num_elem);\n@@ -1462,1 +1993,28 @@\n-  Node* rearrange = gvn().transform(new VectorRearrangeNode(v1, shuffle));\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=3 op=shuffle\/rearrange vlen=%d etype=%s ismask=useload is_masked_op=1\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* rearrange = new VectorRearrangeNode(v1, shuffle);\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      rearrange->add_req(mask);\n+      rearrange->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      const TypeVect* vt = v1->bottom_type()->is_vect();\n+      rearrange = gvn().transform(rearrange);\n+      Node* zero = gvn().makecon(TypeInt::ZERO);\n+      Node* zerovec = gvn().transform(VectorNode::scalar2vector(zero, num_elem, Type::get_const_basic_type(elem_bt)));\n+      rearrange = new VectorBlendNode(zerovec, rearrange, mask);\n+    }\n+  }\n+  rearrange = gvn().transform(rearrange);\n@@ -1528,4 +2086,5 @@\n-\/\/  <V extends Vector<?,?>>\n-\/\/  V broadcastInt(int opr, Class<V> vectorClass, Class<?> elementType, int vlen,\n-\/\/                 V v, int i,\n-\/\/                 VectorBroadcastIntOp<V> defaultImpl) {\n+\/\/  <V extends Vector<?>, M>\n+\/\/  V broadcastInt(int opr, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                 Class<?> elementType, int length,\n+\/\/                 V v, int n, M m,\n+\/\/                 VectorBroadcastIntOp<V> defaultImpl)\n@@ -1536,2 +2095,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1547,2 +2107,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1558,0 +2118,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(7));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1565,1 +2151,1 @@\n-  BasicType elem_bt = elem_type->basic_type();\n+\n@@ -1567,0 +2153,1 @@\n+  BasicType elem_bt = elem_type->basic_type();\n@@ -1568,0 +2155,1 @@\n+\n@@ -1570,0 +2158,1 @@\n+\n@@ -1576,0 +2165,1 @@\n+\n@@ -1583,1 +2173,2 @@\n-  Node* cnt  = argument(5);\n+\n+  Node* cnt  = argument(6);\n@@ -1592,4 +2183,15 @@\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s ismask=no\",\n-                    sopc, num_elem, type2name(elem_bt));\n+\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  bool use_predicate = is_masked_op;\n+\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, checkFlags, has_scalar_args)) {\n+    use_predicate = false;\n+    if (!is_masked_op ||\n+        (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) ||\n+         !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad))) {\n+\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s is_masked_op=%d\",\n+                      sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+      }\n+      return false; \/\/ not supported\n@@ -1597,1 +2199,0 @@\n-    return false; \/\/ not supported\n@@ -1599,1 +2200,2 @@\n-  Node* opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+\n+  Node* opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1614,0 +2216,1 @@\n+\n@@ -1617,1 +2220,0 @@\n-  Node* operation = gvn().transform(VectorNode::make(opc, opd1, opd2, num_elem, elem_bt));\n@@ -1619,0 +2221,23 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\", NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* operation = VectorNode::make(opc, opd1, opd2, num_elem, elem_bt);\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n@@ -1632,1 +2257,1 @@\n-\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl)\n@@ -1741,2 +2366,2 @@\n-  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from);\n-  const TypeVect* dst_type = TypeVect::make(elem_bt_to,   num_elem_to);\n+  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from, is_mask);\n+  const TypeVect* dst_type = TypeVect::make(elem_bt_to, num_elem_to, is_mask);\n@@ -1822,1 +2447,1 @@\n-\/\/           VecInsertOp<V> defaultImpl) {\n+\/\/           VecInsertOp<V> defaultImpl)\n@@ -1915,1 +2540,1 @@\n-\/\/               VecExtractOp<V> defaultImpl) {\n+\/\/               VecExtractOp<V> defaultImpl)\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":816,"deletions":191,"binary":false,"changes":1007,"status":"modified"},{"patch":"@@ -445,0 +445,25 @@\n+VectorNode* VectorNode::make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n+  guarantee(vopc > 0, \"vopc must be > 0\");\n+  const TypeVect* vmask_type = TypeVect::makemask(bt, vlen);\n+  switch (vopc) {\n+    case Op_AndV:\n+      if (Matcher::match_rule_supported_vector(Op_AndVMask, vlen, bt)) {\n+        return new AndVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new AndVNode(n1, n2, vmask_type);\n+    case Op_OrV:\n+      if (Matcher::match_rule_supported_vector(Op_OrVMask, vlen, bt)) {\n+        return new OrVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new OrVNode(n1, n2, vmask_type);\n+    case Op_XorV:\n+      if (Matcher::match_rule_supported_vector(Op_XorVMask, vlen, bt)) {\n+        return new XorVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new XorVNode(n1, n2, vmask_type);\n+    default:\n+      fatal(\"Unsupported mask vector creation for '%s'\", NodeClassNames[vopc]);\n+      return NULL;\n+  }\n+}\n+\n@@ -446,1 +471,1 @@\n-VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt) {\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask) {\n@@ -449,0 +474,5 @@\n+\n+  if (is_mask) {\n+    return make_mask_node(vopc, n1, n2, vt->length(), vt->element_basic_type());\n+  }\n+\n@@ -555,1 +585,1 @@\n-VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t) {\n+VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask) {\n@@ -557,2 +587,7 @@\n-  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen)\n-                                          : TypeVect::make(bt, vlen);\n+  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen, is_mask)\n+                                          : TypeVect::make(bt, vlen, is_mask);\n+\n+  if (is_mask && Matcher::match_rule_supported_vector(Op_MaskAll, vlen, bt)) {\n+    return new MaskAllNode(s, vt);\n+  }\n+\n@@ -752,0 +787,17 @@\n+Node* StoreVectorNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ StoreVector (VectorStoreMask src)  ==>  (StoreVectorMask src).\n+  Node* value = in(MemNode::ValueIn);\n+  if (value->Opcode() == Op_VectorStoreMask) {\n+    assert(vect_type()->element_basic_type() == T_BOOLEAN, \"Invalid basic type to store mask\");\n+    const TypeVect* type = value->in(1)->bottom_type()->is_vect();\n+    if (Matcher::match_rule_supported_vector(Op_StoreVectorMask, type->length(), type->element_basic_type())) {\n+      const TypeVect* mem_type = TypeVect::make(T_BOOLEAN, type->length());\n+      return new StoreVectorMaskNode(in(MemNode::Control),\n+                                     in(MemNode::Memory),\n+                                     in(MemNode::Address),\n+                                     adr_type(), value->in(1), mem_type);\n+    }\n+  }\n+  return StoreNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -1007,0 +1059,15 @@\n+Node* VectorLoadMaskNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ (VectorLoadMask (LoadVector mem))  ==> (LoadVectorMask mem)\n+  LoadVectorNode* load = this->in(1)->isa_LoadVector();\n+  BasicType out_bt = vect_type()->element_basic_type();\n+  if (load != NULL &&\n+      Matcher::match_rule_supported_vector(Op_LoadVectorMask, length(), out_bt)) {\n+    const TypeVect* mem_type = TypeVect::make(T_BOOLEAN, length());\n+    return new LoadVectorMaskNode(load->in(MemNode::Control),\n+                                  load->in(MemNode::Memory),\n+                                  load->in(MemNode::Address),\n+                                  load->adr_type(), vect_type(), mem_type);\n+  }\n+  return NULL;\n+}\n+\n@@ -1108,0 +1175,1 @@\n+          return gvn.makecon(TypeInt::make(max_jbyte));\n@@ -1109,0 +1177,1 @@\n+          return gvn.makecon(TypeInt::make(max_jshort));\n@@ -1123,0 +1192,1 @@\n+          return gvn.makecon(TypeInt::make(min_jbyte));\n@@ -1124,0 +1194,1 @@\n+          return gvn.makecon(TypeInt::make(min_jshort));\n@@ -1316,0 +1387,1 @@\n+          const TypeVect* vmask_type = TypeVect::makemask(out_vt->element_basic_type(), out_vt->length());\n@@ -1321,1 +1393,1 @@\n-            return new VectorMaskCastNode(value, out_vt);\n+            return new VectorMaskCastNode(value, vmask_type);\n@@ -1325,1 +1397,1 @@\n-          return new VectorLoadMaskNode(value, out_vt);\n+          return new VectorLoadMaskNode(value, vmask_type);\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":78,"deletions":6,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -69,1 +69,6 @@\n-  virtual uint ideal_reg() const { return Matcher::vector_ideal_reg(vect_type()->length_in_bytes()); }\n+  virtual uint ideal_reg() const {\n+    if (vect_type()->isa_vectmask()) {\n+      return Op_RegVectMask;\n+    }\n+    return Matcher::vector_ideal_reg(vect_type()->length_in_bytes());\n+  }\n@@ -71,1 +76,1 @@\n-  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t);\n+  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask = false);\n@@ -74,1 +79,1 @@\n-  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt);\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask = false);\n@@ -77,0 +82,1 @@\n+  static VectorNode* make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt);\n@@ -776,0 +782,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -801,2 +808,2 @@\n-                                                     idx == MemNode::ValueIn ||\n-                                                     idx == MemNode::ValueIn + 1; }\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1; }\n@@ -811,1 +818,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -831,1 +838,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -845,0 +852,39 @@\n+\/\/-------------------------------LoadVectorGatherMaskedNode---------------------------------\n+\/\/ Load Vector from memory via index map under the influence of a predicate register(mask).\n+class LoadVectorGatherMaskedNode : public LoadVectorNode {\n+ public:\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+    : LoadVectorNode(c, mem, adr, at, vt) {\n+    init_class_id(Class_LoadVector);\n+    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+    add_req(indices);\n+    add_req(mask);\n+    assert(req() == MemNode::ValueIn + 2, \"match_edge expects that last input is in MemNode::ValueIn+1\");\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                   idx == MemNode::ValueIn ||\n+                                                   idx == MemNode::ValueIn + 1; }\n+};\n+\n+\/\/------------------------------StoreVectorScatterMaskedNode--------------------------------\n+\/\/ Store Vector into memory via index map under the influence of a predicate register(mask).\n+class StoreVectorScatterMaskedNode : public StoreVectorNode {\n+  public:\n+   StoreVectorScatterMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* indices, Node* mask)\n+     : StoreVectorNode(c, mem, adr, at, val) {\n+     init_class_id(Class_StoreVector);\n+     assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+     assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+     add_req(indices);\n+     add_req(mask);\n+     assert(req() == MemNode::ValueIn + 3, \"match_edge expects that last input is in MemNode::ValueIn+2\");\n+   }\n+   virtual int Opcode() const;\n+   virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1 ||\n+                                                    idx == MemNode::ValueIn + 2; }\n+};\n@@ -859,1 +905,0 @@\n-\n@@ -916,0 +961,72 @@\n+class LoadVectorMaskNode : public LoadVectorNode {\n+ private:\n+  \/**\n+   * The type of the accessed memory, whose basic element type is T_BOOLEAN for mask vector.\n+   * It is different with the basic element type of the node, which can be T_BYTE, T_SHORT,\n+   * T_INT, T_LONG, T_FLOAT or T_DOUBLE.\n+   **\/\n+  const TypeVect* _mem_type;\n+\n+ public:\n+  LoadVectorMaskNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, const TypeVect* mt)\n+   : LoadVectorNode(c, mem, adr, at, vt), _mem_type(mt) {\n+    assert(_mem_type->element_basic_type() == T_BOOLEAN, \"Memory type must be T_BOOLEAN\");\n+    init_class_id(Class_LoadVector);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual int memory_size() const { return _mem_type->length_in_bytes(); }\n+  virtual int store_Opcode() const { return Op_StoreVectorMask; }\n+  virtual uint ideal_reg() const  { return vect_type()->ideal_reg(); }\n+  virtual uint size_of() const { return sizeof(LoadVectorMaskNode); }\n+};\n+\n+class StoreVectorMaskNode : public StoreVectorNode {\n+ private:\n+  \/**\n+   * The type of the accessed memory, whose basic element type is T_BOOLEAN for mask vector.\n+   * It is different with the basic element type of the src value, which can be T_BYTE, T_SHORT,\n+   * T_INT, T_LONG, T_FLOAT or T_DOUBLE.\n+   **\/\n+  const TypeVect* _mem_type;\n+\n+ public:\n+  StoreVectorMaskNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* src, const TypeVect* mt)\n+   : StoreVectorNode(c, mem, adr, at, src), _mem_type(mt) {\n+    assert(_mem_type->element_basic_type() == T_BOOLEAN, \"Memory type must be T_BOOLEAN\");\n+    init_class_id(Class_StoreVector);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual int memory_size() const { return _mem_type->length_in_bytes(); }\n+  virtual uint size_of() const { return sizeof(StoreVectorMaskNode); }\n+};\n+\n+\/\/-------------------------- Vector mask broadcast -----------------------------------\n+class MaskAllNode : public VectorNode {\n+ public:\n+  MaskAllNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical and --------------------------------\n+class AndVMaskNode : public VectorNode {\n+ public:\n+  AndVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical or ---------------------------------\n+class OrVMaskNode : public VectorNode {\n+ public:\n+  OrVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical xor --------------------------------\n+class XorVMaskNode : public VectorNode {\n+ public:\n+  XorVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -1290,0 +1407,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":126,"deletions":8,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -1829,0 +1829,2 @@\n+  declare_c2_type(LoadVectorMaskNode, LoadVectorNode)                     \\\n+  declare_c2_type(StoreVectorMaskNode, StoreVectorNode)                   \\\n@@ -1847,0 +1849,4 @@\n+  declare_c2_type(MaskAllNode, VectorNode)                                \\\n+  declare_c2_type(AndVMaskNode, VectorNode)                               \\\n+  declare_c2_type(OrVMaskNode, VectorNode)                                \\\n+  declare_c2_type(XorVMaskNode, VectorNode)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -428,0 +428,43 @@\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                               int length, ByteBuffer bb, int offset, M m, S s,\n+                               VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, E, S, M> defaultImpl) {\n+        try {\n+            return loadFromByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    bb, offset, m,\n+                    s,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope, Class<? extends V> vmClass,\n+                                     Class<M> maskClass, Class<E> e, int length,\n+                                     ByteBuffer bb, int offset, M m,\n+                                     S s,\n+                                     VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, E, S, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            return VectorSupport.loadMasked(vmClass, maskClass, e, length,\n+                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset), m,\n+                    bb, offset, s,\n+                    defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n+\n@@ -473,0 +516,42 @@\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                                   int length, V v, M m,\n+                                   ByteBuffer bb, int offset,\n+                                   VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            storeIntoByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    v, m,\n+                    bb, offset,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope,\n+                                         Class<? extends V> vmClass, Class<M> maskClass,\n+                                         Class<E> e, int length, V v, M m,\n+                                         ByteBuffer bb, int offset,\n+                                         VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            VectorSupport.storeMasked(vmClass, maskClass, e, length,\n+                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset),\n+                    v, m,\n+                    bb, offset,\n+                    defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/X-ScopedMemoryAccess.java.template","additions":85,"deletions":0,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -219,4 +219,4 @@\n-    <V extends Vector<?>>\n-    long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int length,\n-                          V v,\n-                          Function<V,Long> defaultImpl) {\n+    <V, M>\n+    long reductionCoerced(int oprId, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+                          Class<?> elementType, int length, V v, M m,\n+                          ReductionOperation<V, M> defaultImpl) {\n@@ -224,1 +224,5 @@\n-        return defaultImpl.apply(v);\n+        return defaultImpl.apply(v, m);\n+    }\n+\n+    public interface ReductionOperation<V, M> {\n+        long apply(V v, M mask);\n@@ -227,0 +231,1 @@\n+\n@@ -263,4 +268,4 @@\n-    <VM>\n-    VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-               VM vm,\n-               Function<VM, VM> defaultImpl) {\n+    <V, M>\n+    V unaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+              Class<?> elementType, int length, V v, M m,\n+              UnaryOperation<V, M> defaultImpl) {\n@@ -268,1 +273,5 @@\n-        return defaultImpl.apply(vm);\n+        return defaultImpl.apply(v, m);\n+    }\n+\n+    public interface UnaryOperation<V, M> {\n+        V apply(V v, M mask);\n@@ -275,4 +284,4 @@\n-    <VM>\n-    VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                VM vm1, VM vm2,\n-                BiFunction<VM, VM, VM> defaultImpl) {\n+    <V, M>\n+    V binaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+               Class<?> elementType, int length, V v1, V v2, M m,\n+               BinaryOperation<V, M> defaultImpl) {\n@@ -280,1 +289,1 @@\n-        return defaultImpl.apply(vm1, vm2);\n+        return defaultImpl.apply(v1, v2, m);\n@@ -283,4 +292,2 @@\n-    \/* ============================================================================ *\/\n-\n-    public interface TernaryOperation<V> {\n-        V apply(V v1, V v2, V v3);\n+    public interface BinaryOperation<V, M> {\n+        V apply(V v1, V v2, M mask);\n@@ -289,0 +296,2 @@\n+    \/* ============================================================================ *\/\n+\n@@ -291,4 +300,4 @@\n-    <VM>\n-    VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                 VM vm1, VM vm2, VM vm3,\n-                 TernaryOperation<VM> defaultImpl) {\n+    <V, M>\n+    V ternaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+                Class<?> elementType, int length, V v1, V v2, V v3, M m,\n+                TernaryOperation<V, M> defaultImpl) {\n@@ -296,1 +305,5 @@\n-        return defaultImpl.apply(vm1, vm2, vm3);\n+        return defaultImpl.apply(v1, v2, v3, m);\n+    }\n+\n+    public interface TernaryOperation<V, M> {\n+        V apply(V v1, V v2, V v3, M mask);\n@@ -320,2 +333,2 @@\n-    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>> {\n-        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s);\n+    public interface LoadVectorMaskedOperation<C, V, E, S extends VectorSpecies<E>, M extends VectorMask<E>> {\n+        V load(C container, int index, S s, M m);\n@@ -326,2 +339,23 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>, E, S extends VectorSpecies<E>>\n-    V loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n+    <C, V, E, S extends VectorSpecies<E>,\n+     M extends VectorMask<E>>\n+    V loadMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+                 int length, Object base, long offset, M m,\n+                 C container, int index, S s,  \/\/ Arguments for default implementation\n+                 LoadVectorMaskedOperation<C, V, E, S, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.load(container, index, s, m);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>,\n+                                                M extends VectorMask<E>> {\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>, W extends Vector<Integer>, E,\n+     S extends VectorSpecies<E>, M extends VectorMask<E>>\n+    V loadWithMap(Class<?> vectorClass, Class<M> maskClass, Class<E> E, int length,\n+                  Class<?> vectorIndexClass,\n@@ -329,1 +363,1 @@\n-                  W index_vector,\n+                  W index_vector, M m,\n@@ -331,1 +365,1 @@\n-                  LoadVectorOperationWithMap<C, V, E, S> defaultImpl) {\n+                  LoadVectorOperationWithMap<C, V, E, S, M> defaultImpl) {\n@@ -333,1 +367,1 @@\n-        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s);\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s, m);\n@@ -354,0 +388,17 @@\n+    public interface StoreVectorMaskedOperation<C, V extends Vector<?>, M extends VectorMask<?>> {\n+        void store(C container, int index, V v, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>,\n+     M extends VectorMask<?>>\n+    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+                     int length, Object base, long offset,   \/\/ Unsafe addressing\n+                     V v, M m,\n+                     C container, int index,      \/\/ Arguments for default implementation\n+                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v, m);\n+    }\n+\n@@ -356,2 +407,2 @@\n-    public interface StoreVectorOperationWithMap<C, V extends Vector<?>> {\n-        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM);\n+    public interface StoreVectorOperationWithMap<C, V extends Vector<?>, M extends VectorMask<?>> {\n+        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM, M m);\n@@ -362,4 +413,4 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>>\n-    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-                      Object base, long offset,    \/\/ Unsafe addressing\n-                      W index_vector, V v,\n+    <C, V extends Vector<?>, W extends Vector<Integer>, M extends VectorMask<?>>\n+    void storeWithMap(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+                      int length, Class<?> vectorIndexClass, Object base, long offset,    \/\/ Unsafe addressing\n+                      W index_vector, V v, M m,\n@@ -367,1 +418,1 @@\n-                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+                      StoreVectorOperationWithMap<C, V, M> defaultImpl) {\n@@ -369,1 +420,1 @@\n-        defaultImpl.storeWithMap(container, index, v, indexMap, indexM);\n+        defaultImpl.storeWithMap(container, index, v, indexMap, indexM, m);\n@@ -387,1 +438,1 @@\n-        M apply(int cond, V v1, V v2);\n+        M apply(int cond, V v1, V v2, M m);\n@@ -395,1 +446,1 @@\n-              V v1, V v2,\n+              V v1, V v2, M m,\n@@ -398,1 +449,1 @@\n-        return defaultImpl.apply(cond, v1, v2);\n+        return defaultImpl.apply(cond, v1, v2, m);\n@@ -402,1 +453,0 @@\n-\n@@ -405,0 +455,1 @@\n+            M  extends VectorMask<E>,\n@@ -406,1 +457,1 @@\n-        V apply(V v1, Sh shuffle);\n+        V apply(V v1, Sh shuffle, M mask);\n@@ -413,0 +464,1 @@\n+            M  extends VectorMask<E>,\n@@ -414,3 +466,3 @@\n-    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<?> elementType, int vlen,\n-                  V v1, Sh sh,\n-                  VectorRearrangeOp<V,Sh, E> defaultImpl) {\n+    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+                  V v1, Sh sh, M m,\n+                  VectorRearrangeOp<V,Sh,M,E> defaultImpl) {\n@@ -418,1 +470,1 @@\n-        return defaultImpl.apply(v1, sh);\n+        return defaultImpl.apply(v1, sh, m);\n@@ -443,2 +495,2 @@\n-    public interface VectorBroadcastIntOp<V extends Vector<?>> {\n-        V apply(V v, int n);\n+    public interface VectorBroadcastIntOp<V extends Vector<?>, M> {\n+        V apply(V v, int n, M m);\n@@ -449,4 +501,5 @@\n-    <V extends Vector<?>>\n-    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<?> elementType, int length,\n-                   V v, int n,\n-                   VectorBroadcastIntOp<V> defaultImpl) {\n+    <V extends Vector<?>, M>\n+    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+                   Class<?> elementType, int length,\n+                   V v, int n, M m,\n+                   VectorBroadcastIntOp<V, M> defaultImpl) {\n@@ -454,1 +507,1 @@\n-        return defaultImpl.apply(v, n);\n+        return defaultImpl.apply(v, n, m);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":108,"deletions":55,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -117,0 +117,18 @@\n+    @Override\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    public\n+    <F> VectorMask<F> check(Class<? extends VectorMask<F>> maskClass, Vector<F> vector) {\n+        if (!sameSpecies(maskClass, vector)) {\n+            throw AbstractSpecies.checkFailed(this, vector);\n+        }\n+        return (VectorMask<F>) this;\n+    }\n+\n+    @ForceInline\n+    private <F> boolean sameSpecies(Class<? extends VectorMask<F>> maskClass, Vector<F> vector) {\n+        boolean same = getClass() == maskClass;\n+        assert (same == (vectorSpecies() == vector.species())) : same;\n+        return same;\n+    }\n+\n@@ -218,1 +236,0 @@\n-            \/\/ This requires a split test.\n@@ -220,6 +237,3 @@\n-            int elemCount = Math.min(vlength, (alength - clipOffset) \/ esize);\n-            badMask = checkIndex0(0, elemCount, iota, vlength);\n-            clipOffset &= (esize - 1);  \/\/ power of two, so OK\n-            VectorMask<E> badMask2 = checkIndex0(clipOffset \/ esize, vlength,\n-                                                 iota, vlength);\n-            badMask = badMask.or(badMask2);\n+            badMask = checkIndex0(clipOffset, alength,\n+                                  iota.lanewise(VectorOperators.MUL, esize),\n+                                  vlength * esize);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractMask.java","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte128Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte128Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseShiftTemplate(op, Byte128Mask.class, e, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte128Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v1, v2, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte128Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte128Mask.class, op, v, (Byte128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte128Mask.class,\n@@ -650,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte128Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +695,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte128Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +705,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte128Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -806,0 +841,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -814,0 +857,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -821,0 +871,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -828,0 +885,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte128Mask.class, bb, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -835,0 +899,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n@@ -842,0 +921,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte128Mask.class, bb, offset, (Byte128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte256Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte256Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseShiftTemplate(op, Byte256Mask.class, e, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte256Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v1, v2, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte256Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte256Mask.class, op, v, (Byte256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte256Mask.class,\n@@ -682,3 +717,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte256Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -692,3 +727,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte256Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -702,3 +737,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte256Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -838,0 +873,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -846,0 +889,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -853,0 +903,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -860,0 +917,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte256Mask.class, bb, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -867,0 +931,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n@@ -874,0 +953,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte256Mask.class, bb, offset, (Byte256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte512Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte512Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseShiftTemplate(op, Byte512Mask.class, e, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte512Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v1, v2, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte512Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte512Mask.class, op, v, (Byte512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte512Mask.class,\n@@ -746,3 +781,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte512Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -756,3 +791,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte512Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -766,3 +801,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte512Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -902,0 +937,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -910,0 +953,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -917,0 +967,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -924,0 +981,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte512Mask.class, bb, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -931,0 +995,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n@@ -938,0 +1017,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte512Mask.class, bb, offset, (Byte512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte64Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte64Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseShiftTemplate(op, Byte64Mask.class, e, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte64Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v1, v2, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte64Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte64Mask.class, op, v, (Byte64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte64Mask.class,\n@@ -634,3 +669,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte64Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +679,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte64Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +689,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte64Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -790,0 +825,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -798,0 +841,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +855,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -812,0 +869,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte64Mask.class, bb, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -819,0 +883,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n@@ -826,0 +905,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte64Mask.class, bb, offset, (Byte64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public ByteMaxVector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public ByteMaxVector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline ByteMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseShiftTemplate(op, ByteMaxMask.class, e, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    ByteMaxVector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v1, v2, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final ByteMaxMask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(ByteMaxMask.class, op, v, (ByteMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    ByteMaxMask.class,\n@@ -620,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, ByteMaxMask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, ByteMaxMask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, ByteMaxMask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -776,0 +811,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -784,0 +827,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +841,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +855,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(ByteMaxMask.class, bb, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +869,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n@@ -812,0 +891,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(ByteMaxMask.class, bb, offset, (ByteMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    byte rOp(byte v, FBinOp f);\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    byte rOpTemplate(byte v, VectorMask<Byte> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        byte[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Byte>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (byte) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (byte) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ByteVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ByteVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ByteVector, VectorMask<Byte>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ByteVector.class);\n+\n+    private static UnaryOperation<ByteVector, VectorMask<Byte>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Byte> eqz = that.eq((byte)0);\n+                VectorMask<Byte> eqz = that.eq((byte) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ByteVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ByteVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Byte> m) {\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v, VectorMask<Byte> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Byte> eqz = that.eq((byte)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Byte> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (byte) 0);\n+                that = that.blend((byte) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Byte> eqz = that.eq((byte)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ByteVector, VectorMask<Byte>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ByteVector.class);\n+\n+    private static BinaryOperation<ByteVector, VectorMask<Byte>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (byte) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        byte e1 = (byte) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract ByteVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ByteVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Byte>> maskClass,\n+                          int e, VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ByteVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ByteVector, VectorMask<Byte>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ByteVector, VectorMask<Byte>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ByteVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ByteVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v1,\n+                                          Vector<Byte> v2,\n+                                          VectorMask<Byte> m) {\n+        ByteVector that = (ByteVector) v1;\n+        ByteVector tother = (ByteVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ByteVector, VectorMask<Byte>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ByteVector.class);\n+\n+    private static TernaryOperation<ByteVector, VectorMask<Byte>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ByteSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Byte> v, M m) {\n+        ByteVector that = (ByteVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, byte.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Byte> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Byte> compare(VectorOperators.Comparison op,\n-                                  Vector<Byte> v,\n-                                  VectorMask<Byte> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, byte.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, byte.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Byte>>\n+    <S extends VectorShuffle<Byte>, M extends VectorMask<Byte>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Byte> m) {\n-        ByteVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, byte.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((byte)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, byte.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Byte>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ByteVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ByteVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((byte)1, (i, a, b) -> (byte)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (byte) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (byte) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((byte)-1, (i, a, b) -> (byte)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ByteVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ByteVector.class);\n+    ImplCache<Associative, ReductionOperation<ByteVector, VectorMask<Byte>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ByteVector.class);\n+\n+    private static ReductionOperation<ByteVector, VectorMask<Byte>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((byte)1, m, (i, a, b) -> (byte)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (byte) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (byte) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((byte)-1, m, (i, a, b) -> (byte)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ByteVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2924,1 +3092,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3102,3 +3270,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3342,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3344,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3332,1 +3497,0 @@\n-            \/\/ FIXME: optimize\n@@ -3335,1 +3499,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -3454,1 +3618,0 @@\n-            \/\/ FIXME: optimize\n@@ -3457,3 +3620,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3490,1 +3651,0 @@\n-            \/\/ FIXME: optimize\n@@ -3496,3 +3656,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3536,0 +3694,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3552,0 +3728,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n+\n@@ -3570,0 +3763,19 @@\n+    abstract\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get(o + i * 1));\n+            });\n+    }\n+\n@@ -3586,0 +3798,18 @@\n+    abstract\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get(o + i * 1));\n+                });\n+    }\n+\n@@ -3605,0 +3835,36 @@\n+    abstract\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+\n@@ -3622,0 +3888,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put(o + i * 1, e));\n+            });\n+    }\n+\n@@ -3636,0 +3921,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+                });\n+    }\n+\n+\n@@ -3962,1 +4266,1 @@\n-                                      AbstractMask<Byte> m,\n+                                      VectorMask<Byte> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":494,"deletions":190,"binary":false,"changes":684,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double128Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double128Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double128Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v1, v2, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double128Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double128Mask.class, op, v, (Double128Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double128Mask.class,\n@@ -618,3 +646,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -628,3 +656,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -638,3 +666,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double128Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -774,0 +802,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, indexMap, mapOffset, (Double128Mask) m);\n+    }\n+\n@@ -783,0 +825,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -790,0 +839,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double128Mask.class, bb, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +853,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, indexMap, mapOffset, (Double128Mask) m);\n+    }\n+\n+\n@@ -804,0 +875,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double128Mask.class, bb, offset, (Double128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double128Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double256Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double256Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double256Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v1, v2, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double256Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double256Mask.class, op, v, (Double256Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double256Mask.class,\n@@ -622,3 +650,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +660,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +670,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double256Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -778,0 +806,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, indexMap, mapOffset, (Double256Mask) m);\n+    }\n+\n@@ -787,0 +829,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double256Mask.class, bb, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +857,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, indexMap, mapOffset, (Double256Mask) m);\n+    }\n+\n+\n@@ -808,0 +879,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double256Mask.class, bb, offset, (Double256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double256Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double512Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double512Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double512Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v1, v2, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double512Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double512Mask.class, op, v, (Double512Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double512Mask.class,\n@@ -630,3 +658,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -640,3 +668,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -650,3 +678,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double512Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -786,0 +814,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, indexMap, mapOffset, (Double512Mask) m);\n+    }\n+\n@@ -795,0 +837,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -802,0 +851,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double512Mask.class, bb, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -809,0 +865,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, indexMap, mapOffset, (Double512Mask) m);\n+    }\n+\n+\n@@ -816,0 +887,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double512Mask.class, bb, offset, (Double512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double512Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double64Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double64Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double64Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v1, v2, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double64Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double64Mask.class, op, v, (Double64Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double64Mask.class,\n@@ -616,3 +644,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -626,3 +654,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -636,3 +664,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double64Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -772,0 +800,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, indexMap, mapOffset, (Double64Mask) m);\n+    }\n+\n@@ -781,0 +823,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -788,0 +837,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double64Mask.class, bb, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -795,0 +851,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, indexMap, mapOffset, (Double64Mask) m);\n+    }\n+\n+\n@@ -802,0 +873,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double64Mask.class, bb, offset, (Double64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double64Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public DoubleMaxVector lanewise(Unary op, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public DoubleMaxVector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    DoubleMaxVector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v1, v2, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final DoubleMaxMask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(DoubleMaxMask.class, op, v, (DoubleMaxMask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    DoubleMaxMask.class,\n@@ -615,3 +643,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, DoubleMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -625,3 +653,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, DoubleMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -635,3 +663,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, DoubleMaxMask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -771,0 +799,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, indexMap, mapOffset, (DoubleMaxMask) m);\n+    }\n+\n@@ -780,0 +822,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -787,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(DoubleMaxMask.class, bb, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +850,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, indexMap, mapOffset, (DoubleMaxMask) m);\n+    }\n+\n+\n@@ -801,0 +872,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(DoubleMaxMask.class, bb, offset, (DoubleMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleMaxVector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    double rOp(double v, FBinOp f);\n+    double rOp(double v, VectorMask<Double> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    double rOpTemplate(double v, VectorMask<Double> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        double[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Double>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (double) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<DoubleVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, DoubleVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          VectorMask<Double> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<DoubleVector, VectorMask<Double>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, DoubleVector.class);\n+\n+    private static UnaryOperation<DoubleVector, VectorMask<Double>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<DoubleVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, DoubleVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v, VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<DoubleVector, VectorMask<Double>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, DoubleVector.class);\n+\n+    private static BinaryOperation<DoubleVector, VectorMask<Double>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        double e1 = (double) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<DoubleVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, DoubleVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v1,\n+                                          Vector<Double> v2,\n+                                          VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v1;\n+        DoubleVector tother = (DoubleVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<DoubleVector, VectorMask<Double>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, DoubleVector.class);\n+\n+    private static TernaryOperation<DoubleVector, VectorMask<Double>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1650,2 +1747,0 @@\n-        Objects.requireNonNull(v);\n-        DoubleSpecies vsp = vspecies();\n@@ -1657,2 +1752,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1668,0 +1763,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Double> v, M m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, double.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Double> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1681,12 +1798,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Double> compare(VectorOperators.Comparison op,\n-                                  Vector<Double> v,\n-                                  VectorMask<Double> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1751,1 +1856,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2002,3 +2107,3 @@\n-            getClass(), shuffletype, double.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, double.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2021,1 +2126,1 @@\n-    <S extends VectorShuffle<Double>>\n+    <S extends VectorShuffle<Double>, M extends VectorMask<Double>>\n@@ -2023,0 +2128,1 @@\n+                                           Class<M> masktype,\n@@ -2024,9 +2130,3 @@\n-                                           VectorMask<Double> m) {\n-        DoubleVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, double.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2038,1 +2138,7 @@\n-        return broadcast((double)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, double.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2061,3 +2167,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2069,3 +2175,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2312,0 +2418,1 @@\n+                               Class<? extends VectorMask<Double>> maskClass,\n@@ -2313,2 +2420,10 @@\n-        DoubleVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            DoubleVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2329,14 +2444,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((double)0, (i, a, b) -> (double)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((double)1, (i, a, b) -> (double)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (double) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (double) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2344,0 +2448,1 @@\n+\n@@ -2345,2 +2450,16 @@\n-    ImplCache<Associative,Function<DoubleVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, DoubleVector.class);\n+    ImplCache<Associative, ReductionOperation<DoubleVector, VectorMask<Double>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, DoubleVector.class);\n+\n+    private static ReductionOperation<DoubleVector, VectorMask<Double>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((double)0, m, (i, a, b) -> (double)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((double)1, m, (i, a, b) -> (double)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (double) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (double) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2552,3 +2671,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2616,2 +2733,1 @@\n-            DoubleVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2693,3 +2809,3 @@\n-            vectorType, double.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2697,1 +2813,1 @@\n-            (double[] c, int idx, int[] iMap, int idy, DoubleSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2699,1 +2815,1 @@\n-        }\n+    }\n@@ -2747,1 +2863,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2749,1 +2864,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2843,3 +2958,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2917,1 +3030,0 @@\n-            \/\/ FIXME: optimize\n@@ -2920,1 +3032,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2983,1 +3095,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2986,1 +3098,1 @@\n-            this,\n+            this, null,\n@@ -2988,1 +3100,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3035,6 +3147,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3070,1 +3177,0 @@\n-            \/\/ FIXME: optimize\n@@ -3073,3 +3179,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3106,1 +3210,0 @@\n-            \/\/ FIXME: optimize\n@@ -3112,3 +3215,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3152,0 +3253,69 @@\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends DoubleVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return DoubleVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Double species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3172,0 +3342,19 @@\n+    abstract\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+            });\n+    }\n+\n@@ -3188,0 +3377,18 @@\n+    abstract\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+                });\n+    }\n+\n@@ -3207,0 +3414,71 @@\n+    abstract\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(double[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Double species is S_MAX_BIT. and the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3224,0 +3502,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putDouble(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3238,0 +3535,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3555,1 +3871,1 @@\n-                                      AbstractMask<Double> m,\n+                                      VectorMask<Double> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":510,"deletions":194,"binary":false,"changes":704,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float128Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float128Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float128Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v1, v2, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float128Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float128Mask.class, op, v, (Float128Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float128Mask.class,\n@@ -622,3 +650,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +660,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +670,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float128Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -778,0 +806,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, indexMap, mapOffset, (Float128Mask) m);\n+    }\n+\n@@ -787,0 +829,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float128Mask.class, bb, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +857,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, indexMap, mapOffset, (Float128Mask) m);\n+    }\n+\n+\n@@ -808,0 +879,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float128Mask.class, bb, offset, (Float128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float128Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float256Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float256Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float256Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v1, v2, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float256Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float256Mask.class, op, v, (Float256Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float256Mask.class,\n@@ -630,3 +658,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -640,3 +668,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -650,3 +678,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float256Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -786,0 +814,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, indexMap, mapOffset, (Float256Mask) m);\n+    }\n+\n@@ -795,0 +837,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -802,0 +851,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float256Mask.class, bb, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -809,0 +865,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, indexMap, mapOffset, (Float256Mask) m);\n+    }\n+\n+\n@@ -816,0 +887,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float256Mask.class, bb, offset, (Float256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float256Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float512Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float512Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float512Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v1, v2, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float512Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float512Mask.class, op, v, (Float512Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float512Mask.class,\n@@ -646,3 +674,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -656,3 +684,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -666,3 +694,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float512Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -802,0 +830,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, indexMap, mapOffset, (Float512Mask) m);\n+    }\n+\n@@ -811,0 +853,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -818,0 +867,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float512Mask.class, bb, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -825,0 +881,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, indexMap, mapOffset, (Float512Mask) m);\n+    }\n+\n+\n@@ -832,0 +903,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float512Mask.class, bb, offset, (Float512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float512Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float64Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float64Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float64Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v1, v2, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float64Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float64Mask.class, op, v, (Float64Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float64Mask.class,\n@@ -618,3 +646,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -628,3 +656,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -638,3 +666,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float64Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -774,0 +802,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, indexMap, mapOffset, (Float64Mask) m);\n+    }\n+\n@@ -783,0 +825,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -790,0 +839,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float64Mask.class, bb, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +853,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, indexMap, mapOffset, (Float64Mask) m);\n+    }\n+\n+\n@@ -804,0 +875,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float64Mask.class, bb, offset, (Float64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float64Vector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public FloatMaxVector lanewise(Unary op, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public FloatMaxVector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    FloatMaxVector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v1, v2, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final FloatMaxMask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(FloatMaxMask.class, op, v, (FloatMaxMask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    FloatMaxMask.class,\n@@ -615,3 +643,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, FloatMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -625,3 +653,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, FloatMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -635,3 +663,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, FloatMaxMask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -771,0 +799,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, indexMap, mapOffset, (FloatMaxMask) m);\n+    }\n+\n@@ -780,0 +822,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -787,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(FloatMaxMask.class, bb, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +850,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, indexMap, mapOffset, (FloatMaxMask) m);\n+    }\n+\n+\n@@ -801,0 +872,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(FloatMaxMask.class, bb, offset, (FloatMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatMaxVector.java","additions":100,"deletions":14,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    float rOp(float v, FBinOp f);\n+    float rOp(float v, VectorMask<Float> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    float rOpTemplate(float v, VectorMask<Float> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        float[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Float>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (float) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<FloatVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, FloatVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          VectorMask<Float> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<FloatVector, VectorMask<Float>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, FloatVector.class);\n+\n+    private static UnaryOperation<FloatVector, VectorMask<Float>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<FloatVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, FloatVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v, VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<FloatVector, VectorMask<Float>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, FloatVector.class);\n+\n+    private static BinaryOperation<FloatVector, VectorMask<Float>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        float e1 = (float) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<FloatVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, FloatVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v1,\n+                                          Vector<Float> v2,\n+                                          VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v1;\n+        FloatVector tother = (FloatVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<FloatVector, VectorMask<Float>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, FloatVector.class);\n+\n+    private static TernaryOperation<FloatVector, VectorMask<Float>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1662,2 +1759,0 @@\n-        Objects.requireNonNull(v);\n-        FloatSpecies vsp = vspecies();\n@@ -1669,2 +1764,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1680,0 +1775,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Float> v, M m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, float.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Float> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1693,12 +1810,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Float> compare(VectorOperators.Comparison op,\n-                                  Vector<Float> v,\n-                                  VectorMask<Float> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1763,1 +1868,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2014,3 +2119,3 @@\n-            getClass(), shuffletype, float.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, float.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2033,1 +2138,1 @@\n-    <S extends VectorShuffle<Float>>\n+    <S extends VectorShuffle<Float>, M extends VectorMask<Float>>\n@@ -2035,0 +2140,1 @@\n+                                           Class<M> masktype,\n@@ -2036,9 +2142,3 @@\n-                                           VectorMask<Float> m) {\n-        FloatVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, float.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2050,1 +2150,7 @@\n-        return broadcast((float)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, float.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2073,3 +2179,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2081,3 +2187,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2332,0 +2438,1 @@\n+                               Class<? extends VectorMask<Float>> maskClass,\n@@ -2333,2 +2440,10 @@\n-        FloatVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            FloatVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2349,14 +2464,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((float)0, (i, a, b) -> (float)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((float)1, (i, a, b) -> (float)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (float) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (float) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2364,0 +2468,1 @@\n+\n@@ -2365,2 +2470,16 @@\n-    ImplCache<Associative,Function<FloatVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, FloatVector.class);\n+    ImplCache<Associative, ReductionOperation<FloatVector, VectorMask<Float>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, FloatVector.class);\n+\n+    private static ReductionOperation<FloatVector, VectorMask<Float>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((float)0, m, (i, a, b) -> (float)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((float)1, m, (i, a, b) -> (float)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (float) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (float) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2576,3 +2695,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2640,2 +2757,1 @@\n-            FloatVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2699,3 +2815,3 @@\n-            vectorType, float.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2703,1 +2819,1 @@\n-            (float[] c, int idx, int[] iMap, int idy, FloatSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2705,1 +2821,1 @@\n-        }\n+    }\n@@ -2753,1 +2869,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2755,1 +2870,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2849,3 +2964,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2923,1 +3036,0 @@\n-            \/\/ FIXME: optimize\n@@ -2926,1 +3038,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2970,1 +3082,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2973,1 +3085,1 @@\n-            this,\n+            this, null,\n@@ -2975,1 +3087,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3022,6 +3134,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3057,1 +3164,0 @@\n-            \/\/ FIXME: optimize\n@@ -3060,3 +3166,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3093,1 +3197,0 @@\n-            \/\/ FIXME: optimize\n@@ -3099,3 +3202,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3139,0 +3240,51 @@\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends FloatVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3159,0 +3311,19 @@\n+    abstract\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+            });\n+    }\n+\n@@ -3175,0 +3346,18 @@\n+    abstract\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+                });\n+    }\n+\n@@ -3194,0 +3383,52 @@\n+    abstract\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(float[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3211,0 +3452,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putFloat(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3225,0 +3485,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3542,1 +3821,1 @@\n-                                      AbstractMask<Float> m,\n+                                      VectorMask<Float> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":473,"deletions":194,"binary":false,"changes":667,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int128Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int128Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseShiftTemplate(op, Int128Mask.class, e, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int128Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v1, v2, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int128Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int128Mask.class, op, v, (Int128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int128Mask.class,\n@@ -626,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -636,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -646,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int128Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -782,0 +817,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, indexMap, mapOffset, (Int128Mask) m);\n+    }\n+\n@@ -791,0 +840,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +854,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int128Mask.class, bb, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +868,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, indexMap, mapOffset, (Int128Mask) m);\n+    }\n+\n+\n@@ -812,0 +890,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int128Mask.class, bb, offset, (Int128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int128Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int256Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int256Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseShiftTemplate(op, Int256Mask.class, e, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int256Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v1, v2, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int256Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int256Mask.class, op, v, (Int256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int256Mask.class,\n@@ -634,3 +669,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +679,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +689,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int256Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -790,0 +825,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, indexMap, mapOffset, (Int256Mask) m);\n+    }\n+\n@@ -799,0 +848,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -806,0 +862,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int256Mask.class, bb, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -813,0 +876,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, indexMap, mapOffset, (Int256Mask) m);\n+    }\n+\n+\n@@ -820,0 +898,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int256Mask.class, bb, offset, (Int256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int256Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int512Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int512Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseShiftTemplate(op, Int512Mask.class, e, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int512Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v1, v2, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int512Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int512Mask.class, op, v, (Int512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int512Mask.class,\n@@ -650,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +695,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +705,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int512Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -806,0 +841,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, indexMap, mapOffset, (Int512Mask) m);\n+    }\n+\n@@ -815,0 +864,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -822,0 +878,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int512Mask.class, bb, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -829,0 +892,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, indexMap, mapOffset, (Int512Mask) m);\n+    }\n+\n+\n@@ -836,0 +914,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int512Mask.class, bb, offset, (Int512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int512Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int64Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int64Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseShiftTemplate(op, Int64Mask.class, e, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int64Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v1, v2, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int64Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int64Mask.class, op, v, (Int64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int64Mask.class,\n@@ -622,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +677,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int64Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -778,0 +813,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, indexMap, mapOffset, (Int64Mask) m);\n+    }\n+\n@@ -787,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +850,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int64Mask.class, bb, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +864,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, indexMap, mapOffset, (Int64Mask) m);\n+    }\n+\n+\n@@ -808,0 +886,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int64Mask.class, bb, offset, (Int64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int64Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public IntMaxVector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public IntMaxVector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline IntMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseShiftTemplate(op, IntMaxMask.class, e, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    IntMaxVector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v1, v2, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final IntMaxMask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(IntMaxMask.class, op, v, (IntMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    IntMaxMask.class,\n@@ -620,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, IntMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, IntMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, IntMaxMask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -787,0 +822,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, indexMap, mapOffset, (IntMaxMask) m);\n+    }\n+\n@@ -796,0 +845,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -803,0 +859,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(IntMaxMask.class, bb, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -810,0 +873,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, indexMap, mapOffset, (IntMaxMask) m);\n+    }\n+\n+\n@@ -817,0 +895,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(IntMaxMask.class, bb, offset, (IntMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntMaxVector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    int rOp(int v, FBinOp f);\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    int rOpTemplate(int v, VectorMask<Integer> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        int[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Integer>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (int) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (int) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<IntVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, IntVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<IntVector, VectorMask<Integer>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, IntVector.class);\n+\n+    private static UnaryOperation<IntVector, VectorMask<Integer>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Integer> eqz = that.eq((int)0);\n+                VectorMask<Integer> eqz = that.eq((int) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<IntVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, IntVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Integer> m) {\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v, VectorMask<Integer> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Integer> eqz = that.eq((int)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Integer> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (int) 0);\n+                that = that.blend((int) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Integer> eqz = that.eq((int)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<IntVector, VectorMask<Integer>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, IntVector.class);\n+\n+    private static BinaryOperation<IntVector, VectorMask<Integer>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (int) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        int e1 = (int) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract IntVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final IntVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Integer>> maskClass,\n+                          int e, VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<IntVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<IntVector, VectorMask<Integer>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<IntVector, VectorMask<Integer>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -880,6 +1000,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n@@ -887,3 +1004,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<IntVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, IntVector.class);\n@@ -897,2 +1011,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -902,2 +1016,37 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v1,\n+                                          Vector<Integer> v2,\n+                                          VectorMask<Integer> m) {\n+        IntVector that = (IntVector) v1;\n+        IntVector tother = (IntVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<IntVector, VectorMask<Integer>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, IntVector.class);\n+\n+    private static TernaryOperation<IntVector, VectorMask<Integer>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -960,1 +1109,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1018,1 +1167,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1075,1 +1224,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1747,2 +1896,0 @@\n-        Objects.requireNonNull(v);\n-        IntSpecies vsp = vspecies();\n@@ -1754,2 +1901,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1765,0 +1912,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Integer> v, M m) {\n+        IntVector that = (IntVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, int.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Integer> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1782,12 +1951,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Integer> compare(VectorOperators.Comparison op,\n-                                  Vector<Integer> v,\n-                                  VectorMask<Integer> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1852,1 +2009,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2103,3 +2260,3 @@\n-            getClass(), shuffletype, int.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, int.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2122,1 +2279,1 @@\n-    <S extends VectorShuffle<Integer>>\n+    <S extends VectorShuffle<Integer>, M extends VectorMask<Integer>>\n@@ -2124,0 +2281,1 @@\n+                                           Class<M> masktype,\n@@ -2125,9 +2283,3 @@\n-                                           VectorMask<Integer> m) {\n-        IntVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, int.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2139,1 +2291,7 @@\n-        return broadcast((int)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, int.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2162,3 +2320,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2170,3 +2328,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2435,0 +2593,1 @@\n+                               Class<? extends VectorMask<Integer>> maskClass,\n@@ -2436,2 +2595,10 @@\n-        IntVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            IntVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2452,20 +2619,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((int)1, (i, a, b) -> (int)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (int) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (int) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((int)-1, (i, a, b) -> (int)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2473,0 +2623,1 @@\n+\n@@ -2474,2 +2625,22 @@\n-    ImplCache<Associative,Function<IntVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, IntVector.class);\n+    ImplCache<Associative, ReductionOperation<IntVector, VectorMask<Integer>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, IntVector.class);\n+\n+    private static ReductionOperation<IntVector, VectorMask<Integer>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((int)1, m, (i, a, b) -> (int)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (int) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (int) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((int)-1, m, (i, a, b) -> (int)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2694,3 +2865,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2758,2 +2927,1 @@\n-            IntVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2817,3 +2985,3 @@\n-            vectorType, int.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2821,1 +2989,1 @@\n-            (int[] c, int idx, int[] iMap, int idy, IntSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2823,1 +2991,1 @@\n-        }\n+    }\n@@ -2871,1 +3039,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2873,1 +3040,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2967,3 +3134,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3041,1 +3206,0 @@\n-            \/\/ FIXME: optimize\n@@ -3044,1 +3208,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3088,1 +3252,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3091,1 +3255,1 @@\n-            this,\n+            this, null,\n@@ -3093,1 +3257,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3140,6 +3304,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3175,1 +3334,0 @@\n-            \/\/ FIXME: optimize\n@@ -3178,3 +3336,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3211,1 +3367,0 @@\n-            \/\/ FIXME: optimize\n@@ -3217,3 +3372,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3257,0 +3410,51 @@\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends IntVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3277,0 +3481,19 @@\n+    abstract\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getInt(o + i * 4));\n+            });\n+    }\n+\n@@ -3293,0 +3516,18 @@\n+    abstract\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getInt(o + i * 4));\n+                });\n+    }\n+\n@@ -3312,0 +3553,52 @@\n+    abstract\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(int[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3329,0 +3622,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putInt(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3343,0 +3655,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3660,1 +3991,1 @@\n-                                      AbstractMask<Integer> m,\n+                                      VectorMask<Integer> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":534,"deletions":203,"binary":false,"changes":737,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long128Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long128Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseShiftTemplate(op, Long128Mask.class, e, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long128Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v1, v2, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long128Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long128Mask.class, op, v, (Long128Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long128Mask.class,\n@@ -612,3 +647,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -622,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -632,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long128Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -768,0 +803,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, indexMap, mapOffset, (Long128Mask) m);\n+    }\n+\n@@ -777,0 +826,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -784,0 +840,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long128Mask.class, bb, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +854,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, indexMap, mapOffset, (Long128Mask) m);\n+    }\n+\n+\n@@ -798,0 +876,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long128Mask.class, bb, offset, (Long128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long128Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long256Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long256Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseShiftTemplate(op, Long256Mask.class, e, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long256Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v1, v2, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long256Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long256Mask.class, op, v, (Long256Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long256Mask.class,\n@@ -616,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -626,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -636,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long256Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -772,0 +807,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, indexMap, mapOffset, (Long256Mask) m);\n+    }\n+\n@@ -781,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -788,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long256Mask.class, bb, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -795,0 +858,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, indexMap, mapOffset, (Long256Mask) m);\n+    }\n+\n+\n@@ -802,0 +880,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long256Mask.class, bb, offset, (Long256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long256Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long512Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long512Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseShiftTemplate(op, Long512Mask.class, e, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long512Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v1, v2, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long512Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long512Mask.class, op, v, (Long512Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long512Mask.class,\n@@ -624,3 +659,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -634,3 +669,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -644,3 +679,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long512Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -780,0 +815,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, indexMap, mapOffset, (Long512Mask) m);\n+    }\n+\n@@ -789,0 +838,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -796,0 +852,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long512Mask.class, bb, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -803,0 +866,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, indexMap, mapOffset, (Long512Mask) m);\n+    }\n+\n+\n@@ -810,0 +888,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long512Mask.class, bb, offset, (Long512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long512Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long64Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long64Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseShiftTemplate(op, Long64Mask.class, e, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long64Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v1, v2, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long64Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long64Mask.class, op, v, (Long64Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long64Mask.class,\n@@ -610,3 +645,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -620,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -630,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long64Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -766,0 +801,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, indexMap, mapOffset, (Long64Mask) m);\n+    }\n+\n@@ -775,0 +824,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -782,0 +838,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long64Mask.class, bb, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -789,0 +852,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, indexMap, mapOffset, (Long64Mask) m);\n+    }\n+\n+\n@@ -796,0 +874,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long64Mask.class, bb, offset, (Long64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long64Vector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public LongMaxVector lanewise(Unary op, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public LongMaxVector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline LongMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseShiftTemplate(op, LongMaxMask.class, e, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    LongMaxVector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v1, v2, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final LongMaxMask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(LongMaxMask.class, op, v, (LongMaxMask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    LongMaxMask.class,\n@@ -610,3 +645,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, LongMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -620,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, LongMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -630,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, LongMaxMask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -766,0 +801,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, indexMap, mapOffset, (LongMaxMask) m);\n+    }\n+\n@@ -775,0 +824,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -782,0 +838,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(LongMaxMask.class, bb, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -789,0 +852,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, indexMap, mapOffset, (LongMaxMask) m);\n+    }\n+\n+\n@@ -796,0 +874,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(LongMaxMask.class, bb, offset, (LongMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongMaxVector.java","additions":107,"deletions":14,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    long rOp(long v, FBinOp f);\n+    long rOp(long v, VectorMask<Long> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    long rOpTemplate(long v, VectorMask<Long> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        long[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Long>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -510,1 +533,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -513,1 +536,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -518,10 +541,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (long) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (long) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n@@ -529,3 +545,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<LongVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, LongVector.class);\n@@ -536,2 +549,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -539,2 +552,36 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<LongVector, VectorMask<Long>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, LongVector.class);\n+\n+    private static UnaryOperation<LongVector, VectorMask<Long>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) Math.abs(a));\n+            default: return null;\n+        }\n@@ -560,0 +607,1 @@\n+\n@@ -578,1 +626,1 @@\n-                VectorMask<Long> eqz = that.eq((long)0);\n+                VectorMask<Long> eqz = that.eq((long) 0);\n@@ -584,0 +632,1 @@\n+\n@@ -586,34 +635,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n@@ -621,3 +639,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<LongVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, LongVector.class);\n@@ -629,2 +644,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -633,1 +648,6 @@\n-                                  VectorMask<Long> m) {\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v, VectorMask<Long> m) {\n@@ -635,4 +655,10 @@\n-        if (op == DIV) {\n-            VectorMask<Long> eqz = that.eq((long)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Long> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (long) 0);\n+                that = that.blend((long) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n@@ -640,3 +666,61 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Long> eqz = that.eq((long)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<LongVector, VectorMask<Long>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, LongVector.class);\n+\n+    private static BinaryOperation<LongVector, VectorMask<Long>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n@@ -644,1 +728,0 @@\n-        return blend(lanewise(op, v), m);\n@@ -646,0 +729,1 @@\n+\n@@ -708,1 +792,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (long) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -726,16 +816,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n@@ -743,0 +820,22 @@\n+\n+    \/*package-private*\/\n+    abstract LongVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final LongVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Long>> maskClass,\n+                          int e, VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n+    }\n+\n@@ -744,1 +843,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<LongVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<LongVector, VectorMask<Long>>> BIN_INT_IMPL\n@@ -747,0 +846,16 @@\n+    private static VectorBroadcastIntOp<LongVector, VectorMask<Long>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -798,6 +913,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n@@ -805,3 +917,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<LongVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, LongVector.class);\n@@ -815,2 +924,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -820,2 +929,37 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v1,\n+                                          Vector<Long> v2,\n+                                          VectorMask<Long> m) {\n+        LongVector that = (LongVector) v1;\n+        LongVector tother = (LongVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<LongVector, VectorMask<Long>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, LongVector.class);\n+\n+    private static TernaryOperation<LongVector, VectorMask<Long>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -878,1 +1022,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -936,1 +1080,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -993,1 +1137,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1665,2 +1809,0 @@\n-        Objects.requireNonNull(v);\n-        LongSpecies vsp = vspecies();\n@@ -1672,2 +1814,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1683,0 +1825,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Long> v, M m) {\n+        LongVector that = (LongVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, long.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Long> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1700,12 +1864,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Long> compare(VectorOperators.Comparison op,\n-                                  Vector<Long> v,\n-                                  VectorMask<Long> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1770,1 +1922,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -1974,3 +2126,3 @@\n-            getClass(), shuffletype, long.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, long.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -1993,1 +2145,1 @@\n-    <S extends VectorShuffle<Long>>\n+    <S extends VectorShuffle<Long>, M extends VectorMask<Long>>\n@@ -1995,0 +2147,1 @@\n+                                           Class<M> masktype,\n@@ -1996,9 +2149,3 @@\n-                                           VectorMask<Long> m) {\n-        LongVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, long.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2010,1 +2157,7 @@\n-        return broadcast((long)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, long.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2033,3 +2186,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2041,3 +2194,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2306,0 +2459,1 @@\n+                               Class<? extends VectorMask<Long>> maskClass,\n@@ -2307,2 +2461,10 @@\n-        LongVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            LongVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2323,20 +2485,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((long)1, (i, a, b) -> (long)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (long) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (long) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((long)-1, (i, a, b) -> (long)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2344,0 +2489,1 @@\n+\n@@ -2345,2 +2491,22 @@\n-    ImplCache<Associative,Function<LongVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, LongVector.class);\n+    ImplCache<Associative, ReductionOperation<LongVector, VectorMask<Long>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, LongVector.class);\n+\n+    private static ReductionOperation<LongVector, VectorMask<Long>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((long)1, m, (i, a, b) -> (long)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (long) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (long) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((long)-1, m, (i, a, b) -> (long)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2560,3 +2726,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2624,2 +2788,1 @@\n-            LongVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2701,3 +2864,3 @@\n-            vectorType, long.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2705,1 +2868,1 @@\n-            (long[] c, int idx, int[] iMap, int idy, LongSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2707,1 +2870,1 @@\n-        }\n+    }\n@@ -2755,1 +2918,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2757,1 +2919,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2851,3 +3013,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2925,1 +3085,0 @@\n-            \/\/ FIXME: optimize\n@@ -2928,1 +3087,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2991,1 +3150,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2994,1 +3153,1 @@\n-            this,\n+            this, null,\n@@ -2996,1 +3155,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3043,6 +3202,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3078,1 +3232,0 @@\n-            \/\/ FIXME: optimize\n@@ -3081,3 +3234,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3114,1 +3265,0 @@\n-            \/\/ FIXME: optimize\n@@ -3120,3 +3270,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3160,0 +3308,69 @@\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends LongVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return LongVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Long species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3180,0 +3397,19 @@\n+    abstract\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getLong(o + i * 8));\n+            });\n+    }\n+\n@@ -3196,0 +3432,18 @@\n+    abstract\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getLong(o + i * 8));\n+                });\n+    }\n+\n@@ -3215,0 +3469,71 @@\n+    abstract\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(long[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Long species is S_MAX_BIT. and the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3232,0 +3557,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putLong(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3246,0 +3590,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3554,1 +3917,1 @@\n-                                      AbstractMask<Long> m,\n+                                      VectorMask<Long> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":563,"deletions":200,"binary":false,"changes":763,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short128Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short128Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseShiftTemplate(op, Short128Mask.class, e, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short128Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v1, v2, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short128Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short128Mask.class, op, v, (Short128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short128Mask.class,\n@@ -634,3 +669,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short128Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +679,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short128Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +689,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short128Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -790,0 +825,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -797,0 +840,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +855,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -812,0 +869,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short128Mask.class, bb, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -819,0 +883,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n+\n+\n@@ -826,0 +899,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short128Mask.class, bb, offset, (Short128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short256Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short256Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseShiftTemplate(op, Short256Mask.class, e, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short256Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v1, v2, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short256Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short256Mask.class, op, v, (Short256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short256Mask.class,\n@@ -650,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short256Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +695,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short256Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +705,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short256Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -806,0 +841,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -813,0 +856,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -821,0 +871,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -828,0 +885,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short256Mask.class, bb, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -835,0 +899,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n+\n+\n@@ -842,0 +915,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short256Mask.class, bb, offset, (Short256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short512Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short512Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseShiftTemplate(op, Short512Mask.class, e, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short512Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v1, v2, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short512Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short512Mask.class, op, v, (Short512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short512Mask.class,\n@@ -682,3 +717,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short512Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -692,3 +727,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short512Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -702,3 +737,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short512Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -838,0 +873,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -845,0 +888,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -853,0 +903,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -860,0 +917,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short512Mask.class, bb, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -867,0 +931,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n+\n+\n@@ -874,0 +947,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short512Mask.class, bb, offset, (Short512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short64Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short64Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseShiftTemplate(op, Short64Mask.class, e, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short64Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v1, v2, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short64Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short64Mask.class, op, v, (Short64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short64Mask.class,\n@@ -626,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short64Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -636,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short64Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -646,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short64Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -782,0 +817,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -789,0 +832,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -804,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short64Mask.class, bb, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -811,0 +875,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n+\n+\n@@ -818,0 +891,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short64Mask.class, bb, offset, (Short64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public ShortMaxVector lanewise(Unary op, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public ShortMaxVector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline ShortMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseShiftTemplate(op, ShortMaxMask.class, e, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    ShortMaxVector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v1, v2, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final ShortMaxMask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(ShortMaxMask.class, op, v, (ShortMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    ShortMaxMask.class,\n@@ -620,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, ShortMaxMask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, ShortMaxMask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, ShortMaxMask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -776,0 +811,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -783,0 +826,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +841,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +855,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(ShortMaxMask.class, bb, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +869,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n+\n+\n@@ -812,0 +885,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(ShortMaxMask.class, bb, offset, (ShortMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":108,"deletions":14,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    short rOp(short v, FBinOp f);\n+    short rOp(short v, VectorMask<Short> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    short rOpTemplate(short v, VectorMask<Short> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        short[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Short>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (short) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (short) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ShortVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ShortVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ShortVector, VectorMask<Short>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ShortVector.class);\n+\n+    private static UnaryOperation<ShortVector, VectorMask<Short>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Short> eqz = that.eq((short)0);\n+                VectorMask<Short> eqz = that.eq((short) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ShortVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ShortVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Short> m) {\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v, VectorMask<Short> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Short> eqz = that.eq((short)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Short> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n+                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Short> eqz = that.eq((short)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ShortVector, VectorMask<Short>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ShortVector.class);\n+\n+    private static BinaryOperation<ShortVector, VectorMask<Short>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (short) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        short e1 = (short) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,24 @@\n-            opc, getClass(), short.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract ShortVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ShortVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Short>> maskClass,\n+                          int e, VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n@@ -825,0 +928,1 @@\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ShortVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ShortVector, VectorMask<Short>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ShortVector, VectorMask<Short>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ShortVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ShortVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v1,\n+                                          Vector<Short> v2,\n+                                          VectorMask<Short> m) {\n+        ShortVector that = (ShortVector) v1;\n+        ShortVector tother = (ShortVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ShortVector, VectorMask<Short>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ShortVector.class);\n+\n+    private static TernaryOperation<ShortVector, VectorMask<Short>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ShortSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Short> v, M m) {\n+        ShortVector that = (ShortVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, short.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Short> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Short> compare(VectorOperators.Comparison op,\n-                                  Vector<Short> v,\n-                                  VectorMask<Short> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, short.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, short.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Short>>\n+    <S extends VectorShuffle<Short>, M extends VectorMask<Short>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Short> m) {\n-        ShortVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, short.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((short)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, short.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Short>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ShortVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ShortVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((short)1, (i, a, b) -> (short)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (short) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (short) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((short)-1, (i, a, b) -> (short)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ShortVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ShortVector.class);\n+    ImplCache<Associative, ReductionOperation<ShortVector, VectorMask<Short>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ShortVector.class);\n+\n+    private static ReductionOperation<ShortVector, VectorMask<Short>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((short)1, m, (i, a, b) -> (short)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (short) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (short) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((short)-1, m, (i, a, b) -> (short)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2916,2 +3084,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3102,3 +3269,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3341,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3343,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3324,1 +3488,0 @@\n-            \/\/ FIXME: optimize\n@@ -3327,1 +3490,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -3441,1 +3604,0 @@\n-            \/\/ FIXME: optimize\n@@ -3444,3 +3606,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3477,1 +3637,0 @@\n-            \/\/ FIXME: optimize\n@@ -3483,3 +3642,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3523,0 +3680,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3538,0 +3713,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n+\n@@ -3557,0 +3749,19 @@\n+    abstract\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getShort(o + i * 2));\n+            });\n+    }\n+\n@@ -3573,0 +3784,18 @@\n+    abstract\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getShort(o + i * 2));\n+                });\n+    }\n+\n@@ -3592,0 +3821,19 @@\n+    abstract\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+\n@@ -3609,0 +3857,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putShort(o + i * 2, e));\n+            });\n+    }\n+\n@@ -3623,0 +3890,36 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+                });\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+\n@@ -3957,1 +4260,1 @@\n-                                      AbstractMask<Short> m,\n+                                      VectorMask<Short> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":494,"deletions":191,"binary":false,"changes":685,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -556,0 +556,18 @@\n+    \/**\n+     * Checks that this mask has the same class with the given mask class,\n+     * and it has the same species with given vector's species,\n+     * and returns this mask unchanged.\n+     * The effect is similar to this pseudocode:\n+     * {@code getClass() == maskClass &&\n+     *        vectorSpecies() == vector.species()\n+     *        ? this\n+     *        : throw new ClassCastException()}.\n+     *\n+     * @param maskClass the class required for this mask\n+     * @param vector its species required for this mask\n+     * @param <F> the boxed element type of the required species\n+     * @return the same mask\n+     * @throws ClassCastException if the species is wrong\n+     *\/\n+    public abstract <F> VectorMask<F> check(Class<? extends VectorMask<F>> maskClass, Vector<F> vector);\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorMask.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -180,0 +179,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -223,0 +225,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -272,0 +277,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -287,1 +295,16 @@\n-    $type$ rOp($type$ v, FBinOp f);\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    $type$ rOpTemplate($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        $type$[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<$Boxtype$>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -575,1 +598,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -578,1 +601,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -584,44 +607,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.abs(a));\n-#if[FP]\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log1p(a));\n-#end[FP]\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n@@ -629,3 +611,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<$abstractvectortype$>> UN_IMPL\n-        = new ImplCache<>(Unary.class, $Type$Vector.class);\n@@ -636,2 +615,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -639,2 +618,72 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+#if[BITWISE]\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+#end[BITWISE]\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, $Type$Vector.class);\n+\n+    private static UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.abs(a));\n+#if[FP]\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log1p(a));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -660,0 +709,1 @@\n+\n@@ -687,1 +737,1 @@\n-                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$) 0);\n@@ -694,0 +744,1 @@\n+\n@@ -696,44 +747,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.min(a, b));\n-#if[BITWISE]\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-#end[BITWISE]\n-#if[FP]\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n@@ -741,3 +751,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<$abstractvectortype$>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, $Type$Vector.class);\n@@ -749,2 +756,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -753,2 +760,6 @@\n-                                  VectorMask<$Boxtype$> m) {\n-#if[BITWISE]\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n@@ -756,4 +767,34 @@\n-        if (op == DIV) {\n-            VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL {#if[!FP]? | VO_SHIFT})) {\n+            if (op == FIRST_NONZERO) {\n+#if[FP]\n+                return blend(lanewise(op, v), m);\n+#else[FP]\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<$Boxbitstype$> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n+                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+#end[FP]\n+            }\n+#if[BITWISE]\n+#if[!FP]\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+#end[!FP]\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -761,3 +802,1 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+#end[BITWISE]\n@@ -765,0 +804,43 @@\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, $Type$Vector.class);\n+\n+    private static BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.min(a, b));\n+#if[BITWISE]\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n@@ -766,1 +848,12 @@\n-        return blend(lanewise(op, v), m);\n+#if[FP]\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.hypot(a, b));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -768,0 +861,1 @@\n+\n@@ -832,1 +926,9 @@\n-        return blend(lanewise(op, e), m);\n+#if[BITWISE]\n+        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = ($type$) ~e;\n+        }\n+#end[BITWISE]\n+        return lanewise(op, broadcast(e), m);\n@@ -851,1 +953,0 @@\n-        if ((long)e1 != e\n@@ -853,0 +954,1 @@\n+        if ((long)e1 != e\n@@ -854,1 +956,3 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n@@ -856,1 +960,0 @@\n-            ) {\n@@ -876,1 +979,11 @@\n-        return blend(lanewise(op, e), m);\n+        $type$ e1 = ($type$) e;\n+#if[BITWISE]\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n+#end[BITWISE]\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -895,16 +1008,24 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract $abstractvectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final $abstractvectortype$\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                          int e, VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n@@ -912,0 +1033,1 @@\n+\n@@ -913,1 +1035,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>>> BIN_INT_IMPL\n@@ -916,0 +1038,16 @@\n+    private static VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -975,10 +1113,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-#if[FP]\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n@@ -986,3 +1117,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<$abstractvectortype$>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, $Type$Vector.class);\n@@ -996,2 +1124,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -1001,2 +1129,43 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v1,\n+                                          Vector<$Boxtype$> v2,\n+                                          VectorMask<$Boxtype$> m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v1;\n+        $abstractvectortype$ tother = ($abstractvectortype$) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+#if[BITWISE]\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+#end[BITWISE]\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, $Type$Vector.class);\n+\n+    private static TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+#if[FP]\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -1059,1 +1228,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1117,1 +1286,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1174,1 +1343,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -2019,2 +2188,0 @@\n-        Objects.requireNonNull(v);\n-        $Type$Species vsp = vspecies();\n@@ -2026,2 +2193,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -2037,0 +2204,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<$Boxtype$> v, M m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, $type$.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<$Boxtype$> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -2056,12 +2245,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<$Boxtype$> compare(VectorOperators.Comparison op,\n-                                  Vector<$Boxtype$> v,\n-                                  VectorMask<$Boxtype$> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -2126,1 +2303,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2381,3 +2558,3 @@\n-            getClass(), shuffletype, $type$.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, $type$.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2400,1 +2577,1 @@\n-    <S extends VectorShuffle<$Boxtype$>>\n+    <S extends VectorShuffle<$Boxtype$>, M extends VectorMask<$Boxtype$>>\n@@ -2402,0 +2579,1 @@\n+                                           Class<M> masktype,\n@@ -2403,9 +2581,3 @@\n-                                           VectorMask<$Boxtype$> m) {\n-        $abstractvectortype$ unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2417,1 +2589,7 @@\n-        return broadcast(($type$)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, $type$.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2440,3 +2618,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2448,3 +2626,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2842,0 +3020,1 @@\n+                               Class<? extends VectorMask<$Boxtype$>> maskClass,\n@@ -2843,2 +3022,10 @@\n-        $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n@@ -2859,12 +3046,19 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp(($type$)1, (i, a, b) -> ($type$)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> ($type$) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> ($type$) Math.max(a, b)));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n+    }\n+\n+    private static final\n+    ImplCache<Associative, ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, $Type$Vector.class);\n+\n+    private static ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp(($type$)1, m, (i, a, b) -> ($type$)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> ($type$) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> ($type$) Math.max(a, b)));\n@@ -2872,6 +3066,6 @@\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp(($type$)-1, (i, a, b) -> ($type$)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a ^ b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp(($type$)-1, m, (i, a, b) -> ($type$)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a ^ b)));\n@@ -2879,2 +3073,2 @@\n-              default: return null;\n-              }})));\n+            default: return null;\n+        }\n@@ -2882,3 +3076,0 @@\n-    private static final\n-    ImplCache<Associative,Function<$abstractvectortype$,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, $Type$Vector.class);\n@@ -3178,3 +3369,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -3242,2 +3431,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -3336,3 +3524,3 @@\n-            vectorType, $type$.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -3340,1 +3528,1 @@\n-            ($type$[] c, int idx, int[] iMap, int idy, $Type$Species s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -3342,1 +3530,1 @@\n-        }\n+    }\n@@ -3402,1 +3590,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -3404,1 +3591,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -3465,2 +3652,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3626,1 +3812,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3817,3 +4003,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3891,1 +4075,0 @@\n-            \/\/ FIXME: optimize\n@@ -3894,1 +4077,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3976,1 +4159,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3979,1 +4162,1 @@\n-            this,\n+            this, null,\n@@ -3981,1 +4164,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -4042,6 +4225,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -4115,1 +4293,0 @@\n-            \/\/ FIXME: optimize\n@@ -4118,1 +4295,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -4278,1 +4455,0 @@\n-            \/\/ FIXME: optimize\n@@ -4281,1 +4457,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -4401,1 +4577,0 @@\n-            \/\/ FIXME: optimize\n@@ -4404,3 +4579,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -4437,1 +4610,0 @@\n-            \/\/ FIXME: optimize\n@@ -4443,3 +4615,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -4483,0 +4653,78 @@\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+#if[!byteOrShort]\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+          return $abstractvectortype$.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of $Type$ species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#end[!byteOrShort]\n+\n@@ -4498,0 +4746,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n@@ -4515,0 +4780,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n@@ -4535,0 +4817,19 @@\n+    abstract\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+            });\n+    }\n+\n@@ -4551,0 +4852,18 @@\n+    abstract\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+                });\n+    }\n+\n@@ -4570,0 +4889,99 @@\n+    abstract\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+#if[!byteOrShort]\n+    abstract\n+    void intoArray0($type$[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of $Type$ species is S_MAX_BIT. and the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+#end[byte]\n+\n@@ -4587,0 +5005,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            });\n+    }\n+\n@@ -4601,0 +5038,38 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+                });\n+    }\n+\n+#if[short]\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+#end[short]\n+\n@@ -4976,1 +5451,1 @@\n-                                      AbstractMask<$Boxtype$> m,\n+                                      VectorMask<$Boxtype$> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":736,"deletions":261,"binary":false,"changes":997,"status":"modified"},{"patch":"@@ -241,2 +241,2 @@\n-    $type$ rOp($type$ v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -278,0 +278,6 @@\n+    @Override\n+    @ForceInline\n+    public $vectortype$ lanewise(Unary op, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +290,6 @@\n+    @Override\n+    @ForceInline\n+    public $vectortype$ lanewise(Binary op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -291,0 +303,7 @@\n+\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline $vectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseShiftTemplate(op, $masktype$.class, e, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -298,1 +317,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2) {\n+    lanewise(Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2) {\n@@ -302,0 +321,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    $vectortype$\n+    lanewise(Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v1, v2, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -321,1 +348,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialized\n@@ -334,1 +361,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialized\n@@ -372,0 +399,7 @@\n+    @Override\n+    @ForceInline\n+    public final $masktype$ compare(Comparison op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return super.compareTemplate($masktype$.class, op, v, ($masktype$) m);\n+    }\n+\n+\n@@ -428,0 +462,1 @@\n+                                    $masktype$.class,\n@@ -893,3 +928,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -903,3 +938,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -913,3 +948,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -1064,0 +1099,16 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n+#if[!byteOrShort]\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, int[] indexMap, int mapOffset, VectorMask<$Boxtype$> m) {\n+        return super.fromArray0Template($masktype$.class, a, offset, indexMap, mapOffset, ($masktype$) m);\n+    }\n+#end[!byteOrShort]\n+\n@@ -1071,0 +1122,7 @@\n+\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromCharArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -1080,0 +1138,7 @@\n+\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -1089,0 +1154,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromByteArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -1096,0 +1168,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromByteBuffer0Template($masktype$.class, bb, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -1103,0 +1182,25 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+\n+#if[!byteOrShort]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, int[] indexMap, int mapOffset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, indexMap, mapOffset, ($masktype$) m);\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+#end[byte]\n+\n@@ -1110,0 +1214,23 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoByteArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoByteBuffer0Template($masktype$.class, bb, offset, ($masktype$) m);\n+    }\n+\n+#if[short]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoCharArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+#end[short]\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":141,"deletions":14,"binary":false,"changes":155,"status":"modified"},{"patch":"@@ -1556,0 +1556,17 @@\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, p0, 0);\",                       \"lsl\\tz0.b, p0\/m, z0.b, #0\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, p0, 5);\",                       \"lsl\\tz0.b, p0\/m, z0.b, #5\"],\n+                        [\"lsl\",     \"__ sve_lsl(z1, __ H, p1, 15);\",                      \"lsl\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"lsl\",     \"__ sve_lsl(z2, __ S, p2, 31);\",                      \"lsl\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"lsl\",     \"__ sve_lsl(z3, __ D, p3, 63);\",                      \"lsl\\tz3.d, p3\/m, z3.d, #63\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, p0, 1);\",                       \"lsr\\tz0.b, p0\/m, z0.b, #1\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, p0, 8);\",                       \"lsr\\tz0.b, p0\/m, z0.b, #8\"],\n+                        [\"lsr\",     \"__ sve_lsr(z1, __ H, p1, 15);\",                      \"lsr\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"lsr\",     \"__ sve_lsr(z2, __ S, p2, 7);\",                       \"lsr\\tz2.s, p2\/m, z2.s, #7\"],\n+                        [\"lsr\",     \"__ sve_lsr(z2, __ S, p2, 31);\",                      \"lsr\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"lsr\",     \"__ sve_lsr(z3, __ D, p3, 63);\",                      \"lsr\\tz3.d, p3\/m, z3.d, #63\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ B, p0, 1);\",                       \"asr\\tz0.b, p0\/m, z0.b, #1\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ B, p0, 7);\",                       \"asr\\tz0.b, p0\/m, z0.b, #7\"],\n+                        [\"asr\",     \"__ sve_asr(z1, __ H, p1, 5);\",                       \"asr\\tz1.h, p1\/m, z1.h, #5\"],\n+                        [\"asr\",     \"__ sve_asr(z1, __ H, p1, 15);\",                      \"asr\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"asr\",     \"__ sve_asr(z2, __ S, p2, 31);\",                      \"asr\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"asr\",     \"__ sve_asr(z3, __ D, p3, 63);\",                      \"asr\\tz3.d, p3\/m, z3.d, #63\"],\n@@ -1652,0 +1669,13 @@\n+                        [\"and\",     \"__ sve_and(p0, p1, p2, p3);\",                        \"and\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"ands\",    \"__ sve_ands(p4, p5, p6, p0);\",                       \"ands\\tp4.b, p5\/z, p6.b, p0.b\"],\n+                        [\"eor\",     \"__ sve_eor(p0, p1, p2, p3);\",                        \"eor\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"eors\",    \"__ sve_eors(p5, p6, p0, p1);\",                       \"eors\\tp5.b, p6\/z, p0.b, p1.b\"],\n+                        [\"orr\",     \"__ sve_orr(p0, p1, p2, p3);\",                        \"orr\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"orrs\",    \"__ sve_orrs(p9, p1, p4, p5);\",                       \"orrs\\tp9.b, p1\/z, p4.b, p5.b\"],\n+                        [\"bic\",     \"__ sve_bic(p10, p7, p9, p11);\",                      \"bic\\tp10.b, p7\/z, p9.b, p11.b\"],\n+                        [\"ptest\",   \"__ sve_ptest(p7, p1);\",                              \"ptest\\tp7, p1.b\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p1, __ B);\",                            \"ptrue\\tp1.b\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p2, __ H);\",                            \"ptrue\\tp2.h\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p3, __ S);\",                            \"ptrue\\tp3.s\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p4, __ D);\",                            \"ptrue\\tp4.d\"],\n+                        [\"pfalse\",  \"__ sve_pfalse(p7);\",                                 \"pfalse\\tp7.b\"],\n@@ -1689,0 +1719,1 @@\n+                       [\"and\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1691,0 +1722,1 @@\n+                       [\"eor\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1696,0 +1728,1 @@\n+                       [\"orr\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1711,0 +1744,1 @@\n+                       [\"fmad\", \"ZPZZ\", \"m\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -745,0 +745,17 @@\n+    __ sve_lsl(z0, __ B, p0, 0);                       \/\/       lsl     z0.b, p0\/m, z0.b, #0\n+    __ sve_lsl(z0, __ B, p0, 5);                       \/\/       lsl     z0.b, p0\/m, z0.b, #5\n+    __ sve_lsl(z1, __ H, p1, 15);                      \/\/       lsl     z1.h, p1\/m, z1.h, #15\n+    __ sve_lsl(z2, __ S, p2, 31);                      \/\/       lsl     z2.s, p2\/m, z2.s, #31\n+    __ sve_lsl(z3, __ D, p3, 63);                      \/\/       lsl     z3.d, p3\/m, z3.d, #63\n+    __ sve_lsr(z0, __ B, p0, 1);                       \/\/       lsr     z0.b, p0\/m, z0.b, #1\n+    __ sve_lsr(z0, __ B, p0, 8);                       \/\/       lsr     z0.b, p0\/m, z0.b, #8\n+    __ sve_lsr(z1, __ H, p1, 15);                      \/\/       lsr     z1.h, p1\/m, z1.h, #15\n+    __ sve_lsr(z2, __ S, p2, 7);                       \/\/       lsr     z2.s, p2\/m, z2.s, #7\n+    __ sve_lsr(z2, __ S, p2, 31);                      \/\/       lsr     z2.s, p2\/m, z2.s, #31\n+    __ sve_lsr(z3, __ D, p3, 63);                      \/\/       lsr     z3.d, p3\/m, z3.d, #63\n+    __ sve_asr(z0, __ B, p0, 1);                       \/\/       asr     z0.b, p0\/m, z0.b, #1\n+    __ sve_asr(z0, __ B, p0, 7);                       \/\/       asr     z0.b, p0\/m, z0.b, #7\n+    __ sve_asr(z1, __ H, p1, 5);                       \/\/       asr     z1.h, p1\/m, z1.h, #5\n+    __ sve_asr(z1, __ H, p1, 15);                      \/\/       asr     z1.h, p1\/m, z1.h, #15\n+    __ sve_asr(z2, __ S, p2, 31);                      \/\/       asr     z2.s, p2\/m, z2.s, #31\n+    __ sve_asr(z3, __ D, p3, 63);                      \/\/       asr     z3.d, p3\/m, z3.d, #63\n@@ -841,0 +858,13 @@\n+    __ sve_and(p0, p1, p2, p3);                        \/\/       and     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_ands(p4, p5, p6, p0);                       \/\/       ands    p4.b, p5\/z, p6.b, p0.b\n+    __ sve_eor(p0, p1, p2, p3);                        \/\/       eor     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_eors(p5, p6, p0, p1);                       \/\/       eors    p5.b, p6\/z, p0.b, p1.b\n+    __ sve_orr(p0, p1, p2, p3);                        \/\/       orr     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_orrs(p9, p1, p4, p5);                       \/\/       orrs    p9.b, p1\/z, p4.b, p5.b\n+    __ sve_bic(p10, p7, p9, p11);                      \/\/       bic     p10.b, p7\/z, p9.b, p11.b\n+    __ sve_ptest(p7, p1);                              \/\/       ptest   p7, p1.b\n+    __ sve_ptrue(p1, __ B);                            \/\/       ptrue   p1.b\n+    __ sve_ptrue(p2, __ H);                            \/\/       ptrue   p2.h\n+    __ sve_ptrue(p3, __ S);                            \/\/       ptrue   p3.s\n+    __ sve_ptrue(p4, __ D);                            \/\/       ptrue   p4.d\n+    __ sve_pfalse(p7);                                 \/\/       pfalse  p7.b\n@@ -984,38 +1014,42 @@\n-    __ sve_asr(z26, __ H, p5, z28);                    \/\/       asr     z26.h, p5\/m, z26.h, z28.h\n-    __ sve_cnt(z13, __ D, p7, z16);                    \/\/       cnt     z13.d, p7\/m, z16.d\n-    __ sve_lsl(z5, __ H, p0, z13);                     \/\/       lsl     z5.h, p0\/m, z5.h, z13.h\n-    __ sve_lsr(z15, __ S, p2, z26);                    \/\/       lsr     z15.s, p2\/m, z15.s, z26.s\n-    __ sve_mul(z11, __ S, p1, z22);                    \/\/       mul     z11.s, p1\/m, z11.s, z22.s\n-    __ sve_neg(z4, __ S, p0, z19);                     \/\/       neg     z4.s, p0\/m, z19.s\n-    __ sve_not(z17, __ H, p3, z14);                    \/\/       not     z17.h, p3\/m, z14.h\n-    __ sve_smax(z2, __ S, p4, z3);                     \/\/       smax    z2.s, p4\/m, z2.s, z3.s\n-    __ sve_smin(z23, __ B, p1, z6);                    \/\/       smin    z23.b, p1\/m, z23.b, z6.b\n-    __ sve_sub(z17, __ S, p3, z27);                    \/\/       sub     z17.s, p3\/m, z17.s, z27.s\n-    __ sve_fabs(z16, __ D, p1, z2);                    \/\/       fabs    z16.d, p1\/m, z2.d\n-    __ sve_fadd(z3, __ D, p1, z6);                     \/\/       fadd    z3.d, p1\/m, z3.d, z6.d\n-    __ sve_fdiv(z19, __ D, p3, z12);                   \/\/       fdiv    z19.d, p3\/m, z19.d, z12.d\n-    __ sve_fmax(z8, __ D, p6, z19);                    \/\/       fmax    z8.d, p6\/m, z8.d, z19.d\n-    __ sve_fmin(z0, __ S, p2, z23);                    \/\/       fmin    z0.s, p2\/m, z0.s, z23.s\n-    __ sve_fmul(z19, __ D, p7, z13);                   \/\/       fmul    z19.d, p7\/m, z19.d, z13.d\n-    __ sve_fneg(z6, __ S, p0, z7);                     \/\/       fneg    z6.s, p0\/m, z7.s\n-    __ sve_frintm(z17, __ S, p6, z8);                  \/\/       frintm  z17.s, p6\/m, z8.s\n-    __ sve_frintn(z22, __ D, p5, z22);                 \/\/       frintn  z22.d, p5\/m, z22.d\n-    __ sve_frintp(z2, __ D, p0, z15);                  \/\/       frintp  z2.d, p0\/m, z15.d\n-    __ sve_fsqrt(z20, __ D, p1, z4);                   \/\/       fsqrt   z20.d, p1\/m, z4.d\n-    __ sve_fsub(z7, __ D, p0, z8);                     \/\/       fsub    z7.d, p0\/m, z7.d, z8.d\n-    __ sve_fmla(z19, __ S, p5, z4, z15);               \/\/       fmla    z19.s, p5\/m, z4.s, z15.s\n-    __ sve_fmls(z22, __ D, p2, z25, z5);               \/\/       fmls    z22.d, p2\/m, z25.d, z5.d\n-    __ sve_fnmla(z16, __ S, p3, z22, z11);             \/\/       fnmla   z16.s, p3\/m, z22.s, z11.s\n-    __ sve_fnmls(z13, __ D, p2, z20, z16);             \/\/       fnmls   z13.d, p2\/m, z20.d, z16.d\n-    __ sve_mla(z15, __ H, p1, z4, z17);                \/\/       mla     z15.h, p1\/m, z4.h, z17.h\n-    __ sve_mls(z6, __ S, p7, z4, z28);                 \/\/       mls     z6.s, p7\/m, z4.s, z28.s\n-    __ sve_and(z29, z26, z9);                          \/\/       and     z29.d, z26.d, z9.d\n-    __ sve_eor(z2, z11, z28);                          \/\/       eor     z2.d, z11.d, z28.d\n-    __ sve_orr(z7, z1, z26);                           \/\/       orr     z7.d, z1.d, z26.d\n-    __ sve_bic(z17, z14, z8);                          \/\/       bic     z17.d, z14.d, z8.d\n-    __ sve_cmpeq(p5, __ S, p6, z5, z19);               \/\/       cmpeq   p5.s, p6\/z, z5.s, z19.s\n-    __ sve_cmpge(p4, __ S, p5, z16, z29);              \/\/       cmpge   p4.s, p5\/z, z16.s, z29.s\n-    __ sve_cmpgt(p5, __ D, p0, z4, z17);               \/\/       cmpgt   p5.d, p0\/z, z4.d, z17.d\n-    __ sve_cmpne(p1, __ D, p5, z4, z23);               \/\/       cmpne   p1.d, p5\/z, z4.d, z23.d\n-    __ sve_uzp1(z19, __ H, z2, z8);                    \/\/       uzp1    z19.h, z2.h, z8.h\n-    __ sve_uzp2(z14, __ D, z24, z17);                  \/\/       uzp2    z14.d, z24.d, z17.d\n+    __ sve_and(z26, __ H, p5, z28);                    \/\/       and     z26.h, p5\/m, z26.h, z28.h\n+    __ sve_asr(z13, __ D, p7, z16);                    \/\/       asr     z13.d, p7\/m, z13.d, z16.d\n+    __ sve_cnt(z5, __ H, p0, z13);                     \/\/       cnt     z5.h, p0\/m, z13.h\n+    __ sve_eor(z15, __ S, p2, z26);                    \/\/       eor     z15.s, p2\/m, z15.s, z26.s\n+    __ sve_lsl(z11, __ S, p1, z22);                    \/\/       lsl     z11.s, p1\/m, z11.s, z22.s\n+    __ sve_lsr(z4, __ S, p0, z19);                     \/\/       lsr     z4.s, p0\/m, z4.s, z19.s\n+    __ sve_mul(z17, __ H, p3, z14);                    \/\/       mul     z17.h, p3\/m, z17.h, z14.h\n+    __ sve_neg(z2, __ S, p4, z3);                      \/\/       neg     z2.s, p4\/m, z3.s\n+    __ sve_not(z23, __ B, p1, z6);                     \/\/       not     z23.b, p1\/m, z6.b\n+    __ sve_orr(z17, __ S, p3, z27);                    \/\/       orr     z17.s, p3\/m, z17.s, z27.s\n+    __ sve_smax(z16, __ D, p1, z2);                    \/\/       smax    z16.d, p1\/m, z16.d, z2.d\n+    __ sve_smin(z3, __ S, p1, z6);                     \/\/       smin    z3.s, p1\/m, z3.s, z6.s\n+    __ sve_sub(z19, __ S, p3, z12);                    \/\/       sub     z19.s, p3\/m, z19.s, z12.s\n+    __ sve_fabs(z8, __ D, p6, z19);                    \/\/       fabs    z8.d, p6\/m, z19.d\n+    __ sve_fadd(z0, __ S, p2, z23);                    \/\/       fadd    z0.s, p2\/m, z0.s, z23.s\n+    __ sve_fdiv(z19, __ D, p7, z13);                   \/\/       fdiv    z19.d, p7\/m, z19.d, z13.d\n+    __ sve_fmax(z6, __ S, p0, z7);                     \/\/       fmax    z6.s, p0\/m, z6.s, z7.s\n+    __ sve_fmin(z17, __ S, p6, z8);                    \/\/       fmin    z17.s, p6\/m, z17.s, z8.s\n+    __ sve_fmul(z22, __ D, p5, z22);                   \/\/       fmul    z22.d, p5\/m, z22.d, z22.d\n+    __ sve_fneg(z2, __ D, p0, z15);                    \/\/       fneg    z2.d, p0\/m, z15.d\n+    __ sve_frintm(z20, __ D, p1, z4);                  \/\/       frintm  z20.d, p1\/m, z4.d\n+    __ sve_frintn(z7, __ D, p0, z8);                   \/\/       frintn  z7.d, p0\/m, z8.d\n+    __ sve_frintp(z19, __ D, p5, z4);                  \/\/       frintp  z19.d, p5\/m, z4.d\n+    __ sve_fsqrt(z9, __ D, p5, z11);                   \/\/       fsqrt   z9.d, p5\/m, z11.d\n+    __ sve_fsub(z5, __ S, p7, z16);                    \/\/       fsub    z5.s, p7\/m, z5.s, z16.s\n+    __ sve_fmad(z22, __ S, p3, z1, z13);               \/\/       fmad    z22.s, p3\/m, z1.s, z13.s\n+    __ sve_fmla(z20, __ S, p4, z25, z15);              \/\/       fmla    z20.s, p4\/m, z25.s, z15.s\n+    __ sve_fmls(z4, __ D, p4, z8, z6);                 \/\/       fmls    z4.d, p4\/m, z8.d, z6.d\n+    __ sve_fnmla(z4, __ D, p7, z16, z29);              \/\/       fnmla   z4.d, p7\/m, z16.d, z29.d\n+    __ sve_fnmls(z9, __ D, p3, z2, z11);               \/\/       fnmls   z9.d, p3\/m, z2.d, z11.d\n+    __ sve_mla(z3, __ S, p1, z1, z26);                 \/\/       mla     z3.s, p1\/m, z1.s, z26.s\n+    __ sve_mls(z17, __ S, p3, z8, z17);                \/\/       mls     z17.s, p3\/m, z8.s, z17.s\n+    __ sve_and(z24, z5, z19);                          \/\/       and     z24.d, z5.d, z19.d\n+    __ sve_eor(z17, z22, z16);                         \/\/       eor     z17.d, z22.d, z16.d\n+    __ sve_orr(z20, z19, z0);                          \/\/       orr     z20.d, z19.d, z0.d\n+    __ sve_bic(z17, z23, z4);                          \/\/       bic     z17.d, z23.d, z4.d\n+    __ sve_cmpeq(p1, __ B, p6, z25, z19);              \/\/       cmpeq   p1.b, p6\/z, z25.b, z19.b\n+    __ sve_cmpge(p2, __ S, p2, z14, z24);              \/\/       cmpge   p2.s, p2\/z, z14.s, z24.s\n+    __ sve_cmpgt(p7, __ B, p5, z4, z30);               \/\/       cmpgt   p7.b, p5\/z, z4.b, z30.b\n+    __ sve_cmpne(p2, __ H, p5, z12, z0);               \/\/       cmpne   p2.h, p5\/z, z12.h, z0.h\n+    __ sve_uzp1(z7, __ B, z24, z17);                   \/\/       uzp1    z7.b, z24.b, z17.b\n+    __ sve_uzp2(z27, __ D, z6, z9);                    \/\/       uzp2    z27.d, z6.d, z9.d\n@@ -1024,9 +1058,9 @@\n-    __ sve_andv(v21, __ B, p1, z30);                   \/\/       andv b21, p1, z30.b\n-    __ sve_orv(v10, __ B, p5, z12);                    \/\/       orv b10, p5, z12.b\n-    __ sve_eorv(v9, __ S, p1, z24);                    \/\/       eorv s9, p1, z24.s\n-    __ sve_smaxv(v4, __ H, p6, z6);                    \/\/       smaxv h4, p6, z6.h\n-    __ sve_sminv(v27, __ S, p6, z13);                  \/\/       sminv s27, p6, z13.s\n-    __ sve_fminv(v30, __ D, p5, z22);                  \/\/       fminv d30, p5, z22.d\n-    __ sve_fmaxv(v30, __ S, p7, z9);                   \/\/       fmaxv s30, p7, z9.s\n-    __ sve_fadda(v19, __ D, p1, z20);                  \/\/       fadda d19, p1, d19, z20.d\n-    __ sve_uaddv(v9, __ H, p2, z13);                   \/\/       uaddv d9, p2, z13.h\n+    __ sve_andv(v23, __ D, p3, z16);                   \/\/       andv d23, p3, z16.d\n+    __ sve_orv(v22, __ D, p5, z20);                    \/\/       orv d22, p5, z20.d\n+    __ sve_eorv(v28, __ S, p2, z13);                   \/\/       eorv s28, p2, z13.s\n+    __ sve_smaxv(v7, __ H, p5, z28);                   \/\/       smaxv h7, p5, z28.h\n+    __ sve_sminv(v11, __ S, p3, z11);                  \/\/       sminv s11, p3, z11.s\n+    __ sve_fminv(v1, __ D, p6, z8);                    \/\/       fminv d1, p6, z8.d\n+    __ sve_fmaxv(v13, __ D, p4, z17);                  \/\/       fmaxv d13, p4, z17.d\n+    __ sve_fadda(v4, __ S, p0, z3);                    \/\/       fadda s4, p0, s4, z3.s\n+    __ sve_uaddv(v7, __ S, p3, z14);                   \/\/       uaddv d7, p3, z14.s\n@@ -1051,7 +1085,7 @@\n-    0x14000000,     0x17ffffd7,     0x14000352,     0x94000000,\n-    0x97ffffd4,     0x9400034f,     0x3400000a,     0x34fffa2a,\n-    0x3400698a,     0x35000008,     0x35fff9c8,     0x35006928,\n-    0xb400000b,     0xb4fff96b,     0xb40068cb,     0xb500001d,\n-    0xb5fff91d,     0xb500687d,     0x10000013,     0x10fff8b3,\n-    0x10006813,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36306796,     0x3758000c,     0x375ff7cc,     0x3758672c,\n+    0x14000000,     0x17ffffd7,     0x14000374,     0x94000000,\n+    0x97ffffd4,     0x94000371,     0x3400000a,     0x34fffa2a,\n+    0x34006dca,     0x35000008,     0x35fff9c8,     0x35006d68,\n+    0xb400000b,     0xb4fff96b,     0xb4006d0b,     0xb500001d,\n+    0xb5fff91d,     0xb5006cbd,     0x10000013,     0x10fff8b3,\n+    0x10006c53,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36306bd6,     0x3758000c,     0x375ff7cc,     0x37586b6c,\n@@ -1062,13 +1096,13 @@\n-    0x54006500,     0x54000001,     0x54fff541,     0x540064a1,\n-    0x54000002,     0x54fff4e2,     0x54006442,     0x54000002,\n-    0x54fff482,     0x540063e2,     0x54000003,     0x54fff423,\n-    0x54006383,     0x54000003,     0x54fff3c3,     0x54006323,\n-    0x54000004,     0x54fff364,     0x540062c4,     0x54000005,\n-    0x54fff305,     0x54006265,     0x54000006,     0x54fff2a6,\n-    0x54006206,     0x54000007,     0x54fff247,     0x540061a7,\n-    0x54000008,     0x54fff1e8,     0x54006148,     0x54000009,\n-    0x54fff189,     0x540060e9,     0x5400000a,     0x54fff12a,\n-    0x5400608a,     0x5400000b,     0x54fff0cb,     0x5400602b,\n-    0x5400000c,     0x54fff06c,     0x54005fcc,     0x5400000d,\n-    0x54fff00d,     0x54005f6d,     0x5400000e,     0x54ffefae,\n-    0x54005f0e,     0x5400000f,     0x54ffef4f,     0x54005eaf,\n+    0x54006940,     0x54000001,     0x54fff541,     0x540068e1,\n+    0x54000002,     0x54fff4e2,     0x54006882,     0x54000002,\n+    0x54fff482,     0x54006822,     0x54000003,     0x54fff423,\n+    0x540067c3,     0x54000003,     0x54fff3c3,     0x54006763,\n+    0x54000004,     0x54fff364,     0x54006704,     0x54000005,\n+    0x54fff305,     0x540066a5,     0x54000006,     0x54fff2a6,\n+    0x54006646,     0x54000007,     0x54fff247,     0x540065e7,\n+    0x54000008,     0x54fff1e8,     0x54006588,     0x54000009,\n+    0x54fff189,     0x54006529,     0x5400000a,     0x54fff12a,\n+    0x540064ca,     0x5400000b,     0x54fff0cb,     0x5400646b,\n+    0x5400000c,     0x54fff06c,     0x5400640c,     0x5400000d,\n+    0x54fff00d,     0x540063ad,     0x5400000e,     0x54ffefae,\n+    0x5400634e,     0x5400000f,     0x54ffef4f,     0x540062ef,\n@@ -1106,1 +1140,1 @@\n-    0xbd1b1869,     0x58004efb,     0x1800000b,     0xf8945060,\n+    0xbd1b1869,     0x5800533b,     0x1800000b,     0xf8945060,\n@@ -1198,67 +1232,75 @@\n-    0x0461943e,     0x04a19020,     0x042053ff,     0x047f5401,\n-    0x25208028,     0x2538cfe0,     0x2578d001,     0x25b8efe2,\n-    0x25f8f007,     0x2538dfea,     0x25b8dfeb,     0xa400a3e0,\n-    0xa420a7e0,     0xa4484be0,     0xa467afe0,     0xa4a8a7ea,\n-    0xa547a814,     0xa4084ffe,     0xa55c53e0,     0xa5e1540b,\n-    0xe400fbf6,     0xe408ffff,     0xe420e7e0,     0xe4484be0,\n-    0xe460efe0,     0xe547e400,     0xe4014be0,     0xe4a84fe0,\n-    0xe5f15000,     0x858043e0,     0x85a043ff,     0xe59f5d08,\n-    0x0420e3e9,     0x0460e3ea,     0x04a0e3eb,     0x04e0e3ec,\n-    0x25104042,     0x25104871,     0x25904861,     0x25904c92,\n-    0x05344020,     0x05744041,     0x05b44062,     0x05f44083,\n-    0x252c8840,     0x253c1420,     0x25681572,     0x25a21ce3,\n-    0x25ea1e34,     0x0522c020,     0x05e6c0a4,     0x2401a001,\n-    0x2443a051,     0x24858881,     0x24c78cd1,     0x240b8142,\n-    0x24918213,     0x250f9001,     0x25508051,     0x25802491,\n-    0x25df28c1,     0x25850c81,     0x251e10d1,     0x65816001,\n-    0x65c36051,     0x65854891,     0x65c74cc1,     0x658b4152,\n-    0x65d14203,     0x05733820,     0x05b238a4,     0x05f138e6,\n-    0x0570396a,     0x65d0a001,     0x65d6a443,     0x65d4a826,\n-    0x6594ac26,     0x6554ac26,     0x6556ac26,     0x6552ac26,\n-    0x65cbac85,     0x65caac01,     0x65dea833,     0x659ca509,\n-    0x65d8a801,     0x65dcac01,     0x655cb241,     0x0520a1e0,\n-    0x0521a601,     0x052281e0,     0x05238601,     0x04a14026,\n-    0x0568aca7,     0x05b23230,     0x853040af,     0xc5b040af,\n-    0xe57080af,     0xe5b080af,     0x1e601000,     0x1e603000,\n-    0x1e621000,     0x1e623000,     0x1e641000,     0x1e643000,\n-    0x1e661000,     0x1e663000,     0x1e681000,     0x1e683000,\n-    0x1e6a1000,     0x1e6a3000,     0x1e6c1000,     0x1e6c3000,\n-    0x1e6e1000,     0x1e6e3000,     0x1e701000,     0x1e703000,\n-    0x1e721000,     0x1e723000,     0x1e741000,     0x1e743000,\n-    0x1e761000,     0x1e763000,     0x1e781000,     0x1e783000,\n-    0x1e7a1000,     0x1e7a3000,     0x1e7c1000,     0x1e7c3000,\n-    0x1e7e1000,     0x1e7e3000,     0xf8208193,     0xf83101b6,\n-    0xf83c13fe,     0xf821239a,     0xf824309e,     0xf826535e,\n-    0xf8304109,     0xf82c7280,     0xf8216058,     0xf8a08309,\n-    0xf8ba03d0,     0xf8a312ea,     0xf8aa21e4,     0xf8a2310b,\n-    0xf8aa522f,     0xf8a2418a,     0xf8ac71af,     0xf8a26287,\n-    0xf8fa8090,     0xf8e20184,     0xf8f01215,     0xf8f022ab,\n-    0xf8f7334c,     0xf8f751dc,     0xf8eb4038,     0xf8ec715f,\n-    0xf8f06047,     0xf863826d,     0xf8710070,     0xf86113cb,\n-    0xf86521e8,     0xf87d301e,     0xf8745287,     0xf87742bc,\n-    0xf87b70b9,     0xf8616217,     0xb83f8185,     0xb82901fc,\n-    0xb83d13f6,     0xb83320bf,     0xb82e33f0,     0xb830529b,\n-    0xb830416c,     0xb82973c6,     0xb831639b,     0xb8be8147,\n-    0xb8b4008a,     0xb8b81231,     0xb8b623a3,     0xb8af3276,\n-    0xb8b35056,     0xb8af4186,     0xb8b071ab,     0xb8b763c1,\n-    0xb8f38225,     0xb8e202d0,     0xb8ed12aa,     0xb8fd219b,\n-    0xb8fb3023,     0xb8ff5278,     0xb8f14389,     0xb8fb70ef,\n-    0xb8f563f7,     0xb87983e2,     0xb87b0150,     0xb8771073,\n-    0xb8702320,     0xb87a3057,     0xb870508c,     0xb87c43be,\n-    0xb87070db,     0xb86961fd,     0xce273c87,     0xce080ac9,\n-    0xce7e8e9b,     0xce808b45,     0xce79806e,     0xce758768,\n-    0xcec0835a,     0xce608ad8,     0x043100c4,     0x046105e3,\n-    0x65c900a6,     0x65d60a87,     0x65c80545,     0x0416a63e,\n-    0x04001f8b,     0x0450979a,     0x04dabe0d,     0x045381a5,\n-    0x04918b4f,     0x049006cb,     0x0497a264,     0x045eadd1,\n-    0x04881062,     0x040a04d7,     0x04810f71,     0x04dca450,\n-    0x65c084c3,     0x65cd8d93,     0x65c69a68,     0x65878ae0,\n-    0x65c29db3,     0x049da0e6,     0x6582b911,     0x65c0b6d6,\n-    0x65c1a1e2,     0x65cda494,     0x65c18107,     0x65af1493,\n-    0x65e52b36,     0x65ab4ed0,     0x65f06a8d,     0x0451448f,\n-    0x049c7c86,     0x0429335d,     0x04bc3162,     0x047a3027,\n-    0x04e831d1,     0x2493b8a5,     0x249d9604,     0x24d18095,\n-    0x24d7b491,     0x05686853,     0x05f16f0e,     0x041a27d5,\n-    0x0418358a,     0x04992709,     0x044838c4,     0x048a39bb,\n-    0x65c736de,     0x65863d3e,     0x65d82693,     0x044129a9,\n-\n+    0x0461943e,     0x04a19020,     0x04038100,     0x040381a0,\n+    0x040387e1,     0x04438be2,     0x04c38fe3,     0x040181e0,\n+    0x04018100,     0x04018621,     0x04418b22,     0x04418822,\n+    0x04818c23,     0x040081e0,     0x04008120,     0x04008761,\n+    0x04008621,     0x04408822,     0x04808c23,     0x042053ff,\n+    0x047f5401,     0x25208028,     0x2538cfe0,     0x2578d001,\n+    0x25b8efe2,     0x25f8f007,     0x2538dfea,     0x25b8dfeb,\n+    0xa400a3e0,     0xa420a7e0,     0xa4484be0,     0xa467afe0,\n+    0xa4a8a7ea,     0xa547a814,     0xa4084ffe,     0xa55c53e0,\n+    0xa5e1540b,     0xe400fbf6,     0xe408ffff,     0xe420e7e0,\n+    0xe4484be0,     0xe460efe0,     0xe547e400,     0xe4014be0,\n+    0xe4a84fe0,     0xe5f15000,     0x858043e0,     0x85a043ff,\n+    0xe59f5d08,     0x0420e3e9,     0x0460e3ea,     0x04a0e3eb,\n+    0x04e0e3ec,     0x25104042,     0x25104871,     0x25904861,\n+    0x25904c92,     0x05344020,     0x05744041,     0x05b44062,\n+    0x05f44083,     0x252c8840,     0x253c1420,     0x25681572,\n+    0x25a21ce3,     0x25ea1e34,     0x0522c020,     0x05e6c0a4,\n+    0x2401a001,     0x2443a051,     0x24858881,     0x24c78cd1,\n+    0x240b8142,     0x24918213,     0x250f9001,     0x25508051,\n+    0x25802491,     0x25df28c1,     0x25850c81,     0x251e10d1,\n+    0x65816001,     0x65c36051,     0x65854891,     0x65c74cc1,\n+    0x658b4152,     0x65d14203,     0x05733820,     0x05b238a4,\n+    0x05f138e6,     0x0570396a,     0x65d0a001,     0x65d6a443,\n+    0x65d4a826,     0x6594ac26,     0x6554ac26,     0x6556ac26,\n+    0x6552ac26,     0x65cbac85,     0x65caac01,     0x65dea833,\n+    0x659ca509,     0x65d8a801,     0x65dcac01,     0x655cb241,\n+    0x0520a1e0,     0x0521a601,     0x052281e0,     0x05238601,\n+    0x04a14026,     0x0568aca7,     0x05b23230,     0x853040af,\n+    0xc5b040af,     0xe57080af,     0xe5b080af,     0x25034440,\n+    0x254054c4,     0x25034640,     0x25415a05,     0x25834440,\n+    0x25c54489,     0x250b5d3a,     0x2550dc20,     0x2518e3e1,\n+    0x2558e3e2,     0x2598e3e3,     0x25d8e3e4,     0x2518e407,\n+    0x1e601000,     0x1e603000,     0x1e621000,     0x1e623000,\n+    0x1e641000,     0x1e643000,     0x1e661000,     0x1e663000,\n+    0x1e681000,     0x1e683000,     0x1e6a1000,     0x1e6a3000,\n+    0x1e6c1000,     0x1e6c3000,     0x1e6e1000,     0x1e6e3000,\n+    0x1e701000,     0x1e703000,     0x1e721000,     0x1e723000,\n+    0x1e741000,     0x1e743000,     0x1e761000,     0x1e763000,\n+    0x1e781000,     0x1e783000,     0x1e7a1000,     0x1e7a3000,\n+    0x1e7c1000,     0x1e7c3000,     0x1e7e1000,     0x1e7e3000,\n+    0xf8208193,     0xf83101b6,     0xf83c13fe,     0xf821239a,\n+    0xf824309e,     0xf826535e,     0xf8304109,     0xf82c7280,\n+    0xf8216058,     0xf8a08309,     0xf8ba03d0,     0xf8a312ea,\n+    0xf8aa21e4,     0xf8a2310b,     0xf8aa522f,     0xf8a2418a,\n+    0xf8ac71af,     0xf8a26287,     0xf8fa8090,     0xf8e20184,\n+    0xf8f01215,     0xf8f022ab,     0xf8f7334c,     0xf8f751dc,\n+    0xf8eb4038,     0xf8ec715f,     0xf8f06047,     0xf863826d,\n+    0xf8710070,     0xf86113cb,     0xf86521e8,     0xf87d301e,\n+    0xf8745287,     0xf87742bc,     0xf87b70b9,     0xf8616217,\n+    0xb83f8185,     0xb82901fc,     0xb83d13f6,     0xb83320bf,\n+    0xb82e33f0,     0xb830529b,     0xb830416c,     0xb82973c6,\n+    0xb831639b,     0xb8be8147,     0xb8b4008a,     0xb8b81231,\n+    0xb8b623a3,     0xb8af3276,     0xb8b35056,     0xb8af4186,\n+    0xb8b071ab,     0xb8b763c1,     0xb8f38225,     0xb8e202d0,\n+    0xb8ed12aa,     0xb8fd219b,     0xb8fb3023,     0xb8ff5278,\n+    0xb8f14389,     0xb8fb70ef,     0xb8f563f7,     0xb87983e2,\n+    0xb87b0150,     0xb8771073,     0xb8702320,     0xb87a3057,\n+    0xb870508c,     0xb87c43be,     0xb87070db,     0xb86961fd,\n+    0xce273c87,     0xce080ac9,     0xce7e8e9b,     0xce808b45,\n+    0xce79806e,     0xce758768,     0xcec0835a,     0xce608ad8,\n+    0x043100c4,     0x046105e3,     0x65c900a6,     0x65d60a87,\n+    0x65c80545,     0x0416a63e,     0x04001f8b,     0x045a179a,\n+    0x04d09e0d,     0x045aa1a5,     0x04990b4f,     0x049386cb,\n+    0x04918264,     0x04500dd1,     0x0497b062,     0x041ea4d7,\n+    0x04980f71,     0x04c80450,     0x048a04c3,     0x04810d93,\n+    0x04dcba68,     0x65808ae0,     0x65cd9db3,     0x658680e6,\n+    0x65879911,     0x65c296d6,     0x04dda1e2,     0x65c2a494,\n+    0x65c0a107,     0x65c1b493,     0x65cdb569,     0x65819e05,\n+    0x65ad8c36,     0x65af1334,     0x65e63104,     0x65fd5e04,\n+    0x65eb6c49,     0x049a4423,     0x04916d11,     0x043330b8,\n+    0x04b032d1,     0x04603274,     0x04e432f1,     0x2413bb21,\n+    0x249889c2,     0x241e9497,     0x2440b592,     0x05316b07,\n+    0x05e96cdb,     0x04da2e17,     0x04d83696,     0x049929bc,\n+    0x04483787,     0x048a2d6b,     0x65c73901,     0x65c6322d,\n+    0x65982064,     0x04812dc7,\n@@ -1267,1 +1309,0 @@\n-\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":177,"deletions":136,"binary":false,"changes":313,"status":"modified"}]}