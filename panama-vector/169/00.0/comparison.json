{"files":[{"patch":"@@ -1202,0 +1202,3 @@\n+reg_class p0_reg(P0);\n+reg_class p1_reg(P1);\n+\n@@ -2689,0 +2692,41 @@\n+bool can_combine_with_imm(Node* binary_node, Node* replicate_node) {\n+  if (UseSVE == 0 || !VectorNode::is_invariant_vector(replicate_node)){\n+    return false;\n+  }\n+  Node* imm_node = replicate_node->in(1);\n+  if (!imm_node->is_Con()) {\n+    return false;\n+  }\n+\n+  const Type* t = imm_node->bottom_type();\n+  if (!(t->isa_int() || t->isa_long())) {\n+    return false;\n+  }\n+\n+  switch (binary_node->Opcode()) {\n+  case Op_AndV:\n+  case Op_OrV:\n+  case Op_XorV: {\n+    Assembler::SIMD_RegVariant T = Assembler::elemType_to_regVariant(Matcher::vector_element_basic_type(binary_node));\n+    uint64_t value = t->isa_long() ? (uint64_t)imm_node->get_long() : (uint64_t)imm_node->get_int();\n+    return Assembler::operand_valid_for_sve_logical_immediate(Assembler::regVariant_to_elemBits(T), value);\n+  }\n+  case Op_AddVB:\n+    return (imm_node->get_int() <= 255 && imm_node->get_int() >= -255);\n+  case Op_AddVS:\n+  case Op_AddVI:\n+    return Assembler::operand_valid_for_sve_add_sub_immediate((int64_t)imm_node->get_int());\n+  case Op_AddVL:\n+    return Assembler::operand_valid_for_sve_add_sub_immediate(imm_node->get_long());\n+  default:\n+    return false;\n+  }\n+}\n+\n+bool is_vector_arith_imm_pattern(Node* n, Node* m) {\n+  if (n != NULL && m != NULL) {\n+    return can_combine_with_imm(n, m);\n+  }\n+  return false;\n+}\n+\n@@ -2693,0 +2737,1 @@\n+  \/\/ Binary src (Replicate con)\n@@ -2694,1 +2739,2 @@\n-      (UseSVE > 0 && m->Opcode() == Op_VectorStoreMask && n->Opcode() == Op_StoreVector)) {\n+      (UseSVE > 0 && m->Opcode() == Op_VectorStoreMask && n->Opcode() == Op_StoreVector) ||\n+      is_vector_arith_imm_pattern(n, m)) {\n@@ -3906,1 +3952,1 @@\n-               \/*release*\/ true, \/*weak*\/ false, noreg); \/\/ Sets flags for result\n+               \/*release*\/ true, \/*weak*\/ false, rscratch1); \/\/ Sets flags for result\n@@ -3915,0 +3961,9 @@\n+    __ br(Assembler::EQ, cont); \/\/ CAS success means locking succeeded\n+\n+    __ cmp(rscratch1, rthread);\n+    __ br(Assembler::NE, cont); \/\/ Check for recursive locking\n+\n+    \/\/ Recursive lock case\n+    __ increment(Address(disp_hdr, ObjectMonitor::recursions_offset_in_bytes() - markWord::monitor_value), 1);\n+    \/\/ flag == EQ still from the cmp above, checking if this is a reentrant lock\n+\n@@ -3958,3 +4013,3 @@\n-    __ eor(rscratch1, rscratch1, rthread); \/\/ Will be 0 if we are the owner.\n-    __ orr(rscratch1, rscratch1, disp_hdr); \/\/ Will be 0 if there are 0 recursions\n-    __ cmp(rscratch1, zr); \/\/ Sets flags for result\n+\n+    Label notRecursive;\n+    __ cmp(rscratch1, rthread);\n@@ -3963,0 +4018,9 @@\n+    __ cbz(disp_hdr, notRecursive);\n+\n+    \/\/ Recursive lock\n+    __ sub(disp_hdr, disp_hdr, 1u);\n+    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+    \/\/ flag == EQ was set in the ownership check above\n+    __ b(cont);\n+\n+    __ bind(notRecursive);\n@@ -4615,0 +4679,11 @@\n+\/\/ 8 bit integer valid for vector add sub immediate\n+operand immBAddSubV()\n+%{\n+  predicate(n->get_int() <= 255 && n->get_int() >= -255);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4625,0 +4700,11 @@\n+\/\/ 32 bit integer valid for vector add sub immediate\n+operand immIAddSubV()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_add_sub_immediate((int64_t)n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4626,1 +4712,21 @@\n-\/\/ TODO -- check this is right when e.g the mask is 0x80000000\n+\n+operand immBLog()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_logical_immediate(BitsPerByte, (uint64_t)n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immSLog()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_logical_immediate(BitsPerShort, (uint64_t)n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4704,0 +4810,11 @@\n+\/\/ 64 bit integer valid for addv subv immediate\n+operand immLAddSubV()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_add_sub_immediate(n->get_long()));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -5572,0 +5689,18 @@\n+operand pRegGov_P0()\n+%{\n+  constraint(ALLOC_IN_RC(p0_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand pRegGov_P1()\n+%{\n+  constraint(ALLOC_IN_RC(p1_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -16550,1 +16685,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU));\n@@ -16560,1 +16695,1 @@\n-                      fnoreg, fnoreg, fnoreg, StrIntrinsicNode::UU);\n+                      fnoreg, fnoreg, fnoreg, pnoreg, pnoreg, StrIntrinsicNode::UU);\n@@ -16568,1 +16703,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL));\n@@ -16577,1 +16712,1 @@\n-                      fnoreg, fnoreg, fnoreg, StrIntrinsicNode::LL);\n+                      fnoreg, fnoreg, fnoreg, pnoreg, pnoreg, StrIntrinsicNode::LL);\n@@ -16586,1 +16721,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL));\n@@ -16597,1 +16732,1 @@\n-                      $vtmp3$$FloatRegister, StrIntrinsicNode::UL);\n+                      $vtmp3$$FloatRegister, pnoreg, pnoreg, StrIntrinsicNode::UL);\n@@ -16606,1 +16741,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU));\n@@ -16617,1 +16752,1 @@\n-                      $vtmp3$$FloatRegister,StrIntrinsicNode::LU);\n+                      $vtmp3$$FloatRegister, pnoreg, pnoreg, StrIntrinsicNode::LU);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":149,"deletions":14,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -940,0 +940,211 @@\n+\/\/ vector add reg imm (unpredicated)\n+\n+instruct vaddImmB(vReg dst_src, immBAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVB dst_src (ReplicateB con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ B, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ B, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddImmS(vReg dst_src, immIAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVS dst_src (ReplicateS con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ H, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ H, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddImmI(vReg dst_src, immIAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVI dst_src (ReplicateI con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ S, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ S, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddImmL(vReg dst_src, immLAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVL dst_src (ReplicateL con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ D, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ D, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector binary op reg imm (unpredicated)\n+\n+instruct vandB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateB con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ B,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vandH(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateS con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ H,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vandS(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateI con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ S,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vandD(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateL con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ D,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateB con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ B,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorH(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateS con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ H,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorS(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateI con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ S,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorD(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateL con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ D,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateB con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ B,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorH(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateS con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ H,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorS(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateI con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ S,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorD(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateL con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ D,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -5345,0 +5556,99 @@\n+\/\/ Intrisics for String.compareTo()\n+\n+\/\/ Note that Z registers alias the corresponding NEON registers, we declare the vector operands of\n+\/\/ these string_compare variants as NEON register type for convenience so that the prototype of\n+\/\/ string_compare can be shared with all variants.\n+\n+\n+instruct string_compareLL_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareLU_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::LU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUL_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUU_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":310,"deletions":0,"binary":false,"changes":310,"status":"modified"},{"patch":"@@ -608,0 +608,19 @@\n+dnl\n+dnl ADD_IMM($1,          $2,   $3      )\n+dnl ADD_IMM(name_suffix, size, imm_type)\n+define(`ADD_IMM', `\n+instruct vaddImm$1(vReg dst_src, $3 con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddV$1 dst_src (Replicate$1 con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ $2, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ $2, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n@@ -609,0 +628,35 @@\n+\/\/ vector add reg imm (unpredicated)\n+ADD_IMM(B, B, immBAddSubV)\n+ADD_IMM(S, H, immIAddSubV)\n+ADD_IMM(I, S, immIAddSubV)\n+ADD_IMM(L, D, immLAddSubV)\n+dnl\n+dnl BITWISE_OP_IMM($1,        $2        $3,   $4    $5      )\n+dnl BITWISE_OP_IMM(insn_name, op_name1, size, type, op_name2)\n+define(`BITWISE_OP_IMM', `\n+instruct $1(vReg dst_src, imm$4Log con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src (Replicate$4 con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"$5 $dst_src, $dst_src, $con\\t # vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $5(as_FloatRegister($dst_src$$reg), __ $3,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+\/\/ vector binary op reg imm (unpredicated)\n+BITWISE_OP_IMM(vandB, AndV, B, B, sve_and)\n+BITWISE_OP_IMM(vandH, AndV, H, S, sve_and)\n+BITWISE_OP_IMM(vandS, AndV, S, I, sve_and)\n+BITWISE_OP_IMM(vandD, AndV, D, L, sve_and)\n+BITWISE_OP_IMM(vorB,  OrV,  B, B, sve_orr)\n+BITWISE_OP_IMM(vorH,  OrV,  H, S, sve_orr)\n+BITWISE_OP_IMM(vorS,  OrV,  S, I, sve_orr)\n+BITWISE_OP_IMM(vorD,  OrV,  D, L, sve_orr)\n+BITWISE_OP_IMM(vxorB, XorV, B, B, sve_eor)\n+BITWISE_OP_IMM(vxorH, XorV, H, S, sve_eor)\n+BITWISE_OP_IMM(vxorS, XorV, S, I, sve_eor)\n+BITWISE_OP_IMM(vxorD, XorV, D, L, sve_eor)\n+dnl\n@@ -2995,0 +3049,36 @@\n+\/\/ Intrisics for String.compareTo()\n+\n+\/\/ Note that Z registers alias the corresponding NEON registers, we declare the vector operands of\n+\/\/ these string_compare variants as NEON register type for convenience so that the prototype of\n+\/\/ string_compare can be shared with all variants.\n+\n+dnl\n+define(`STRING_COMPARETO', `\n+instruct string_compare$1_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::$1));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::$1);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}')dnl\n+dnl              $1\n+STRING_COMPARETO(LL)\n+STRING_COMPARETO(LU)\n+STRING_COMPARETO(UL)\n+STRING_COMPARETO(UU)\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":90,"deletions":0,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -162,0 +162,2 @@\n+  uint32_t encode_sve_logical_immediate(unsigned elembits, uint64_t imm);\n+  bool operand_valid_for_immediate_bits(int64_t imm, unsigned nbits);\n@@ -1519,0 +1521,2 @@\n+  \/\/ Return the corresponding bits for different SIMD_RegVariant value.\n+  static unsigned regVariant_to_elemBits(SIMD_RegVariant T);\n@@ -2956,0 +2960,26 @@\n+\/\/ SVE integer add\/subtract immediate (unpredicated)\n+#define INSN(NAME, op)                                                  \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, unsigned imm8) {       \\\n+    starti;                                                             \\\n+    \/* The immediate is an unsigned value in the range 0 to 255, and    \\\n+     * for element width of 16 bits or higher it may also be a          \\\n+     * positive multiple of 256 in the range 256 to 65280.              \\\n+     *\/                                                                 \\\n+    assert(T != Q, \"invalid size\");                                     \\\n+    int sh = 0;                                                         \\\n+    if (imm8 <= 0xff) {                                                 \\\n+      sh = 0;                                                           \\\n+    } else if (T != B && imm8 <= 0xff00 && (imm8 & 0xff) == 0) {        \\\n+      sh = 1;                                                           \\\n+      imm8 = (imm8 >> 8);                                               \\\n+    } else {                                                            \\\n+      guarantee(false, \"invalid immediate\");                            \\\n+    }                                                                   \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(0b10000, 21, 17);            \\\n+    f(op, 16, 14), f(sh, 13), f(imm8, 12, 5), rf(Zd, 0);                \\\n+  }\n+\n+  INSN(sve_add, 0b011);\n+  INSN(sve_sub, 0b111);\n+#undef INSN\n+\n@@ -3093,0 +3123,14 @@\n+\/\/ SVE bitwise logical with immediate (unpredicated)\n+#define INSN(NAME, opc)                                                      \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, uint64_t imm) {             \\\n+    starti;                                                                  \\\n+    unsigned elembits = regVariant_to_elemBits(T);                           \\\n+    uint32_t val = encode_sve_logical_immediate(elembits, imm);              \\\n+    f(0b00000101, 31, 24), f(opc, 23, 22), f(0b0000, 21, 18);                \\\n+    f(val, 17, 5), rf(Zd, 0);                                                \\\n+  }\n+  INSN(sve_and, 0b10);\n+  INSN(sve_eor, 0b01);\n+  INSN(sve_orr, 0b00);\n+#undef INSN\n+\n@@ -3650,0 +3694,1 @@\n+  static bool operand_valid_for_sve_logical_immediate(unsigned elembits, uint64_t imm);\n@@ -3651,0 +3696,1 @@\n+  static bool operand_valid_for_sve_add_sub_immediate(int64_t imm);\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -577,1 +577,6 @@\n-  \/\/ Intentional fall-through into DONE_LABEL ...\n+  jcc(Assembler::equal, DONE_LABEL);           \/\/ CAS above succeeded; propagate ZF = 1 (success)\n+\n+  cmpptr(r15_thread, rax);                     \/\/ Check if we are already the owner (recursive lock)\n+  jcc(Assembler::notEqual, DONE_LABEL);        \/\/ If not recursive, ZF = 0 at this point (fail)\n+  incq(Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  xorq(rax, rax); \/\/ Set ZF = 1 (success) for recursive lock, denoting locking success\n@@ -673,4 +678,0 @@\n-  \/\/ I'd like to add more cases in fast_lock() and fast_unlock() --\n-  \/\/ such as recursive enter and exit -- but we have to be wary of\n-  \/\/ I$ bloat, T$ effects and BP$ effects.\n-  \/\/\n@@ -724,3 +725,10 @@\n-  xorptr(boxReg, boxReg);\n-  orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-  jccb  (Assembler::notZero, DONE_LABEL);\n+  Label LNotRecursive, LSuccess, LGoSlowPath;\n+\n+  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+  jccb(Assembler::equal, LNotRecursive);\n+\n+  \/\/ Recursive inflated unlock\n+  decq(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  jmpb(LSuccess);\n+\n+  bind(LNotRecursive);\n@@ -735,1 +743,0 @@\n-  Label LSuccess, LGoSlowPath ;\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -6227,1 +6227,1 @@\n-  predicate(Matcher::vector_length(n) <= 8 && VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(Matcher::vector_length(n) <= 8 && !n->as_ShiftV()->is_var_shift());\n@@ -6247,1 +6247,1 @@\n-  predicate(Matcher::vector_length(n) == 16 && VectorNode::is_vshift_cnt(n->in(2)) &&\n+  predicate(Matcher::vector_length(n) == 16 && !n->as_ShiftV()->is_var_shift() &&\n@@ -6272,1 +6272,1 @@\n-  predicate(Matcher::vector_length(n) == 16 && VectorNode::is_vshift_cnt(n->in(2)) &&\n+  predicate(Matcher::vector_length(n) == 16 && !n->as_ShiftV()->is_var_shift() &&\n@@ -6293,1 +6293,1 @@\n-  predicate(Matcher::vector_length(n) == 32 && VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(Matcher::vector_length(n) == 32 && !n->as_ShiftV()->is_var_shift());\n@@ -6318,1 +6318,1 @@\n-  predicate(Matcher::vector_length(n) == 64 && VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(Matcher::vector_length(n) == 64 && !n->as_ShiftV()->is_var_shift());\n@@ -6351,1 +6351,1 @@\n-  predicate(VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(!n->as_ShiftV()->is_var_shift());\n@@ -6382,1 +6382,1 @@\n-  predicate(VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(!n->as_ShiftV()->is_var_shift());\n@@ -6436,1 +6436,1 @@\n-  predicate(VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(!n->as_ShiftV()->is_var_shift());\n@@ -6477,1 +6477,1 @@\n-  predicate(VectorNode::is_vshift_cnt(n->in(2)) && UseAVX <= 2);\n+  predicate(!n->as_ShiftV()->is_var_shift() && UseAVX <= 2);\n@@ -6506,1 +6506,1 @@\n-  predicate(VectorNode::is_vshift_cnt(n->in(2)) && UseAVX > 2);\n+  predicate(!n->as_ShiftV()->is_var_shift() && UseAVX > 2);\n@@ -6520,1 +6520,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6540,1 +6540,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6568,1 +6568,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6604,1 +6604,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6623,1 +6623,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6647,1 +6647,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6672,1 +6672,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6707,1 +6707,1 @@\n-  predicate(!VectorNode::is_vshift_cnt(n->in(2)) &&\n+  predicate(n->as_ShiftV()->is_var_shift() &&\n@@ -6728,1 +6728,1 @@\n-  predicate(!VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(n->as_ShiftV()->is_var_shift());\n@@ -6745,1 +6745,1 @@\n-  predicate(!VectorNode::is_vshift_cnt(n->in(2)));\n+  predicate(n->as_ShiftV()->is_var_shift());\n@@ -6762,1 +6762,1 @@\n-            !VectorNode::is_vshift_cnt(n->in(2)) &&\n+            n->as_ShiftV()->is_var_shift() &&\n@@ -6777,1 +6777,1 @@\n-  predicate(!VectorNode::is_vshift_cnt(n->in(2)) &&\n+  predicate(n->as_ShiftV()->is_var_shift() &&\n@@ -9092,0 +9092,1 @@\n+  predicate(!n->as_ShiftV()->is_var_shift());\n@@ -9100,2 +9101,17 @@\n-    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n-                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, false);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshiftv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(n->as_ShiftV()->is_var_shift());\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshiftv_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, true);\n@@ -9138,0 +9154,1 @@\n+  predicate(!n->as_ShiftV()->is_var_shift());\n@@ -9146,2 +9163,17 @@\n-    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n-                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, false);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshiftv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(n->as_ShiftV()->is_var_shift());\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshiftv_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, true);\n@@ -9184,0 +9216,1 @@\n+  predicate(!n->as_ShiftV()->is_var_shift());\n@@ -9192,2 +9225,17 @@\n-    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n-                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, false);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshiftv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(n->as_ShiftV()->is_var_shift());\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshiftv_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, true);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":76,"deletions":28,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -178,0 +178,1 @@\n+class ShiftVNode;\n@@ -717,3 +718,4 @@\n-        DEFINE_CLASS_ID(CompressV, Vector, 3)\n-        DEFINE_CLASS_ID(ExpandV, Vector, 4)\n-        DEFINE_CLASS_ID(CompressM, Vector, 5)\n+        DEFINE_CLASS_ID(ShiftV, Vector, 3)\n+        DEFINE_CLASS_ID(CompressV, Vector, 4)\n+        DEFINE_CLASS_ID(ExpandV, Vector, 5)\n+        DEFINE_CLASS_ID(CompressM, Vector, 6)\n@@ -949,4 +951,4 @@\n-  DEFINE_CLASS_QUERY(VectorReinterpret);\n-  DEFINE_CLASS_QUERY(CompressV);\n-  DEFINE_CLASS_QUERY(ExpandV);\n-  DEFINE_CLASS_QUERY(CompressM);\n+  DEFINE_CLASS_QUERY(VectorReinterpret)\n+  DEFINE_CLASS_QUERY(CompressV)\n+  DEFINE_CLASS_QUERY(ExpandV)\n+  DEFINE_CLASS_QUERY(CompressM)\n@@ -957,0 +959,1 @@\n+  DEFINE_CLASS_QUERY(ShiftV)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -532,1 +532,1 @@\n-        operation = VectorNode::make(sopc, opd1, opd2, vt, is_vector_mask(vbox_klass));\n+        operation = VectorNode::make(sopc, opd1, opd2, vt, is_vector_mask(vbox_klass), VectorNode::is_shift_opcode(opc));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -227,0 +227,8 @@\n+  case Op_ConvI2F:\n+    return Op_VectorCastI2X;\n+  case Op_ConvL2D:\n+    return Op_VectorCastL2X;\n+  case Op_ConvF2I:\n+    return Op_VectorCastF2X;\n+  case Op_ConvD2L:\n+    return Op_VectorCastD2X;\n@@ -475,1 +483,1 @@\n-VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask) {\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask, bool is_var_shift) {\n@@ -529,4 +537,4 @@\n-  case Op_LShiftVB: return new LShiftVBNode(n1, n2, vt);\n-  case Op_LShiftVS: return new LShiftVSNode(n1, n2, vt);\n-  case Op_LShiftVI: return new LShiftVINode(n1, n2, vt);\n-  case Op_LShiftVL: return new LShiftVLNode(n1, n2, vt);\n+  case Op_LShiftVB: return new LShiftVBNode(n1, n2, vt, is_var_shift);\n+  case Op_LShiftVS: return new LShiftVSNode(n1, n2, vt, is_var_shift);\n+  case Op_LShiftVI: return new LShiftVINode(n1, n2, vt, is_var_shift);\n+  case Op_LShiftVL: return new LShiftVLNode(n1, n2, vt, is_var_shift);\n@@ -534,4 +542,4 @@\n-  case Op_RShiftVB: return new RShiftVBNode(n1, n2, vt);\n-  case Op_RShiftVS: return new RShiftVSNode(n1, n2, vt);\n-  case Op_RShiftVI: return new RShiftVINode(n1, n2, vt);\n-  case Op_RShiftVL: return new RShiftVLNode(n1, n2, vt);\n+  case Op_RShiftVB: return new RShiftVBNode(n1, n2, vt, is_var_shift);\n+  case Op_RShiftVS: return new RShiftVSNode(n1, n2, vt, is_var_shift);\n+  case Op_RShiftVI: return new RShiftVINode(n1, n2, vt, is_var_shift);\n+  case Op_RShiftVL: return new RShiftVLNode(n1, n2, vt, is_var_shift);\n@@ -539,4 +547,4 @@\n-  case Op_URShiftVB: return new URShiftVBNode(n1, n2, vt);\n-  case Op_URShiftVS: return new URShiftVSNode(n1, n2, vt);\n-  case Op_URShiftVI: return new URShiftVINode(n1, n2, vt);\n-  case Op_URShiftVL: return new URShiftVLNode(n1, n2, vt);\n+  case Op_URShiftVB: return new URShiftVBNode(n1, n2, vt, is_var_shift);\n+  case Op_URShiftVS: return new URShiftVSNode(n1, n2, vt, is_var_shift);\n+  case Op_URShiftVI: return new URShiftVINode(n1, n2, vt, is_var_shift);\n+  case Op_URShiftVL: return new URShiftVLNode(n1, n2, vt, is_var_shift);\n@@ -562,1 +570,1 @@\n-VectorNode* VectorNode::make(int opc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n+VectorNode* VectorNode::make(int opc, Node* n1, Node* n2, uint vlen, BasicType bt, bool is_var_shift) {\n@@ -567,1 +575,1 @@\n-  return make(vopc, n1, n2, vt);\n+  return make(vopc, n1, n2, vt, false, is_var_shift);\n@@ -1296,2 +1304,2 @@\n-  return new OrVNode(phase->transform(VectorNode::make(shiftLOpc, src, shiftLCnt, vlen, bt)),\n-                     phase->transform(VectorNode::make(shiftROpc, src, shiftRCnt, vlen, bt)),\n+  return new OrVNode(phase->transform(VectorNode::make(shiftLOpc, src, shiftLCnt, vlen, bt, is_binary_vector_op)),\n+                     phase->transform(VectorNode::make(shiftROpc, src, shiftRCnt, vlen, bt, is_binary_vector_op)),\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":25,"deletions":17,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -75,2 +75,2 @@\n-  static VectorNode* make(int opc, Node* n1, Node* n2, uint vlen, BasicType bt);\n-  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask = false);\n+  static VectorNode* make(int opc, Node* n1, Node* n2, uint vlen, BasicType bt, bool is_var_shift = false);\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask = false, bool is_var_shift = false);\n@@ -534,0 +534,1 @@\n+ bool _is_var_shift;\n@@ -535,1 +536,4 @@\n-  ShiftVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1,in2,vt) {}\n+  ShiftVNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift) :\n+    VectorNode(in1,in2,vt), _is_var_shift(is_var_shift) {\n+    init_class_id(Class_ShiftV);\n+  }\n@@ -538,0 +542,2 @@\n+  bool is_var_shift() { return _is_var_shift;}\n+  virtual  uint  size_of() const { return sizeof(ShiftVNode); }\n@@ -544,1 +550,2 @@\n-  LShiftVBNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  LShiftVBNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -552,1 +559,2 @@\n-  LShiftVSNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  LShiftVSNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -560,1 +568,2 @@\n-  LShiftVINode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  LShiftVINode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -568,1 +577,2 @@\n-  LShiftVLNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  LShiftVLNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -576,1 +586,2 @@\n-  RShiftVBNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  RShiftVBNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -584,1 +595,2 @@\n-  RShiftVSNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  RShiftVSNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -592,1 +604,2 @@\n-  RShiftVINode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  RShiftVINode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -600,1 +613,2 @@\n-  RShiftVLNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  RShiftVLNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -608,1 +622,2 @@\n-  URShiftVBNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  URShiftVBNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -616,1 +631,2 @@\n-  URShiftVSNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  URShiftVSNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -624,1 +640,2 @@\n-  URShiftVINode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  URShiftVINode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+    ShiftVNode(in1,in2,vt,is_var_shift) {}\n@@ -632,1 +649,2 @@\n-  URShiftVLNode(Node* in1, Node* in2, const TypeVect* vt) : ShiftVNode(in1,in2,vt) {}\n+  URShiftVLNode(Node* in1, Node* in2, const TypeVect* vt, bool is_var_shift=false) :\n+     ShiftVNode(in1,in2,vt,is_var_shift) {}\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":33,"deletions":15,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+        case T_LONG:   return Op_NegL;\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -10,0 +10,49 @@\n+# These tables are legal immediate logical operands\n+immediates8 \\\n+     = [0x1, 0x0c, 0x3e, 0x60, 0x7c, 0x80, 0x83,\n+        0xe1, 0xbf, 0xef, 0xf3, 0xfe]\n+\n+immediates16 \\\n+     = [0x1, 0x38, 0x7e, 0xff, 0x1fc, 0x1ff, 0x3f0,\n+        0x7e0, 0xfc0, 0x1f80, 0x3ff0, 0x7e00, 0x7e00,\n+        0x8000, 0x81ff, 0xc1ff, 0xc003, 0xc7ff, 0xdfff,\n+        0xe03f, 0xe10f, 0xe1ff, 0xf801, 0xfc00, 0xfc07,\n+        0xff03, 0xfffe]\n+\n+immediates32 \\\n+     = [0x1, 0x3f, 0x1f0, 0x7e0,\n+        0x1c00, 0x3ff0, 0x8000, 0x1e000,\n+        0x3e000, 0x78000, 0xe0000, 0x100000,\n+        0x1fffe0, 0x3fe000, 0x780000, 0x7ffff8,\n+        0xff8000, 0x1800180, 0x1fffc00, 0x3c003c0,\n+        0x3ffff00, 0x7c00000, 0x7fffe00, 0xf000f00,\n+        0xfffe000, 0x18181818, 0x1ffc0000, 0x1ffffffe,\n+        0x3f003f00, 0x3fffe000, 0x60006000, 0x7f807f80,\n+        0x7ffffc00, 0x800001ff, 0x803fffff, 0x9f9f9f9f,\n+        0xc0000fff, 0xc0c0c0c0, 0xe0000000, 0xe003e003,\n+        0xe3ffffff, 0xf0000fff, 0xf0f0f0f0, 0xf80000ff,\n+        0xf83ff83f, 0xfc00007f, 0xfc1fffff, 0xfe0001ff,\n+        0xfe3fffff, 0xff003fff, 0xff800003, 0xff87ff87,\n+        0xffc00fff, 0xffe0000f, 0xffefffef, 0xfff1fff1,\n+        0xfff83fff, 0xfffc0fff, 0xfffe0fff, 0xffff3fff,\n+        0xffffc007, 0xffffe1ff, 0xfffff80f, 0xfffffe07,\n+        0xffffffbf, 0xfffffffd]\n+\n+immediates64 \\\n+     = [0x1, 0x1f80, 0x3fff0, 0x3ffffc,\n+        0x3fe0000, 0x1ffc0000, 0xf8000000, 0x3ffffc000,\n+        0xffffffe00, 0x3ffffff800, 0xffffc00000, 0x3f000000000,\n+        0x7fffffff800, 0x1fe000001fe0, 0x3ffffff80000, 0xc00000000000,\n+        0x1ffc000000000, 0x3ffff0003ffff, 0x7ffffffe00000, 0xfffffffffc000,\n+        0x1ffffffffffc00, 0x3fffffffffff00, 0x7ffffffffffc00, 0xffffffffff8000,\n+        0x1ffffffff800000, 0x3fffffc03fffffc, 0x7fffc0000000000, 0xff80ff80ff80ff8,\n+        0x1c00000000000000, 0x1fffffffffff0000, 0x3fffff803fffff80, 0x7fc000007fc00000,\n+        0x8000000000000000, 0x803fffff803fffff, 0xc000007fc000007f, 0xe00000000000ffff,\n+        0xe3ffffffffffffff, 0xf007f007f007f007, 0xf80003ffffffffff, 0xfc000003fc000003,\n+        0xfe000000007fffff, 0xff00000000007fff, 0xff800000000003ff, 0xffc00000000000ff,\n+        0xffe00000000003ff, 0xfff0000000003fff, 0xfff80000001fffff, 0xfffc0000fffc0000,\n+        0xfffe003fffffffff, 0xffff3fffffffffff, 0xffffc0000007ffff, 0xffffe01fffffe01f,\n+        0xfffff800000007ff, 0xfffffc0fffffffff, 0xffffff00003fffff, 0xffffffc0000007ff,\n+        0xfffffff0000001ff, 0xfffffffc00003fff, 0xffffffff07ffffff, 0xffffffffe003ffff,\n+        0xfffffffffc01ffff, 0xffffffffffc00003, 0xfffffffffffc000f, 0xffffffffffffe07f]\n+\n@@ -354,39 +403,0 @@\n-\n-     # These tables are legal immediate logical operands\n-     immediates32 \\\n-         = [0x1, 0x3f, 0x1f0, 0x7e0,\n-            0x1c00, 0x3ff0, 0x8000, 0x1e000,\n-            0x3e000, 0x78000, 0xe0000, 0x100000,\n-            0x1fffe0, 0x3fe000, 0x780000, 0x7ffff8,\n-            0xff8000, 0x1800180, 0x1fffc00, 0x3c003c0,\n-            0x3ffff00, 0x7c00000, 0x7fffe00, 0xf000f00,\n-            0xfffe000, 0x18181818, 0x1ffc0000, 0x1ffffffe,\n-            0x3f003f00, 0x3fffe000, 0x60006000, 0x7f807f80,\n-            0x7ffffc00, 0x800001ff, 0x803fffff, 0x9f9f9f9f,\n-            0xc0000fff, 0xc0c0c0c0, 0xe0000000, 0xe003e003,\n-            0xe3ffffff, 0xf0000fff, 0xf0f0f0f0, 0xf80000ff,\n-            0xf83ff83f, 0xfc00007f, 0xfc1fffff, 0xfe0001ff,\n-            0xfe3fffff, 0xff003fff, 0xff800003, 0xff87ff87,\n-            0xffc00fff, 0xffe0000f, 0xffefffef, 0xfff1fff1,\n-            0xfff83fff, 0xfffc0fff, 0xfffe0fff, 0xffff3fff,\n-            0xffffc007, 0xffffe1ff, 0xfffff80f, 0xfffffe07,\n-            0xffffffbf, 0xfffffffd]\n-\n-     immediates \\\n-         = [0x1, 0x1f80, 0x3fff0, 0x3ffffc,\n-            0x3fe0000, 0x1ffc0000, 0xf8000000, 0x3ffffc000,\n-            0xffffffe00, 0x3ffffff800, 0xffffc00000, 0x3f000000000,\n-            0x7fffffff800, 0x1fe000001fe0, 0x3ffffff80000, 0xc00000000000,\n-            0x1ffc000000000, 0x3ffff0003ffff, 0x7ffffffe00000, 0xfffffffffc000,\n-            0x1ffffffffffc00, 0x3fffffffffff00, 0x7ffffffffffc00, 0xffffffffff8000,\n-            0x1ffffffff800000, 0x3fffffc03fffffc, 0x7fffc0000000000, 0xff80ff80ff80ff8,\n-            0x1c00000000000000, 0x1fffffffffff0000, 0x3fffff803fffff80, 0x7fc000007fc00000,\n-            0x8000000000000000, 0x803fffff803fffff, 0xc000007fc000007f, 0xe00000000000ffff,\n-            0xe3ffffffffffffff, 0xf007f007f007f007, 0xf80003ffffffffff, 0xfc000003fc000003,\n-            0xfe000000007fffff, 0xff00000000007fff, 0xff800000000003ff, 0xffc00000000000ff,\n-            0xffe00000000003ff, 0xfff0000000003fff, 0xfff80000001fffff, 0xfffc0000fffc0000,\n-            0xfffe003fffffffff, 0xffff3fffffffffff, 0xffffc0000007ffff, 0xffffe01fffffe01f,\n-            0xfffff800000007ff, 0xfffffc0fffffffff, 0xffffff00003fffff, 0xffffffc0000007ff,\n-            0xfffffff0000001ff, 0xfffffffc00003fff, 0xffffffff07ffffff, 0xffffffffe003ffff,\n-            0xfffffffffc01ffff, 0xffffffffffc00003, 0xfffffffffffc000f, 0xffffffffffffe07f]\n-\n@@ -396,1 +406,1 @@\n-              self.immediates32[random.randint(0, len(self.immediates32)-1)] \\\n+              immediates32[random.randint(0, len(immediates32)-1)] \\\n@@ -398,1 +408,1 @@\n-              self.immediates[random.randint(0, len(self.immediates)-1)]\n+              immediates64[random.randint(0, len(immediates64)-1)]\n@@ -409,0 +419,38 @@\n+class SVEBinaryImmOp(Instruction):\n+    def __init__(self, name):\n+        reg = SVEVectorRegister().generate()\n+        self.reg = [reg, reg]\n+        self.numRegs = len(self.reg)\n+        self._width = RegVariant(0, 3)\n+        self._isLogical = False\n+        if name in [\"and\", \"eor\", \"orr\"]:\n+            self._isLogical = True\n+        Instruction.__init__(self, name)\n+\n+    def generate(self):\n+        Instruction.generate(self)\n+        self.immed = random.randint(0, (1<<8)-1)\n+        if self._isLogical:\n+            vectype = self._width.cstr()\n+            if vectype == \"__ B\":\n+                self.immed = immediates8[random.randint(0, len(immediates8)-1)]\n+            elif vectype == \"__ H\":\n+                self.immed = immediates16[random.randint(0, len(immediates16)-1)]\n+            elif vectype == \"__ S\":\n+                self.immed = immediates32[random.randint(0, len(immediates32)-1)]\n+            elif vectype == \"__ D\":\n+                self.immed = immediates64[random.randint(0, len(immediates64)-1)]\n+        return self\n+\n+    def cstr(self):\n+        formatStr = \"%s%s, %s, %su);\"\n+        return (formatStr\n+                % tuple([\"__ sve_\" + self._name + \"(\"] +\n+                        [str(self.reg[0]), self._width.cstr(), self.immed]))\n+\n+    def astr(self):\n+        formatStr = \"%s%s, %s, #0x%x\"\n+        Regs = [str(self.reg[i]) + self._width.astr() for i in range(0, self.numRegs)]\n+        return (formatStr\n+                % tuple([Instruction.astr(self)] + Regs + [self.immed]))\n+\n@@ -1724,0 +1772,3 @@\n+for i in range(6):\n+    generate(SVEBinaryImmOp, [\"add\", \"sub\", \"and\", \"eor\", \"orr\"])\n+\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":92,"deletions":41,"binary":false,"changes":133,"status":"modified"}]}