{"files":[{"patch":"@@ -1905,1 +1905,1 @@\n-  if (C->max_vector_size() >= 16) {\n+  if (C->max_vector_size() > 0) {\n@@ -2391,1 +2391,1 @@\n-  if (!match_rule_supported(opcode) || !vector_size_supported(bt, vlen)) {\n+  if (!match_rule_supported(opcode)) {\n@@ -2399,1 +2399,1 @@\n-    return op_sve_supported(opcode);\n+    return op_sve_supported(opcode, vlen, bt);\n@@ -2416,0 +2416,3 @@\n+    case Op_LoadVectorGather:\n+    case Op_StoreVectorScatter:\n+      return false;\n@@ -2420,1 +2423,1 @@\n-  return true; \/\/ Per default match rules are supported.\n+  return vector_size_supported(bt, vlen);\n@@ -2470,0 +2473,1 @@\n+\n@@ -2472,15 +2476,8 @@\n-  if ((UseSVE > 0) && (MaxVectorSize >= 16)) {\n-    \/\/ Currently vector length less than SVE vector register size is not supported.\n-    return max_size;\n-  } else { \/\/ NEON\n-    \/\/ Limit the vector size to 8 bytes\n-    int size = 8 \/ type2aelembytes(bt);\n-    if (bt == T_BYTE) {\n-      \/\/ To support vector api shuffle\/rearrange.\n-      size = 4;\n-    } else if (bt == T_BOOLEAN) {\n-      \/\/ To support vector api load\/store mask.\n-      size = 2;\n-    }\n-    if (size < 2) size = 2;\n-    return MIN2(size,max_size);\n+  \/\/ Limit the min vector size to 8 bytes.\n+  int size = 8 \/ type2aelembytes(bt);\n+  if (bt == T_BYTE) {\n+    \/\/ To support vector api shuffle\/rearrange.\n+    size = 4;\n+  } else if (bt == T_BOOLEAN) {\n+    \/\/ To support vector api load\/store mask.\n+    size = 2;\n@@ -2488,0 +2485,2 @@\n+  if (size < 2) size = 2;\n+  return MIN2(size, max_size);\n@@ -2497,1 +2496,1 @@\n-  if (UseSVE > 0 && 16 <= len && len <= 256) {\n+  if (UseSVE > 0 && 2 <= len && len <= 256) {\n@@ -3662,1 +3661,1 @@\n-    if (Compile::current()->max_vector_size() >= 16 && uncommon_trap_request() == 0) {\n+    if (Compile::current()->max_vector_size() > 0 && uncommon_trap_request() == 0) {\n@@ -3674,1 +3673,1 @@\n-    } else if (Compile::current()->max_vector_size() >= 16) {\n+    } else if (Compile::current()->max_vector_size() > 0) {\n@@ -3712,1 +3711,1 @@\n-    if (Compile::current()->max_vector_size() >= 16) {\n+    if (Compile::current()->max_vector_size() > 0) {\n@@ -3725,1 +3724,1 @@\n-    if (Compile::current()->max_vector_size() >= 16) {\n+    if (Compile::current()->max_vector_size() > 0) {\n@@ -4106,0 +4105,10 @@\n+operand immI_gt_1()\n+%{\n+  predicate(n->get_int() > 1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":33,"deletions":24,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 2);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 2);\n@@ -47,1 +47,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 4);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 4);\n@@ -58,1 +58,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 8);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 8);\n@@ -2476,3 +2476,4 @@\n-  predicate((n->as_Vector()->length() == 2 || n->as_Vector()->length() == 4 ||\n-             n->as_Vector()->length() == 8) &&\n-             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  predicate(UseSVE == 0 &&\n+           (n->as_Vector()->length() == 2 || n->as_Vector()->length() == 4 ||\n+            n->as_Vector()->length() == 8) &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n@@ -2491,1 +2492,1 @@\n-  predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  predicate(UseSVE == 0 && n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n@@ -2948,2 +2949,2 @@\n-  predicate(n->as_Vector()->length() == 4 ||\n-            n->as_Vector()->length() == 8);\n+  predicate(UseSVE == 0 && (n->as_Vector()->length() == 8 ||\n+                            n->as_Vector()->length() == 4));\n@@ -2973,2 +2974,2 @@\n-  predicate(n->as_Vector()->length() == 4 ||\n-            n->as_Vector()->length() == 8);\n+  predicate(UseSVE == 0 && (n->as_Vector()->length() == 8 ||\n+                            n->as_Vector()->length() == 4));\n@@ -2998,2 +2999,2 @@\n-  predicate(n->as_Vector()->length() == 2 ||\n-            n->as_Vector()->length() == 4);\n+  predicate(UseSVE == 0 && (n->as_Vector()->length() == 4 ||\n+                            n->as_Vector()->length() == 2));\n@@ -3023,2 +3024,2 @@\n-  predicate(n->as_Vector()->length() == 2 ||\n-            n->as_Vector()->length() == 4);\n+  predicate(UseSVE == 0 && (n->as_Vector()->length() == 4 ||\n+                            n->as_Vector()->length() == 2));\n@@ -3048,1 +3049,1 @@\n-  predicate(n->as_Vector()->length() == 2);\n+  predicate(UseSVE == 0 && n->as_Vector()->length() == 2);\n@@ -3072,1 +3073,1 @@\n-  predicate(n->as_Vector()->length() == 2);\n+  predicate(UseSVE == 0 && n->as_Vector()->length() == 2);\n@@ -3122,1 +3123,1 @@\n-  predicate(n->as_Vector()->length() == 2);\n+  predicate(UseSVE == 0 && n->as_Vector()->length() == 2);\n@@ -4252,2 +4253,2 @@\n-  predicate(n->as_Vector()->length_in_bytes() == 4 ||\n-            n->as_Vector()->length_in_bytes() == 8);\n+  predicate(UseSVE == 0 && (n->as_Vector()->length_in_bytes() == 4 ||\n+            n->as_Vector()->length_in_bytes() == 8));\n@@ -4264,1 +4265,1 @@\n-  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  predicate(UseSVE == 0 && (n->as_Vector()->length_in_bytes() == 16));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":22,"deletions":21,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -72,3 +72,3 @@\n-VLoadStore(ldrh, H, load,  2,  D, 16,  dst, )\n-VLoadStore(ldrs, S, load,  4,  D, 32,  dst, )\n-VLoadStore(ldrd, D, load,  8,  D, 64,  dst, )\n+VLoadStore(ldrh, H, load,  2,  D, 16,  dst, UseSVE == 0 && )\n+VLoadStore(ldrs, S, load,  4,  D, 32,  dst, UseSVE == 0 && )\n+VLoadStore(ldrd, D, load,  8,  D, 64,  dst, UseSVE == 0 && )\n@@ -1199,4 +1199,5 @@\n-`predicate((n->as_Vector()->length() == 2 || n->as_Vector()->length() == 4 ||\n-             n->as_Vector()->length() == 8) &&\n-             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);',\n-`predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);')')dnl\n+`predicate(UseSVE == 0 &&\n+           (n->as_Vector()->length() == 2 || n->as_Vector()->length() == 4 ||\n+            n->as_Vector()->length() == 8) &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);',\n+`predicate(UseSVE == 0 && n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);')')dnl\n@@ -1469,3 +1470,4 @@\n-  predicate(ifelse($8, UseSVE == 0 && , $8,\n-                   $8, , , $8`\n-            ')n->as_Vector()->length() == $3);\n+  predicate(UseSVE == 0 && ifelse($8, `',\n+                                  n->as_Vector()->length() == $3,\n+                                  (n->as_Vector()->length() == $3 ||`\n+                            'n->as_Vector()->length() == $8)));\n@@ -1497,18 +1499,18 @@\n-dnl        $1    $2    $3  $4 $5     $6 $7          $8                                $9\n-VREPLICATE(dup,  dup,  8,  B, ,      D, iRegIorL2I, n->as_Vector()->length() == 4 ||, B)\n-VREPLICATE(dup,  dup,  16, B, ,      X, iRegIorL2I, UseSVE == 0 && ,                  B)\n-VREPLICATE(movi, mov,  8,  B, _imm,  D, immI,       n->as_Vector()->length() == 4 ||, B)\n-VREPLICATE(movi, mov,  16, B, _imm,  X, immI,       UseSVE == 0 && ,                  B)\n-VREPLICATE(dup,  dup,  4,  S, ,      D, iRegIorL2I, n->as_Vector()->length() == 2 ||, H)\n-VREPLICATE(dup,  dup,  8,  S, ,      X, iRegIorL2I, UseSVE == 0 && ,                  H)\n-VREPLICATE(movi, mov,  4,  S, _imm,  D, immI,       n->as_Vector()->length() == 2 ||, H)\n-VREPLICATE(movi, mov,  8,  S,  _imm, X, immI,       UseSVE == 0 && ,                  H)\n-VREPLICATE(dup,  dup,  2,  I, ,      D, iRegIorL2I, ,                                 S)\n-VREPLICATE(dup,  dup,  4,  I, ,      X, iRegIorL2I, UseSVE == 0 && ,                  S)\n-VREPLICATE(movi, mov,  2,  I, _imm,  D, immI,       ,                                 S)\n-VREPLICATE(movi, mov,  4,  I,  _imm, X, immI,       UseSVE == 0 && ,                  S)\n-VREPLICATE(dup,  dup,  2,  L, ,      X, iRegL,      UseSVE == 0 && ,                  D)\n-VREPLICATE(movi, eor,  2,  L, _zero, X, immI0,      UseSVE == 0 && ,                  D)\n-VREPLICATE(dup,  dup,  2,  F, ,      D, vRegF,      ,                                 S)\n-VREPLICATE(dup,  dup,  4,  F, ,      X, vRegF,      UseSVE == 0 && ,                  S)\n-VREPLICATE(dup,  dup,  2,  D, ,      X, vRegD,      UseSVE == 0 && ,                  D)\n+dnl        $1    $2    $3  $4 $5     $6 $7          $8 $9\n+VREPLICATE(dup,  dup,  8,  B, ,      D, iRegIorL2I, 4, B)\n+VREPLICATE(dup,  dup,  16, B, ,      X, iRegIorL2I,  , B)\n+VREPLICATE(movi, mov,  8,  B, _imm,  D, immI,       4, B)\n+VREPLICATE(movi, mov,  16, B, _imm,  X, immI,        , B)\n+VREPLICATE(dup,  dup,  4,  S, ,      D, iRegIorL2I, 2, H)\n+VREPLICATE(dup,  dup,  8,  S, ,      X, iRegIorL2I,  , H)\n+VREPLICATE(movi, mov,  4,  S, _imm,  D, immI,       2, H)\n+VREPLICATE(movi, mov,  8,  S,  _imm, X, immI,        , H)\n+VREPLICATE(dup,  dup,  2,  I, ,      D, iRegIorL2I, ,  S)\n+VREPLICATE(dup,  dup,  4,  I, ,      X, iRegIorL2I, ,  S)\n+VREPLICATE(movi, mov,  2,  I, _imm,  D, immI,       ,  S)\n+VREPLICATE(movi, mov,  4,  I,  _imm, X, immI,       ,  S)\n+VREPLICATE(dup,  dup,  2,  L, ,      X, iRegL,      ,  D)\n+VREPLICATE(movi, eor,  2,  L, _zero, X, immI0,      ,  D)\n+VREPLICATE(dup,  dup,  2,  F, ,      D, vRegF,      ,  S)\n+VREPLICATE(dup,  dup,  4,  F, ,      X, vRegF,      ,  S)\n+VREPLICATE(dup,  dup,  2,  D, ,      X, vRegD,      ,  D)\n@@ -1887,2 +1889,2 @@\n-  predicate(ifelse($3, 8, n->as_Vector()->length_in_bytes() == 4 ||`\n-            ')n->as_Vector()->length_in_bytes() == $3);\n+  predicate(UseSVE == 0 && (ifelse($3, 8, n->as_Vector()->length_in_bytes() == 4 ||`\n+            ')n->as_Vector()->length_in_bytes() == $3));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":32,"deletions":30,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -3172,0 +3172,14 @@\n+\/\/ SVE load gather, store scatter (scalar plus vector) - 32-bit scaled offset\n+#define INSN(NAME, op1, type, op2, op3)                                         \\\n+  void NAME(FloatRegister Zt, PRegister Pg, Register Xn, FloatRegister Zm) {    \\\n+    starti;                                                                     \\\n+    f(op1, 31, 25), f(type, 24, 23), f(op2, 22, 21), rf(Zm, 16);                \\\n+    f(op3, 15, 13), pgrf(Pg, 10), srf(Xn, 5), rf(Zt, 0);                        \\\n+  }\n+\n+  INSN(sve_ld1w_gather,  0b1000010, 0b10, 0b01, 0b010);\n+  INSN(sve_ld1d_gather,  0b1100010, 0b11, 0b01, 0b010);\n+  INSN(sve_st1w_scatter, 0b1110010, 0b10, 0b11, 0b100);\n+  INSN(sve_st1d_scatter, 0b1110010, 0b11, 0b01, 0b100);\n+#undef INSN\n+\n@@ -3248,0 +3262,233 @@\n+  \/\/ SVE cpy general-purpose register\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, Register Rn) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b101000101, 21, 13);\n+    pgrf(Pg, 10), srf(Rn, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE cpy immediate\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8, bool isMerge) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    int sh = 0;\n+    if (imm8 <= 127 && imm8 >= -128) {\n+      sh = 0;\n+    } else if (T != B && imm8 <= 32512 && imm8 >= -32768 && (imm8 & 0xff) == 0) {\n+      sh = 1;\n+      imm8 = (imm8 >> 8);\n+    } else {\n+      guarantee(false, \"invalid immediate\");\n+    }\n+    int m = isMerge ? 1 : 0;\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b01, 21, 20);\n+    prf(Pg, 16), f(0b0, 15), f(m, 14), f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE sel (vectors)\n+  void sve_sel(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg,\n+               FloatRegister Zn, FloatRegister Zm) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);\n+    f(0b11, 15, 14), prf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+\/\/ SVE compare vectors\n+#define INSN(NAME, op, cond, fp)  \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pg, FloatRegister Zn, FloatRegister Zm)  { \\\n+    starti;                                                                                       \\\n+    if (fp == 0) {                                                                                \\\n+      assert(T != Q, \"invalid size\");                                                             \\\n+    } else {                                                                                      \\\n+      assert(T != B && T != Q, \"invalid size\");                                                   \\\n+    }                                                                                             \\\n+    f(op, 31, 24), f(T, 23, 22), f(0b0, 21), rf(Zm, 16), f((cond >> 1) & 0x7, 15, 13);            \\\n+    pgrf(Pg, 10), rf(Zn, 5), f(cond & 0x1, 4), prf(Pd, 0);                                        \\\n+  }\n+\n+  INSN(sve_cmpeq, 0b00100100, 0b1010, 0);\n+  INSN(sve_cmpne, 0b00100100, 0b1011, 0);\n+  INSN(sve_cmpge, 0b00100100, 0b1000, 0);\n+  INSN(sve_cmpgt, 0b00100100, 0b1001, 0);\n+  INSN(sve_fcmeq, 0b01100101, 0b0110, 1);\n+  INSN(sve_fcmne, 0b01100101, 0b0111, 1);\n+  INSN(sve_fcmgt, 0b01100101, 0b0101, 1);\n+  INSN(sve_fcmge, 0b01100101, 0b0100, 1);\n+#undef INSN\n+\n+\/\/ SVE compare vector with immediate\n+#define INSN(NAME, cond)  \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pg, FloatRegister Zn, int imm5) { \\\n+    starti;                                                                              \\\n+    assert(T != Q, \"invalid size\");                                                      \\\n+    guarantee(-16 <= imm5 && imm5 <= 15, \"invalid immediate\");                           \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(0b0, 21), sf(imm5, 20, 16),                   \\\n+    f((cond >> 1) & 0x7, 15, 13), pgrf(Pg, 10), rf(Zn, 5), f(cond & 0x1, 4), prf(Pd, 0); \\\n+  }\n+\n+  INSN(sve_cmpeq, 0b1000);\n+  INSN(sve_cmpne, 0b1001);\n+  INSN(sve_cmpgt, 0b0001);\n+  INSN(sve_cmpge, 0b0000);\n+  INSN(sve_cmplt, 0b0010);\n+  INSN(sve_cmple, 0b0011);\n+#undef INSN\n+\n+\/\/ SVE unpack and extend\n+#define INSN(NAME, op) \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn) { \\\n+    starti;                                                          \\\n+    assert(T != B && T != Q, \"invalid size\");                        \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1100, 21, 18);          \\\n+    f(op, 17, 16), f(0b001110, 15, 10), rf(Zn, 5), rf(Zd, 0);        \\\n+  }\n+\n+  INSN(sve_uunpkhi, 0b11);\n+  INSN(sve_uunpklo, 0b10);\n+  INSN(sve_sunpkhi, 0b01);\n+  INSN(sve_sunpklo, 0b00);\n+#undef INSN\n+\n+\/\/ SVE uzp1\/uzp2 (vectors)\n+#define INSN(NAME, op) \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn, FloatRegister Zm) { \\\n+    starti;                                                                            \\\n+    assert(T != Q, \"invalid size\");                                                    \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);                       \\\n+    f(0b01101, 15, 11), f(op, 10), rf(Zn, 5), rf(Zd, 0);                               \\\n+  }\n+\n+  INSN(sve_uzp1, 0b0);\n+  INSN(sve_uzp2, 0b1);\n+#undef INSN\n+\n+\/\/ SVE while[cond]\n+#define INSN(NAME, decode, sf)                                            \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, Register Rn, Register Rm) {  \\\n+    starti;                                                               \\\n+    assert(T != Q, \"invalid register variant\");                           \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(1, 21),                        \\\n+    zrf(Rm, 16), f(0, 15, 13), f(sf, 12), f(decode >> 1, 11, 10),         \\\n+    zrf(Rn, 5), f(decode & 0b1, 4), prf(Pd, 0);                           \\\n+  }\n+\n+  INSN(sve_whilelt,  0b010, 1);\n+  INSN(sve_whileltw, 0b010, 0);\n+  INSN(sve_whilele,  0b011, 1);\n+  INSN(sve_whilelew, 0b011, 0);\n+  INSN(sve_whilelo,  0b110, 1);\n+  INSN(sve_whilelow, 0b110, 0);\n+  INSN(sve_whilels,  0b111, 1);\n+  INSN(sve_whilelsw, 0b111, 0);\n+#undef INSN\n+\n+  \/\/ SVE convert signed integer to floating-point (predicated)\n+  void sve_scvtf(FloatRegister Zd, SIMD_RegVariant T_dst, PRegister Pg,\n+                 FloatRegister Zn, SIMD_RegVariant T_src) {\n+    starti;\n+    assert(T_src != B && T_dst != B && T_src != Q && T_dst != Q &&\n+           (T_src != H || T_dst == T_src), \"invalid register variant\");\n+    int opc = T_dst;\n+    int opc2 = T_src;\n+    \/\/ In most cases we can treat T_dst, T_src as opc, opc2,\n+    \/\/ except for the following two combinations.\n+    \/\/ +-----+------+---+------------------------------------+\n+    \/\/ | opc | opc2 | U |        Instruction Details         |\n+    \/\/ +-----+------+---+------------------------------------+\n+    \/\/ |  11 |   00 | 0 | SCVTF - 32-bit to double-precision |\n+    \/\/ |  11 |   10 | 0 | SCVTF - 64-bit to single-precision |\n+    \/\/ +-----+------+---+------------------------------------+\n+    if (T_src == S && T_dst == D) {\n+      opc = 0b11;\n+      opc2 = 0b00;\n+    } else if (T_src == D && T_dst == S) {\n+      opc = 0b11;\n+      opc2 = 0b10;\n+    }\n+    f(0b01100101, 31, 24), f(opc, 23, 22), f(0b010, 21, 19);\n+    f(opc2, 18, 17), f(0b0101, 16, 13);\n+    pgrf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE floating-point convert to signed integer, rounding toward zero (predicated)\n+  void sve_fcvtzs(FloatRegister Zd, SIMD_RegVariant T_dst, PRegister Pg,\n+                  FloatRegister Zn, SIMD_RegVariant T_src) {\n+    starti;\n+    assert(T_src != B && T_dst != B && T_src != Q && T_dst != Q &&\n+           (T_dst != H || T_src == H), \"invalid register variant\");\n+    int opc = T_src;\n+    int opc2 = T_dst;\n+    \/\/ In most cases we can treat T_src, T_dst as opc, opc2,\n+    \/\/ except for the following two combinations.\n+    \/\/ +-----+------+---+-------------------------------------+\n+    \/\/ | opc | opc2 | U |         Instruction Details         |\n+    \/\/ +-----+------+---+-------------------------------------+\n+    \/\/ |  11 |  10  | 0 | FCVTZS - single-precision to 64-bit |\n+    \/\/ |  11 |  00  | 0 | FCVTZS - double-precision to 32-bit |\n+    \/\/ +-----+------+---+-------------------------------------+\n+    if (T_src == S && T_dst == D) {\n+      opc = 0b11;\n+      opc2 = 0b10;\n+    } else if (T_src == D && T_dst == S) {\n+      opc = 0b11;\n+      opc2 = 0b00;\n+    }\n+    f(0b01100101, 31, 24), f(opc, 23, 22), f(0b011, 21, 19);\n+    f(opc2, 18, 17), f(0b0101, 16, 13);\n+    pgrf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE floating-point convert precision (predicated)\n+  void sve_fcvt(FloatRegister Zd, SIMD_RegVariant T_dst, PRegister Pg,\n+                FloatRegister Zn, SIMD_RegVariant T_src) {\n+    starti;\n+    assert(T_src != B && T_dst != B && T_src != Q && T_dst != Q &&\n+           T_src != T_dst, \"invalid register variant\");\n+    guarantee(T_src != H && T_dst != H, \"half-precision unsupported\");\n+    f(0b01100101, 31, 24), f(0b11, 23, 22), f(0b0010, 21, 18);\n+    f(T_dst, 17, 16), f(0b101, 15, 13);\n+    pgrf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+\/\/ SVE conditionally extract element to general-purpose register\n+#define INSN(NAME, before)                                                      \\\n+  void NAME(Register Rd, SIMD_RegVariant T, PRegister Pg,  FloatRegister Zn) {  \\\n+    starti;                                                                     \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b10000, 21, 17);                    \\\n+    f(before, 16), f(0b101, 15, 13);                                            \\\n+    pgrf(Pg, 10), rf(Zn, 5), rf(Rd, 0);                                         \\\n+  }\n+\n+  INSN(sve_lasta, 0b0);\n+  INSN(sve_lastb, 0b1);\n+#undef INSN\n+\n+#define INSN(NAME, before)                                                           \\\n+  void NAME(FloatRegister Vd, SIMD_RegVariant T, PRegister Pg,  FloatRegister Zn) {  \\\n+    starti;                                                                          \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b10001, 21, 17);                         \\\n+    f(before, 16), f(0b100, 15, 13);                                                 \\\n+    pgrf(Pg, 10), rf(Zn, 5), rf(Vd, 0);                                              \\\n+  }\n+\n+  INSN(sve_lasta, 0b0);\n+  INSN(sve_lastb, 0b1);\n+#undef INSN\n+\n+  \/\/ SVE INDEX (immediates)\n+  void sve_index(FloatRegister Zd, SIMD_RegVariant T, int imm1, int imm2) {\n+    starti;\n+    f(0b00000100, 31, 24), f(T, 23, 22), f(0b1, 21);\n+    sf(imm2, 20, 16), f(0b010000, 15, 10);\n+    sf(imm1, 9, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE programmable table lookup in single vector table\n+  void sve_tbl(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn, FloatRegister Zm) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);\n+    f(0b001100, 15, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":247,"deletions":0,"binary":false,"changes":247,"status":"modified"},{"patch":"@@ -2712,1 +2712,3 @@\n-  if (restore_vectors) {\n+  \/\/ We may use predicate registers and rely on ptrue with SVE,\n+  \/\/ regardless of wide vector (> 8 bytes) used or not.\n+  if (use_sve) {\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -6981,0 +6981,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2127,1 +2127,2 @@\n-    for_igvn()->clear();\n+    Unique_Node_List* old_worklist = for_igvn();\n+    old_worklist->clear();\n@@ -2137,1 +2138,1 @@\n-    set_for_igvn(save_for_igvn);\n+    set_for_igvn(old_worklist); \/\/ new_worklist is dead beyond this point\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -461,1 +461,1 @@\n-    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(1));\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::ge));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1195,0 +1195,1 @@\n+    assert((BoolTest::mask)predicate_node->get_int() == predicate, \"Unmatched predicates\");\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/globalDefinitions_vecApi.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1528,33 +1528,104 @@\n-                        [\"cpy\",    \"__ sve_cpy(z0, __ S, p0, v1);\",                      \"mov\\tz0.s, p0\/m, s1\"],\n-                        [\"inc\",    \"__ sve_inc(r0, __ S);\",                              \"incw\\tx0\"],\n-                        [\"dec\",    \"__ sve_dec(r1, __ H);\",                              \"dech\\tx1\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ B, z1, 7);\",                       \"lsl\\tz0.b, z1.b, #7\"],\n-                        [\"lsl\",    \"__ sve_lsl(z21, __ H, z1, 15);\",                     \"lsl\\tz21.h, z1.h, #15\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ S, z1, 31);\",                      \"lsl\\tz0.s, z1.s, #31\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ D, z1, 63);\",                      \"lsl\\tz0.d, z1.d, #63\"],\n-                        [\"lsr\",    \"__ sve_lsr(z0, __ B, z1, 7);\",                       \"lsr\\tz0.b, z1.b, #7\"],\n-                        [\"asr\",    \"__ sve_asr(z0, __ H, z11, 15);\",                     \"asr\\tz0.h, z11.h, #15\"],\n-                        [\"lsr\",    \"__ sve_lsr(z30, __ S, z1, 31);\",                     \"lsr\\tz30.s, z1.s, #31\"],\n-                        [\"asr\",    \"__ sve_asr(z0, __ D, z1, 63);\",                      \"asr\\tz0.d, z1.d, #63\"],\n-                        [\"addvl\",  \"__ sve_addvl(sp, r0, 31);\",                          \"addvl\\tsp, x0, #31\"],\n-                        [\"addpl\",  \"__ sve_addpl(r1, sp, -32);\",                         \"addpl\\tx1, sp, -32\"],\n-                        [\"cntp\",   \"__ sve_cntp(r8, __ B, p0, p1);\",                     \"cntp\\tx8, p0, p1.b\"],\n-                        [\"dup\",    \"__ sve_dup(z0, __ B, 127);\",                         \"dup\\tz0.b, 127\"],\n-                        [\"dup\",    \"__ sve_dup(z1, __ H, -128);\",                        \"dup\\tz1.h, -128\"],\n-                        [\"dup\",    \"__ sve_dup(z2, __ S, 32512);\",                       \"dup\\tz2.s, 32512\"],\n-                        [\"dup\",    \"__ sve_dup(z7, __ D, -32768);\",                      \"dup\\tz7.d, -32768\"],\n-                        [\"ld1b\",   \"__ sve_ld1b(z0, __ B, p0, Address(sp));\",            \"ld1b\\t{z0.b}, p0\/z, [sp]\"],\n-                        [\"ld1h\",   \"__ sve_ld1h(z10, __ H, p1, Address(sp, -8));\",       \"ld1h\\t{z10.h}, p1\/z, [sp, #-8, MUL VL]\"],\n-                        [\"ld1w\",   \"__ sve_ld1w(z20, __ S, p2, Address(r0, 7));\",        \"ld1w\\t{z20.s}, p2\/z, [x0, #7, MUL VL]\"],\n-                        [\"ld1b\",   \"__ sve_ld1b(z30, __ B, p3, Address(sp, r8));\",       \"ld1b\\t{z30.b}, p3\/z, [sp, x8]\"],\n-                        [\"ld1w\",   \"__ sve_ld1w(z0, __ S, p4, Address(sp, r28));\",       \"ld1w\\t{z0.s}, p4\/z, [sp, x28, LSL #2]\"],\n-                        [\"ld1d\",   \"__ sve_ld1d(z11, __ D, p5, Address(r0, r1));\",       \"ld1d\\t{z11.d}, p5\/z, [x0, x1, LSL #3]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z22, __ B, p6, Address(sp));\",           \"st1b\\t{z22.b}, p6, [sp]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z31, __ B, p7, Address(sp, -8));\",       \"st1b\\t{z31.b}, p7, [sp, #-8, MUL VL]\"],\n-                        [\"st1w\",   \"__ sve_st1w(z0, __ S, p1, Address(r0, 7));\",         \"st1w\\t{z0.s}, p1, [x0, #7, MUL VL]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z0, __ B, p2, Address(sp, r1));\",        \"st1b\\t{z0.b}, p2, [sp, x1]\"],\n-                        [\"st1h\",   \"__ sve_st1h(z0, __ H, p3, Address(sp, r8));\",        \"st1h\\t{z0.h}, p3, [sp, x8, LSL #1]\"],\n-                        [\"st1d\",   \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n-                        [\"ldr\",    \"__ sve_ldr(z0, Address(sp));\",                       \"ldr\\tz0, [sp]\"],\n-                        [\"ldr\",    \"__ sve_ldr(z31, Address(sp, -256));\",                \"ldr\\tz31, [sp, #-256, MUL VL]\"],\n-                        [\"str\",    \"__ sve_str(z8, Address(r8, 255));\",                  \"str\\tz8, [x8, #255, MUL VL]\"],\n+                        [\"cpy\",     \"__ sve_cpy(z0, __ S, p0, v1);\",                      \"mov\\tz0.s, p0\/m, s1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z0, __ B, p0, 127, true);\",               \"mov\\tz0.b, p0\/m, 127\"],\n+                        [\"cpy\",     \"__ sve_cpy(z1, __ H, p0, -128, true);\",              \"mov\\tz1.h, p0\/m, -128\"],\n+                        [\"cpy\",     \"__ sve_cpy(z2, __ S, p0, 32512, true);\",             \"mov\\tz2.s, p0\/m, 32512\"],\n+                        [\"cpy\",     \"__ sve_cpy(z5, __ D, p0, -32768, false);\",           \"mov\\tz5.d, p0\/z, -32768\"],\n+                        [\"cpy\",     \"__ sve_cpy(z10, __ B, p0, -1, false);\",              \"mov\\tz10.b, p0\/z, -1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z11, __ S, p0, -1, false);\",              \"mov\\tz11.s, p0\/z, -1\"],\n+                        [\"inc\",     \"__ sve_inc(r0, __ S);\",                              \"incw\\tx0\"],\n+                        [\"dec\",     \"__ sve_dec(r1, __ H);\",                              \"dech\\tx1\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, z1, 7);\",                       \"lsl\\tz0.b, z1.b, #7\"],\n+                        [\"lsl\",     \"__ sve_lsl(z21, __ H, z1, 15);\",                     \"lsl\\tz21.h, z1.h, #15\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ S, z1, 31);\",                      \"lsl\\tz0.s, z1.s, #31\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ D, z1, 63);\",                      \"lsl\\tz0.d, z1.d, #63\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, z1, 7);\",                       \"lsr\\tz0.b, z1.b, #7\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ H, z11, 15);\",                     \"asr\\tz0.h, z11.h, #15\"],\n+                        [\"lsr\",     \"__ sve_lsr(z30, __ S, z1, 31);\",                     \"lsr\\tz30.s, z1.s, #31\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ D, z1, 63);\",                      \"asr\\tz0.d, z1.d, #63\"],\n+                        [\"addvl\",   \"__ sve_addvl(sp, r0, 31);\",                          \"addvl\\tsp, x0, #31\"],\n+                        [\"addpl\",   \"__ sve_addpl(r1, sp, -32);\",                         \"addpl\\tx1, sp, -32\"],\n+                        [\"cntp\",    \"__ sve_cntp(r8, __ B, p0, p1);\",                     \"cntp\\tx8, p0, p1.b\"],\n+                        [\"dup\",     \"__ sve_dup(z0, __ B, 127);\",                         \"dup\\tz0.b, 127\"],\n+                        [\"dup\",     \"__ sve_dup(z1, __ H, -128);\",                        \"dup\\tz1.h, -128\"],\n+                        [\"dup\",     \"__ sve_dup(z2, __ S, 32512);\",                       \"dup\\tz2.s, 32512\"],\n+                        [\"dup\",     \"__ sve_dup(z7, __ D, -32768);\",                      \"dup\\tz7.d, -32768\"],\n+                        [\"dup\",     \"__ sve_dup(z10, __ B, -1);\",                         \"dup\\tz10.b, -1\"],\n+                        [\"dup\",     \"__ sve_dup(z11, __ S, -1);\",                         \"dup\\tz11.s, -1\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ B, p0, Address(sp));\",            \"ld1b\\t{z0.b}, p0\/z, [sp]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ H, p1, Address(sp));\",            \"ld1b\\t{z0.h}, p1\/z, [sp]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ S, p2, Address(sp, r8));\",        \"ld1b\\t{z0.s}, p2\/z, [sp, x8]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ D, p3, Address(sp, 7));\",         \"ld1b\\t{z0.d}, p3\/z, [sp, #7, MUL VL]\"],\n+                        [\"ld1h\",    \"__ sve_ld1h(z10, __ H, p1, Address(sp, -8));\",       \"ld1h\\t{z10.h}, p1\/z, [sp, #-8, MUL VL]\"],\n+                        [\"ld1w\",    \"__ sve_ld1w(z20, __ S, p2, Address(r0, 7));\",        \"ld1w\\t{z20.s}, p2\/z, [x0, #7, MUL VL]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z30, __ B, p3, Address(sp, r8));\",       \"ld1b\\t{z30.b}, p3\/z, [sp, x8]\"],\n+                        [\"ld1w\",    \"__ sve_ld1w(z0, __ S, p4, Address(sp, r28));\",       \"ld1w\\t{z0.s}, p4\/z, [sp, x28, LSL #2]\"],\n+                        [\"ld1d\",    \"__ sve_ld1d(z11, __ D, p5, Address(r0, r1));\",       \"ld1d\\t{z11.d}, p5\/z, [x0, x1, LSL #3]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z22, __ B, p6, Address(sp));\",           \"st1b\\t{z22.b}, p6, [sp]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z31, __ B, p7, Address(sp, -8));\",       \"st1b\\t{z31.b}, p7, [sp, #-8, MUL VL]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ H, p1, Address(sp));\",            \"st1b\\t{z0.h}, p1, [sp]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ S, p2, Address(sp, r8));\",        \"st1b\\t{z0.s}, p2, [sp, x8]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ D, p3, Address(sp));\",            \"st1b\\t{z0.d}, p3, [sp]\"],\n+                        [\"st1w\",    \"__ sve_st1w(z0, __ S, p1, Address(r0, 7));\",         \"st1w\\t{z0.s}, p1, [x0, #7, MUL VL]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ B, p2, Address(sp, r1));\",        \"st1b\\t{z0.b}, p2, [sp, x1]\"],\n+                        [\"st1h\",    \"__ sve_st1h(z0, __ H, p3, Address(sp, r8));\",        \"st1h\\t{z0.h}, p3, [sp, x8, LSL #1]\"],\n+                        [\"st1d\",    \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n+                        [\"ldr\",     \"__ sve_ldr(z0, Address(sp));\",                       \"ldr\\tz0, [sp]\"],\n+                        [\"ldr\",     \"__ sve_ldr(z31, Address(sp, -256));\",                \"ldr\\tz31, [sp, #-256, MUL VL]\"],\n+                        [\"str\",     \"__ sve_str(z8, Address(r8, 255));\",                  \"str\\tz8, [x8, #255, MUL VL]\"],\n+                        [\"sel\",     \"__ sve_sel(z0, __ B, p0, z1, z2);\",                  \"sel\\tz0.b, p0, z1.b, z2.b\"],\n+                        [\"sel\",     \"__ sve_sel(z4, __ D, p0, z5, z6);\",                  \"sel\\tz4.d, p0, z5.d, z6.d\"],\n+                        [\"cmpeq\",   \"__ sve_cmpeq(p1, __ B, p0, z0, z1);\",                \"cmpeq\\tp1.b, p0\/z, z0.b, z1.b\"],\n+                        [\"cmpne\",   \"__ sve_cmpne(p1, __ H, p0, z2, z3);\",                \"cmpne\\tp1.h, p0\/z, z2.h, z3.h\"],\n+                        [\"cmpge\",   \"__ sve_cmpge(p1, __ S, p2, z4, z5);\",                \"cmpge\\tp1.s, p2\/z, z4.s, z5.s\"],\n+                        [\"cmpgt\",   \"__ sve_cmpgt(p1, __ D, p3, z6, z7);\",                \"cmpgt\\tp1.d, p3\/z, z6.d, z7.d\"],\n+                        [\"cmple\",   \"__ sve_cmpge(p2, __ B, p0, z10, z11);\",              \"cmple\\tp2.b, p0\/z, z11.b, z10.b\"],\n+                        [\"cmplt\",   \"__ sve_cmpgt(p3, __ S, p0, z16, z17);\",              \"cmplt\\tp3.s, p0\/z, z17.s, z16.s\"],\n+                        [\"cmpeq\",   \"__ sve_cmpeq(p1, __ B, p4, z0, 15);\",                \"cmpeq\\tp1.b, p4\/z, z0.b, #15\"],\n+                        [\"cmpne\",   \"__ sve_cmpne(p1, __ H, p0, z2, -16);\",               \"cmpne\\tp1.h, p0\/z, z2.h, #-16\"],\n+                        [\"cmple\",   \"__ sve_cmple(p1, __ S, p1, z4, 0);\",                 \"cmple\\tp1.s, p1\/z, z4.s, #0\"],\n+                        [\"cmplt\",   \"__ sve_cmplt(p1, __ D, p2, z6, -1);\",                \"cmplt\\tp1.d, p2\/z, z6.d, #-1\"],\n+                        [\"cmpge\",   \"__ sve_cmpge(p1, __ S, p3, z4, 5);\",                 \"cmpge\\tp1.s, p3\/z, z4.s, #5\"],\n+                        [\"cmpgt\",   \"__ sve_cmpgt(p1, __ B, p4, z6, -2);\",                \"cmpgt\\tp1.b, p4\/z, z6.b, #-2\"],\n+                        [\"fcmeq\",   \"__ sve_fcmeq(p1, __ S, p0, z0, z1);\",                \"fcmeq\\tp1.s, p0\/z, z0.s, z1.s\"],\n+                        [\"fcmne\",   \"__ sve_fcmne(p1, __ D, p0, z2, z3);\",                \"fcmne\\tp1.d, p0\/z, z2.d, z3.d\"],\n+                        [\"fcmgt\",   \"__ sve_fcmgt(p1, __ S, p2, z4, z5);\",                \"fcmgt\\tp1.s, p2\/z, z4.s, z5.s\"],\n+                        [\"fcmge\",   \"__ sve_fcmge(p1, __ D, p3, z6, z7);\",                \"fcmge\\tp1.d, p3\/z, z6.d, z7.d\"],\n+                        [\"fcmlt\",   \"__ sve_fcmgt(p2, __ S, p0, z10, z11);\",              \"fcmlt\\tp2.s, p0\/z, z11.s, z10.s\"],\n+                        [\"fcmle\",   \"__ sve_fcmge(p3, __ D, p0, z16, z17);\",              \"fcmle\\tp3.d, p0\/z, z17.d, z16.d\"],\n+                        [\"uunpkhi\", \"__ sve_uunpkhi(z0, __ H, z1);\",                      \"uunpkhi\\tz0.h, z1.b\"],\n+                        [\"uunpklo\", \"__ sve_uunpklo(z4, __ S, z5);\",                      \"uunpklo\\tz4.s, z5.h\"],\n+                        [\"sunpkhi\", \"__ sve_sunpkhi(z6, __ D, z7);\",                      \"sunpkhi\\tz6.d, z7.s\"],\n+                        [\"sunpklo\", \"__ sve_sunpklo(z10, __ H, z11);\",                    \"sunpklo\\tz10.h, z11.b\"],\n+                        [\"whilelt\", \"__ sve_whilelt(p0, __ B, r1, r2);\",                  \"whilelt\\tp0.b, x1, x2\"],\n+                        [\"whilelt\", \"__ sve_whileltw(p1, __ H, r3, r4);\",                 \"whilelt\\tp1.h, w3, w4\"],\n+                        [\"whilele\", \"__ sve_whilele(p2, __ S, r5, r6);\",                  \"whilele\\tp2.s, x5, x6\"],\n+                        [\"whilele\", \"__ sve_whilelew(p3, __ D, r10, r11);\",               \"whilele\\tp3.d, w10, w11\"],\n+                        [\"whilelo\", \"__ sve_whilelo(p4, __ B, r1, r2);\",                  \"whilelo\\tp4.b, x1, x2\"],\n+                        [\"whilelo\", \"__ sve_whilelow(p0, __ H, r3, r4);\",                 \"whilelo\\tp0.h, w3, w4\"],\n+                        [\"whilels\", \"__ sve_whilels(p1, __ S, r5, r6);\",                  \"whilels\\tp1.s, x5, x6\"],\n+                        [\"whilels\", \"__ sve_whilelsw(p2, __ D, r10, r11);\",               \"whilels\\tp2.d, w10, w11\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z1, __ D, p0, z0, __ S);\",              \"scvtf\\tz1.d, p0\/m, z0.s\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z3, __ D, p1, z2, __ D);\",              \"scvtf\\tz3.d, p1\/m, z2.d\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z6, __ S, p2, z1, __ D);\",              \"scvtf\\tz6.s, p2\/m, z1.d\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z6, __ S, p3, z1, __ S);\",              \"scvtf\\tz6.s, p3\/m, z1.s\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z6, __ H, p3, z1, __ S);\",              \"scvtf\\tz6.h, p3\/m, z1.s\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z6, __ H, p3, z1, __ D);\",              \"scvtf\\tz6.h, p3\/m, z1.d\"],\n+                        [\"scvtf\",   \"__ sve_scvtf(z6, __ H, p3, z1, __ H);\",              \"scvtf\\tz6.h, p3\/m, z1.h\"],\n+                        [\"fcvt\",    \"__ sve_fcvt(z5, __ D, p3, z4, __ S);\",               \"fcvt\\tz5.d, p3\/m, z4.s\"],\n+                        [\"fcvt\",    \"__ sve_fcvt(z1, __ S, p3, z0, __ D);\",               \"fcvt\\tz1.s, p3\/m, z0.d\"],\n+                        [\"fcvtzs\",  \"__ sve_fcvtzs(z19, __ D, p2, z1, __ D);\",            \"fcvtzs\\tz19.d, p2\/m, z1.d\"],\n+                        [\"fcvtzs\",  \"__ sve_fcvtzs(z9, __ S, p1, z8, __ S);\",             \"fcvtzs\\tz9.s, p1\/m, z8.s\"],\n+                        [\"fcvtzs\",  \"__ sve_fcvtzs(z1, __ S, p2, z0, __ D);\",             \"fcvtzs\\tz1.s, p2\/m, z0.d\"],\n+                        [\"fcvtzs\",  \"__ sve_fcvtzs(z1, __ D, p3, z0, __ S);\",             \"fcvtzs\\tz1.d, p3\/m, z0.s\"],\n+                        [\"fcvtzs\",  \"__ sve_fcvtzs(z1, __ S, p4, z18, __ H);\",            \"fcvtzs\\tz1.s, p4\/m, z18.h\"],\n+                        [\"lasta\",   \"__ sve_lasta(r0, __ B, p0, z15);\",                   \"lasta\\tw0, p0, z15.b\"],\n+                        [\"lastb\",   \"__ sve_lastb(r1, __ B, p1, z16);\",                   \"lastb\\tw1, p1, z16.b\"],\n+                        [\"lasta\",   \"__ sve_lasta(v0, __ B, p0, z15);\",                   \"lasta\\tb0, p0, z15.b\"],\n+                        [\"lastb\",   \"__ sve_lastb(v1, __ B, p1, z16);\",                   \"lastb\\tb1, p1, z16.b\"],\n+                        [\"index\",   \"__ sve_index(z6, __ S, 1, 1);\",                      \"index\\tz6.s, #1, #1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z7, __ H, p3, r5);\",                      \"cpy\\tz7.h, p3\/m, w5\"],\n+                        [\"tbl\",     \"__ sve_tbl(z16, __ S, z17, z18);\",                   \"tbl\\tz16.s, {z17.s}, z18.s\"],\n+                        [\"ld1w\",    \"__ sve_ld1w_gather(z15, p0, r5, z16);\",              \"ld1w\\t{z15.s}, p0\/z, [x5, z16.s, uxtw #2]\"],\n+                        [\"ld1d\",    \"__ sve_ld1d_gather(z15, p0, r5, z16);\",              \"ld1d\\t{z15.d}, p0\/z, [x5, z16.d, uxtw #3]\"],\n+                        [\"st1w\",    \"__ sve_st1w_scatter(z15, p0, r5, z16);\",             \"st1w\\t{z15.s}, p0, [x5, z16.s, uxtw #2]\"],\n+                        [\"st1d\",    \"__ sve_st1d_scatter(z15, p0, r5, z16);\",             \"st1d\\t{z15.d}, p0, [x5, z16.d, uxtw #3]\"],\n@@ -1630,0 +1701,2 @@\n+                       [\"uzp1\", \"ZZZ\"],\n+                       [\"uzp2\", \"ZZZ\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":106,"deletions":33,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -728,0 +728,6 @@\n+    __ sve_cpy(z0, __ B, p0, 127, true);               \/\/       mov     z0.b, p0\/m, 127\n+    __ sve_cpy(z1, __ H, p0, -128, true);              \/\/       mov     z1.h, p0\/m, -128\n+    __ sve_cpy(z2, __ S, p0, 32512, true);             \/\/       mov     z2.s, p0\/m, 32512\n+    __ sve_cpy(z5, __ D, p0, -32768, false);           \/\/       mov     z5.d, p0\/z, -32768\n+    __ sve_cpy(z10, __ B, p0, -1, false);              \/\/       mov     z10.b, p0\/z, -1\n+    __ sve_cpy(z11, __ S, p0, -1, false);              \/\/       mov     z11.s, p0\/z, -1\n@@ -745,0 +751,2 @@\n+    __ sve_dup(z10, __ B, -1);                         \/\/       dup     z10.b, -1\n+    __ sve_dup(z11, __ S, -1);                         \/\/       dup     z11.s, -1\n@@ -746,0 +754,3 @@\n+    __ sve_ld1b(z0, __ H, p1, Address(sp));            \/\/       ld1b    {z0.h}, p1\/z, [sp]\n+    __ sve_ld1b(z0, __ S, p2, Address(sp, r8));        \/\/       ld1b    {z0.s}, p2\/z, [sp, x8]\n+    __ sve_ld1b(z0, __ D, p3, Address(sp, 7));         \/\/       ld1b    {z0.d}, p3\/z, [sp, #7, MUL VL]\n@@ -753,0 +764,3 @@\n+    __ sve_st1b(z0, __ H, p1, Address(sp));            \/\/       st1b    {z0.h}, p1, [sp]\n+    __ sve_st1b(z0, __ S, p2, Address(sp, r8));        \/\/       st1b    {z0.s}, p2, [sp, x8]\n+    __ sve_st1b(z0, __ D, p3, Address(sp));            \/\/       st1b    {z0.d}, p3, [sp]\n@@ -760,0 +774,57 @@\n+    __ sve_sel(z0, __ B, p0, z1, z2);                  \/\/       sel     z0.b, p0, z1.b, z2.b\n+    __ sve_sel(z4, __ D, p0, z5, z6);                  \/\/       sel     z4.d, p0, z5.d, z6.d\n+    __ sve_cmpeq(p1, __ B, p0, z0, z1);                \/\/       cmpeq   p1.b, p0\/z, z0.b, z1.b\n+    __ sve_cmpne(p1, __ H, p0, z2, z3);                \/\/       cmpne   p1.h, p0\/z, z2.h, z3.h\n+    __ sve_cmpge(p1, __ S, p2, z4, z5);                \/\/       cmpge   p1.s, p2\/z, z4.s, z5.s\n+    __ sve_cmpgt(p1, __ D, p3, z6, z7);                \/\/       cmpgt   p1.d, p3\/z, z6.d, z7.d\n+    __ sve_cmpge(p2, __ B, p0, z10, z11);              \/\/       cmple   p2.b, p0\/z, z11.b, z10.b\n+    __ sve_cmpgt(p3, __ S, p0, z16, z17);              \/\/       cmplt   p3.s, p0\/z, z17.s, z16.s\n+    __ sve_cmpeq(p1, __ B, p4, z0, 15);                \/\/       cmpeq   p1.b, p4\/z, z0.b, #15\n+    __ sve_cmpne(p1, __ H, p0, z2, -16);               \/\/       cmpne   p1.h, p0\/z, z2.h, #-16\n+    __ sve_cmple(p1, __ S, p1, z4, 0);                 \/\/       cmple   p1.s, p1\/z, z4.s, #0\n+    __ sve_cmplt(p1, __ D, p2, z6, -1);                \/\/       cmplt   p1.d, p2\/z, z6.d, #-1\n+    __ sve_cmpge(p1, __ S, p3, z4, 5);                 \/\/       cmpge   p1.s, p3\/z, z4.s, #5\n+    __ sve_cmpgt(p1, __ B, p4, z6, -2);                \/\/       cmpgt   p1.b, p4\/z, z6.b, #-2\n+    __ sve_fcmeq(p1, __ S, p0, z0, z1);                \/\/       fcmeq   p1.s, p0\/z, z0.s, z1.s\n+    __ sve_fcmne(p1, __ D, p0, z2, z3);                \/\/       fcmne   p1.d, p0\/z, z2.d, z3.d\n+    __ sve_fcmgt(p1, __ S, p2, z4, z5);                \/\/       fcmgt   p1.s, p2\/z, z4.s, z5.s\n+    __ sve_fcmge(p1, __ D, p3, z6, z7);                \/\/       fcmge   p1.d, p3\/z, z6.d, z7.d\n+    __ sve_fcmgt(p2, __ S, p0, z10, z11);              \/\/       fcmlt   p2.s, p0\/z, z11.s, z10.s\n+    __ sve_fcmge(p3, __ D, p0, z16, z17);              \/\/       fcmle   p3.d, p0\/z, z17.d, z16.d\n+    __ sve_uunpkhi(z0, __ H, z1);                      \/\/       uunpkhi z0.h, z1.b\n+    __ sve_uunpklo(z4, __ S, z5);                      \/\/       uunpklo z4.s, z5.h\n+    __ sve_sunpkhi(z6, __ D, z7);                      \/\/       sunpkhi z6.d, z7.s\n+    __ sve_sunpklo(z10, __ H, z11);                    \/\/       sunpklo z10.h, z11.b\n+    __ sve_whilelt(p0, __ B, r1, r2);                  \/\/       whilelt p0.b, x1, x2\n+    __ sve_whileltw(p1, __ H, r3, r4);                 \/\/       whilelt p1.h, w3, w4\n+    __ sve_whilele(p2, __ S, r5, r6);                  \/\/       whilele p2.s, x5, x6\n+    __ sve_whilelew(p3, __ D, r10, r11);               \/\/       whilele p3.d, w10, w11\n+    __ sve_whilelo(p4, __ B, r1, r2);                  \/\/       whilelo p4.b, x1, x2\n+    __ sve_whilelow(p0, __ H, r3, r4);                 \/\/       whilelo p0.h, w3, w4\n+    __ sve_whilels(p1, __ S, r5, r6);                  \/\/       whilels p1.s, x5, x6\n+    __ sve_whilelsw(p2, __ D, r10, r11);               \/\/       whilels p2.d, w10, w11\n+    __ sve_scvtf(z1, __ D, p0, z0, __ S);              \/\/       scvtf   z1.d, p0\/m, z0.s\n+    __ sve_scvtf(z3, __ D, p1, z2, __ D);              \/\/       scvtf   z3.d, p1\/m, z2.d\n+    __ sve_scvtf(z6, __ S, p2, z1, __ D);              \/\/       scvtf   z6.s, p2\/m, z1.d\n+    __ sve_scvtf(z6, __ S, p3, z1, __ S);              \/\/       scvtf   z6.s, p3\/m, z1.s\n+    __ sve_scvtf(z6, __ H, p3, z1, __ S);              \/\/       scvtf   z6.h, p3\/m, z1.s\n+    __ sve_scvtf(z6, __ H, p3, z1, __ D);              \/\/       scvtf   z6.h, p3\/m, z1.d\n+    __ sve_scvtf(z6, __ H, p3, z1, __ H);              \/\/       scvtf   z6.h, p3\/m, z1.h\n+    __ sve_fcvt(z5, __ D, p3, z4, __ S);               \/\/       fcvt    z5.d, p3\/m, z4.s\n+    __ sve_fcvt(z1, __ S, p3, z0, __ D);               \/\/       fcvt    z1.s, p3\/m, z0.d\n+    __ sve_fcvtzs(z19, __ D, p2, z1, __ D);            \/\/       fcvtzs  z19.d, p2\/m, z1.d\n+    __ sve_fcvtzs(z9, __ S, p1, z8, __ S);             \/\/       fcvtzs  z9.s, p1\/m, z8.s\n+    __ sve_fcvtzs(z1, __ S, p2, z0, __ D);             \/\/       fcvtzs  z1.s, p2\/m, z0.d\n+    __ sve_fcvtzs(z1, __ D, p3, z0, __ S);             \/\/       fcvtzs  z1.d, p3\/m, z0.s\n+    __ sve_fcvtzs(z1, __ S, p4, z18, __ H);            \/\/       fcvtzs  z1.s, p4\/m, z18.h\n+    __ sve_lasta(r0, __ B, p0, z15);                   \/\/       lasta   w0, p0, z15.b\n+    __ sve_lastb(r1, __ B, p1, z16);                   \/\/       lastb   w1, p1, z16.b\n+    __ sve_lasta(v0, __ B, p0, z15);                   \/\/       lasta   b0, p0, z15.b\n+    __ sve_lastb(v1, __ B, p1, z16);                   \/\/       lastb   b1, p1, z16.b\n+    __ sve_index(z6, __ S, 1, 1);                      \/\/       index   z6.s, #1, #1\n+    __ sve_cpy(z7, __ H, p3, r5);                      \/\/       cpy     z7.h, p3\/m, w5\n+    __ sve_tbl(z16, __ S, z17, z18);                   \/\/       tbl     z16.s, {z17.s}, z18.s\n+    __ sve_ld1w_gather(z15, p0, r5, z16);              \/\/       ld1w    {z15.s}, p0\/z, [x5, z16.s, uxtw #2]\n+    __ sve_ld1d_gather(z15, p0, r5, z16);              \/\/       ld1d    {z15.d}, p0\/z, [x5, z16.d, uxtw #3]\n+    __ sve_st1w_scatter(z15, p0, r5, z16);             \/\/       st1w    {z15.s}, p0, [x5, z16.s, uxtw #2]\n+    __ sve_st1d_scatter(z15, p0, r5, z16);             \/\/       st1d    {z15.d}, p0, [x5, z16.d, uxtw #3]\n@@ -896,0 +967,54 @@\n+<<<<<<< HEAD\n+    __ sve_add(z7, __ H, z4, z7);                      \/\/       add     z7.h, z4.h, z7.h\n+    __ sve_sub(z9, __ B, z22, z8);                     \/\/       sub     z9.b, z22.b, z8.b\n+    __ sve_fadd(z27, __ S, z20, z30);                  \/\/       fadd    z27.s, z20.s, z30.s\n+    __ sve_fmul(z26, __ S, z0, z16);                   \/\/       fmul    z26.s, z0.s, z16.s\n+    __ sve_fsub(z3, __ D, z25, z8);                    \/\/       fsub    z3.d, z25.d, z8.d\n+    __ sve_abs(z21, __ D, p6, z26);                    \/\/       abs     z21.d, p6\/m, z26.d\n+    __ sve_add(z22, __ B, p0, z4);                     \/\/       add     z22.b, p0\/m, z22.b, z4.b\n+    __ sve_asr(z17, __ H, p0, z3);                     \/\/       asr     z17.h, p0\/m, z17.h, z3.h\n+    __ sve_cnt(z1, __ B, p2, z6);                      \/\/       cnt     z1.b, p2\/m, z6.b\n+    __ sve_lsl(z9, __ S, p7, z7);                      \/\/       lsl     z9.s, p7\/m, z9.s, z7.s\n+    __ sve_lsr(z22, __ H, p5, z5);                     \/\/       lsr     z22.h, p5\/m, z22.h, z5.h\n+    __ sve_mul(z8, __ B, p4, z30);                     \/\/       mul     z8.b, p4\/m, z8.b, z30.b\n+    __ sve_neg(z17, __ D, p0, z11);                    \/\/       neg     z17.d, p0\/m, z11.d\n+    __ sve_not(z28, __ S, p0, z26);                    \/\/       not     z28.s, p0\/m, z26.s\n+    __ sve_smax(z28, __ D, p3, z13);                   \/\/       smax    z28.d, p3\/m, z28.d, z13.d\n+    __ sve_smin(z16, __ B, p6, z5);                    \/\/       smin    z16.b, p6\/m, z16.b, z5.b\n+    __ sve_sub(z13, __ H, p2, z15);                    \/\/       sub     z13.h, p2\/m, z13.h, z15.h\n+    __ sve_fabs(z26, __ S, p5, z11);                   \/\/       fabs    z26.s, p5\/m, z11.s\n+    __ sve_fadd(z22, __ S, p4, z4);                    \/\/       fadd    z22.s, p4\/m, z22.s, z4.s\n+    __ sve_fdiv(z19, __ S, p4, z17);                   \/\/       fdiv    z19.s, p4\/m, z19.s, z17.s\n+    __ sve_fmax(z14, __ D, p3, z2);                    \/\/       fmax    z14.d, p3\/m, z14.d, z2.d\n+    __ sve_fmin(z3, __ S, p5, z23);                    \/\/       fmin    z3.s, p5\/m, z3.s, z23.s\n+    __ sve_fmul(z6, __ S, p1, z17);                    \/\/       fmul    z6.s, p1\/m, z6.s, z17.s\n+    __ sve_fneg(z27, __ S, p4, z16);                   \/\/       fneg    z27.s, p4\/m, z16.s\n+    __ sve_frintm(z2, __ S, p7, z3);                   \/\/       frintm  z2.s, p7\/m, z3.s\n+    __ sve_frintn(z6, __ S, p4, z19);                  \/\/       frintn  z6.s, p4\/m, z19.s\n+    __ sve_frintp(z12, __ D, p5, z8);                  \/\/       frintp  z12.d, p5\/m, z8.d\n+    __ sve_fsqrt(z19, __ S, p4, z0);                   \/\/       fsqrt   z19.s, p4\/m, z0.s\n+    __ sve_fsub(z23, __ D, p1, z19);                   \/\/       fsub    z23.d, p1\/m, z23.d, z19.d\n+    __ sve_fmla(z13, __ S, p4, z6, z0);                \/\/       fmla    z13.s, p4\/m, z6.s, z0.s\n+    __ sve_fmls(z14, __ S, p4, z25, z8);               \/\/       fmls    z14.s, p4\/m, z25.s, z8.s\n+    __ sve_fnmla(z22, __ S, p5, z22, z27);             \/\/       fnmla   z22.s, p5\/m, z22.s, z27.s\n+    __ sve_fnmls(z3, __ S, p3, z17, z20);              \/\/       fnmls   z3.s, p3\/m, z17.s, z20.s\n+    __ sve_mla(z4, __ H, p7, z7, z0);                  \/\/       mla     z4.h, p7\/m, z7.h, z0.h\n+    __ sve_mls(z16, __ S, p5, z22, z4);                \/\/       mls     z16.s, p5\/m, z22.s, z4.s\n+    __ sve_and(z9, z22, z11);                          \/\/       and     z9.d, z22.d, z11.d\n+    __ sve_eor(z5, z30, z16);                          \/\/       eor     z5.d, z30.d, z16.d\n+    __ sve_orr(z22, z11, z1);                          \/\/       orr     z22.d, z11.d, z1.d\n+    __ sve_bic(z8, z20, z16);                          \/\/       bic     z8.d, z20.d, z16.d\n+    __ sve_uzp1(z15, __ S, z4, z4);                    \/\/       uzp1    z15.s, z4.s, z4.s\n+    __ sve_uzp2(z8, __ B, z6, z29);                    \/\/       uzp2    z8.b, z6.b, z29.b\n+\n+\/\/ SVEReductionOp\n+    __ sve_andv(v28, __ D, p4, z29);                   \/\/       andv d28, p4, z29.d\n+    __ sve_orv(v9, __ H, p3, z2);                      \/\/       orv h9, p3, z2.h\n+    __ sve_eorv(v28, __ B, p0, z7);                    \/\/       eorv b28, p0, z7.b\n+    __ sve_smaxv(v26, __ H, p5, z17);                  \/\/       smaxv h26, p5, z17.h\n+    __ sve_sminv(v8, __ D, p4, z21);                   \/\/       sminv d8, p4, z21.d\n+    __ sve_fminv(v5, __ D, p5, z21);                   \/\/       fminv d5, p5, z21.d\n+    __ sve_fmaxv(v22, __ D, p4, z29);                  \/\/       fmaxv d22, p4, z29.d\n+    __ sve_fadda(v19, __ D, p0, z4);                   \/\/       fadda d19, p0, d19, z4.d\n+    __ sve_uaddv(v23, __ B, p1, z19);                  \/\/       uaddv d23, p1, z19.b\n+=======\n@@ -946,0 +1071,1 @@\n+>>>>>>> master\n@@ -964,0 +1090,9 @@\n+<<<<<<< HEAD\n+    0x14000000,     0x17ffffd7,     0x1400032a,     0x94000000,\n+    0x97ffffd4,     0x94000327,     0x3400000a,     0x34fffa2a,\n+    0x3400648a,     0x35000008,     0x35fff9c8,     0x35006428,\n+    0xb400000b,     0xb4fff96b,     0xb40063cb,     0xb500001d,\n+    0xb5fff91d,     0xb500637d,     0x10000013,     0x10fff8b3,\n+    0x10006313,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36306296,     0x3758000c,     0x375ff7cc,     0x3758622c,\n+=======\n@@ -971,0 +1106,1 @@\n+>>>>>>> master\n@@ -975,0 +1111,15 @@\n+<<<<<<< HEAD\n+    0x54006000,     0x54000001,     0x54fff541,     0x54005fa1,\n+    0x54000002,     0x54fff4e2,     0x54005f42,     0x54000002,\n+    0x54fff482,     0x54005ee2,     0x54000003,     0x54fff423,\n+    0x54005e83,     0x54000003,     0x54fff3c3,     0x54005e23,\n+    0x54000004,     0x54fff364,     0x54005dc4,     0x54000005,\n+    0x54fff305,     0x54005d65,     0x54000006,     0x54fff2a6,\n+    0x54005d06,     0x54000007,     0x54fff247,     0x54005ca7,\n+    0x54000008,     0x54fff1e8,     0x54005c48,     0x54000009,\n+    0x54fff189,     0x54005be9,     0x5400000a,     0x54fff12a,\n+    0x54005b8a,     0x5400000b,     0x54fff0cb,     0x54005b2b,\n+    0x5400000c,     0x54fff06c,     0x54005acc,     0x5400000d,\n+    0x54fff00d,     0x54005a6d,     0x5400000e,     0x54ffefae,\n+    0x54005a0e,     0x5400000f,     0x54ffef4f,     0x540059af,\n+=======\n@@ -988,0 +1139,1 @@\n+>>>>>>> master\n@@ -1019,0 +1171,3 @@\n+<<<<<<< HEAD\n+    0xbd1b1869,     0x580049fb,     0x1800000b,     0xf8945060,\n+=======\n@@ -1020,0 +1175,1 @@\n+>>>>>>> master\n@@ -1084,0 +1240,86 @@\n+<<<<<<< HEAD\n+    0x0eb36651,     0x4ebf67dd,     0x0e3cf77a,     0x4e3ef7bc,\n+    0x4e63f441,     0x0e3d6f9b,     0x4e226c20,     0x0e766eb4,\n+    0x4e7e6fbc,     0x0eb16e0f,     0x4eae6dac,     0x0eacf56a,\n+    0x4ebef7bc,     0x4efef7bc,     0x2e358e93,     0x6e388ef6,\n+    0x2e6c8d6a,     0x6e668ca4,     0x2ea08ffe,     0x6eb68eb4,\n+    0x6eea8d28,     0x0e20e7fe,     0x4e33e651,     0x4e6ce56a,\n+    0x0e3d379b,     0x4e243462,     0x0e7a3738,     0x4e6634a4,\n+    0x0ea53483,     0x4eaa3528,     0x4ef836f6,     0x2eb3e651,\n+    0x6eafe5cd,     0x6ee6e4a4,     0x0e3e3fbc,     0x4e393f17,\n+    0x0e773ed5,     0x4e7b3f59,     0x0eba3f38,     0x4ea53c83,\n+    0x4ef93f17,     0x2e3ce77a,     0x6e39e717,     0x6e70e5ee,\n+    0xba5fd3e3,     0x3a5f03e5,     0xfa411be4,     0x7a42cbe2,\n+    0x93df03ff,     0xc820ffff,     0x8822fc7f,     0xc8247cbf,\n+    0x88267fff,     0x4e010fe0,     0x4e081fe1,     0x4e0c1fe1,\n+    0x4e0a1fe1,     0x4e071fe1,     0x4e042c20,     0x4e062c20,\n+    0x4e052c20,     0x4e083c20,     0x0e0c3c20,     0x0e0a3c20,\n+    0x0e073c20,     0x4cc0ac3f,     0x05a08020,     0x05104fe0,\n+    0x05505001,     0x05906fe2,     0x05d03005,     0x05101fea,\n+    0x05901feb,     0x04b0e3e0,     0x0470e7e1,     0x042f9c20,\n+    0x043f9c35,     0x047f9c20,     0x04ff9c20,     0x04299420,\n+    0x04319160,     0x0461943e,     0x04a19020,     0x042053ff,\n+    0x047f5401,     0x25208028,     0x2538cfe0,     0x2578d001,\n+    0x25b8efe2,     0x25f8f007,     0x2538dfea,     0x25b8dfeb,\n+    0xa400a3e0,     0xa420a7e0,     0xa4484be0,     0xa467afe0,\n+    0xa4a8a7ea,     0xa547a814,     0xa4084ffe,     0xa55c53e0,\n+    0xa5e1540b,     0xe400fbf6,     0xe408ffff,     0xe420e7e0,\n+    0xe4484be0,     0xe460efe0,     0xe547e400,     0xe4014be0,\n+    0xe4a84fe0,     0xe5f15000,     0x858043e0,     0x85a043ff,\n+    0xe59f5d08,     0x0522c020,     0x05e6c0a4,     0x2401a001,\n+    0x2443a051,     0x24858881,     0x24c78cd1,     0x240b8142,\n+    0x24918213,     0x250f9001,     0x25508051,     0x25802491,\n+    0x25df28c1,     0x25850c81,     0x251e10d1,     0x65816001,\n+    0x65c36051,     0x65854891,     0x65c74cc1,     0x658b4152,\n+    0x65d14203,     0x05733820,     0x05b238a4,     0x05f138e6,\n+    0x0570396a,     0x25221420,     0x25640461,     0x25a614b2,\n+    0x25eb0553,     0x25221c24,     0x25640c60,     0x25a61cb1,\n+    0x25eb0d52,     0x65d0a001,     0x65d6a443,     0x65d4a826,\n+    0x6594ac26,     0x6554ac26,     0x6556ac26,     0x6552ac26,\n+    0x65cbac85,     0x65caac01,     0x65dea833,     0x659ca509,\n+    0x65d8a801,     0x65dcac01,     0x655cb241,     0x0520a1e0,\n+    0x0521a601,     0x052281e0,     0x05238601,     0x04a14026,\n+    0x0568aca7,     0x05b23230,     0x853040af,     0xc5b040af,\n+    0xe57080af,     0xe5b080af,     0x1e601000,     0x1e603000,\n+    0x1e621000,     0x1e623000,     0x1e641000,     0x1e643000,\n+    0x1e661000,     0x1e663000,     0x1e681000,     0x1e683000,\n+    0x1e6a1000,     0x1e6a3000,     0x1e6c1000,     0x1e6c3000,\n+    0x1e6e1000,     0x1e6e3000,     0x1e701000,     0x1e703000,\n+    0x1e721000,     0x1e723000,     0x1e741000,     0x1e743000,\n+    0x1e761000,     0x1e763000,     0x1e781000,     0x1e783000,\n+    0x1e7a1000,     0x1e7a3000,     0x1e7c1000,     0x1e7c3000,\n+    0x1e7e1000,     0x1e7e3000,     0xf8358303,     0xf8280299,\n+    0xf8301051,     0xf8212300,     0xf8243183,     0xf83f515c,\n+    0xf83a4182,     0xf830703f,     0xf82d601d,     0xf8b3822c,\n+    0xf8b6038d,     0xf8be103f,     0xf8ba209c,     0xf8be30c4,\n+    0xf8be51fa,     0xf8a94188,     0xf8a07034,     0xf8b86002,\n+    0xf8e98358,     0xf8f0007e,     0xf8ea1157,     0xf8e42050,\n+    0xf8eb3148,     0xf8ef5051,     0xf8ea418c,     0xf8ef704d,\n+    0xf8e76354,     0xf8708044,     0xf86401ec,     0xf87511f0,\n+    0xf86b22f5,     0xf86c32fa,     0xf87c516e,     0xf8784181,\n+    0xf87f720a,     0xf8676062,     0xb82d8233,     0xb8300023,\n+    0xb82b10be,     0xb82823af,     0xb83e3280,     0xb82752f4,\n+    0xb83c4375,     0xb8397025,     0xb83763f0,     0xb8a5812c,\n+    0xb8bc03af,     0xb8b6127f,     0xb8bf21c5,     0xb8b031ff,\n+    0xb8bb5214,     0xb8ac412b,     0xb8a6723e,     0xb8bb63dc,\n+    0xb8e7828a,     0xb8ea0304,     0xb8f112d1,     0xb8e321fd,\n+    0xb8f63273,     0xb8f651e2,     0xb8e6420c,     0xb8eb72ed,\n+    0xb8e1627e,     0xb8658051,     0xb87001b6,     0xb86a13b5,\n+    0xb87b236c,     0xb86333e1,     0xb8785233,     0xb869437c,\n+    0xb86f72a7,     0xb877633f,     0xce3a47c2,     0xce110aca,\n+    0xce788c11,     0xce8296d9,     0xce7b806c,     0xce70879d,\n+    0xcec080da,     0xce718b89,     0x04670087,     0x042806c9,\n+    0x659e029b,     0x6590081a,     0x65c80723,     0x04d6bb55,\n+    0x04000096,     0x04508071,     0x041aa8c1,     0x04939ce9,\n+    0x045194b6,     0x041013c8,     0x04d7a171,     0x049ea35c,\n+    0x04c80dbc,     0x040a18b0,     0x044109ed,     0x049cb57a,\n+    0x65809096,     0x658d9233,     0x65c68c4e,     0x658796e3,\n+    0x65828626,     0x049db21b,     0x6582bc62,     0x6580b266,\n+    0x65c1b50c,     0x658db013,     0x65c18677,     0x65a010cd,\n+    0x65a8332e,     0x65bb56d6,     0x65b46e23,     0x04405ce4,\n+    0x048476d0,     0x042b32c9,     0x04b033c5,     0x04613176,\n+    0x04f03288,     0x05a4688f,     0x053d6cc8,     0x04da33bc,\n+    0x04582c49,     0x041920fc,     0x0448363a,     0x04ca32a8,\n+    0x65c736a5,     0x65c633b6,     0x65d82093,     0x04012677,\n+\n+=======\n@@ -1156,0 +1398,1 @@\n+>>>>>>> master\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":243,"deletions":0,"binary":false,"changes":243,"status":"modified"}]}