{"files":[{"patch":"@@ -1282,1 +1282,1 @@\n-    return MacroAssembler::far_branch_size();\n+    return MacroAssembler::far_codestub_branch_size();\n@@ -1287,1 +1287,1 @@\n-    return 4 * NativeInstruction::instruction_size;\n+    return NativeInstruction::instruction_size + MacroAssembler::far_codestub_branch_size();\n@@ -2401,1 +2401,1 @@\n-  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow\");\n+  assert(__ offset() - offset == (int) size_deopt_handler(), \"overflow\");\n@@ -11609,0 +11609,102 @@\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegI_reg_URShift_reg(iRegINoSp dst,\n+                              immI0 zero, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (SubI zero (URShiftI src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"negw  $dst, $src1, LSR $src2\" %}\n+\n+  ins_encode %{\n+    __ negw(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSR, $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegI_reg_RShift_reg(iRegINoSp dst,\n+                              immI0 zero, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (SubI zero (RShiftI src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"negw  $dst, $src1, ASR $src2\" %}\n+\n+  ins_encode %{\n+    __ negw(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::ASR, $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegI_reg_LShift_reg(iRegINoSp dst,\n+                              immI0 zero, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (SubI zero (LShiftI src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"negw  $dst, $src1, LSL $src2\" %}\n+\n+  ins_encode %{\n+    __ negw(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSL, $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegL_reg_URShift_reg(iRegLNoSp dst,\n+                              immL0 zero, iRegL src1, immI src2) %{\n+  match(Set dst (SubL zero (URShiftL src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"neg  $dst, $src1, LSR $src2\" %}\n+\n+  ins_encode %{\n+    __ neg(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSR, $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegL_reg_RShift_reg(iRegLNoSp dst,\n+                              immL0 zero, iRegL src1, immI src2) %{\n+  match(Set dst (SubL zero (RShiftL src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"neg  $dst, $src1, ASR $src2\" %}\n+\n+  ins_encode %{\n+    __ neg(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::ASR, $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegL_reg_LShift_reg(iRegLNoSp dst,\n+                              immL0 zero, iRegL src1, immI src2) %{\n+  match(Set dst (SubL zero (LShiftL src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"neg  $dst, $src1, LSL $src2\" %}\n+\n+  ins_encode %{\n+    __ neg(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSL, $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":105,"deletions":3,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -353,1 +353,1 @@\n-instruct vcvt2Lto2F(vecD dst, vecX src)\n+instruct vcvt2Lto2F(vecD dst, vecX src, vRegF tmp)\n@@ -357,2 +357,6 @@\n-  format %{ \"scvtfv  T2D, $dst, $src\\n\\t\"\n-            \"fcvtn   $dst, T2S, $dst, T2D\\t# convert 2L to 2F vector\"\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   rscratch1, $src, D, 0\\n\\t\"\n+            \"scvtfs $dst, rscratch1\\n\\t\"\n+            \"umov   rscratch1, $src, D, 1\\n\\t\"\n+            \"scvtfs $tmp, rscratch1\\n\\t\"\n+            \"ins    $dst, S, $tmp, 1, 0\\t# convert 2L to 2F vector\"\n@@ -361,2 +365,5 @@\n-    __ scvtfv(__ T2D, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n-    __ fcvtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($dst$$reg), __ T2D);\n+    __ umov(rscratch1, as_FloatRegister($src$$reg), __ D, 0);\n+    __ scvtfs(as_FloatRegister($dst$$reg), rscratch1);\n+    __ umov(rscratch1, as_FloatRegister($src$$reg), __ D, 1);\n+    __ scvtfs(as_FloatRegister($tmp$$reg), rscratch1);\n+    __ ins(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($tmp$$reg), 1, 0);\n@@ -3258,1 +3265,1 @@\n-instruct replicate2L_zero(vecX dst, immI0 zero)\n+instruct replicate2L_imm(vecX dst, immL con)\n@@ -3261,1 +3268,1 @@\n-  match(Set dst (ReplicateI zero));\n+  match(Set dst (ReplicateL con));\n@@ -3263,1 +3270,1 @@\n-  format %{ \"movi  $dst, $zero\\t# vector (4I)\" %}\n+  format %{ \"movi  $dst, $con\\t# vector (2L)\" %}\n@@ -3265,3 +3272,1 @@\n-    __ eor(as_FloatRegister($dst$$reg), __ T16B,\n-           as_FloatRegister($dst$$reg),\n-           as_FloatRegister($dst$$reg));\n+    __ mov(as_FloatRegister($dst$$reg), __ T2D, $con$$constant);\n@@ -3279,2 +3284,1 @@\n-    __ dup(as_FloatRegister($dst$$reg), __ T2S,\n-           as_FloatRegister($src$$reg));\n+    __ dup(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg));\n@@ -3292,2 +3296,1 @@\n-    __ dup(as_FloatRegister($dst$$reg), __ T4S,\n-           as_FloatRegister($src$$reg));\n+    __ dup(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg));\n@@ -3305,2 +3308,1 @@\n-    __ dup(as_FloatRegister($dst$$reg), __ T2D,\n-           as_FloatRegister($src$$reg));\n+    __ dup(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg));\n@@ -4269,0 +4271,41 @@\n+instruct vnegID(vecD dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() < 16);\n+  match(Set dst (NegVI src));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# vector (8B\/4H\/2S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), false);\n+    __ negr(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(vunop_fp64);\n+%}\n+\n+instruct vnegIX(vecX dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (NegVI src));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# vector (16B\/8H\/4S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), true);\n+    __ negr(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(vunop_fp128);\n+%}\n+\n+instruct vneg2L(vecX dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (NegVL src));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst,$src\\t# vector (2D)\" %}\n+  ins_encode %{\n+    __ negr(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(vunop_fp128);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":60,"deletions":17,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -181,1 +181,1 @@\n-instruct vcvt2Lto2F(vecD dst, vecX src)\n+instruct vcvt2Lto2F(vecD dst, vecX src, vRegF tmp)\n@@ -185,2 +185,6 @@\n-  format %{ \"scvtfv  T2D, $dst, $src\\n\\t\"\n-            \"fcvtn   $dst, T2S, $dst, T2D\\t# convert 2L to 2F vector\"\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   rscratch1, $src, D, 0\\n\\t\"\n+            \"scvtfs $dst, rscratch1\\n\\t\"\n+            \"umov   rscratch1, $src, D, 1\\n\\t\"\n+            \"scvtfs $tmp, rscratch1\\n\\t\"\n+            \"ins    $dst, S, $tmp, 1, 0\\t# convert 2L to 2F vector\"\n@@ -189,2 +193,5 @@\n-    __ scvtfv(__ T2D, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n-    __ fcvtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($dst$$reg), __ T2D);\n+    __ umov(rscratch1, as_FloatRegister($src$$reg), __ D, 0);\n+    __ scvtfs(as_FloatRegister($dst$$reg), rscratch1);\n+    __ umov(rscratch1, as_FloatRegister($src$$reg), __ D, 1);\n+    __ scvtfs(as_FloatRegister($tmp$$reg), rscratch1);\n+    __ ins(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($tmp$$reg), 1, 0);\n@@ -1555,8 +1562,11 @@\n-define(`VREPLICATE', `\n-instruct replicate$3$4$5`'(vec$6 dst, $7 ifelse($7, immI0, zero, $7, immI, con, src))\n-%{\n-  predicate(UseSVE == 0 && ifelse($8, `',\n-                                  n->as_Vector()->length() == $3,\n-                                  (n->as_Vector()->length() == $3 ||`\n-                            'n->as_Vector()->length() == $8)));\n-  match(Set dst (Replicate`'ifelse($7, immI0, I, $4) ifelse($7, immI0, zero, $7, immI, con, $7, zero, I, src)));\n+define(`VREPLICATE_REG', `\n+instruct replicate$2$3`'(vec$4 dst, $5 src)\n+%{\n+  predicate(UseSVE == 0 && ifelse($2$3, 8B,\n+                                  `(n->as_Vector()->length() == 8 ||\n+                            n->as_Vector()->length() == 4)',\n+                                  $2$3, 4S,\n+                                  `(n->as_Vector()->length() == 4 ||\n+                            n->as_Vector()->length() == 2)',\n+                            n->as_Vector()->length() == $2));\n+  match(Set dst (Replicate$3 src));\n@@ -1564,22 +1574,7 @@\n-  format %{ \"$1  $dst, $ifelse($7, immI0, zero, $7, immI, con, src)`\\t# vector ('ifelse($4$7, SimmI, $3H, $2, eor, 4I, $3$4)`)\"' %}\n-  ins_encode %{\n-    __ $2(as_FloatRegister($dst$$reg), __ ifelse(\n-            $2, eor, T16B, T$3`'$9),ifelse(\n-            `$4 $7', `B immI', ` '$con$$constant & 0xff,\n-            `$4 $7', `S immI', ` '$con$$constant & 0xffff,\n-            `$4 $7', `I immI', ` '$con$$constant,\n-            `$2', eor,`\n-           as_FloatRegister($dst$$reg),\n-           as_FloatRegister($dst$$reg)',\n-            `$7', vRegF,`\n-           as_FloatRegister($src$$reg)',\n-            `$7', vRegD,`\n-           as_FloatRegister($src$$reg)',\n-           ` 'as_Register($src$$reg)));\n-  %}\n-  ins_pipe(ifelse($7, immI0, v$1_reg_imm,\n-                  $7, immI,  v$1_reg_imm,\n-                  $7, iRegIorL2I, v$1_reg_reg,\n-                  $7, zero, vmovi_reg_imm,\n-                  $7, iRegL, vdup_reg_reg,\n-                  $4, F, vdup_reg_freg, vdup_reg_dreg)`'ifelse($6, X, 128, 64));\n+  format %{ \"dup  $dst, $src\\t# vector ($2$3)\" %}\n+  ins_encode %{\n+    __ dup(as_FloatRegister($dst$$reg), __ T$2$1, $6($src$$reg));\n+  %}\n+  ins_pipe(ifelse($5, iRegIorL2I, vdup_reg_reg,\n+                  $5, iRegL, vdup_reg_reg,\n+                  $3, F, vdup_reg_freg, vdup_reg_dreg)`'ifelse($4, X, 128, 64));\n@@ -1587,18 +1582,36 @@\n-dnl        $1    $2    $3  $4 $5     $6 $7          $8 $9\n-VREPLICATE(dup,  dup,  8,  B, ,      D, iRegIorL2I, 4, B)\n-VREPLICATE(dup,  dup,  16, B, ,      X, iRegIorL2I,  , B)\n-VREPLICATE(movi, mov,  8,  B, _imm,  D, immI,       4, B)\n-VREPLICATE(movi, mov,  16, B, _imm,  X, immI,        , B)\n-VREPLICATE(dup,  dup,  4,  S, ,      D, iRegIorL2I, 2, H)\n-VREPLICATE(dup,  dup,  8,  S, ,      X, iRegIorL2I,  , H)\n-VREPLICATE(movi, mov,  4,  S, _imm,  D, immI,       2, H)\n-VREPLICATE(movi, mov,  8,  S,  _imm, X, immI,        , H)\n-VREPLICATE(dup,  dup,  2,  I, ,      D, iRegIorL2I, ,  S)\n-VREPLICATE(dup,  dup,  4,  I, ,      X, iRegIorL2I, ,  S)\n-VREPLICATE(movi, mov,  2,  I, _imm,  D, immI,       ,  S)\n-VREPLICATE(movi, mov,  4,  I,  _imm, X, immI,       ,  S)\n-VREPLICATE(dup,  dup,  2,  L, ,      X, iRegL,      ,  D)\n-VREPLICATE(movi, eor,  2,  L, _zero, X, immI0,      ,  D)\n-VREPLICATE(dup,  dup,  2,  F, ,      D, vRegF,      ,  S)\n-VREPLICATE(dup,  dup,  4,  F, ,      X, vRegF,      ,  S)\n-VREPLICATE(dup,  dup,  2,  D, ,      X, vRegD,      ,  D)\n+define(`VREPLICATE_IMM', `\n+instruct replicate$2$3_imm`'(vec$4 dst, $5 con)\n+%{\n+  predicate(UseSVE == 0 && ifelse($2$3, 8B,\n+                                  `(n->as_Vector()->length() == 8 ||\n+                            n->as_Vector()->length() == 4)',\n+                                  $2$3, 4S,\n+                                  `(n->as_Vector()->length() == 4 ||\n+                            n->as_Vector()->length() == 2)',\n+                            n->as_Vector()->length() == $2));\n+  match(Set dst (Replicate$3 con));\n+  ins_cost(INSN_COST);\n+  format %{ \"movi  $dst, $con\\t`#' vector ($2`'ifelse($3, S, H, $3))\" %}\n+  ins_encode %{\n+    __ mov(as_FloatRegister($dst$$reg), __ T$2`'iTYPE2SIMD($3), $con$$constant`'$6);\n+  %}\n+  ins_pipe(vmovi_reg_imm`'ifelse($4, X, 128, 64));\n+%}')dnl\n+dnl            $1 $2  $3 $4 $5          $6\n+VREPLICATE_REG(B, 8,  B, D, iRegIorL2I, as_Register)\n+VREPLICATE_REG(B, 16, B, X, iRegIorL2I, as_Register)\n+VREPLICATE_IMM(B, 8,  B, D, immI, ` & 0xff')\n+VREPLICATE_IMM(B, 16, B, X, immI, ` & 0xff')\n+VREPLICATE_REG(H, 4,  S, D, iRegIorL2I, as_Register)\n+VREPLICATE_REG(H, 8,  S, X, iRegIorL2I, as_Register)\n+VREPLICATE_IMM(H, 4,  S, D, immI, ` & 0xffff')\n+VREPLICATE_IMM(H, 8,  S, X, immI, ` & 0xffff')\n+VREPLICATE_REG(S, 2,  I, D, iRegIorL2I, as_Register)\n+VREPLICATE_REG(S, 4,  I, X, iRegIorL2I, as_Register)\n+VREPLICATE_IMM(S, 2,  I, D, immI)\n+VREPLICATE_IMM(S, 4,  I, X, immI)\n+VREPLICATE_REG(D, 2,  L, X, iRegL,      as_Register)\n+VREPLICATE_IMM(D, 2,  L, X, immL)\n+VREPLICATE_REG(S, 2,  F, D, vRegF,      as_FloatRegister)\n+VREPLICATE_REG(S, 4,  F, X, vRegF,      as_FloatRegister)\n+VREPLICATE_REG(D, 2,  D, X, vRegD,      as_FloatRegister)\n@@ -1926,0 +1939,18 @@\n+define(`VNEGI', `\n+instruct vnegI$1(vec$1 dst, vec$1 src)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() ifelse($1, D, <, ==) 16);\n+  match(Set dst (NegVI src));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# vector ($2)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_Arrangement size = __ esize2arrangement((unsigned)type2aelembytes(bt), ifelse($1, D, false, true));\n+    __ negr(as_FloatRegister($dst$$reg), size, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(vunop_fp`'ifelse($1, D, 64, 128));\n+%}')dnl\n+dnl  $1  $2\n+VNEGI(D, 8B\/4H\/2S)\n+VNEGI(X, 16B\/8H\/4S)\n+dnl\n@@ -1931,1 +1962,1 @@\n-  ins_cost(INSN_COST * 3);\n+  ins_cost(INSN_COST`'ifelse($3, L, `',` * 3'));\n@@ -1934,1 +1965,1 @@\n-    __ $1(as_FloatRegister($dst$$reg), __ T$2`'ifelse($5, L, D, $5),\n+    __ $1(as_FloatRegister($dst$$reg), __ T$2$5,\n@@ -1940,0 +1971,1 @@\n+VNEG(negr, 2,  L, X, D)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":87,"deletions":55,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -308,1 +308,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+  format %{ \"sve_ptrue $pgtmp, vector_length\\n\\t\"\n@@ -312,2 +312,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this));\n@@ -328,1 +328,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+  format %{ \"sve_ptrue $pgtmp, vector_length\\n\\t\"\n@@ -332,2 +332,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $src));\n@@ -345,2 +345,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -359,22 +358,1 @@\n-instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() < MaxVectorSize);\n-  match(Set dst (LoadVectorMasked mem pg));\n-  effect(TEMP pgtmp, KILL cr);\n-  ins_cost(6 * SVE_COST);\n-  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this));\n-    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n-                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -394,108 +372,0 @@\n-instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() < MaxVectorSize);\n-  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n-  effect(TEMP pgtmp, KILL cr);\n-  ins_cost(6 * SVE_COST);\n-  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n-                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ maskAll (full or partial predicate size)\n-\n-instruct vmaskAll_immI(pRegGov dst, immI src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (MaskAll src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_ptrue_lanecnt\/sve_pfalse $dst\\t# mask all (sve) (B\/H\/S)\" %}\n-  ins_encode %{\n-    int con = (int)$src$$constant;\n-    if (con == 0) {\n-      __ sve_pfalse(as_PRegister($dst$$reg));\n-    } else {\n-      assert(con == -1, \"invalid constant value for mask\");\n-      BasicType bt = Matcher::vector_element_basic_type(this);\n-      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-                           Matcher::vector_length(this));\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (MaskAll src));\n-  effect(TEMP tmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_dup $tmp, $src\\n\\t\"\n-            \"sve_ptrue_lanecnt $dst\\n\\t\"\n-            \"sve_cmpne $dst, $dst, $tmp, 0\\t# mask all (sve) (B\/H\/S)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n-    if (length_in_bytes < MaxVectorSize) {\n-      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), size, Matcher::vector_length(this));\n-      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size,\n-                 as_PRegister($dst$$reg), as_FloatRegister($tmp$$reg), 0);\n-    } else {\n-      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vmaskAll_immL(pRegGov dst, immL src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (MaskAll src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_ptrue_lanecnt\/sve_pfalse $dst\\t# mask all (sve) (D)\" %}\n-  ins_encode %{\n-    long con = (long)$src$$constant;\n-    if (con == 0) {\n-      __ sve_pfalse(as_PRegister($dst$$reg));\n-    } else {\n-      assert(con == -1, \"invalid constant value for mask\");\n-      BasicType bt = Matcher::vector_element_basic_type(this);\n-      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-                           Matcher::vector_length(this));\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (MaskAll src));\n-  effect(TEMP tmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_dup $tmp, $src\\n\\t\"\n-            \"sve_ptrue_lanecnt $dst\\n\\t\"\n-            \"sve_cmpne $dst, $dst, $tmp, 0\\t# mask all (sve) (D)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n-    if (length_in_bytes < MaxVectorSize) {\n-      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), size, Matcher::vector_length(this));\n-      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size,\n-                 as_PRegister($dst$$reg), as_FloatRegister($tmp$$reg), 0);\n-    } else {\n-      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -594,1 +464,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n+    __ sve_ptrue_lanecnt(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n@@ -1547,1 +1417,1 @@\n-\/\/ vector fmla - predicated\n+\/\/ vector fmad - predicated\n@@ -1550,1 +1420,1 @@\n-instruct vfmlaF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+instruct vfmadF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n@@ -1563,1 +1433,1 @@\n-instruct vfmlaD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+instruct vfmadD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n@@ -1578,3 +1448,4 @@\n-\/\/ dst_src1 = dst_src1 + src2 * -src3\n-instruct vfmlsF(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n+\/\/ The NegVF must not be predicated.\n+instruct vfmlsF1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n@@ -1582,0 +1453,14 @@\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * -src3\n+\/\/ The NegVF must not be predicated.\n+instruct vfmlsF2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n@@ -1593,3 +1478,4 @@\n-\/\/ dst_src1 = dst_src1 + src2 * -src3\n-instruct vfmlsD(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n+\/\/ The NegVD must not be predicated.\n+instruct vfmlsD1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n@@ -1597,0 +1483,14 @@\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * -src3\n+\/\/ The NegVD must not be predicated.\n+instruct vfmlsD2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n@@ -1607,0 +1507,32 @@\n+\/\/ vector fmsb - predicated\n+\n+\/\/ dst_src1 = dst_src1 * -src2 + src3\n+\/\/ The NegVF must not be predicated.\n+instruct vfmsbF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 (NegVF src2)) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmsb $dst_src1, $pg, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmsb(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 * -src2 + src3\n+\/\/ The NegVD must not be predicated.\n+instruct vfmsbD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 (NegVD src2)) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmsb $dst_src1, $pg, $src2, $src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmsb(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1610,3 +1542,5 @@\n-\/\/ dst_src1 = -dst_src1 + src2 * -src3\n-instruct vfnmlaF(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n+\/\/ The NegVF must not be predicated.\n+instruct vfnmlaF1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n@@ -1614,0 +1548,15 @@\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = -dst_src1 + src2 * -src3\n+\/\/ The NegVF must not be predicated.\n+instruct vfnmlaF2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n@@ -1625,3 +1574,5 @@\n-\/\/ dst_src1 = -dst_src1 + src2 * -src3\n-instruct vfnmlaD(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n+\/\/ The NegVD must not be predicated.\n+instruct vfnmlaD1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n@@ -1629,0 +1580,15 @@\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = -dst_src1 + src2 * -src3\n+\/\/ The NegVD must not be predicated.\n+instruct vfnmlaD2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n@@ -1639,0 +1605,34 @@\n+\/\/ vector fnmad - predicated\n+\n+\/\/ dst_src1 = -src3 + dst_src1 * -src2\n+\/\/ The NegVF must not be predicated.\n+instruct vfnmadF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 (NegVF src2)) (Binary (NegVF src3) pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmad $dst_src1, $pg, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fnmad(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = -src3 + dst_src1 * -src2\n+\/\/ The NegVD must not be predicated.\n+instruct vfnmadD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 (NegVD src2)) (Binary (NegVD src3) pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmad $dst_src1, $pg, $src2, $src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fnmad(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1642,0 +1642,1 @@\n+\/\/ The NegVF must not be predicated.\n@@ -1643,1 +1644,2 @@\n-  predicate(UseFMA && UseSVE > 0);\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector());\n@@ -1655,0 +1657,1 @@\n+\/\/ The NegVD must not be predicated.\n@@ -1656,1 +1659,2 @@\n-  predicate(UseFMA && UseSVE > 0);\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector());\n@@ -1667,0 +1671,32 @@\n+\/\/ vector fnmsb - predicated\n+\n+\/\/ dst_src1 = -src3 + dst_src1 * src2\n+\/\/ The NegVF must not be predicated.\n+instruct vfnmsbF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary (NegVF src3) pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmsb $dst_src1, $pg, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fnmsb(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = -src3 + dst_src1 * src2\n+\/\/ The NegVD must not be predicated.\n+instruct vfnmsbD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary (NegVD src3) pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmsb $dst_src1, $pg, $src2, $src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fnmsb(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1725,0 +1761,58 @@\n+\/\/ vector mla - predicated\n+\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaB_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 (MulVB src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, $pg, src2, src3\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ B, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaS_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVS (Binary dst_src1 (MulVS src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, $pg, src2, src3\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ H, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaI_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVI (Binary dst_src1 (MulVI src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, $pg, src2, src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaL_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVL (Binary dst_src1 (MulVL src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, $pg, src2, src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1783,0 +1877,58 @@\n+\/\/ vector mls - predicated\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsB_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 (MulVB src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, $pg, src2, src3\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ B, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsS_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVS (Binary dst_src1 (MulVS src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, $pg, src2, src3\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ H, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsI_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVI (Binary dst_src1 (MulVI src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, $pg, src2, src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsL_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVL (Binary dst_src1 (MulVL src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, $pg, src2, src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1939,1 +2091,28 @@\n-\/\/ vector fneg\n+\/\/ vector neg\n+\n+instruct vnegI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (NegVI src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t# vector (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegL(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (NegVL src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1967,1 +2146,28 @@\n-\/\/ vector fneg - predicated\n+\/\/ vector neg - predicated\n+\n+instruct vnegI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVI dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst_src, $pg, $dst_src\\t# vector (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_neg(as_FloatRegister($dst_src$$reg), __ elemType_to_regVariant(bt),\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVL dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2081,30 +2287,0 @@\n-\/\/ vector mask compare\n-\n-instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n-  effect(KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n-                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n-  effect(KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n-                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2127,31 +2303,0 @@\n-\/\/ vector load mask\n-\n-instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorLoadMask src));\n-  effect(KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n-  ins_encode %{\n-    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n-               ptrue, as_FloatRegister($src$$reg), 0);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n-  match(Set dst (VectorLoadMask src));\n-  effect(TEMP tmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n-    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2224,1 +2369,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n@@ -2271,1 +2416,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n@@ -2349,2 +2494,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -2367,2 +2511,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -2384,2 +2527,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this, $src2));\n@@ -2400,2 +2542,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -2411,2 +2552,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -2427,2 +2567,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -2442,2 +2581,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -2455,2 +2593,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -2467,78 +2604,0 @@\n-instruct reduce_addI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated partial (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addF_masked_partial(vRegF src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addD_masked_partial(vRegD src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2592,2 +2651,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -2611,2 +2669,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -2624,2 +2681,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG);\n@@ -2641,2 +2697,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n@@ -2655,44 +2710,0 @@\n-instruct reduce_andI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated partial (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_andL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2746,2 +2757,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -2765,2 +2775,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -2778,2 +2787,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG);\n@@ -2795,2 +2803,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n@@ -2809,44 +2816,0 @@\n-instruct reduce_orI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated partial (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_orL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2900,2 +2863,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -2919,2 +2881,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -2932,2 +2893,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG);\n@@ -2949,2 +2909,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n@@ -2963,44 +2922,0 @@\n-instruct reduce_eorI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated partial (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3056,2 +2971,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -3075,2 +2989,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -3109,2 +3022,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this, $src2));\n@@ -3142,2 +3054,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -3155,1 +3066,0 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3174,1 +3084,0 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3188,45 +3097,0 @@\n-instruct reduce_maxI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n-  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_maxL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3235,2 +3099,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n@@ -3250,2 +3113,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n@@ -3263,42 +3125,0 @@\n-instruct reduce_maxF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n-                                    pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_maxD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n-                                    pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3354,2 +3174,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -3373,2 +3192,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -3407,2 +3225,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this, $src2));\n@@ -3440,2 +3257,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -3453,1 +3269,0 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3472,1 +3287,0 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3486,45 +3300,0 @@\n-instruct reduce_minI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n-  match(Set dst (MinReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_minL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (MinReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3533,2 +3302,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n@@ -3548,2 +3316,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n@@ -3561,42 +3328,0 @@\n-instruct reduce_minF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n-                                    pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_minD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n-                                    pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -5106,1 +4831,0 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -5124,1 +4848,0 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -5138,43 +4861,0 @@\n-\n-instruct vtest_alltrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"vtest_alltrue_partial $dst, $src1, $src2\\t# VectorTest partial (sve) - alltrue\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n-                          Matcher::vector_length(this, $src1));\n-    __ sve_eors(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n-    __ csetw(as_Register($dst$$reg), Assembler::EQ);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vtest_anytrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"vtest_anytrue_partial $dst, $src1, $src2\\t# VectorTest partial (sve) - anytrue\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n-                          Matcher::vector_length(this, $src1));\n-    __ sve_ands(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n-    __ csetw(as_Register($dst$$reg), Assembler::NE);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -5408,1 +5088,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n@@ -5425,2 +5105,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n@@ -5438,1 +5117,0 @@\n-            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n@@ -5453,1 +5131,0 @@\n-            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n@@ -5467,42 +5144,0 @@\n-\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n-\n-instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n-                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n-                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -5554,2 +5189,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this, $src));\n@@ -5572,2 +5206,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src));\n@@ -5585,1 +5218,0 @@\n-            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n@@ -5600,1 +5232,0 @@\n-            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n@@ -5614,43 +5245,0 @@\n-\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n-\n-instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n-            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n-                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n-            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n-                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -5712,2 +5300,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -5725,0 +5312,2 @@\n+\/\/ Return the index of the first mask lane that is set, or vector length if none of\n+\/\/ them are set.\n@@ -5726,2 +5315,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -5730,1 +5318,1 @@\n-  ins_cost(2 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -5733,0 +5321,1 @@\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n@@ -5735,1 +5324,8 @@\n-    __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    \/\/ When the input predicate is all-false, the result should be the vector length\n+    \/\/ instead of max vector register size.\n+    if (length_in_bytes == MaxVectorSize) {\n+      __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    } else {\n+      __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+      __ sve_brkb(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), as_PRegister($src$$reg), false);\n+    }\n@@ -5742,2 +5338,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -5755,1 +5350,1 @@\n-instruct vmask_truecount_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -5757,5 +5352,5 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskTrueCount src));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"vmask_truecount_partial $dst, $src\\t# vector mask truecount partial (sve)\" %}\n+            n->in(1)->bottom_type()->is_vect()->length() <= 64);\n+  match(Set dst (VectorMaskToLong src));\n+  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n+  ins_cost(13 * SVE_COST);\n+  format %{ \"vmask_tolong $dst, $src\\t# vector mask tolong (sve)\" %}\n@@ -5763,4 +5358,5 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n-    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_tolong(as_Register($dst$$reg), as_PRegister($src$$reg),\n+                        Matcher::vector_element_basic_type(this, $src),\n+                        Matcher::vector_length(this, $src),\n+                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg),\n+                        as_PRegister($pgtmp$$reg));\n@@ -5771,5 +5367,31 @@\n-instruct vmask_firsttrue_partial(iRegINoSp dst, pReg src, pReg ptmp1, pReg ptmp2, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskFirstTrue src));\n-  effect(TEMP ptmp1, TEMP ptmp2, KILL cr);\n+\/\/ ---------------------------- Vector mask generation ---------------------------\n+\/\/ The rules below set predicate registers. They can guarantee the high bits of dst\n+\/\/ are cleared with zero when the vector length is less than the full size of\n+\/\/ hardware vector register width.\n+\n+\n+\/\/ maskAll (full or partial predicate size)\n+\n+instruct vmaskAll_immI(pRegGov dst, immI src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue_lanecnt\/sve_pfalse $dst\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n@@ -5777,1 +5399,3 @@\n-  format %{ \"vmask_firsttrue_partial $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_ptrue $dst, vector_length\\n\\t\"\n+            \"sve_cmpne $dst, $dst, $tmp, 0\\t# mask all (sve) (B\/H\/S)\" %}\n@@ -5779,1 +5403,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n@@ -5781,4 +5405,9 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp1$$reg), size,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_brkb(as_PRegister($ptmp2$$reg), as_PRegister($ptmp1$$reg), as_PRegister($src$$reg), false);\n-    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp1$$reg), as_PRegister($ptmp2$$reg));\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    if (length_in_bytes < MaxVectorSize) {\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), size, Matcher::vector_length(this));\n+      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size,\n+                 as_PRegister($dst$$reg), as_FloatRegister($tmp$$reg), 0);\n+    } else {\n+      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+    }\n@@ -5789,7 +5418,5 @@\n-instruct vmask_lasttrue_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskLastTrue src));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"vmask_lasttrue_partial $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n+instruct vmaskAll_immL(pRegGov dst, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue_lanecnt\/sve_pfalse $dst\\t# mask all (sve) (D)\" %}\n@@ -5797,1 +5424,23 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_ptrue $dst, vector_length\\n\\t\"\n+            \"sve_cmpne $dst, $dst, $tmp, 0\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n@@ -5799,3 +5448,9 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($ptmp$$reg), ptrue, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n-    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    if (length_in_bytes < MaxVectorSize) {\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), size, Matcher::vector_length(this));\n+      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size,\n+                 as_PRegister($dst$$reg), as_FloatRegister($tmp$$reg), 0);\n+    } else {\n+      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+    }\n@@ -5806,1 +5461,41 @@\n-instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (length_in_bytes == MaxVectorSize) {\n+      __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+                     as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    } else {\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+      __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($dst$$reg), as_FloatRegister($src1$$reg),\n+                     as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -5808,5 +5503,5 @@\n-            n->in(1)->bottom_type()->is_vect()->length() <= 64);\n-  match(Set dst (VectorMaskToLong src));\n-  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n-  ins_cost(13 * SVE_COST);\n-  format %{ \"vmask_tolong $dst, $src\\t# vector mask tolong (sve)\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -5814,5 +5509,17 @@\n-    __ sve_vmask_tolong(as_Register($dst$$reg), as_PRegister($src$$reg),\n-                        Matcher::vector_element_basic_type(this, $src),\n-                        Matcher::vector_length(this, $src),\n-                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg),\n-                        as_PRegister($pgtmp$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n+               ptrue, as_FloatRegister($src$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -5887,1 +5594,0 @@\n-\/\/ ---------------------------- Vector mask generation ---------------------------\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":572,"deletions":866,"binary":false,"changes":1438,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+  format %{ \"sve_ptrue $pgtmp, vector_length\\n\\t\"\n@@ -249,2 +249,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this));\n@@ -265,1 +265,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+  format %{ \"sve_ptrue $pgtmp, vector_length\\n\\t\"\n@@ -269,2 +269,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $src));\n@@ -282,2 +282,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -296,22 +295,1 @@\n-instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() < MaxVectorSize);\n-  match(Set dst (LoadVectorMasked mem pg));\n-  effect(TEMP pgtmp, KILL cr);\n-  ins_cost(6 * SVE_COST);\n-  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this));\n-    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n-                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -331,76 +309,0 @@\n-instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() < MaxVectorSize);\n-  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n-  effect(TEMP pgtmp, KILL cr);\n-  ins_cost(6 * SVE_COST);\n-  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n-                          Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n-                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-dnl\n-dnl MASKALL_IMM($1,   $2  )\n-dnl MASKALL_IMM(type, size)\n-define(`MASKALL_IMM', `\n-instruct vmaskAll_imm$1(pRegGov dst, imm$1 src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (MaskAll src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_ptrue_lanecnt\/sve_pfalse $dst\\t# mask all (sve) ($2)\" %}\n-  ins_encode %{\n-    ifelse($1, `I', int, long) con = (ifelse($1, `I', int, long))$src$$constant;\n-    if (con == 0) {\n-      __ sve_pfalse(as_PRegister($dst$$reg));\n-    } else {\n-      assert(con == -1, \"invalid constant value for mask\");\n-      BasicType bt = Matcher::vector_element_basic_type(this);\n-      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-                           Matcher::vector_length(this));\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-dnl MASKALL($1,   $2  )\n-dnl MASKALL(type, size)\n-define(`MASKALL', `\n-instruct vmaskAll$1(pRegGov dst, ifelse($1, `I', iRegIorL2I, iRegL) src, vReg tmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (MaskAll src));\n-  effect(TEMP tmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_dup $tmp, $src\\n\\t\"\n-            \"sve_ptrue_lanecnt $dst\\n\\t\"\n-            \"sve_cmpne $dst, $dst, $tmp, 0\\t# mask all (sve) ($2)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n-    if (length_in_bytes < MaxVectorSize) {\n-      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), size, Matcher::vector_length(this));\n-      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size,\n-                 as_PRegister($dst$$reg), as_FloatRegister($tmp$$reg), 0);\n-    } else {\n-      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-\/\/ maskAll (full or partial predicate size)\n-MASKALL_IMM(I, B\/H\/S)\n-MASKALL(I, B\/H\/S)\n-MASKALL_IMM(L, D)\n-MASKALL(L, D)\n-\n@@ -476,1 +378,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n+    __ sve_ptrue_lanecnt(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n@@ -527,2 +429,4 @@\n-  ins_encode %{\n-    __ $4(as_FloatRegister($dst$$reg), __ $3,\n+  ins_encode %{dnl\n+ifelse($1, `vnegI', `\n+    BasicType bt = Matcher::vector_element_basic_type(this);', `')\n+    __ $4(as_FloatRegister($dst$$reg), ifelse($1, `vnegI', `__ elemType_to_regVariant(bt)', `__ $3'),\n@@ -551,2 +455,4 @@\n-  ins_encode %{\n-    __ $4(as_FloatRegister($dst_src$$reg), __ $3,\n+  ins_encode %{dnl\n+ifelse($1, `vnegI', `\n+    BasicType bt = Matcher::vector_element_basic_type(this);', `')\n+    __ $4(as_FloatRegister($dst_src$$reg), ifelse($1, `vnegI', `__ elemType_to_regVariant(bt)', `__ $3'),\n@@ -906,2 +812,2 @@\n-dnl VFMLA($1           $2    $3         )\n-dnl VFMLA(name_suffix, size, min_vec_len)\n+dnl VFMLA($1           $2  )\n+dnl VFMLA(name_suffix, size)\n@@ -923,2 +829,2 @@\n-VFMLA(F, S, 4)\n-VFMLA(D, D, 2)\n+VFMLA(F, S)\n+VFMLA(D, D)\n@@ -927,3 +833,3 @@\n-dnl VFMLA_PREDICATE($1,   $2  )\n-dnl VFMLA_PREDICATE(type, size)\n-define(`VFMLA_PREDICATE', `\n+dnl VFMAD_PREDICATE($1           $2  )\n+dnl VFMAD_PREDICATE(name_suffix, size)\n+define(`VFMAD_PREDICATE', `\n@@ -931,1 +837,1 @@\n-instruct vfmla$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+instruct vfmad$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n@@ -943,3 +849,3 @@\n-\/\/ vector fmla - predicated\n-VFMLA_PREDICATE(F, S)\n-VFMLA_PREDICATE(D, D)\n+\/\/ vector fmad - predicated\n+VFMAD_PREDICATE(F, S)\n+VFMAD_PREDICATE(D, D)\n@@ -948,3 +854,3 @@\n-dnl VFMLS($1           $2    $3         )\n-dnl VFMLS(name_suffix, size, min_vec_len)\n-define(`VFMLS', `\n+dnl VFMLS1($1           $2  )\n+dnl VFMLS1(name_suffix, size)\n+define(`VFMLS1', `\n@@ -952,3 +858,4 @@\n-\/\/ dst_src1 = dst_src1 + src2 * -src3\n-instruct vfmls$1(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfmls`$1'1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n@@ -956,0 +863,17 @@\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ $2,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl VFMLS2($1           $2  )\n+dnl VFMLS2(name_suffix, size)\n+define(`VFMLS2', `\n+\/\/ dst_src1 = dst_src1 + src2 * -src3\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfmls`$1'2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n@@ -967,2 +891,4 @@\n-VFMLS(F, S, 4)\n-VFMLS(D, D, 2)\n+VFMLS1(F, S)\n+VFMLS2(F, S)\n+VFMLS1(D, D)\n+VFMLS2(D, D)\n@@ -971,3 +897,26 @@\n-dnl VFNMLA($1           $2    $3         )\n-dnl VFNMLA(name_suffix, size, min_vec_len)\n-define(`VFNMLA', `\n+dnl VFMSB_PREDICATE($1           $2  )\n+dnl VFMSB_PREDICATE(name_suffix, size)\n+define(`VFMSB_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 * -src2 + src3\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfmsb$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 (NegV$1 src2)) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmsb $dst_src1, $pg, $src2, $src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fmsb(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fmsb - predicated\n+VFMSB_PREDICATE(F, S)\n+VFMSB_PREDICATE(D, D)\n+\n+dnl\n+dnl VFNMLA1($1           $2  )\n+dnl VFNMLA1(name_suffix, size)\n+define(`VFNMLA1', `\n@@ -975,3 +924,5 @@\n-\/\/ dst_src1 = -dst_src1 + src2 * -src3\n-instruct vfnmla$1(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfnmla`$1'1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n@@ -979,0 +930,18 @@\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ $2,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl VFNMLA2($1           $2  )\n+dnl VFNMLA2(name_suffix, size)\n+define(`VFNMLA2', `\n+\/\/ dst_src1 = -dst_src1 + src2 * -src3\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfnmla`$1'2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n@@ -990,2 +959,28 @@\n-VFNMLA(F, S, 4)\n-VFNMLA(D, D, 2)\n+VFNMLA1(F, S)\n+VFNMLA2(F, S)\n+VFNMLA1(D, D)\n+VFNMLA2(D, D)\n+\n+dnl\n+dnl VFNMAD_PREDICATE($1           $2  )\n+dnl VFNMAD_PREDICATE(name_suffix, size)\n+define(`VFNMAD_PREDICATE', `\n+\/\/ dst_src1 = -src3 + dst_src1 * -src2\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfnmad$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 (NegV$1 src2)) (Binary (NegV$1 src3) pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmad $dst_src1, $pg, $src2, $src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fnmad(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fnmad - predicated\n+VFNMAD_PREDICATE(F, S)\n+VFNMAD_PREDICATE(D, D)\n@@ -994,2 +989,2 @@\n-dnl VFNMLS($1           $2    $3         )\n-dnl VFNMLS(name_suffix, size, min_vec_len)\n+dnl VFNMLS($1           $2  )\n+dnl VFNMLS(name_suffix, size)\n@@ -998,0 +993,1 @@\n+\/\/ The NegV$1 must not be predicated.\n@@ -999,1 +995,2 @@\n-  predicate(UseFMA && UseSVE > 0);\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector());\n@@ -1011,2 +1008,2 @@\n-VFNMLS(F, S, 4)\n-VFNMLS(D, D, 2)\n+VFNMLS(F, S)\n+VFNMLS(D, D)\n@@ -1015,2 +1012,25 @@\n-dnl VMLA($1           $2    $3         )\n-dnl VMLA(name_suffix, size, min_vec_len)\n+dnl VFNMSB_PREDICATE($1           $2  )\n+dnl VFNMSB_PREDICATE(name_suffix, size)\n+define(`VFNMSB_PREDICATE', `\n+\/\/ dst_src1 = -src3 + dst_src1 * src2\n+\/\/ The NegV$1 must not be predicated.\n+instruct vfnmsb$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 src2) (Binary (NegV$1 src3) pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmsb $dst_src1, $pg, $src2, $src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fnmsb(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fnmsb - predicated\n+VFNMSB_PREDICATE(F, S)\n+VFNMSB_PREDICATE(D, D)\n+\n+dnl\n+dnl VMLA($1           $2  )\n+dnl VMLA(name_suffix, size)\n@@ -1033,4 +1053,4 @@\n-VMLA(B, B, 16)\n-VMLA(S, H, 8)\n-VMLA(I, S, 4)\n-VMLA(L, D, 2)\n+VMLA(B, B)\n+VMLA(S, H)\n+VMLA(I, S)\n+VMLA(L, D)\n@@ -1039,2 +1059,26 @@\n-dnl VMLS($1           $2    $3         )\n-dnl VMLS(name_suffix, size, min_vec_len)\n+dnl VMLA_PREDICATE($1           $2  )\n+dnl VMLA_PREDICATE(name_suffix, size)\n+define(`VMLA_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmla$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddV$1 (Binary dst_src1 (MulV$1 src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, $pg, src2, src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector mla - predicated\n+VMLA_PREDICATE(B, B)\n+VMLA_PREDICATE(S, H)\n+VMLA_PREDICATE(I, S)\n+VMLA_PREDICATE(L, D)\n+\n+dnl\n+dnl VMLS($1           $2  )\n+dnl VMLS(name_suffix, size)\n@@ -1057,4 +1101,28 @@\n-VMLS(B, B, 16)\n-VMLS(S, H, 8)\n-VMLS(I, S, 4)\n-VMLS(L, D, 2)\n+VMLS(B, B)\n+VMLS(S, H)\n+VMLS(I, S)\n+VMLS(L, D)\n+\n+dnl\n+dnl VMLS_PREDICATE($1           $2  )\n+dnl VMLS_PREDICATE(name_suffix, size)\n+define(`VMLS_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmls$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubV$1 (Binary dst_src1 (MulV$1 src2 src3)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, $pg, src2, src3\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector mls - predicated\n+VMLS_PREDICATE(B, B)\n+VMLS_PREDICATE(S, H)\n+VMLS_PREDICATE(I, S)\n+VMLS_PREDICATE(L, D)\n@@ -1094,1 +1162,3 @@\n-\/\/ vector fneg\n+\/\/ vector neg\n+UNARY_OP_TRUE_PREDICATE(vnegI, NegVI, B\/H\/S, sve_neg)\n+UNARY_OP_TRUE_PREDICATE(vnegL, NegVL, D, sve_neg)\n@@ -1098,1 +1168,3 @@\n-\/\/ vector fneg - predicated\n+\/\/ vector neg - predicated\n+UNARY_OP_PREDICATE(vnegI, NegVI, B\/H\/S, sve_neg)\n+UNARY_OP_PREDICATE(vnegL, NegVL, D, sve_neg)\n@@ -1174,30 +1246,0 @@\n-\/\/ vector mask compare\n-\n-instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n-  effect(KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n-                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n-  effect(KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n-                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -1220,31 +1262,0 @@\n-\/\/ vector load mask\n-\n-instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorLoadMask src));\n-  effect(KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n-  ins_encode %{\n-    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n-               ptrue, as_FloatRegister($src$$reg), 0);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n-  match(Set dst (VectorLoadMask src));\n-  effect(TEMP tmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n-    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -1317,1 +1328,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n@@ -1364,1 +1375,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n@@ -1436,2 +1447,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -1461,2 +1471,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -1498,2 +1507,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ $4, Matcher::vector_length(this, $src2));\n@@ -1512,0 +1520,1 @@\n+       `predicate(UseSVE > 0);',\n@@ -1513,4 +1522,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n-       `predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG);')\n@@ -1535,0 +1541,1 @@\n+       `predicate(UseSVE > 0);',\n@@ -1536,4 +1543,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n-       `predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);')\n@@ -1552,56 +1556,0 @@\n-dnl REDUCE_I_PREDICATE_PARTIAL($1,        $2     )\n-dnl REDUCE_I_PREDICATE_PARTIAL(insn_name, op_name)\n-define(`REDUCE_I_PREDICATE_PARTIAL', `\n-instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  ifelse($2, AddReductionVI,\n-       `predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n-       `predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n-  match(Set dst ($2 (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve) (may extend)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-dnl REDUCE_L_PREDICATE_PARTIAL($1,        $2    )\n-dnl REDUCE_L_PREDICATE_PARTIAL(insn_name, op_name)\n-define(`REDUCE_L_PREDICATE_PARTIAL', `\n-instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  ifelse($2, AddReductionVL,\n-       `predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n-       `predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n-  match(Set dst ($2 (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n@@ -1612,18 +1560,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated (sve)\" %}\n-  ins_encode %{\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n-                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-dnl REDUCE_ADDF_PREDICATE_PARTIAL($1,        $2,      $3,      $4  )\n-dnl REDUCE_ADDF_PREDICATE_PARTIAL(insn_name, op_name, reg_dst, size)\n-define(`REDUCE_ADDF_PREDICATE_PARTIAL', `\n-instruct reduce_$1_masked_partial($3 src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -1631,2 +1562,1 @@\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated partial (sve)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated (sve)\" %}\n@@ -1635,5 +1565,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1661,4 +1587,0 @@\n-REDUCE_I_PREDICATE_PARTIAL(add, AddReductionVI)\n-REDUCE_L_PREDICATE_PARTIAL(add, AddReductionVL)\n-REDUCE_ADDF_PREDICATE_PARTIAL(addF, AddReductionVF, vRegF, S)\n-REDUCE_ADDF_PREDICATE_PARTIAL(addD, AddReductionVD, vRegD, D)\n@@ -1675,2 +1597,0 @@\n-REDUCE_I_PREDICATE_PARTIAL(and, AndReductionV)\n-REDUCE_L_PREDICATE_PARTIAL(and, AndReductionV)\n@@ -1687,2 +1607,0 @@\n-REDUCE_I_PREDICATE_PARTIAL(or, OrReductionV)\n-REDUCE_L_PREDICATE_PARTIAL(or, OrReductionV)\n@@ -1699,2 +1617,0 @@\n-REDUCE_I_PREDICATE_PARTIAL(eor, XorReductionV)\n-REDUCE_L_PREDICATE_PARTIAL(eor, XorReductionV)\n@@ -1759,2 +1675,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), variant, Matcher::vector_length(this, $src2));\n@@ -1781,2 +1696,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src2));\n@@ -1796,1 +1710,0 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1818,1 +1731,0 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1832,51 +1744,0 @@\n-dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL($1     , $2     )\n-dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min_max, op_name)\n-define(`REDUCE_MAXMIN_I_PREDICATE_PARTIAL', `\n-instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n-  match(Set dst ($2 (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL($1     , $2     )\n-dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min_max, op_name)\n-define(`REDUCE_MAXMIN_L_PREDICATE_PARTIAL', `\n-instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst ($2 (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n-                           $src1$$Register, as_FloatRegister($src2$$reg),\n-                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n@@ -1914,2 +1775,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n-                          Matcher::vector_length(this, $src2));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ $4, Matcher::vector_length(this, $src2));\n@@ -1927,2 +1787,1 @@\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3);\n@@ -1940,23 +1799,0 @@\n-dnl REDUCE_FMINMAX_PREDICATE_PARTIAL($1,      $2,          $3,           $4,   $5         )\n-dnl REDUCE_FMINMAX_PREDICATE_PARTIAL(min_max, name_suffix, element_type, size, reg_src_dst)\n-define(`REDUCE_FMINMAX_PREDICATE_PARTIAL', `\n-instruct reduce_$1$2_masked_partial($5 dst, $5 src1, vReg src2, pRegGov pg,\n-                                    pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n-            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n-  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n@@ -1976,2 +1812,0 @@\n-REDUCE_MAXMIN_I_PREDICATE_PARTIAL(max, MaxReductionV)\n-REDUCE_MAXMIN_L_PREDICATE_PARTIAL(max, MaxReductionV)\n@@ -1980,2 +1814,0 @@\n-REDUCE_FMINMAX_PREDICATE_PARTIAL(max, F, T_FLOAT,  S, vRegF)\n-REDUCE_FMINMAX_PREDICATE_PARTIAL(max, D, T_DOUBLE, D, vRegD)\n@@ -1996,2 +1828,0 @@\n-REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min, MinReductionV)\n-REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min, MinReductionV)\n@@ -2000,2 +1830,0 @@\n-REDUCE_FMINMAX_PREDICATE_PARTIAL(min, F, T_FLOAT,  S, vRegF)\n-REDUCE_FMINMAX_PREDICATE_PARTIAL(min, D, T_DOUBLE, D, vRegD)\n@@ -2609,1 +2437,0 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -2627,1 +2454,0 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -2642,28 +2468,0 @@\n-dnl\n-dnl VTEST_PARTIAL($1,      $2,   $3,   $4  )\n-dnl VTEST_PARTIAL(op_name, pred, inst, cond)\n-define(`VTEST_PARTIAL', `\n-instruct vtest_$1_partial`'(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::$2);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"vtest_$1_partial $dst, $src1, $src2\\t# VectorTest partial (sve) - $1\" %}\n-  ins_encode %{\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n-                          Matcher::vector_length(this, $src1));\n-    __ $3(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n-    __ csetw(as_Register($dst$$reg), Assembler::$4);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-VTEST_PARTIAL(alltrue, overflow, sve_eors, EQ)\n-VTEST_PARTIAL(anytrue, ne,       sve_ands, NE)\n-\n@@ -2881,1 +2679,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n@@ -2898,2 +2696,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n@@ -2911,1 +2708,0 @@\n-            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n@@ -2926,1 +2722,0 @@\n-            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n@@ -2940,42 +2735,0 @@\n-\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n-\n-instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n-                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n-                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3027,2 +2780,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this, $src));\n@@ -3045,2 +2797,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src));\n+    __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this, $src));\n@@ -3058,1 +2809,0 @@\n-            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n@@ -3073,1 +2823,0 @@\n-            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n@@ -3087,43 +2836,0 @@\n-\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n-\n-instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n-            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n-                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n-            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n-               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n-                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3171,2 +2877,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -3184,0 +2889,2 @@\n+\/\/ Return the index of the first mask lane that is set, or vector length if none of\n+\/\/ them are set.\n@@ -3185,2 +2892,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -3189,1 +2895,1 @@\n-  ins_cost(2 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -3192,0 +2898,1 @@\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n@@ -3194,1 +2901,8 @@\n-    __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    \/\/ When the input predicate is all-false, the result should be the vector length\n+    \/\/ instead of max vector register size.\n+    if (length_in_bytes == MaxVectorSize) {\n+      __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    } else {\n+      __ sve_ptrue_lanecnt(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+      __ sve_brkb(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), as_PRegister($src$$reg), false);\n+    }\n@@ -3201,2 +2915,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  predicate(UseSVE > 0);\n@@ -3214,1 +2927,1 @@\n-instruct vmask_truecount_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -3216,5 +2929,5 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskTrueCount src));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"vmask_truecount_partial $dst, $src\\t# vector mask truecount partial (sve)\" %}\n+            n->in(1)->bottom_type()->is_vect()->length() <= 64);\n+  match(Set dst (VectorMaskToLong src));\n+  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n+  ins_cost(13 * SVE_COST);\n+  format %{ \"vmask_tolong $dst, $src\\t# vector mask tolong (sve)\" %}\n@@ -3222,4 +2935,5 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n-    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_tolong(as_Register($dst$$reg), as_PRegister($src$$reg),\n+                        Matcher::vector_element_basic_type(this, $src),\n+                        Matcher::vector_length(this, $src),\n+                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg),\n+                        as_PRegister($pgtmp$$reg));\n@@ -3230,5 +2944,30 @@\n-instruct vmask_firsttrue_partial(iRegINoSp dst, pReg src, pReg ptmp1, pReg ptmp2, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskFirstTrue src));\n-  effect(TEMP ptmp1, TEMP ptmp2, KILL cr);\n+\/\/ ---------------------------- Vector mask generation ---------------------------\n+\/\/ The rules below set predicate registers. They can guarantee the high bits of dst\n+\/\/ are cleared with zero when the vector length is less than the full size of\n+\/\/ hardware vector register width.\n+\n+define(`MASKALL_IMM', `\n+instruct vmaskAll_imm$1(pRegGov dst, imm$1 src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue_lanecnt\/sve_pfalse $dst\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    ifelse($1, `I', int, long) con = (ifelse($1, `I', int, long))$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+define(`MASKALL', `\n+instruct vmaskAll$1(pRegGov dst, ifelse($1, `I', iRegIorL2I, iRegL) src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n@@ -3236,1 +2975,3 @@\n-  format %{ \"vmask_firsttrue_partial $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_ptrue $dst, vector_length\\n\\t\"\n+            \"sve_cmpne $dst, $dst, $tmp, 0\\t# mask all (sve) ($2)\" %}\n@@ -3238,1 +2979,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n@@ -3240,4 +2981,39 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp1$$reg), size,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_brkb(as_PRegister($ptmp2$$reg), as_PRegister($ptmp1$$reg), as_PRegister($src$$reg), false);\n-    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp1$$reg), as_PRegister($ptmp2$$reg));\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    if (length_in_bytes < MaxVectorSize) {\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), size, Matcher::vector_length(this));\n+      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size,\n+                 as_PRegister($dst$$reg), as_FloatRegister($tmp$$reg), 0);\n+    } else {\n+      __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ maskAll (full or partial predicate size)\n+MASKALL_IMM(I, B\/H\/S)\n+MASKALL(I, B\/H\/S)\n+MASKALL_IMM(L, D)\n+MASKALL(L, D)\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (length_in_bytes == MaxVectorSize) {\n+      __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+                     as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    } else {\n+      __ sve_ptrue_lanecnt(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+      __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($dst$$reg), as_FloatRegister($src1$$reg),\n+                     as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    }\n@@ -3248,7 +3024,6 @@\n-instruct vmask_lasttrue_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskLastTrue src));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"vmask_lasttrue_partial $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -3256,5 +3031,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n-    __ sve_and(as_PRegister($ptmp$$reg), ptrue, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n-    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n@@ -3265,1 +3038,3 @@\n-instruct vmask_tolong(iRegLNoSp dst, pReg src, vReg vtmp1, vReg vtmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -3267,5 +3042,5 @@\n-            n->in(1)->bottom_type()->is_vect()->length() <= 64);\n-  match(Set dst (VectorMaskToLong src));\n-  effect(TEMP vtmp1, TEMP vtmp2, TEMP pgtmp, KILL cr);\n-  ins_cost(13 * SVE_COST);\n-  format %{ \"vmask_tolong $dst, $src\\t# vector mask tolong (sve)\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -3273,5 +3048,17 @@\n-    __ sve_vmask_tolong(as_Register($dst$$reg), as_PRegister($src$$reg),\n-                        Matcher::vector_element_basic_type(this, $src),\n-                        Matcher::vector_length(this, $src),\n-                        as_FloatRegister($vtmp1$$reg), as_FloatRegister($vtmp2$$reg),\n-                        as_PRegister($pgtmp$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n+               ptrue, as_FloatRegister($src$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -3346,1 +3133,0 @@\n-\/\/ ---------------------------- Vector mask generation ---------------------------\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":382,"deletions":596,"binary":false,"changes":978,"status":"modified"},{"patch":"@@ -3177,2 +3177,5 @@\n-  INSN(sve_mla,   0b00000100, 0, 0b010); \/\/ multiply-add: Zda = Zda + Zn*Zm\n-  INSN(sve_mls,   0b00000100, 0, 0b011); \/\/ multiply-subtract: Zda = Zda + -Zn*Zm\n+  INSN(sve_fmsb,  0b01100101, 1, 0b101); \/\/ floating-point fused multiply-subtract, writing multiplicand: Zda = Zm + -Zda * Zn\n+  INSN(sve_fnmad, 0b01100101, 1, 0b110); \/\/ floating-point negated fused multiply-add, writing multiplicand: Zda = -Zm + -Zda * Zn\n+  INSN(sve_fnmsb, 0b01100101, 1, 0b111); \/\/ floating-point negated fused multiply-subtract, writing multiplicand: Zda = -Zm + Zda * Zn\n+  INSN(sve_mla,   0b00000100, 0, 0b010); \/\/ multiply-add, writing addend: Zda = Zda + Zn*Zm\n+  INSN(sve_mls,   0b00000100, 0, 0b011); \/\/ multiply-subtract, writing addend: Zda = Zda + -Zn*Zm\n@@ -3794,0 +3797,1 @@\n+  static int  operand_valid_for_movi_immediate(uint64_t imm64, SIMD_Arrangement T);\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -94,8 +94,0 @@\n-  \/\/ Generate predicate through whilelo, by comparing ZR with an unsigned\n-  \/\/ immediate. rscratch1 will be clobbered.\n-  inline void sve_whilelo_zr_imm(PRegister pd, SIMD_RegVariant size, uint imm) {\n-    assert(UseSVE > 0, \"not supported\");\n-    mov(rscratch1, imm);\n-    sve_whilelo(pd, size, zr, rscratch1);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1998,0 +1998,7 @@\n+void Assembler::cvtss2sil(Register dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(as_XMMRegister(dst->encoding()), xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x2D, (0xC0 | encode));\n+}\n+\n@@ -2091,0 +2098,15 @@\n+void Assembler::vcvtps2dq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5B, (0xC0 | encode));\n+}\n+\n+void Assembler::evcvtpd2qq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x7B, (0xC0 | encode));\n+}\n+\n@@ -6532,1 +6554,0 @@\n-\n@@ -12483,0 +12504,7 @@\n+void Assembler::cvtsd2siq(Register dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(as_XMMRegister(dst->encoding()), xnoreg, src, VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x2D, (0xC0 | encode));\n+}\n+\n@@ -12536,0 +12564,5 @@\n+void Assembler::divq(Register src) {\n+  int encode = prefixq_and_encode(src->encoding());\n+  emit_int16((unsigned char)0xF7, (0xF0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1152,0 +1152,1 @@\n+  void cvtsd2siq(Register dst, XMMRegister src);\n@@ -1160,0 +1161,1 @@\n+  void cvtss2sil(Register dst, XMMRegister src);\n@@ -1169,0 +1171,1 @@\n+  void vcvtps2dq(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1176,0 +1179,1 @@\n+  void evcvtpd2qq(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1367,0 +1371,1 @@\n+  void divq(Register src); \/\/ Unsigned division\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -51,14 +51,0 @@\n-void C2_MacroAssembler::setvectmask(Register dst, Register src, KRegister mask) {\n-  guarantee(PostLoopMultiversioning, \"must be\");\n-  Assembler::movl(dst, 1);\n-  Assembler::shlxl(dst, dst, src);\n-  Assembler::decl(dst);\n-  Assembler::kmovdl(mask, dst);\n-  Assembler::movl(dst, src);\n-}\n-\n-void C2_MacroAssembler::restorevectmask(KRegister mask) {\n-  guarantee(PostLoopMultiversioning, \"must be\");\n-  Assembler::knotwl(mask, k0);\n-}\n-\n@@ -1950,1 +1936,0 @@\n-  assert(ArrayOperationPartialInlineSize > 0 && ArrayOperationPartialInlineSize <= 64, \"invalid\");\n@@ -4064,7 +4049,6 @@\n- * Algorithm for vector D2L and F2I conversions:-\n- * a) Perform vector D2L\/F2I cast.\n- * b) Choose fast path if none of the result vector lane contains 0x80000000 value.\n- *    It signifies that source value could be any of the special floating point\n- *    values(NaN,-Inf,Inf,Max,-Min).\n- * c) Set destination to zero if source is NaN value.\n- * d) Replace 0x80000000 with MaxInt if source lane contains a +ve value.\n+ * Following routine handles special floating point values(NaN\/Inf\/-Inf\/Max\/Min) for casting operation.\n+ * If src is NaN, the result is 0.\n+ * If the src is negative infinity or any value less than or equal to the value of Integer.MIN_VALUE,\n+ * the result is equal to the value of Integer.MIN_VALUE.\n+ * If the src is positive infinity or any value greater than or equal to the value of Integer.MAX_VALUE,\n+ * the result is equal to the value of Integer.MAX_VALUE.\n@@ -4072,4 +4056,4 @@\n-\n-void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n-                                            Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_cast_float_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                                            Register scratch, AddressLiteral float_sign_flip,\n+                                                            int vec_enc) {\n@@ -4077,22 +4061,0 @@\n-  evcvttpd2qq(dst, src, vec_enc);\n-  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n-  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);\n-  kortestwl(ktmp1, ktmp1);\n-  jccb(Assembler::equal, done);\n-\n-  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n-  evcmppd(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n-  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n-\n-  kxorwl(ktmp1, ktmp1, ktmp2);\n-  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n-  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n-  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n-  bind(done);\n-}\n-\n-void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n-                                           AddressLiteral float_sign_flip, Register scratch, int vec_enc) {\n-  Label done;\n-  vcvttps2dq(dst, src, vec_enc);\n@@ -4123,3 +4085,4 @@\n-void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n-                                            Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_cast_float_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                             XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                             Register scratch, AddressLiteral float_sign_flip,\n+                                                             int vec_enc) {\n@@ -4127,1 +4090,0 @@\n-  vcvttps2dq(dst, src, vec_enc);\n@@ -4144,0 +4106,109 @@\n+\/*\n+ * Following routine handles special floating point values(NaN\/Inf\/-Inf\/Max\/Min) for casting operation.\n+ * If src is NaN, the result is 0.\n+ * If the src is negative infinity or any value less than or equal to the value of Long.MIN_VALUE,\n+ * the result is equal to the value of Long.MIN_VALUE.\n+ * If the src is positive infinity or any value greater than or equal to the value of Long.MAX_VALUE,\n+ * the result is equal to the value of Long.MAX_VALUE.\n+ *\/\n+void C2_MacroAssembler::vector_cast_double_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                              XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                              Register scratch, AddressLiteral double_sign_flip,\n+                                                              int vec_enc) {\n+  Label done;\n+  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n+  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmppd(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n+\/*\n+ * Algorithm for vector D2L and F2I conversions:-\n+ * a) Perform vector D2L\/F2I cast.\n+ * b) Choose fast path if none of the result vector lane contains 0x80000000 value.\n+ *    It signifies that source value could be any of the special floating point\n+ *    values(NaN,-Inf,Inf,Max,-Min).\n+ * c) Set destination to zero if source is NaN value.\n+ * d) Replace 0x80000000 with MaxInt if source lane contains a +ve value.\n+ *\/\n+\n+void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  evcvttpd2qq(dst, src, vec_enc);\n+  vector_cast_double_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                           AddressLiteral float_sign_flip, Register scratch, int vec_enc) {\n+  vcvttps2dq(dst, src, vec_enc);\n+  vector_cast_float_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, scratch, float_sign_flip, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  vcvttps2dq(dst, src, vec_enc);\n+  vector_cast_float_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, float_sign_flip, vec_enc);\n+}\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::vector_round_double_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                 KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                                 AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+  \/\/ Perform floor(val+0.5) operation under the influence of MXCSR.RC mode roundTowards -inf.\n+  \/\/ and re-instantiate original MXCSR.RC mode after that.\n+  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+  ldmxcsr(new_mxcsr, scratch);\n+  mov64(scratch, julong_cast(0.5L));\n+  evpbroadcastq(xtmp1, scratch, vec_enc);\n+  vaddpd(xtmp1, src , xtmp1, vec_enc);\n+  evcvtpd2qq(dst, xtmp1, vec_enc);\n+  vector_cast_double_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n+  ldmxcsr(mxcsr_std, scratch);\n+}\n+\n+void C2_MacroAssembler::vector_round_float_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                                                AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+  \/\/ Perform floor(val+0.5) operation under the influence of MXCSR.RC mode roundTowards -inf.\n+  \/\/ and re-instantiate original MXCSR.RC mode after that.\n+  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+  ldmxcsr(new_mxcsr, scratch);\n+  movl(scratch, jint_cast(0.5));\n+  movq(xtmp1, scratch);\n+  vbroadcastss(xtmp1, xtmp1, vec_enc);\n+  vaddps(xtmp1, src , xtmp1, vec_enc);\n+  vcvtps2dq(dst, xtmp1, vec_enc);\n+  vector_cast_float_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, float_sign_flip, vec_enc);\n+  ldmxcsr(mxcsr_std, scratch);\n+}\n+\n+void C2_MacroAssembler::vector_round_float_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                               XMMRegister xtmp3, XMMRegister xtmp4, AddressLiteral float_sign_flip,\n+                                               AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+  \/\/ Perform floor(val+0.5) operation under the influence of MXCSR.RC mode roundTowards -inf.\n+  \/\/ and re-instantiate original MXCSR.RC mode after that.\n+  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+  ldmxcsr(new_mxcsr, scratch);\n+  movl(scratch, jint_cast(0.5));\n+  movq(xtmp1, scratch);\n+  vbroadcastss(xtmp1, xtmp1, vec_enc);\n+  vaddps(xtmp1, src , xtmp1, vec_enc);\n+  vcvtps2dq(dst, xtmp1, vec_enc);\n+  vector_cast_float_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, scratch, float_sign_flip, vec_enc);\n+  ldmxcsr(mxcsr_std, scratch);\n+}\n+#endif\n+\n@@ -4726,0 +4797,164 @@\n+\n+void C2_MacroAssembler::udivI(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpl(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divl(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Integer.SIZE - 1)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.divideUnsigned()\n+  movl(rdx, rax);\n+  subl(rdx, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnl(rax, rdx, rax);\n+  } else {\n+    notl(rdx);\n+    andl(rax, rdx);\n+  }\n+  shrl(rax, 31);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::umodI(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpl(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divl(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath when divisor < 0:\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Integer.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.remainderUnsigned()\n+  movl(rdx, rax);\n+  subl(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnl(rax, rax, rdx);\n+  } else {\n+    notl(rax);\n+    andl(rax, rdx);\n+  }\n+  sarl(rax, 31);\n+  andl(rax, divisor);\n+  subl(rdx, rax);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::udivmodI(Register rax, Register divisor, Register rdx, Register tmp) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+\n+  cmpl(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divl(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Integer.SIZE - 1)\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Integer.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in\n+  \/\/ java.lang.Long.divideUnsigned() and java.lang.Long.remainderUnsigned()\n+  movl(rdx, rax);\n+  subl(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnl(rax, rax, rdx);\n+  } else {\n+    notl(rax);\n+    andl(rax, rdx);\n+  }\n+  movl(tmp, rax);\n+  shrl(rax, 31); \/\/ quotient\n+  sarl(tmp, 31);\n+  andl(tmp, divisor);\n+  subl(rdx, tmp); \/\/ remainder\n+  bind(done);\n+}\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::udivL(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpq(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divq(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Long.SIZE - 1)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.divideUnsigned()\n+  movq(rdx, rax);\n+  subq(rdx, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnq(rax, rdx, rax);\n+  } else {\n+    notq(rdx);\n+    andq(rax, rdx);\n+  }\n+  shrq(rax, 63);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::umodL(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpq(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorq(rdx, rdx);\n+  divq(divisor);\n+  jmp(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath when divisor < 0:\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Long.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.remainderUnsigned()\n+  movq(rdx, rax);\n+  subq(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnq(rax, rax, rdx);\n+  } else {\n+    notq(rax);\n+    andq(rax, rdx);\n+  }\n+  sarq(rax, 63);\n+  andq(rax, divisor);\n+  subq(rdx, rax);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::udivmodL(Register rax, Register divisor, Register rdx, Register tmp) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpq(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorq(rdx, rdx);\n+  divq(divisor);\n+  jmp(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Long.SIZE - 1)\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Long.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in\n+  \/\/ java.lang.Long.divideUnsigned() and java.lang.Long.remainderUnsigned()\n+  movq(rdx, rax);\n+  subq(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnq(rax, rax, rdx);\n+  } else {\n+    notq(rax);\n+    andq(rax, rdx);\n+  }\n+  movq(tmp, rax);\n+  shrq(rax, 63); \/\/ quotient\n+  sarq(tmp, 63);\n+  andq(tmp, divisor);\n+  subq(rdx, tmp); \/\/ remainder\n+  bind(done);\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":287,"deletions":52,"binary":false,"changes":339,"status":"modified"},{"patch":"@@ -33,4 +33,0 @@\n-  \/\/ special instructions for EVEX\n-  void setvectmask(Register dst, Register src, KRegister mask);\n-  void restorevectmask(KRegister mask);\n-\n@@ -311,0 +307,1 @@\n+\n@@ -318,0 +315,27 @@\n+  void vector_cast_double_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                             KRegister ktmp1, KRegister ktmp2, Register scratch, AddressLiteral double_sign_flip,\n+                                             int vec_enc);\n+\n+  void vector_cast_float_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, Register scratch, AddressLiteral float_sign_flip,\n+                                            int vec_enc);\n+\n+  void vector_cast_float_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                           Register scratch, AddressLiteral float_sign_flip,\n+                                           int vec_enc);\n+\n+#ifdef _LP64\n+  void vector_round_double_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+\n+  void vector_round_float_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                               KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                               AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+\n+  void vector_round_float_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                              XMMRegister xtmp3, XMMRegister xtmp4, AddressLiteral float_sign_flip,\n+                              AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+#endif\n+\n@@ -332,0 +356,10 @@\n+  void udivI(Register rax, Register divisor, Register rdx);\n+  void umodI(Register rax, Register divisor, Register rdx);\n+  void udivmodI(Register rax, Register divisor, Register rdx, Register tmp);\n+\n+#ifdef _LP64\n+  void udivL(Register rax, Register divisor, Register rdx);\n+  void umodL(Register rax, Register divisor, Register rdx);\n+  void udivmodL(Register rax, Register divisor, Register rdx, Register tmp);\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":38,"deletions":4,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"c1\/c1_FrameMap.hpp\"\n@@ -2256,1 +2255,1 @@\n-void MacroAssembler::ldmxcsr(AddressLiteral src) {\n+void MacroAssembler::ldmxcsr(AddressLiteral src, Register scratchReg) {\n@@ -2260,2 +2259,2 @@\n-    lea(rscratch1, src);\n-    Assembler::ldmxcsr(Address(rscratch1, 0));\n+    lea(scratchReg, src);\n+    Assembler::ldmxcsr(Address(scratchReg, 0));\n@@ -3608,0 +3607,1 @@\n+  int num_xmm_registers = XMMRegisterImpl::available_xmm_registers();\n@@ -3610,2 +3610,2 @@\n-  if (FrameMap::get_num_caller_save_xmms() > 16) {\n-     result += XMMRegSet::range(xmm16, as_XMMRegister(FrameMap::get_num_caller_save_xmms() - 1));\n+  if (num_xmm_registers > 16) {\n+     result += XMMRegSet::range(xmm16, as_XMMRegister(num_xmm_registers - 1));\n@@ -3615,1 +3615,1 @@\n-  return XMMRegSet::range(xmm0, as_XMMRegister(FrameMap::get_num_caller_save_xmms() - 1));\n+  return XMMRegSet::range(xmm0, as_XMMRegister(num_xmm_registers - 1));\n@@ -4685,10 +4685,0 @@\n-  \/\/ Reset k1 to 0xffff.\n-\n-#ifdef COMPILER2\n-  if (PostLoopMultiversioning && VM_Version::supports_evex()) {\n-    push(rcx);\n-    movl(rcx, 0xffff);\n-    kmovwl(k1, rcx);\n-    pop(rcx);\n-  }\n-#endif \/\/ COMPILER2\n@@ -9134,0 +9124,74 @@\n+void MacroAssembler::round_float(Register dst, XMMRegister src, Register rtmp, Register rcx) {\n+  \/\/ Following code is line by line assembly translation rounding algorithm.\n+  \/\/ Please refer to java.lang.Math.round(float) algorithm for details.\n+  const int32_t FloatConsts_EXP_BIT_MASK = 0x7F800000;\n+  const int32_t FloatConsts_SIGNIFICAND_WIDTH = 24;\n+  const int32_t FloatConsts_EXP_BIAS = 127;\n+  const int32_t FloatConsts_SIGNIF_BIT_MASK = 0x007FFFFF;\n+  const int32_t MINUS_32 = 0xFFFFFFE0;\n+  Label L_special_case, L_block1, L_exit;\n+  movl(rtmp, FloatConsts_EXP_BIT_MASK);\n+  movdl(dst, src);\n+  andl(dst, rtmp);\n+  sarl(dst, FloatConsts_SIGNIFICAND_WIDTH - 1);\n+  movl(rtmp, FloatConsts_SIGNIFICAND_WIDTH - 2 + FloatConsts_EXP_BIAS);\n+  subl(rtmp, dst);\n+  movl(rcx, rtmp);\n+  movl(dst, MINUS_32);\n+  testl(rtmp, dst);\n+  jccb(Assembler::notEqual, L_special_case);\n+  movdl(dst, src);\n+  andl(dst, FloatConsts_SIGNIF_BIT_MASK);\n+  orl(dst, FloatConsts_SIGNIF_BIT_MASK + 1);\n+  movdl(rtmp, src);\n+  testl(rtmp, rtmp);\n+  jccb(Assembler::greaterEqual, L_block1);\n+  negl(dst);\n+  bind(L_block1);\n+  sarl(dst);\n+  addl(dst, 0x1);\n+  sarl(dst, 0x1);\n+  jmp(L_exit);\n+  bind(L_special_case);\n+  convert_f2i(dst, src);\n+  bind(L_exit);\n+}\n+\n+void MacroAssembler::round_double(Register dst, XMMRegister src, Register rtmp, Register rcx) {\n+  \/\/ Following code is line by line assembly translation rounding algorithm.\n+  \/\/ Please refer to java.lang.Math.round(double) algorithm for details.\n+  const int64_t DoubleConsts_EXP_BIT_MASK = 0x7FF0000000000000L;\n+  const int64_t DoubleConsts_SIGNIFICAND_WIDTH = 53;\n+  const int64_t DoubleConsts_EXP_BIAS = 1023;\n+  const int64_t DoubleConsts_SIGNIF_BIT_MASK = 0x000FFFFFFFFFFFFFL;\n+  const int64_t MINUS_64 = 0xFFFFFFFFFFFFFFC0L;\n+  Label L_special_case, L_block1, L_exit;\n+  mov64(rtmp, DoubleConsts_EXP_BIT_MASK);\n+  movq(dst, src);\n+  andq(dst, rtmp);\n+  sarq(dst, DoubleConsts_SIGNIFICAND_WIDTH - 1);\n+  mov64(rtmp, DoubleConsts_SIGNIFICAND_WIDTH - 2 + DoubleConsts_EXP_BIAS);\n+  subq(rtmp, dst);\n+  movq(rcx, rtmp);\n+  mov64(dst, MINUS_64);\n+  testq(rtmp, dst);\n+  jccb(Assembler::notEqual, L_special_case);\n+  movq(dst, src);\n+  mov64(rtmp, DoubleConsts_SIGNIF_BIT_MASK);\n+  andq(dst, rtmp);\n+  mov64(rtmp, DoubleConsts_SIGNIF_BIT_MASK + 1);\n+  orq(dst, rtmp);\n+  movq(rtmp, src);\n+  testq(rtmp, rtmp);\n+  jccb(Assembler::greaterEqual, L_block1);\n+  negq(dst);\n+  bind(L_block1);\n+  sarq(dst);\n+  addq(dst, 0x1);\n+  sarq(dst, 0x1);\n+  jmp(L_exit);\n+  bind(L_special_case);\n+  convert_d2l(dst, src);\n+  bind(L_exit);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":81,"deletions":17,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -909,1 +909,1 @@\n-  void ldmxcsr(AddressLiteral src);\n+  void ldmxcsr(AddressLiteral src, Register scratchReg = rscratch1);\n@@ -2002,0 +2002,2 @@\n+  void round_double(Register dst, XMMRegister src, Register rtmp, Register rcx);\n+  void round_float(Register dst, XMMRegister src, Register rtmp, Register rcx);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3748,34 +3748,0 @@\n-  \/\/ Safefetch stubs.\n-  void generate_safefetch(const char* name, int size, address* entry,\n-                          address* fault_pc, address* continuation_pc) {\n-    \/\/ safefetch signatures:\n-    \/\/   int      SafeFetch32(int*      adr, int      errValue);\n-    \/\/   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Entry point, pc or function descriptor.\n-    *entry = __ pc();\n-\n-    __ movl(rax, Address(rsp, 0x8));\n-    __ movl(rcx, Address(rsp, 0x4));\n-    \/\/ Load *adr into eax, may fault.\n-    *fault_pc = __ pc();\n-    switch (size) {\n-      case 4:\n-        \/\/ int32_t\n-        __ movl(rax, Address(rcx, 0));\n-        break;\n-      case 8:\n-        \/\/ int64_t\n-        Unimplemented();\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-    }\n-\n-    \/\/ Return errValue or *adr.\n-    *continuation_pc = __ pc();\n-    __ ret(0);\n-  }\n-\n@@ -4080,8 +4046,0 @@\n-\n-    \/\/ Safefetch stubs.\n-    generate_safefetch(\"SafeFetch32\", sizeof(int), &StubRoutines::_safefetch32_entry,\n-                                                   &StubRoutines::_safefetch32_fault_pc,\n-                                                   &StubRoutines::_safefetch32_continuation_pc);\n-    StubRoutines::_safefetchN_entry           = StubRoutines::_safefetch32_entry;\n-    StubRoutines::_safefetchN_fault_pc        = StubRoutines::_safefetch32_fault_pc;\n-    StubRoutines::_safefetchN_continuation_pc = StubRoutines::_safefetch32_continuation_pc;\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -3899,40 +3899,0 @@\n-  \/\/ Safefetch stubs.\n-  void generate_safefetch(const char* name, int size, address* entry,\n-                          address* fault_pc, address* continuation_pc) {\n-    \/\/ safefetch signatures:\n-    \/\/   int      SafeFetch32(int*      adr, int      errValue);\n-    \/\/   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);\n-    \/\/\n-    \/\/ arguments:\n-    \/\/   c_rarg0 = adr\n-    \/\/   c_rarg1 = errValue\n-    \/\/\n-    \/\/ result:\n-    \/\/   PPC_RET  = *adr or errValue\n-\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Entry point, pc or function descriptor.\n-    *entry = __ pc();\n-\n-    \/\/ Load *adr into c_rarg1, may fault.\n-    *fault_pc = __ pc();\n-    switch (size) {\n-      case 4:\n-        \/\/ int32_t\n-        __ movl(c_rarg1, Address(c_rarg0, 0));\n-        break;\n-      case 8:\n-        \/\/ int64_t\n-        __ movq(c_rarg1, Address(c_rarg0, 0));\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-    }\n-\n-    \/\/ return errValue or *adr\n-    *continuation_pc = __ pc();\n-    __ movq(rax, c_rarg1);\n-    __ ret(0);\n-  }\n-\n@@ -7735,8 +7695,0 @@\n-\n-    \/\/ Safefetch stubs.\n-    generate_safefetch(\"SafeFetch32\", sizeof(int),     &StubRoutines::_safefetch32_entry,\n-                                                       &StubRoutines::_safefetch32_fault_pc,\n-                                                       &StubRoutines::_safefetch32_continuation_pc);\n-    generate_safefetch(\"SafeFetchN\", sizeof(intptr_t), &StubRoutines::_safefetchN_entry,\n-                                                       &StubRoutines::_safefetchN_fault_pc,\n-                                                       &StubRoutines::_safefetchN_continuation_pc);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -1471,0 +1471,10 @@\n+    case Op_RoundVF:\n+      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVD:\n+      if (UseAVX < 3) {\n+        return false;  \/\/ enabled for AVX3 only\n+      }\n+      break;\n@@ -1575,0 +1585,6 @@\n+    case Op_RoundF:\n+    case Op_RoundD:\n+      if (!is_LP64) {\n+        return false;\n+      }\n+      break;\n@@ -1836,0 +1852,5 @@\n+    case Op_RoundVD:\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n@@ -2869,17 +2890,0 @@\n-\/\/ =================================EVEX special===============================\n-\/\/ Existing partial implementation for post-loop multi-versioning computes\n-\/\/ the mask corresponding to tail loop in K1 opmask register. This may then be\n-\/\/ used for predicating instructions in loop body during last post-loop iteration.\n-\/\/ TODO: Remove hard-coded K1 usage while fixing existing post-loop\n-\/\/ multiversioning support.\n-instruct setMask(rRegI dst, rRegI src, kReg_K1 mask) %{\n-  predicate(PostLoopMultiversioning && Matcher::has_predicated_vectors());\n-  match(Set dst (SetVectMaskI  src));\n-  effect(TEMP dst);\n-  format %{ \"setvectmask   $dst, $src\" %}\n-  ins_encode %{\n-    __ setvectmask($dst$$Register, $src$$Register, $mask$$KRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -7217,1 +7221,2 @@\n-instruct vcastFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+\n+instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n@@ -7223,1 +7228,1 @@\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3 and $xtmp4 as TEMP\" %}\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n@@ -7233,1 +7238,1 @@\n-instruct vcastFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n@@ -7239,1 +7244,1 @@\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n@@ -7260,1 +7265,1 @@\n-instruct vcastDtoL_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castDtoL_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n@@ -7264,1 +7269,1 @@\n-  format %{ \"vector_cast_d2l $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  format %{ \"vector_cast_d2l $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n@@ -7290,0 +7295,50 @@\n+#ifdef _LP64\n+instruct vround_float_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n) < 64 &&\n+            Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n+  format %{ \"vector_round_float $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    __ vector_round_float_avx($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                              $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,\n+                              ExternalAddress(vector_float_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vround_float_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate((VM_Version::supports_avx512vl() ||\n+             Matcher::vector_length_in_bytes(n) == 64) &&\n+             Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_round_float $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    __ vector_round_float_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                               $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                               ExternalAddress(vector_float_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vround_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (RoundVD src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_round_long $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    __ vector_round_double_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                                ExternalAddress(vector_double_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":78,"deletions":23,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -4215,1 +4215,1 @@\n-    \"NegVF\",\"NegVD\",\"NegVI\",\n+    \"NegVF\",\"NegVD\",\"NegVI\",\"NegVL\",\n@@ -4244,0 +4244,1 @@\n+    \"RoundVF\", \"RoundVD\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -137,0 +137,1 @@\n+  do_name(round_name, \"round\")                                                                                          \\\n@@ -188,0 +189,2 @@\n+  do_intrinsic(_roundD,                   java_lang_Math,         round_name,         double_long_signature,     F_S)   \\\n+  do_intrinsic(_roundF,                   java_lang_Math,         round_name,         float_int_signature,       F_S)   \\\n@@ -216,0 +219,6 @@\n+  do_intrinsic(_divideUnsigned_i,         java_lang_Integer,      divideUnsigned_name,     int2_int_signature,   F_S)   \\\n+  do_intrinsic(_remainderUnsigned_i,      java_lang_Integer,      remainderUnsigned_name,  int2_int_signature,   F_S)   \\\n+    do_name(    divideUnsigned_name,                                   \"divideUnsigned\")                                \\\n+  do_intrinsic(_divideUnsigned_l,         java_lang_Long,         divideUnsigned_name,     long2_long_signature, F_S)   \\\n+  do_intrinsic(_remainderUnsigned_l,      java_lang_Long,         remainderUnsigned_name,  long2_long_signature, F_S)   \\\n+    do_name(    remainderUnsigned_name,                                \"remainderUnsigned\")                             \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -270,0 +270,12 @@\n+  case vmIntrinsics::_divideUnsigned_i:\n+    if (!Matcher::match_rule_supported(Op_UDivI)) return false;\n+    break;\n+  case vmIntrinsics::_remainderUnsigned_i:\n+    if (!Matcher::match_rule_supported(Op_UModI)) return false;\n+    break;\n+  case vmIntrinsics::_divideUnsigned_l:\n+    if (!Matcher::match_rule_supported(Op_UDivL)) return false;\n+    break;\n+  case vmIntrinsics::_remainderUnsigned_l:\n+    if (!Matcher::match_rule_supported(Op_UModL)) return false;\n+    break;\n@@ -530,0 +542,2 @@\n+  case vmIntrinsics::_roundD:\n+  case vmIntrinsics::_roundF:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -170,0 +170,2 @@\n+macro(UDivI)\n+macro(UDivL)\n@@ -173,0 +175,2 @@\n+macro(UDivModI)\n+macro(UDivModL)\n@@ -222,1 +226,0 @@\n-macro(SetVectMaskI)\n@@ -240,0 +243,2 @@\n+macro(UModI)\n+macro(UModL)\n@@ -322,0 +327,2 @@\n+macro(RoundF)\n+macro(RoundD)\n@@ -397,0 +404,1 @@\n+macro(NegVL)\n@@ -459,0 +467,2 @@\n+macro(RoundVF)\n+macro(RoundVD)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -24,1 +24,0 @@\n-\n@@ -3518,0 +3517,30 @@\n+  case Op_UModI:\n+    if (UseDivMod) {\n+      \/\/ Check if a%b and a\/b both exist\n+      Node* d = n->find_similar(Op_UDivI);\n+      if (d) {\n+        \/\/ Replace them with a fused unsigned divmod if supported\n+        if (Matcher::has_match_rule(Op_UDivModI)) {\n+          UDivModINode* divmod = UDivModINode::make(n);\n+          d->subsume_by(divmod->div_proj(), this);\n+          n->subsume_by(divmod->mod_proj(), this);\n+        }\n+      }\n+    }\n+    break;\n+\n+  case Op_UModL:\n+    if (UseDivMod) {\n+      \/\/ Check if a%b and a\/b both exist\n+      Node* d = n->find_similar(Op_UDivL);\n+      if (d) {\n+        \/\/ Replace them with a fused unsigned divmod if supported\n+        if (Matcher::has_match_rule(Op_UDivModL)) {\n+          UDivModLNode* divmod = UDivModLNode::make(n);\n+          d->subsume_by(divmod->div_proj(), this);\n+          n->subsume_by(divmod->mod_proj(), this);\n+        }\n+      }\n+    }\n+    break;\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":30,"deletions":1,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -272,0 +272,2 @@\n+  case vmIntrinsics::_roundF:\n+  case vmIntrinsics::_roundD:\n@@ -528,0 +530,5 @@\n+  case vmIntrinsics::_divideUnsigned_i:\n+  case vmIntrinsics::_divideUnsigned_l:\n+  case vmIntrinsics::_remainderUnsigned_i:\n+  case vmIntrinsics::_remainderUnsigned_l:      return inline_divmod_methods(intrinsic_id());\n+\n@@ -1610,0 +1617,1 @@\n+\/\/ public static double Math.round(double)\n@@ -1621,0 +1629,1 @@\n+  case vmIntrinsics::_roundD: n = new RoundDNode(arg); break;\n@@ -1642,0 +1651,1 @@\n+  case vmIntrinsics::_roundF: n = new RoundFNode(arg); break;\n@@ -1757,0 +1767,1 @@\n+  case vmIntrinsics::_roundD: return Matcher::match_rule_supported(Op_RoundD) ? inline_double_math(id) : false;\n@@ -1760,0 +1771,1 @@\n+\n@@ -1779,0 +1791,1 @@\n+  case vmIntrinsics::_roundF: return Matcher::match_rule_supported(Op_RoundF) ? inline_math(id) : false;\n@@ -2185,0 +2198,50 @@\n+\/\/--------------------------inline_unsigned_divmod_methods-----------------------------\n+\/\/ inline int Integer.divideUnsigned(int, int)\n+\/\/ inline int Integer.remainderUnsigned(int, int)\n+\/\/ inline long Long.divideUnsigned(long, long)\n+\/\/ inline long Long.remainderUnsigned(long, long)\n+bool LibraryCallKit::inline_divmod_methods(vmIntrinsics::ID id) {\n+  Node* n = NULL;\n+  switch (id) {\n+    case vmIntrinsics::_divideUnsigned_i: {\n+      zero_check_int(argument(1));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UDivINode(control(), argument(0), argument(1));\n+      break;\n+    }\n+    case vmIntrinsics::_divideUnsigned_l: {\n+      zero_check_long(argument(2));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UDivLNode(control(), argument(0), argument(2));\n+      break;\n+    }\n+    case vmIntrinsics::_remainderUnsigned_i: {\n+      zero_check_int(argument(1));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UModINode(control(), argument(0), argument(1));\n+      break;\n+    }\n+    case vmIntrinsics::_remainderUnsigned_l: {\n+      zero_check_long(argument(2));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UModLNode(control(), argument(0), argument(2));\n+      break;\n+    }\n+    default:  fatal_unexpected_iid(id);  break;\n+  }\n+  set_result(_gvn.transform(n));\n+  return true;\n+}\n+\n@@ -6560,1 +6623,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6566,1 +6629,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6572,1 +6635,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6578,1 +6641,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[J\");\n+    state = get_state_from_digest_object(digestBase_obj, T_LONG);\n@@ -6584,1 +6647,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[B\");\n+    state = get_state_from_digest_object(digestBase_obj, T_BYTE);\n@@ -6648,1 +6711,1 @@\n-  const char* state_type = \"[I\";\n+  BasicType elem_type = T_INT;\n@@ -6677,1 +6740,1 @@\n-      state_type = \"[J\";\n+      elem_type = T_LONG;\n@@ -6685,1 +6748,1 @@\n-      state_type = \"[B\";\n+      elem_type = T_BYTE;\n@@ -6703,1 +6766,1 @@\n-    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, state_type, stub_addr, stub_name, src_start, ofs, limit);\n+    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, elem_type, stub_addr, stub_name, src_start, ofs, limit);\n@@ -6710,1 +6773,1 @@\n-                                                      const char* state_type, address stubAddr, const char *stubName,\n+                                                      BasicType elem_type, address stubAddr, const char *stubName,\n@@ -6717,1 +6780,1 @@\n-  Node* state = get_state_from_digest_object(digest_obj, state_type);\n+  Node* state = get_state_from_digest_object(digest_obj, elem_type);\n@@ -6877,1 +6940,8 @@\n-Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, const char *state_type) {\n+Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, BasicType elem_type) {\n+  const char* state_type;\n+  switch (elem_type) {\n+    case T_BYTE: state_type = \"[B\"; break;\n+    case T_INT:  state_type = \"[I\"; break;\n+    case T_LONG: state_type = \"[J\"; break;\n+    default: ShouldNotReachHere();\n+  }\n@@ -6883,1 +6953,1 @@\n-  Node* state = array_element_address(digest_state, intcon(0), T_INT);\n+  Node* state = array_element_address(digest_state, intcon(0), elem_type);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":84,"deletions":14,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -267,0 +267,1 @@\n+  bool inline_divmod_methods(vmIntrinsics::ID id);\n@@ -284,1 +285,1 @@\n-                                        const char* state_type, address stubAddr, const char *stubName,\n+                                        BasicType elem_type, address stubAddr, const char *stubName,\n@@ -286,1 +287,1 @@\n-  Node* get_state_from_digest_object(Node *digestBase_object, const char* state_type);\n+  Node* get_state_from_digest_object(Node *digestBase_object, BasicType elem_type);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -973,0 +973,4 @@\n+      case Op_RoundF: body_size += 30; break;\n+      case Op_RoundD: body_size += 30; break;\n+      case Op_RoundVF: body_size += 30; break;\n+      case Op_RoundVD: body_size += 30; break;\n@@ -3674,1 +3678,2 @@\n-    if (should_unroll && !should_peel && PostLoopMultiversioning) {\n+    if (should_unroll && !should_peel && PostLoopMultiversioning &&\n+        Matcher::has_predicated_vectors()) {\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -183,6 +183,0 @@\n-\/\/ The type of all node counts and indexes.\n-\/\/ It must hold at least 16 bits, but must also be fast to load and store.\n-\/\/ This type, if less than 32 bits, could limit the number of possible nodes.\n-\/\/ (To make this type platform-specific, move to globalDefinitions_xxx.hpp.)\n-typedef unsigned int node_idx_t;\n-\n@@ -1872,0 +1866,8 @@\n+inline int Op_ConIL(BasicType bt) {\n+  assert(bt == T_INT || bt == T_LONG, \"only for int or longs\");\n+  if (bt == T_INT) {\n+    return Op_ConI;\n+  }\n+  return Op_ConL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -203,0 +203,10 @@\n+#endif\n+      return false;\n+    }\n+  } else if (VectorNode::is_vector_integral_negate(sopc)) {\n+    if (!VectorNode::is_vector_integral_negate_supported(sopc, num_elem, type, false)) {\n+#ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** Rejected vector op (%s,%s,%d) because architecture does not support integral vector negate\",\n+                      NodeClassNames[sopc], type2name(type), num_elem);\n+      }\n@@ -283,2 +293,10 @@\n-    if (!Matcher::has_predicated_vectors() ||\n-        !Matcher::match_rule_supported_vector_masked(sopc, num_elem, type)) {\n+    bool is_supported = false;\n+    if (Matcher::has_predicated_vectors()) {\n+      if (VectorNode::is_vector_integral_negate(sopc)) {\n+        is_supported = VectorNode::is_vector_integral_negate_supported(sopc, num_elem, type, true);\n+      } else {\n+        is_supported = Matcher::match_rule_supported_vector_masked(sopc, num_elem, type);\n+      }\n+    }\n+\n+    if (!is_supported) {\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -138,1 +138,8 @@\n-    return (bt == T_INT ? Op_NegVI : 0);\n+    switch (bt) {\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT: return Op_NegVI;\n+      default: return 0;\n+    }\n+  case Op_NegL:\n+    return (bt == T_LONG ? Op_NegVL : 0);\n@@ -153,0 +160,4 @@\n+  case Op_RoundF:\n+    return (bt == T_INT ? Op_RoundVF : 0);\n+  case Op_RoundD:\n+    return (bt == T_LONG ? Op_RoundVD : 0);\n@@ -295,0 +306,3 @@\n+    if (VectorNode::is_vector_integral_negate(vopc)) {\n+      return is_vector_integral_negate_supported(vopc, vlen, bt, false);\n+    }\n@@ -369,0 +383,32 @@\n+\/\/ Check whether the architecture supports the vector negate instructions. If not, then check\n+\/\/ whether the alternative vector nodes used to implement vector negation are supported.\n+\/\/ Return false if neither of them is supported.\n+bool VectorNode::is_vector_integral_negate_supported(int opc, uint vlen, BasicType bt, bool use_predicate) {\n+  if (!use_predicate) {\n+    \/\/ Check whether the NegVI\/L is supported by the architecture.\n+    if (Matcher::match_rule_supported_vector(opc, vlen, bt)) {\n+      return true;\n+    }\n+    \/\/ Negate is implemented with \"(SubVI\/L (ReplicateI\/L 0) src)\", if NegVI\/L is not supported.\n+    int sub_opc = (bt == T_LONG) ? Op_SubL : Op_SubI;\n+    if (Matcher::match_rule_supported_vector(VectorNode::opcode(sub_opc, bt), vlen, bt) &&\n+        Matcher::match_rule_supported_vector(VectorNode::replicate_opcode(bt), vlen, bt)) {\n+      return true;\n+    }\n+  } else {\n+    \/\/ Check whether the predicated NegVI\/L is supported by the architecture.\n+    if (Matcher::match_rule_supported_vector_masked(opc, vlen, bt)) {\n+      return true;\n+    }\n+    \/\/ Predicated negate is implemented with \"(AddVI\/L (XorV src (ReplicateI\/L -1)) (ReplicateI\/L 1))\",\n+    \/\/ if predicated NegVI\/L is not supported.\n+    int add_opc = (bt == T_LONG) ? Op_AddL : Op_AddI;\n+    if (Matcher::match_rule_supported_vector_masked(Op_XorV, vlen, bt) &&\n+        Matcher::match_rule_supported_vector_masked(VectorNode::opcode(add_opc, bt), vlen, bt) &&\n+        Matcher::match_rule_supported_vector(VectorNode::replicate_opcode(bt), vlen, bt)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -556,0 +602,1 @@\n+  case Op_NegVL: return new NegVLNode(n1, vt);\n@@ -565,0 +612,3 @@\n+  case Op_RoundVF: return new RoundVFNode(n1, vt);\n+  case Op_RoundVD: return new RoundVDNode(n1, vt);\n+\n@@ -696,0 +746,4 @@\n+bool VectorNode::is_vector_integral_negate(int opc) {\n+  return opc == Op_NegVI || opc == Op_NegVL;\n+}\n+\n@@ -1570,0 +1624,63 @@\n+\/\/ Generate other vector nodes to implement the masked\/non-masked vector negation.\n+Node* NegVNode::degenerate_integral_negate(PhaseGVN* phase, bool is_predicated) {\n+  const TypeVect* vt = vect_type();\n+  BasicType bt = vt->element_basic_type();\n+  uint vlen = length();\n+\n+  \/\/ Transformation for predicated NegVI\/L\n+  if (is_predicated) {\n+      \/\/ (NegVI\/L src m) ==> (AddVI\/L (XorV src (ReplicateI\/L -1) m) (ReplicateI\/L 1) m)\n+      Node* const_minus_one = NULL;\n+      Node* const_one = NULL;\n+      int add_opc;\n+      if (bt == T_LONG) {\n+        const_minus_one = phase->longcon(-1L);\n+        const_one = phase->longcon(1L);\n+        add_opc = Op_AddL;\n+      } else {\n+        const_minus_one = phase->intcon(-1);\n+        const_one = phase->intcon(1);\n+        add_opc = Op_AddI;\n+      }\n+      const_minus_one = phase->transform(VectorNode::scalar2vector(const_minus_one, vlen, Type::get_const_basic_type(bt)));\n+      Node* xorv = VectorNode::make(Op_XorV, in(1), const_minus_one, vt);\n+      xorv->add_req(in(2));\n+      xorv->add_flag(Node::Flag_is_predicated_vector);\n+      phase->transform(xorv);\n+      const_one = phase->transform(VectorNode::scalar2vector(const_one, vlen, Type::get_const_basic_type(bt)));\n+      Node* addv = VectorNode::make(VectorNode::opcode(add_opc, bt), xorv, const_one, vt);\n+      addv->add_req(in(2));\n+      addv->add_flag(Node::Flag_is_predicated_vector);\n+      return addv;\n+  }\n+\n+  \/\/ NegVI\/L ==> (SubVI\/L (ReplicateI\/L 0) src)\n+  Node* const_zero = NULL;\n+  int sub_opc;\n+  if (bt == T_LONG) {\n+    const_zero = phase->longcon(0L);\n+    sub_opc = Op_SubL;\n+  } else {\n+    const_zero = phase->intcon(0);\n+    sub_opc = Op_SubI;\n+  }\n+  const_zero = phase->transform(VectorNode::scalar2vector(const_zero, vlen, Type::get_const_basic_type(bt)));\n+  return VectorNode::make(VectorNode::opcode(sub_opc, bt), const_zero, in(1), vt);\n+}\n+\n+Node* NegVNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  BasicType bt = vect_type()->element_basic_type();\n+  uint vlen = length();\n+  int opc = Opcode();\n+  if (is_vector_integral_negate(opc)) {\n+    if (is_predicated_vector()) {\n+      if (!Matcher::match_rule_supported_vector_masked(opc, vlen, bt)) {\n+        return degenerate_integral_negate(phase, true);\n+      }\n+    } else if (!Matcher::match_rule_supported_vector(opc, vlen, bt)) {\n+      return degenerate_integral_negate(phase, false);\n+    }\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":118,"deletions":1,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -100,0 +100,1 @@\n+  static bool is_vector_integral_negate_supported(int opc, uint vlen, BasicType bt, bool use_predicate);\n@@ -112,0 +113,1 @@\n+  static bool is_vector_integral_negate(int opc);\n@@ -477,0 +479,12 @@\n+\/\/------------------------------NegVNode---------------------------------------\n+\/\/ Vector Neg parent class (not for code generation).\n+class NegVNode : public VectorNode {\n+ public:\n+  NegVNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const = 0;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n+ private:\n+  Node* degenerate_integral_negate(PhaseGVN* phase, bool is_predicated);\n+};\n+\n@@ -478,2 +492,10 @@\n-\/\/ Vector Neg int\n-class NegVINode : public VectorNode {\n+\/\/ Vector Neg byte\/short\/int\n+class NegVINode : public NegVNode {\n+ public:\n+  NegVINode(Node* in, const TypeVect* vt) : NegVNode(in, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/------------------------------NegVLNode--------------------------------------\n+\/\/ Vector Neg long\n+class NegVLNode : public NegVNode {\n@@ -481,1 +503,1 @@\n-  NegVINode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  NegVLNode(Node* in, const TypeVect* vt) : NegVNode(in, vt) {}\n@@ -487,1 +509,1 @@\n-class NegVFNode : public VectorNode {\n+class NegVFNode : public NegVNode {\n@@ -489,1 +511,1 @@\n-  NegVFNode(Node* in, const TypeVect* vt) : VectorNode(in,vt) {}\n+  NegVFNode(Node* in, const TypeVect* vt) : NegVNode(in, vt) {}\n@@ -495,1 +517,1 @@\n-class NegVDNode : public VectorNode {\n+class NegVDNode : public NegVNode {\n@@ -497,1 +519,1 @@\n-  NegVDNode(Node* in, const TypeVect* vt) : VectorNode(in,vt) {}\n+  NegVDNode(Node* in, const TypeVect* vt) : NegVNode(in, vt) {}\n@@ -1308,11 +1330,0 @@\n-\/\/------------------------------SetVectMaskINode-------------------------------\n-\/\/ Provide a mask for a vector predicate machine\n-class SetVectMaskINode : public Node {\n-public:\n-  SetVectMaskINode(Node *c, Node *in1) : Node(c, in1) {}\n-  virtual int Opcode() const;\n-  const Type *bottom_type() const { return TypeInt::INT; }\n-  virtual uint ideal_reg() const { return Op_RegI; }\n-  virtual const Type *Value(PhaseGVN *phase) const { return TypeInt::INT; }\n-};\n-\n@@ -1555,0 +1566,8 @@\n+class RoundVFNode : public VectorNode {\n+ public:\n+  RoundVFNode(Node* in, const TypeVect* vt) :VectorNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_FLOAT, \"must be float\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n@@ -1563,0 +1582,8 @@\n+class RoundVDNode : public VectorNode {\n+ public:\n+  RoundVDNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE, \"must be double\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":45,"deletions":18,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -719,1 +719,0 @@\n-  nonstatic_field(JavaThread,                  _pending_async_exception,                      oop)                                   \\\n@@ -1561,1 +1560,0 @@\n-  declare_c2_type(SetVectMaskINode, Node)                                 \\\n@@ -1584,0 +1582,2 @@\n+  declare_c2_type(UDivINode, Node)                                        \\\n+  declare_c2_type(UDivLNode, Node)                                        \\\n@@ -1588,0 +1588,2 @@\n+  declare_c2_type(UModINode, Node)                                        \\\n+  declare_c2_type(UModLNode, Node)                                        \\\n@@ -1591,0 +1593,2 @@\n+  declare_c2_type(UDivModINode, DivModNode)                               \\\n+  declare_c2_type(UDivModLNode, DivModNode)                               \\\n@@ -1760,3 +1764,5 @@\n-  declare_c2_type(NegVINode, VectorNode)                                  \\\n-  declare_c2_type(NegVFNode, VectorNode)                                  \\\n-  declare_c2_type(NegVDNode, VectorNode)                                  \\\n+  declare_c2_type(NegVNode, VectorNode)                                   \\\n+  declare_c2_type(NegVINode, NegVNode)                                    \\\n+  declare_c2_type(NegVLNode, NegVNode)                                    \\\n+  declare_c2_type(NegVFNode, NegVNode)                                    \\\n+  declare_c2_type(NegVDNode, NegVNode)                                    \\\n@@ -2126,6 +2132,0 @@\n-  \/*****************************\/                                         \\\n-  \/* Thread::SuspendFlags enum *\/                                         \\\n-  \/*****************************\/                                         \\\n-                                                                          \\\n-  declare_constant(JavaThread::_has_async_exception)                      \\\n-                                                                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -820,0 +820,3 @@\n+inline bool same_type_or_subword_size(BasicType t1, BasicType t2) {\n+  return (t1 == t2) || (is_subword_type(t1) && type2aelembytes(t1) == type2aelembytes(t2));\n+}\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -674,3 +674,0 @@\n-            } else if (op == NEG) {\n-                \/\/ FIXME: Support this in the JIT.\n-                return broadcast(0).lanewise(SUB, this);\n@@ -705,2 +702,0 @@\n-            } else if (op == NEG) {\n-                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -674,3 +674,0 @@\n-            } else if (op == NEG) {\n-                \/\/ FIXME: Support this in the JIT.\n-                return broadcast(0).lanewise(SUB, this);\n@@ -705,2 +702,0 @@\n-            } else if (op == NEG) {\n-                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -632,3 +632,0 @@\n-            } else if (op == NEG) {\n-                \/\/ FIXME: Support this in the JIT.\n-                return broadcast(0).lanewise(SUB, this);\n@@ -663,2 +660,0 @@\n-            } else if (op == NEG) {\n-                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -674,3 +674,0 @@\n-            } else if (op == NEG) {\n-                \/\/ FIXME: Support this in the JIT.\n-                return broadcast(0).lanewise(SUB, this);\n@@ -705,2 +702,0 @@\n-            } else if (op == NEG) {\n-                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -697,3 +697,0 @@\n-            } else if (op == NEG) {\n-                \/\/ FIXME: Support this in the JIT.\n-                return broadcast(0).lanewise(SUB, this);\n@@ -730,2 +727,0 @@\n-            } else if (op == NEG) {\n-                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1861,0 +1861,3 @@\n+                       [\"fmsb\", \"ZPZZ\", \"m\"],\n+                       [\"fnmad\", \"ZPZZ\", \"m\"],\n+                       [\"fnmsb\", \"ZPZZ\", \"m\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1165,10 +1165,13 @@\n-    __ sve_fnmla(z7, __ S, p6, z21, z13);              \/\/       fnmla   z7.s, p6\/m, z21.s, z13.s\n-    __ sve_fnmls(z7, __ D, p6, z5, z21);               \/\/       fnmls   z7.d, p6\/m, z5.d, z21.d\n-    __ sve_mla(z17, __ S, p0, z3, z9);                 \/\/       mla     z17.s, p0\/m, z3.s, z9.s\n-    __ sve_mls(z11, __ S, p2, z11, z14);               \/\/       mls     z11.s, p2\/m, z11.s, z14.s\n-    __ sve_and(z17, z11, z13);                         \/\/       and     z17.d, z11.d, z13.d\n-    __ sve_eor(z17, z30, z17);                         \/\/       eor     z17.d, z30.d, z17.d\n-    __ sve_orr(z15, z14, z26);                         \/\/       orr     z15.d, z14.d, z26.d\n-    __ sve_bic(z27, z22, z7);                          \/\/       bic     z27.d, z22.d, z7.d\n-    __ sve_uzp1(z5, __ H, z27, z27);                   \/\/       uzp1    z5.h, z27.h, z27.h\n-    __ sve_uzp2(z0, __ S, z14, z24);                   \/\/       uzp2    z0.s, z14.s, z24.s\n+    __ sve_fmsb(z7, __ S, p6, z21, z13);               \/\/       fmsb    z7.s, p6\/m, z21.s, z13.s\n+    __ sve_fnmad(z7, __ D, p6, z5, z21);               \/\/       fnmad   z7.d, p6\/m, z5.d, z21.d\n+    __ sve_fnmsb(z17, __ D, p0, z3, z9);               \/\/       fnmsb   z17.d, p0\/m, z3.d, z9.d\n+    __ sve_fnmla(z11, __ D, p2, z11, z14);             \/\/       fnmla   z11.d, p2\/m, z11.d, z14.d\n+    __ sve_fnmls(z17, __ D, p2, z13, z24);             \/\/       fnmls   z17.d, p2\/m, z13.d, z24.d\n+    __ sve_mla(z30, __ H, p4, z8, z15);                \/\/       mla     z30.h, p4\/m, z8.h, z15.h\n+    __ sve_mls(z26, __ H, p5, z27, z22);               \/\/       mls     z26.h, p5\/m, z27.h, z22.h\n+    __ sve_and(z8, z5, z27);                           \/\/       and     z8.d, z5.d, z27.d\n+    __ sve_eor(z10, z0, z14);                          \/\/       eor     z10.d, z0.d, z14.d\n+    __ sve_orr(z21, z20, z0);                          \/\/       orr     z21.d, z20.d, z0.d\n+    __ sve_bic(z22, z25, z5);                          \/\/       bic     z22.d, z25.d, z5.d\n+    __ sve_uzp1(z29, __ B, z17, z17);                  \/\/       uzp1    z29.b, z17.b, z17.b\n+    __ sve_uzp2(z12, __ H, z14, z29);                  \/\/       uzp2    z12.h, z14.h, z29.h\n@@ -1177,9 +1180,9 @@\n-    __ sve_andv(v20, __ S, p0, z3);                    \/\/       andv s20, p0, z3.s\n-    __ sve_orv(v25, __ D, p1, z25);                    \/\/       orv d25, p1, z25.d\n-    __ sve_eorv(v17, __ H, p4, z1);                    \/\/       eorv h17, p4, z1.h\n-    __ sve_smaxv(v14, __ B, p7, z13);                  \/\/       smaxv b14, p7, z13.b\n-    __ sve_sminv(v17, __ S, p0, z30);                  \/\/       sminv s17, p0, z30.s\n-    __ sve_fminv(v22, __ S, p5, z29);                  \/\/       fminv s22, p5, z29.s\n-    __ sve_fmaxv(v8, __ S, p0, z0);                    \/\/       fmaxv s8, p0, z0.s\n-    __ sve_fadda(v23, __ D, p5, z0);                   \/\/       fadda d23, p5, d23, z0.d\n-    __ sve_uaddv(v25, __ H, p6, z23);                  \/\/       uaddv d25, p6, z23.h\n+    __ sve_andv(v0, __ D, p4, z2);                     \/\/       andv d0, p4, z2.d\n+    __ sve_orv(v20, __ D, p5, z21);                    \/\/       orv d20, p5, z21.d\n+    __ sve_eorv(v12, __ B, p2, z2);                    \/\/       eorv b12, p2, z2.b\n+    __ sve_smaxv(v14, __ B, p5, z22);                  \/\/       smaxv b14, p5, z22.b\n+    __ sve_sminv(v19, __ D, p6, z26);                  \/\/       sminv d19, p6, z26.d\n+    __ sve_fminv(v12, __ S, p5, z21);                  \/\/       fminv s12, p5, z21.s\n+    __ sve_fmaxv(v1, __ S, p2, z19);                   \/\/       fmaxv s1, p2, z19.s\n+    __ sve_fadda(v19, __ S, p6, z23);                  \/\/       fadda s19, p6, s19, z23.s\n+    __ sve_uaddv(v30, __ S, p4, z19);                  \/\/       uaddv d30, p4, z19.s\n@@ -1204,7 +1207,7 @@\n-    0x14000000,     0x17ffffd7,     0x140003db,     0x94000000,\n-    0x97ffffd4,     0x940003d8,     0x3400000a,     0x34fffa2a,\n-    0x34007aaa,     0x35000008,     0x35fff9c8,     0x35007a48,\n-    0xb400000b,     0xb4fff96b,     0xb40079eb,     0xb500001d,\n-    0xb5fff91d,     0xb500799d,     0x10000013,     0x10fff8b3,\n-    0x10007933,     0x90000013,     0x36300016,     0x3637f836,\n-    0x363078b6,     0x3758000c,     0x375ff7cc,     0x3758784c,\n+    0x14000000,     0x17ffffd7,     0x140003de,     0x94000000,\n+    0x97ffffd4,     0x940003db,     0x3400000a,     0x34fffa2a,\n+    0x34007b0a,     0x35000008,     0x35fff9c8,     0x35007aa8,\n+    0xb400000b,     0xb4fff96b,     0xb4007a4b,     0xb500001d,\n+    0xb5fff91d,     0xb50079fd,     0x10000013,     0x10fff8b3,\n+    0x10007993,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36307916,     0x3758000c,     0x375ff7cc,     0x375878ac,\n@@ -1215,13 +1218,13 @@\n-    0x54007620,     0x54000001,     0x54fff541,     0x540075c1,\n-    0x54000002,     0x54fff4e2,     0x54007562,     0x54000002,\n-    0x54fff482,     0x54007502,     0x54000003,     0x54fff423,\n-    0x540074a3,     0x54000003,     0x54fff3c3,     0x54007443,\n-    0x54000004,     0x54fff364,     0x540073e4,     0x54000005,\n-    0x54fff305,     0x54007385,     0x54000006,     0x54fff2a6,\n-    0x54007326,     0x54000007,     0x54fff247,     0x540072c7,\n-    0x54000008,     0x54fff1e8,     0x54007268,     0x54000009,\n-    0x54fff189,     0x54007209,     0x5400000a,     0x54fff12a,\n-    0x540071aa,     0x5400000b,     0x54fff0cb,     0x5400714b,\n-    0x5400000c,     0x54fff06c,     0x540070ec,     0x5400000d,\n-    0x54fff00d,     0x5400708d,     0x5400000e,     0x54ffefae,\n-    0x5400702e,     0x5400000f,     0x54ffef4f,     0x54006fcf,\n+    0x54007680,     0x54000001,     0x54fff541,     0x54007621,\n+    0x54000002,     0x54fff4e2,     0x540075c2,     0x54000002,\n+    0x54fff482,     0x54007562,     0x54000003,     0x54fff423,\n+    0x54007503,     0x54000003,     0x54fff3c3,     0x540074a3,\n+    0x54000004,     0x54fff364,     0x54007444,     0x54000005,\n+    0x54fff305,     0x540073e5,     0x54000006,     0x54fff2a6,\n+    0x54007386,     0x54000007,     0x54fff247,     0x54007327,\n+    0x54000008,     0x54fff1e8,     0x540072c8,     0x54000009,\n+    0x54fff189,     0x54007269,     0x5400000a,     0x54fff12a,\n+    0x5400720a,     0x5400000b,     0x54fff0cb,     0x540071ab,\n+    0x5400000c,     0x54fff06c,     0x5400714c,     0x5400000d,\n+    0x54fff00d,     0x540070ed,     0x5400000e,     0x54ffefae,\n+    0x5400708e,     0x5400000f,     0x54ffef4f,     0x5400702f,\n@@ -1446,6 +1449,7 @@\n-    0x65a902c3,     0x65f33c78,     0x65ad5aa7,     0x65f578a7,\n-    0x04894071,     0x048e696b,     0x042d3171,     0x04b133d1,\n-    0x047a31cf,     0x04e732db,     0x057b6b65,     0x05b86dc0,\n-    0x049a2074,     0x04d82739,     0x04593031,     0x04083dae,\n-    0x048a23d1,     0x658737b6,     0x65862008,     0x65d83417,\n-    0x04413af9,\n+    0x65a902c3,     0x65f33c78,     0x65adbaa7,     0x65f5d8a7,\n+    0x65e9e071,     0x65ee496b,     0x65f869b1,     0x044f511e,\n+    0x0456777a,     0x043b30a8,     0x04ae300a,     0x04603295,\n+    0x04e53336,     0x05316a3d,     0x057d6dcc,     0x04da3040,\n+    0x04d836b4,     0x0419284c,     0x040836ce,     0x04ca3b53,\n+    0x658736ac,     0x65862a61,     0x65983af3,     0x0481327e,\n+\n@@ -1454,1 +1458,0 @@\n-\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":49,"deletions":46,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final byte CONST_SHIFT = Byte.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        byte apply(byte a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        byte apply(byte a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2625,1 +2671,1 @@\n-        return (byte)(ROR_scalar(a,b));\n+        return (byte)(ROR_scalar(a, b));\n@@ -2667,1 +2713,1 @@\n-        return (byte)(ROL_scalar(a,b));\n+        return (byte)(ROL_scalar(a, b));\n@@ -2707,0 +2753,209 @@\n+\n+\n+\n+    static byte LSHR_binary_const(byte a) {\n+        return (byte)(((a & 0xFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHRByte128VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHRByte128VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+    static byte LSHL_binary_const(byte a) {\n+        return (byte)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHLByte128VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHLByte128VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static byte ASHR_binary_const(byte a) {\n+        return (byte)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ASHRByte128VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ASHRByte128VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROR_binary_const(byte a) {\n+        return (byte)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void RORByte128VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void RORByte128VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROL_binary_const(byte a) {\n+        return (byte)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ROLByte128VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte128VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ROLByte128VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte128VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte128VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final byte CONST_SHIFT = Byte.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        byte apply(byte a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        byte apply(byte a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2625,1 +2671,1 @@\n-        return (byte)(ROR_scalar(a,b));\n+        return (byte)(ROR_scalar(a, b));\n@@ -2667,1 +2713,1 @@\n-        return (byte)(ROL_scalar(a,b));\n+        return (byte)(ROL_scalar(a, b));\n@@ -2707,0 +2753,209 @@\n+\n+\n+\n+    static byte LSHR_binary_const(byte a) {\n+        return (byte)(((a & 0xFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHRByte256VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHRByte256VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+    static byte LSHL_binary_const(byte a) {\n+        return (byte)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHLByte256VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHLByte256VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static byte ASHR_binary_const(byte a) {\n+        return (byte)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ASHRByte256VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ASHRByte256VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROR_binary_const(byte a) {\n+        return (byte)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void RORByte256VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void RORByte256VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROL_binary_const(byte a) {\n+        return (byte)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ROLByte256VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte256VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ROLByte256VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte256VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte256VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final byte CONST_SHIFT = Byte.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        byte apply(byte a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        byte apply(byte a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2625,1 +2671,1 @@\n-        return (byte)(ROR_scalar(a,b));\n+        return (byte)(ROR_scalar(a, b));\n@@ -2667,1 +2713,1 @@\n-        return (byte)(ROL_scalar(a,b));\n+        return (byte)(ROL_scalar(a, b));\n@@ -2707,0 +2753,209 @@\n+\n+\n+\n+    static byte LSHR_binary_const(byte a) {\n+        return (byte)(((a & 0xFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHRByte512VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHRByte512VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+    static byte LSHL_binary_const(byte a) {\n+        return (byte)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHLByte512VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHLByte512VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static byte ASHR_binary_const(byte a) {\n+        return (byte)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ASHRByte512VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ASHRByte512VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROR_binary_const(byte a) {\n+        return (byte)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void RORByte512VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void RORByte512VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROL_binary_const(byte a) {\n+        return (byte)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ROLByte512VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte512VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ROLByte512VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte512VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte512VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final byte CONST_SHIFT = Byte.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        byte apply(byte a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        byte apply(byte a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2625,1 +2671,1 @@\n-        return (byte)(ROR_scalar(a,b));\n+        return (byte)(ROR_scalar(a, b));\n@@ -2667,1 +2713,1 @@\n-        return (byte)(ROL_scalar(a,b));\n+        return (byte)(ROL_scalar(a, b));\n@@ -2707,0 +2753,209 @@\n+\n+\n+\n+    static byte LSHR_binary_const(byte a) {\n+        return (byte)(((a & 0xFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHRByte64VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHRByte64VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+    static byte LSHL_binary_const(byte a) {\n+        return (byte)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHLByte64VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHLByte64VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static byte ASHR_binary_const(byte a) {\n+        return (byte)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ASHRByte64VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ASHRByte64VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROR_binary_const(byte a) {\n+        return (byte)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void RORByte64VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void RORByte64VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROL_binary_const(byte a) {\n+        return (byte)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ROLByte64VectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Byte64VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ROLByte64VectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Byte64VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Byte64VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -68,0 +68,2 @@\n+    private static final byte CONST_SHIFT = Byte.SIZE \/ 2;\n+\n@@ -517,0 +519,44 @@\n+    interface FBinConstOp {\n+        byte apply(byte a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        byte apply(byte a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(byte[] r, byte[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2630,1 +2676,1 @@\n-        return (byte)(ROR_scalar(a,b));\n+        return (byte)(ROR_scalar(a, b));\n@@ -2672,1 +2718,1 @@\n-        return (byte)(ROL_scalar(a,b));\n+        return (byte)(ROL_scalar(a, b));\n@@ -2712,0 +2758,209 @@\n+\n+\n+\n+    static byte LSHR_binary_const(byte a) {\n+        return (byte)(((a & 0xFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHRByteMaxVectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ByteMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHRByteMaxVectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ByteMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+    static byte LSHL_binary_const(byte a) {\n+        return (byte)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void LSHLByteMaxVectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ByteMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void LSHLByteMaxVectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ByteMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static byte ASHR_binary_const(byte a) {\n+        return (byte)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ASHRByteMaxVectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ByteMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ASHRByteMaxVectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ByteMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROR_binary_const(byte a) {\n+        return (byte)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void RORByteMaxVectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ByteMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void RORByteMaxVectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ByteMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static byte ROL_binary_const(byte a) {\n+        return (byte)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"byteUnaryOpProvider\")\n+    static void ROLByteMaxVectorTestsScalarShiftConst(IntFunction<byte[]> fa) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ByteMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"byteUnaryOpMaskProvider\")\n+    static void ROLByteMaxVectorTestsScalarShiftMaskedConst(IntFunction<byte[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        byte[] a = fa.apply(SPECIES.length());\n+        byte[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Byte> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ByteVector av = ByteVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ByteMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/ByteMaxVectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final double CONST_SHIFT = Double.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        double apply(double a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        double apply(double a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2004,0 +2050,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double128VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final double CONST_SHIFT = Double.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        double apply(double a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        double apply(double a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2004,0 +2050,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double256VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final double CONST_SHIFT = Double.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        double apply(double a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        double apply(double a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2004,0 +2050,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double512VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final double CONST_SHIFT = Double.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        double apply(double a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        double apply(double a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2004,0 +2050,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Double64VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -68,0 +68,2 @@\n+    private static final double CONST_SHIFT = Double.SIZE \/ 2;\n+\n@@ -517,0 +519,44 @@\n+    interface FBinConstOp {\n+        double apply(double a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        double apply(double a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(double[] r, double[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2009,0 +2055,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/DoubleMaxVectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final float CONST_SHIFT = Float.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        float apply(float a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        float apply(float a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2014,0 +2060,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float128VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final float CONST_SHIFT = Float.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        float apply(float a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        float apply(float a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2014,0 +2060,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float256VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final float CONST_SHIFT = Float.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        float apply(float a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        float apply(float a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2014,0 +2060,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float512VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final float CONST_SHIFT = Float.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        float apply(float a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        float apply(float a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2014,0 +2060,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Float64VectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -68,0 +68,2 @@\n+    private static final float CONST_SHIFT = Float.SIZE \/ 2;\n+\n@@ -517,0 +519,44 @@\n+    interface FBinConstOp {\n+        float apply(float a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        float apply(float a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(float[] r, float[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2019,0 +2065,14 @@\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/FloatMaxVectorTests.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final int CONST_SHIFT = Integer.SIZE \/ 2;\n+\n@@ -513,0 +515,44 @@\n+    interface FBinConstOp {\n+        int apply(int a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        int apply(int a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2672,1 +2718,1 @@\n-        return (int)(ROR_scalar(a,b));\n+        return (int)(ROR_scalar(a, b));\n@@ -2714,1 +2760,1 @@\n-        return (int)(ROL_scalar(a,b));\n+        return (int)(ROL_scalar(a, b));\n@@ -2754,0 +2800,209 @@\n+\n+    static int LSHR_binary_const(int a) {\n+        return (int)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHRInt128VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHRInt128VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static int LSHL_binary_const(int a) {\n+        return (int)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHLInt128VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHLInt128VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static int ASHR_binary_const(int a) {\n+        return (int)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ASHRInt128VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ASHRInt128VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static int ROR_binary_const(int a) {\n+        return (int)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void RORInt128VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void RORInt128VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static int ROL_binary_const(int a) {\n+        return (int)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ROLInt128VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int128VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ROLInt128VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int128VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int128VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final int CONST_SHIFT = Integer.SIZE \/ 2;\n+\n@@ -513,0 +515,44 @@\n+    interface FBinConstOp {\n+        int apply(int a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        int apply(int a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2672,1 +2718,1 @@\n-        return (int)(ROR_scalar(a,b));\n+        return (int)(ROR_scalar(a, b));\n@@ -2714,1 +2760,1 @@\n-        return (int)(ROL_scalar(a,b));\n+        return (int)(ROL_scalar(a, b));\n@@ -2754,0 +2800,209 @@\n+\n+    static int LSHR_binary_const(int a) {\n+        return (int)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHRInt256VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHRInt256VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static int LSHL_binary_const(int a) {\n+        return (int)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHLInt256VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHLInt256VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static int ASHR_binary_const(int a) {\n+        return (int)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ASHRInt256VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ASHRInt256VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static int ROR_binary_const(int a) {\n+        return (int)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void RORInt256VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void RORInt256VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static int ROL_binary_const(int a) {\n+        return (int)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ROLInt256VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int256VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ROLInt256VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int256VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int256VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final int CONST_SHIFT = Integer.SIZE \/ 2;\n+\n@@ -513,0 +515,44 @@\n+    interface FBinConstOp {\n+        int apply(int a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        int apply(int a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2672,1 +2718,1 @@\n-        return (int)(ROR_scalar(a,b));\n+        return (int)(ROR_scalar(a, b));\n@@ -2714,1 +2760,1 @@\n-        return (int)(ROL_scalar(a,b));\n+        return (int)(ROL_scalar(a, b));\n@@ -2754,0 +2800,209 @@\n+\n+    static int LSHR_binary_const(int a) {\n+        return (int)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHRInt512VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHRInt512VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static int LSHL_binary_const(int a) {\n+        return (int)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHLInt512VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHLInt512VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static int ASHR_binary_const(int a) {\n+        return (int)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ASHRInt512VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ASHRInt512VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static int ROR_binary_const(int a) {\n+        return (int)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void RORInt512VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void RORInt512VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static int ROL_binary_const(int a) {\n+        return (int)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ROLInt512VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int512VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ROLInt512VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int512VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int512VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final int CONST_SHIFT = Integer.SIZE \/ 2;\n+\n@@ -513,0 +515,44 @@\n+    interface FBinConstOp {\n+        int apply(int a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        int apply(int a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2672,1 +2718,1 @@\n-        return (int)(ROR_scalar(a,b));\n+        return (int)(ROR_scalar(a, b));\n@@ -2714,1 +2760,1 @@\n-        return (int)(ROL_scalar(a,b));\n+        return (int)(ROL_scalar(a, b));\n@@ -2754,0 +2800,209 @@\n+\n+    static int LSHR_binary_const(int a) {\n+        return (int)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHRInt64VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHRInt64VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static int LSHL_binary_const(int a) {\n+        return (int)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHLInt64VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHLInt64VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static int ASHR_binary_const(int a) {\n+        return (int)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ASHRInt64VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ASHRInt64VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static int ROR_binary_const(int a) {\n+        return (int)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void RORInt64VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void RORInt64VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static int ROL_binary_const(int a) {\n+        return (int)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ROLInt64VectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Int64VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ROLInt64VectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Int64VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Int64VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -69,0 +69,2 @@\n+    private static final int CONST_SHIFT = Integer.SIZE \/ 2;\n+\n@@ -518,0 +520,44 @@\n+    interface FBinConstOp {\n+        int apply(int a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        int apply(int a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(int[] r, int[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2677,1 +2723,1 @@\n-        return (int)(ROR_scalar(a,b));\n+        return (int)(ROR_scalar(a, b));\n@@ -2719,1 +2765,1 @@\n-        return (int)(ROL_scalar(a,b));\n+        return (int)(ROL_scalar(a, b));\n@@ -2759,0 +2805,209 @@\n+\n+    static int LSHR_binary_const(int a) {\n+        return (int)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHRIntMaxVectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, IntMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHRIntMaxVectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, IntMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static int LSHL_binary_const(int a) {\n+        return (int)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void LSHLIntMaxVectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, IntMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void LSHLIntMaxVectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, IntMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static int ASHR_binary_const(int a) {\n+        return (int)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ASHRIntMaxVectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, IntMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ASHRIntMaxVectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, IntMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static int ROR_binary_const(int a) {\n+        return (int)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void RORIntMaxVectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, IntMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void RORIntMaxVectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, IntMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static int ROL_binary_const(int a) {\n+        return (int)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"intUnaryOpProvider\")\n+    static void ROLIntMaxVectorTestsScalarShiftConst(IntFunction<int[]> fa) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, IntMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"intUnaryOpMaskProvider\")\n+    static void ROLIntMaxVectorTestsScalarShiftMaskedConst(IntFunction<int[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        int[] a = fa.apply(SPECIES.length());\n+        int[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Integer> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                IntVector av = IntVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, IntMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/IntMaxVectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final long CONST_SHIFT = Long.SIZE \/ 2;\n+\n@@ -470,0 +472,44 @@\n+    interface FBinConstOp {\n+        long apply(long a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        long apply(long a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2694,1 +2740,1 @@\n-        return (long)(ROR_scalar(a,b));\n+        return (long)(ROR_scalar(a, b));\n@@ -2736,1 +2782,1 @@\n-        return (long)(ROL_scalar(a,b));\n+        return (long)(ROL_scalar(a, b));\n@@ -2776,0 +2822,209 @@\n+\n+    static long LSHR_binary_const(long a) {\n+        return (long)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHRLong128VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHRLong128VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static long LSHL_binary_const(long a) {\n+        return (long)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHLLong128VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHLLong128VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static long ASHR_binary_const(long a) {\n+        return (long)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ASHRLong128VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ASHRLong128VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static long ROR_binary_const(long a) {\n+        return (long)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void RORLong128VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void RORLong128VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static long ROL_binary_const(long a) {\n+        return (long)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ROLLong128VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long128VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ROLLong128VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long128VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long128VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final long CONST_SHIFT = Long.SIZE \/ 2;\n+\n@@ -470,0 +472,44 @@\n+    interface FBinConstOp {\n+        long apply(long a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        long apply(long a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2694,1 +2740,1 @@\n-        return (long)(ROR_scalar(a,b));\n+        return (long)(ROR_scalar(a, b));\n@@ -2736,1 +2782,1 @@\n-        return (long)(ROL_scalar(a,b));\n+        return (long)(ROL_scalar(a, b));\n@@ -2776,0 +2822,209 @@\n+\n+    static long LSHR_binary_const(long a) {\n+        return (long)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHRLong256VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHRLong256VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static long LSHL_binary_const(long a) {\n+        return (long)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHLLong256VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHLLong256VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static long ASHR_binary_const(long a) {\n+        return (long)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ASHRLong256VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ASHRLong256VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static long ROR_binary_const(long a) {\n+        return (long)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void RORLong256VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void RORLong256VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static long ROL_binary_const(long a) {\n+        return (long)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ROLLong256VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long256VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ROLLong256VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long256VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long256VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final long CONST_SHIFT = Long.SIZE \/ 2;\n+\n@@ -470,0 +472,44 @@\n+    interface FBinConstOp {\n+        long apply(long a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        long apply(long a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2694,1 +2740,1 @@\n-        return (long)(ROR_scalar(a,b));\n+        return (long)(ROR_scalar(a, b));\n@@ -2736,1 +2782,1 @@\n-        return (long)(ROL_scalar(a,b));\n+        return (long)(ROL_scalar(a, b));\n@@ -2776,0 +2822,209 @@\n+\n+    static long LSHR_binary_const(long a) {\n+        return (long)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHRLong512VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHRLong512VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static long LSHL_binary_const(long a) {\n+        return (long)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHLLong512VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHLLong512VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static long ASHR_binary_const(long a) {\n+        return (long)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ASHRLong512VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ASHRLong512VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static long ROR_binary_const(long a) {\n+        return (long)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void RORLong512VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void RORLong512VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static long ROL_binary_const(long a) {\n+        return (long)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ROLLong512VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long512VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ROLLong512VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long512VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long512VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+    private static final long CONST_SHIFT = Long.SIZE \/ 2;\n+\n@@ -470,0 +472,44 @@\n+    interface FBinConstOp {\n+        long apply(long a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        long apply(long a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2694,1 +2740,1 @@\n-        return (long)(ROR_scalar(a,b));\n+        return (long)(ROR_scalar(a, b));\n@@ -2736,1 +2782,1 @@\n-        return (long)(ROL_scalar(a,b));\n+        return (long)(ROL_scalar(a, b));\n@@ -2776,0 +2822,209 @@\n+\n+    static long LSHR_binary_const(long a) {\n+        return (long)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHRLong64VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHRLong64VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static long LSHL_binary_const(long a) {\n+        return (long)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHLLong64VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHLLong64VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static long ASHR_binary_const(long a) {\n+        return (long)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ASHRLong64VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ASHRLong64VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static long ROR_binary_const(long a) {\n+        return (long)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void RORLong64VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void RORLong64VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static long ROL_binary_const(long a) {\n+        return (long)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ROLLong64VectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Long64VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ROLLong64VectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Long64VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Long64VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -69,0 +69,2 @@\n+    private static final long CONST_SHIFT = Long.SIZE \/ 2;\n+\n@@ -475,0 +477,44 @@\n+    interface FBinConstOp {\n+        long apply(long a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        long apply(long a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(long[] r, long[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2699,1 +2745,1 @@\n-        return (long)(ROR_scalar(a,b));\n+        return (long)(ROR_scalar(a, b));\n@@ -2741,1 +2787,1 @@\n-        return (long)(ROL_scalar(a,b));\n+        return (long)(ROL_scalar(a, b));\n@@ -2781,0 +2827,209 @@\n+\n+    static long LSHR_binary_const(long a) {\n+        return (long)((a >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHRLongMaxVectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, LongMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHRLongMaxVectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, LongMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+\n+\n+\n+\n+    static long LSHL_binary_const(long a) {\n+        return (long)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void LSHLLongMaxVectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, LongMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void LSHLLongMaxVectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, LongMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static long ASHR_binary_const(long a) {\n+        return (long)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ASHRLongMaxVectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, LongMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ASHRLongMaxVectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, LongMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static long ROR_binary_const(long a) {\n+        return (long)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void RORLongMaxVectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, LongMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void RORLongMaxVectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, LongMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static long ROL_binary_const(long a) {\n+        return (long)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"longUnaryOpProvider\")\n+    static void ROLLongMaxVectorTestsScalarShiftConst(IntFunction<long[]> fa) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, LongMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"longUnaryOpMaskProvider\")\n+    static void ROLLongMaxVectorTestsScalarShiftMaskedConst(IntFunction<long[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        long[] a = fa.apply(SPECIES.length());\n+        long[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Long> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                LongVector av = LongVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, LongMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/LongMaxVectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final short CONST_SHIFT = Short.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        short apply(short a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        short apply(short a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2616,1 +2662,1 @@\n-        return (short)(ROR_scalar(a,b));\n+        return (short)(ROR_scalar(a, b));\n@@ -2658,1 +2704,1 @@\n-        return (short)(ROL_scalar(a,b));\n+        return (short)(ROL_scalar(a, b));\n@@ -2698,0 +2744,209 @@\n+\n+\n+\n+\n+\n+    static short LSHR_binary_const(short a) {\n+        return (short)(((a & 0xFFFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHRShort128VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHRShort128VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short128VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    static short LSHL_binary_const(short a) {\n+        return (short)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHLShort128VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHLShort128VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short128VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static short ASHR_binary_const(short a) {\n+        return (short)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ASHRShort128VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ASHRShort128VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short128VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static short ROR_binary_const(short a) {\n+        return (short)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void RORShort128VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void RORShort128VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short128VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static short ROL_binary_const(short a) {\n+        return (short)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ROLShort128VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short128VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ROLShort128VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short128VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short128VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final short CONST_SHIFT = Short.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        short apply(short a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        short apply(short a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2616,1 +2662,1 @@\n-        return (short)(ROR_scalar(a,b));\n+        return (short)(ROR_scalar(a, b));\n@@ -2658,1 +2704,1 @@\n-        return (short)(ROL_scalar(a,b));\n+        return (short)(ROL_scalar(a, b));\n@@ -2698,0 +2744,209 @@\n+\n+\n+\n+\n+\n+    static short LSHR_binary_const(short a) {\n+        return (short)(((a & 0xFFFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHRShort256VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHRShort256VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short256VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    static short LSHL_binary_const(short a) {\n+        return (short)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHLShort256VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHLShort256VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short256VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static short ASHR_binary_const(short a) {\n+        return (short)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ASHRShort256VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ASHRShort256VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short256VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static short ROR_binary_const(short a) {\n+        return (short)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void RORShort256VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void RORShort256VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short256VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static short ROL_binary_const(short a) {\n+        return (short)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ROLShort256VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short256VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ROLShort256VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short256VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short256VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final short CONST_SHIFT = Short.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        short apply(short a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        short apply(short a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2616,1 +2662,1 @@\n-        return (short)(ROR_scalar(a,b));\n+        return (short)(ROR_scalar(a, b));\n@@ -2658,1 +2704,1 @@\n-        return (short)(ROL_scalar(a,b));\n+        return (short)(ROL_scalar(a, b));\n@@ -2698,0 +2744,209 @@\n+\n+\n+\n+\n+\n+    static short LSHR_binary_const(short a) {\n+        return (short)(((a & 0xFFFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHRShort512VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHRShort512VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short512VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    static short LSHL_binary_const(short a) {\n+        return (short)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHLShort512VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHLShort512VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short512VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static short ASHR_binary_const(short a) {\n+        return (short)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ASHRShort512VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ASHRShort512VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short512VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static short ROR_binary_const(short a) {\n+        return (short)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void RORShort512VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void RORShort512VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short512VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static short ROL_binary_const(short a) {\n+        return (short)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ROLShort512VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short512VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ROLShort512VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short512VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short512VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+    private static final short CONST_SHIFT = Short.SIZE \/ 2;\n+\n@@ -512,0 +514,44 @@\n+    interface FBinConstOp {\n+        short apply(short a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        short apply(short a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2616,1 +2662,1 @@\n-        return (short)(ROR_scalar(a,b));\n+        return (short)(ROR_scalar(a, b));\n@@ -2658,1 +2704,1 @@\n-        return (short)(ROL_scalar(a,b));\n+        return (short)(ROL_scalar(a, b));\n@@ -2698,0 +2744,209 @@\n+\n+\n+\n+\n+\n+    static short LSHR_binary_const(short a) {\n+        return (short)(((a & 0xFFFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHRShort64VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHRShort64VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short64VectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    static short LSHL_binary_const(short a) {\n+        return (short)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHLShort64VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHLShort64VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short64VectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static short ASHR_binary_const(short a) {\n+        return (short)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ASHRShort64VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ASHRShort64VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short64VectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static short ROR_binary_const(short a) {\n+        return (short)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void RORShort64VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void RORShort64VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short64VectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static short ROL_binary_const(short a) {\n+        return (short)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ROLShort64VectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, Short64VectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ROLShort64VectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, Short64VectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/Short64VectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -68,0 +68,2 @@\n+    private static final short CONST_SHIFT = Short.SIZE \/ 2;\n+\n@@ -517,0 +519,44 @@\n+    interface FBinConstOp {\n+        short apply(short a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        short apply(short a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals(short[] r, short[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n+        }\n+    }\n+\n@@ -2621,1 +2667,1 @@\n-        return (short)(ROR_scalar(a,b));\n+        return (short)(ROR_scalar(a, b));\n@@ -2663,1 +2709,1 @@\n-        return (short)(ROL_scalar(a,b));\n+        return (short)(ROL_scalar(a, b));\n@@ -2703,0 +2749,209 @@\n+\n+\n+\n+\n+\n+    static short LSHR_binary_const(short a) {\n+        return (short)(((a & 0xFFFF) >>> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHRShortMaxVectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ShortMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHRShortMaxVectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ShortMaxVectorTests::LSHR_binary_const);\n+    }\n+\n+\n+\n+    static short LSHL_binary_const(short a) {\n+        return (short)((a << CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void LSHLShortMaxVectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ShortMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void LSHLShortMaxVectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.LSHL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ShortMaxVectorTests::LSHL_binary_const);\n+    }\n+\n+\n+\n+    static short ASHR_binary_const(short a) {\n+        return (short)((a >> CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ASHRShortMaxVectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ShortMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ASHRShortMaxVectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ASHR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ShortMaxVectorTests::ASHR_binary_const);\n+    }\n+\n+\n+\n+    static short ROR_binary_const(short a) {\n+        return (short)(ROR_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void RORShortMaxVectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ShortMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void RORShortMaxVectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROR, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ShortMaxVectorTests::ROR_binary_const);\n+    }\n+\n+\n+\n+    static short ROL_binary_const(short a) {\n+        return (short)(ROL_scalar(a, CONST_SHIFT));\n+    }\n+\n+    @Test(dataProvider = \"shortUnaryOpProvider\")\n+    static void ROLShortMaxVectorTestsScalarShiftConst(IntFunction<short[]> fa) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, ShortMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n+\n+    @Test(dataProvider = \"shortUnaryOpMaskProvider\")\n+    static void ROLShortMaxVectorTestsScalarShiftMaskedConst(IntFunction<short[]> fa,\n+                                          IntFunction<boolean[]> fm) {\n+        short[] a = fa.apply(SPECIES.length());\n+        short[] r = fr.apply(SPECIES.length());\n+        boolean[] mask = fm.apply(SPECIES.length());\n+        VectorMask<Short> vmask = VectorMask.fromArray(SPECIES, mask, 0);\n+\n+        for (int ic = 0; ic < INVOC_COUNT; ic++) {\n+            for (int i = 0; i < a.length; i += SPECIES.length()) {\n+                ShortVector av = ShortVector.fromArray(SPECIES, a, i);\n+                av.lanewise(VectorOperators.ROL, CONST_SHIFT, vmask).intoArray(r, i);\n+            }\n+        }\n+\n+        assertShiftConstEquals(r, a, mask, ShortMaxVectorTests::ROL_binary_const);\n+    }\n+\n+\n","filename":"test\/jdk\/jdk\/incubator\/vector\/ShortMaxVectorTests.java","additions":257,"deletions":2,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -73,0 +73,2 @@\n+shift_const_template=\"Shift-Const-op\"\n+shift_masked_const_template=\"Shift-Masked-Const-op\"\n@@ -268,1 +270,1 @@\n-function gen_shift_cst_op {\n+function gen_shift_op {\n@@ -274,0 +276,6 @@\n+function gen_shift_cst_op {\n+  echo \"Generating Shift constant op $1 ($2)...\"\n+  gen_op_tmpl $shift_const_template \"$@\"\n+  gen_op_tmpl $shift_masked_const_template \"$@\"\n+}\n+\n@@ -430,9 +438,9 @@\n-gen_shift_cst_op  \"LSHL\" \"(a << b)\" \"intOrLong\"\n-gen_shift_cst_op  \"LSHL\" \"(a << (b \\& 7))\" \"byte\"\n-gen_shift_cst_op  \"LSHL\" \"(a << (b \\& 15))\" \"short\"\n-gen_shift_cst_op  \"LSHR\" \"(a >>> b)\" \"intOrLong\"\n-gen_shift_cst_op  \"LSHR\" \"((a \\& 0xFF) >>> (b \\& 7))\" \"byte\"\n-gen_shift_cst_op  \"LSHR\" \"((a \\& 0xFFFF) >>> (b \\& 15))\" \"short\"\n-gen_shift_cst_op  \"ASHR\" \"(a >> b)\" \"intOrLong\"\n-gen_shift_cst_op  \"ASHR\" \"(a >> (b \\& 7))\" \"byte\"\n-gen_shift_cst_op  \"ASHR\" \"(a >> (b \\& 15))\" \"short\"\n+gen_shift_op  \"LSHL\" \"(a << b)\" \"intOrLong\"\n+gen_shift_op  \"LSHL\" \"(a << (b \\& 7))\" \"byte\"\n+gen_shift_op  \"LSHL\" \"(a << (b \\& 15))\" \"short\"\n+gen_shift_op  \"LSHR\" \"(a >>> b)\" \"intOrLong\"\n+gen_shift_op  \"LSHR\" \"((a \\& 0xFF) >>> (b \\& 7))\" \"byte\"\n+gen_shift_op  \"LSHR\" \"((a \\& 0xFFFF) >>> (b \\& 15))\" \"short\"\n+gen_shift_op  \"ASHR\" \"(a >> b)\" \"intOrLong\"\n+gen_shift_op  \"ASHR\" \"(a >> (b \\& 7))\" \"byte\"\n+gen_shift_op  \"ASHR\" \"(a >> (b \\& 15))\" \"short\"\n@@ -441,2 +449,11 @@\n-gen_shift_cst_op  \"ROR\" \"ROR_scalar(a,b)\" \"BITWISE\"\n-gen_shift_cst_op  \"ROL\" \"ROL_scalar(a,b)\" \"BITWISE\"\n+gen_shift_op  \"ROR\" \"ROR_scalar(a, b)\" \"BITWISE\"\n+gen_shift_op  \"ROL\" \"ROL_scalar(a, b)\" \"BITWISE\"\n+\n+# Constant Shifts\n+gen_shift_cst_op  \"LSHR\" \"(a >>> CONST_SHIFT)\" \"intOrLong\"\n+gen_shift_cst_op  \"LSHR\" \"((a \\& 0xFF) >>> CONST_SHIFT)\" \"byte\"\n+gen_shift_cst_op  \"LSHR\" \"((a \\& 0xFFFF) >>> CONST_SHIFT)\" \"short\"\n+gen_shift_cst_op  \"LSHL\" \"(a << CONST_SHIFT)\" \"BITWISE\"\n+gen_shift_cst_op  \"ASHR\" \"(a >> CONST_SHIFT)\" \"BITWISE\"\n+gen_shift_cst_op  \"ROR\" \"ROR_scalar(a, CONST_SHIFT)\" \"BITWISE\"\n+gen_shift_cst_op  \"ROL\" \"ROL_scalar(a, CONST_SHIFT)\" \"BITWISE\"\n","filename":"test\/jdk\/jdk\/incubator\/vector\/gen-template.sh","additions":29,"deletions":12,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -94,0 +94,2 @@\n+    private static final $type$ CONST_SHIFT = $Boxtype$.SIZE \/ 2;\n+\n@@ -542,0 +544,44 @@\n+        }\n+    }\n+\n+    interface FBinConstOp {\n+        $type$ apply($type$ a);\n+    }\n+\n+    interface FBinConstMaskOp {\n+        $type$ apply($type$ a, boolean m);\n+\n+        static FBinConstMaskOp lift(FBinConstOp f) {\n+            return (a, m) -> m ? f.apply(a) : a;\n+        }\n+    }\n+\n+    static void assertShiftConstEquals($type$[] r, $type$[] a, FBinConstOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j]));\n+                }\n+            }\n+        } catch (AssertionError e) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j]), \"at index #\" + i + \", \" + j);\n+        }\n+    }\n+\n+    static void assertShiftConstEquals($type$[] r, $type$[] a, boolean[] mask, FBinConstOp f) {\n+        assertShiftConstEquals(r, a, mask, FBinConstMaskOp.lift(f));\n+    }\n+\n+    static void assertShiftConstEquals($type$[] r, $type$[] a, boolean[] mask, FBinConstMaskOp f) {\n+        int i = 0;\n+        int j = 0;\n+        try {\n+            for (; j < a.length; j += SPECIES.length()) {\n+                for (i = 0; i < SPECIES.length(); i++) {\n+                    Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]));\n+                }\n+            }\n+        } catch (AssertionError err) {\n+            Assert.assertEquals(r[i+j], f.apply(a[i+j], mask[i]), \"at index #\" + i + \", input1 = \" + a[i+j] + \", mask = \" + mask[i]);\n","filename":"test\/jdk\/jdk\/incubator\/vector\/templates\/Unit-header.template","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"}]}