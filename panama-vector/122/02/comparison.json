{"files":[{"patch":"@@ -2515,1 +2515,1 @@\n-  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -2672,0 +2672,7 @@\n+void Assembler::knotdl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n@@ -2776,0 +2783,8 @@\n+void Assembler::kshiftrwl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n@@ -8284,0 +8299,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8297,0 +8313,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8311,0 +8328,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8324,0 +8342,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8338,0 +8357,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8351,0 +8371,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8365,0 +8386,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8378,0 +8400,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8446,0 +8469,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8459,0 +8483,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8473,0 +8498,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8486,0 +8512,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8500,0 +8527,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8513,0 +8541,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8527,0 +8556,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8540,0 +8570,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8581,0 +8612,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8594,0 +8626,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8635,0 +8668,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8648,0 +8682,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8662,0 +8697,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8675,0 +8711,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8688,0 +8725,59 @@\n+void Assembler::evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x51, (0xC0 | encode));\n+}\n+\n+void Assembler::evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x51);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x51, (0xC0 | encode));\n+}\n+\n+void Assembler::evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x51);\n+  emit_operand(dst, src);\n+}\n+\n+\n@@ -8689,0 +8785,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8702,0 +8799,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8716,0 +8814,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8729,0 +8828,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8801,0 +8901,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8816,0 +8917,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8830,0 +8932,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8845,0 +8948,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8859,0 +8963,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8872,0 +8977,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8886,0 +8992,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -8899,0 +9006,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9016,0 +9124,114 @@\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n@@ -9029,0 +9251,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9041,0 +9264,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9065,0 +9289,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9077,0 +9302,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9101,0 +9327,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9113,0 +9340,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9137,0 +9365,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9149,0 +9378,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9173,0 +9403,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9185,0 +9416,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9209,0 +9441,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9221,0 +9454,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9285,0 +9519,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9297,0 +9532,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9311,0 +9547,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9323,0 +9560,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9390,0 +9628,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9402,0 +9641,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9416,0 +9656,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -9428,0 +9669,1 @@\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -10833,0 +11075,96 @@\n+void Assembler::evprord(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm0->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprorq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm0->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprorvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::evprorvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::evprold(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm1->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprolq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm1->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprolvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n+void Assembler::evprolvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":339,"deletions":1,"binary":false,"changes":340,"status":"modified"},{"patch":"@@ -1497,0 +1497,1 @@\n+  void knotdl(KRegister dst, KRegister src);\n@@ -1507,0 +1508,1 @@\n+  void kshiftrwl(KRegister dst, KRegister src, int imm8);\n@@ -2338,0 +2340,15 @@\n+  void evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+\n@@ -2376,0 +2393,8 @@\n+  void evprold(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprolq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprolvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprolvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprord(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprorq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprorvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprorvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1464,0 +1464,6 @@\n+void C2_MacroAssembler::load_vector_mask64(KRegister dst, XMMRegister src, XMMRegister xtmp, Register scratch) {\n+  vpmovsxbd(xtmp, src, Assembler::AVX_512bit);\n+  evpcmpd(dst, k0, xtmp, ExternalAddress(StubRoutines::x86::vector_int_mask_cmp_bits()),\n+          Assembler::eq, true, Assembler::AVX_512bit, scratch);\n+}\n+\n@@ -3830,0 +3836,30 @@\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, int imm8, bool merge, int vlen_enc) {\n+  switch(ideal_opc) {\n+    case Op_LShiftVS:\n+      Assembler::evpsllw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_LShiftVI:\n+      Assembler::evpslld(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_LShiftVL:\n+      Assembler::evpsllq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVS:\n+      Assembler::evpsraw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVI:\n+      Assembler::evpsrad(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVL:\n+      Assembler::evpsraq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVS:\n+      Assembler::evpsrlw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVI:\n+      Assembler::evpsrld(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVL:\n+      Assembler::evpsrlq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RotateRightV:\n+      evrord(eType, dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RotateLeftV:\n+      evrold(eType, dst, mask, src1, imm8, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n@@ -3872,0 +3908,4 @@\n+    case Op_SqrtVF:\n+      evsqrtps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SqrtVD:\n+      evsqrtpd(dst, mask, src1, src2, merge, vlen_enc); break;\n@@ -3904,0 +3944,4 @@\n+    case Op_RotateLeftV:\n+      evrold(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_RotateRightV:\n+      evrord(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n@@ -3960,0 +4004,4 @@\n+    case Op_SqrtVF:\n+      evsqrtps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SqrtVD:\n+      evsqrtpd(dst, mask, src1, src2, merge, vlen_enc); break;\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -145,0 +145,2 @@\n+  void load_vector_mask64(KRegister dst, XMMRegister src, XMMRegister xtmp, Register scratch);\n+\n@@ -285,0 +287,3 @@\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                   XMMRegister src1, int imm8, bool merge, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -8208,0 +8208,32 @@\n+void MacroAssembler::knot(uint masklen, KRegister dst, KRegister src, KRegister ktmp, Register rtmp) {\n+  switch(masklen) {\n+    case 2:\n+       knotbl(dst, src);\n+       movl(rtmp, 3);\n+       kmovbl(ktmp, rtmp);\n+       kandbl(dst, ktmp, dst);\n+       break;\n+    case 4:\n+       knotbl(dst, src);\n+       movl(rtmp, 15);\n+       kmovbl(ktmp, rtmp);\n+       kandbl(dst, ktmp, dst);\n+       break;\n+    case 8:\n+       knotbl(dst, src);\n+       break;\n+    case 16:\n+       knotwl(dst, src);\n+       break;\n+    case 32:\n+       knotdl(dst, src);\n+       break;\n+    case 64:\n+       knotql(dst, src);\n+       break;\n+    default:\n+      fatal(\"Unexpected vector length %d\", masklen);\n+      break;\n+  }\n+}\n+\n@@ -8504,0 +8536,45 @@\n+\n+void MacroAssembler::evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprold(dst, mask, src, shift, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprolq(dst, mask, src, shift, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprord(dst, mask, src, shift, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprorq(dst, mask, src, shift, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprolvd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprolvq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprorvd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprorvq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1692,0 +1692,1 @@\n+  void knot(uint masklen, KRegister dst, KRegister src, KRegister ktmp = knoreg, Register rtmp = noreg);\n@@ -1708,0 +1709,5 @@\n+  void evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc);\n+  void evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc);\n+  void evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc);\n+  void evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4004,1 +4004,2 @@\n-    StubRoutines::x86::_vector_masked_cmp_bits = generate_vector_mask(\"vector_masked_cmp_bits\", 0x01010101);\n+    StubRoutines::x86::_vector_mask_cmp_bits = generate_vector_mask(\"vector_mask_cmp_bits\", 0x01010101);\n+    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x00000001);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -7584,1 +7584,2 @@\n-    StubRoutines::x86::_vector_masked_cmp_bits = generate_vector_mask(\"vector_masked_cmp_bits\", 0x0101010101010101);\n+    StubRoutines::x86::_vector_mask_cmp_bits = generate_vector_mask(\"vector_mask_cmp_bits\", 0x0101010101010101);\n+    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -51,1 +51,2 @@\n-address StubRoutines::x86::_vector_masked_cmp_bits = NULL;\n+address StubRoutines::x86::_vector_mask_cmp_bits = NULL;\n+address StubRoutines::x86::_vector_int_mask_cmp_bits = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -168,1 +168,2 @@\n-  static address _vector_masked_cmp_bits;\n+  static address _vector_mask_cmp_bits;\n+  static address _vector_int_mask_cmp_bits;\n@@ -291,2 +292,2 @@\n-  static address vector_masked_cmp_bits() {\n-    return _vector_masked_cmp_bits;\n+  static address vector_int_mask_cmp_bits() {\n+    return _vector_int_mask_cmp_bits;\n@@ -294,0 +295,5 @@\n+\n+  static address vector_mask_cmp_bits() {\n+    return _vector_mask_cmp_bits;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1377,1 +1377,2 @@\n-  static address vector_masked_cmp_bits() { return StubRoutines::x86::vector_masked_cmp_bits(); }\n+  static address vector_mask_cmp_bits() { return StubRoutines::x86::vector_mask_cmp_bits(); }\n+  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n@@ -1821,1 +1822,7 @@\n-      if(!is_LP64 || !VM_Version::supports_avx512vlbw()) {\n+      if (!is_LP64 || !VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n@@ -1844,5 +1851,1 @@\n-  \/\/ Needed for loadmask pattern which populates opmask register\n-  \/\/ consumed by masked instructions.\n-  if (!VM_Version::supports_avx512vlbw()) {\n-    return false;\n-  }\n+\n@@ -1851,0 +1854,3 @@\n+  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n+    return false;\n+  }\n@@ -1855,0 +1861,3 @@\n+      if(!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n@@ -1873,1 +1882,0 @@\n-      assert(VM_Version::supports_avx512bw(), \"\");\n@@ -1875,0 +1883,3 @@\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n@@ -1887,0 +1898,2 @@\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n@@ -1888,0 +1901,7 @@\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorLoadMask:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n@@ -1905,0 +1925,2 @@\n+    case Op_SqrtVF:\n+    case Op_SqrtVD:\n@@ -1919,0 +1941,3 @@\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n@@ -1925,1 +1950,3 @@\n-      assert(!is_subword_type(bt) || VM_Version::supports_avx512bw(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n@@ -1929,1 +1956,3 @@\n-      assert(bt != T_SHORT || VM_Version::supports_avx512bw(), \"\");\n+      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n@@ -1941,3 +1970,1 @@\n-      assert(bt != T_INT  || VM_Version::supports_avx512bw(), \"\");\n-      assert(bt != T_LONG || VM_Version::supports_avx512bw(), \"\");\n-      if (bt == T_BYTE && !VM_Version::supports_avx512dq()) {\n+      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n@@ -1946,0 +1973,2 @@\n+      return true;\n+\n@@ -1947,1 +1976,0 @@\n-      assert(VM_Version::supports_avx512bw(), \"\");\n@@ -2019,1 +2047,1 @@\n-  return new TypeVectMask(TypeInt::BOOL, length);\n+  return new TypeVectMask(elemTy, length);\n@@ -3459,2 +3487,3 @@\n-            n->as_VectorReinterpret()->src_elem_type() == T_SHORT &&\n-            n->as_VectorReinterpret()->dst_elem_type() == T_BYTE); \/\/ dst == src\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_SHORT &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n@@ -3478,3 +3507,4 @@\n-            (n->as_VectorReinterpret()->src_elem_type() == T_INT ||\n-            n->as_VectorReinterpret()->src_elem_type() == T_FLOAT) &&\n-            n->as_VectorReinterpret()->dst_elem_type() == T_BYTE); \/\/ dst == src\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_INT ||\n+             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_FLOAT) &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n@@ -3498,3 +3528,4 @@\n-            (n->as_VectorReinterpret()->src_elem_type() == T_LONG ||\n-            n->as_VectorReinterpret()->src_elem_type() == T_DOUBLE) &&\n-            n->as_VectorReinterpret()->dst_elem_type() == T_BYTE); \/\/ dst == src\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_LONG ||\n+             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_DOUBLE) &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n@@ -3834,1 +3865,0 @@\n-  predicate(VM_Version::supports_avx512vl());\n@@ -3876,1 +3906,0 @@\n-  predicate(VM_Version::supports_avx512vl());\n@@ -6105,0 +6134,1 @@\n+  ins_cost(400);\n@@ -6117,0 +6147,1 @@\n+  ins_cost(400);\n@@ -6129,0 +6160,1 @@\n+  ins_cost(400);\n@@ -6141,0 +6173,1 @@\n+  ins_cost(400);\n@@ -7149,1 +7182,1 @@\n-  predicate(!VM_Version::supports_avx512vl() &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7168,2 +7201,2 @@\n-  predicate(!VM_Version::supports_avx512vl() &&\n-            Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n+  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n+            n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7190,1 +7223,1 @@\n-  predicate(VM_Version::supports_avx512vl() &&\n+  predicate(n->bottom_type()->isa_vectmask() &&\n@@ -7209,1 +7242,1 @@\n-  predicate(!VM_Version::supports_avx512vl() &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7227,1 +7260,1 @@\n-  predicate((UseAVX == 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7246,1 +7279,1 @@\n-  predicate((UseAVX == 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7264,2 +7297,1 @@\n-  predicate(UseAVX > 2 &&\n-            (!VM_Version::supports_avx512vl() &&\n+  predicate((n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7300,2 +7332,1 @@\n-  predicate(UseAVX > 2 &&\n-            VM_Version::supports_avx512vl() && \/\/ src1\n+  predicate(n->bottom_type()->isa_vectmask() &&\n@@ -7834,1 +7865,1 @@\n-  predicate(!VM_Version::supports_avx512vlbw());\n+  predicate(n->bottom_type()->isa_vectmask() == NULL && !VM_Version::supports_avx512vlbw());\n@@ -7847,0 +7878,12 @@\n+instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI scratch) %{\n+  predicate(n->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP xtmp, TEMP scratch);\n+  format %{ \"vector_loadmask_64byte $dst,$src\\n\\t\" %}\n+  ins_encode %{\n+    __ load_vector_mask64($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister, $scratch$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n@@ -7848,1 +7891,1 @@\n-  predicate(VM_Version::supports_avx512vlbw());\n+  predicate(n->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());\n@@ -7854,1 +7897,1 @@\n-    __ evpcmp(T_BYTE, $dst$$KRegister, k0, $src$$XMMRegister, ExternalAddress(vector_masked_cmp_bits()),\n+    __ evpcmp(T_BYTE, $dst$$KRegister, k0, $src$$XMMRegister, ExternalAddress(vector_mask_cmp_bits()),\n@@ -7894,1 +7937,2 @@\n-  predicate(Matcher::vector_length(n) == 16 && !VM_Version::supports_avx512bw());\n+  predicate(Matcher::vector_length(n) == 16 && !VM_Version::supports_avx512bw() &&\n+            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -8012,0 +8056,13 @@\n+instruct vstoreMask64(vec dst, kReg mask, immI size, rRegI scratch) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP_DEF dst, TEMP scratch);\n+  format %{ \"vector_store_mask64 $dst,$mask\\t!\" %}\n+  ins_encode %{\n+    __ evmovdqul($dst$$XMMRegister, $mask$$KRegister, ExternalAddress(vector_int_mask_cmp_bits()),\n+                 false, Assembler::AVX_512bit, $scratch$$Register);\n+    __ evpmovdb($dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_512bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -8013,1 +8070,1 @@\n-  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask());\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());\n@@ -8677,1 +8734,1 @@\n-  format %{ \"vpadd_masked $dst, $dst, $src2\\t! add masked operation\" %}\n+  format %{ \"vpadd_masked $dst, $dst, $src2, $mask\\t! add masked operation\" %}\n@@ -8695,1 +8752,1 @@\n-  format %{ \"vpadd_masked $dst, $dst, $src2\\t! add masked operation\" %}\n+  format %{ \"vpadd_masked $dst, $dst, $src2, $mask\\t! add masked operation\" %}\n@@ -8708,1 +8765,1 @@\n-  format %{ \"vxor_masked $dst, $dst, $src2\\t! xor masked operation\" %}\n+  format %{ \"vxor_masked $dst, $dst, $src2, $mask\\t! xor masked operation\" %}\n@@ -8721,1 +8778,1 @@\n-  format %{ \"vxor_masked $dst, $dst, $src2\\t! xor masked operation\" %}\n+  format %{ \"vxor_masked $dst, $dst, $src2, $mask\\t! xor masked operation\" %}\n@@ -8734,1 +8791,1 @@\n-  format %{ \"vor_masked $dst, $dst, $src2\\t! or masked operation\" %}\n+  format %{ \"vor_masked $dst, $dst, $src2, $mask\\t! or masked operation\" %}\n@@ -8747,1 +8804,1 @@\n-  format %{ \"vor_masked $dst, $dst, $src2\\t! or masked operation\" %}\n+  format %{ \"vor_masked $dst, $dst, $src2, $mask\\t! or masked operation\" %}\n@@ -8760,1 +8817,1 @@\n-  format %{ \"vand_masked $dst, $dst, $src2\\t! and masked operation\" %}\n+  format %{ \"vand_masked $dst, $dst, $src2, $mask\\t! and masked operation\" %}\n@@ -8773,1 +8830,1 @@\n-  format %{ \"vand_masked $dst, $dst, $src2\\t! and masked operation\" %}\n+  format %{ \"vand_masked $dst, $dst, $src2, $mask\\t! and masked operation\" %}\n@@ -8791,1 +8848,1 @@\n-  format %{ \"vpsub_masked $dst, $dst, $src2\\t! sub masked operation\" %}\n+  format %{ \"vpsub_masked $dst, $dst, $src2, $mask\\t! sub masked operation\" %}\n@@ -8809,1 +8866,1 @@\n-  format %{ \"vpsub_masked $dst, $dst, $src2\\t! sub masked operation\" %}\n+  format %{ \"vpsub_masked $dst, $dst, $src2, $mask\\t! sub masked operation\" %}\n@@ -8826,1 +8883,1 @@\n-  format %{ \"vpmul_masked $dst, $dst, $src2\\t! mul masked operation\" %}\n+  format %{ \"vpmul_masked $dst, $dst, $src2, $mask\\t! mul masked operation\" %}\n@@ -8843,1 +8900,1 @@\n-  format %{ \"vpmul_masked $dst, $dst, $src2\\t! mul masked operation\" %}\n+  format %{ \"vpmul_masked $dst, $dst, $src2, $mask\\t! mul masked operation\" %}\n@@ -8854,0 +8911,31 @@\n+instruct vsqrt_reg_masked(vec dst, kReg mask) %{\n+  match(Set dst (SqrtVF dst mask));\n+  match(Set dst (SqrtVD dst mask));\n+  ins_cost(100);\n+  format %{ \"vpsqrt_masked $dst, $mask\\t! sqrt masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsqrt_mem_masked(vec dst, memory src, kReg mask) %{\n+  match(Set dst (SqrtVF src mask));\n+  match(Set dst (SqrtVD src mask));\n+  ins_cost(100);\n+  format %{ \"vpsqrt_masked $dst, $src, $mask\\t! sqrt masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n@@ -8857,1 +8945,1 @@\n-  format %{ \"vpdiv_masked $dst, $dst, $src2\\t! div masked operation\" %}\n+  format %{ \"vpdiv_masked $dst, $dst, $src2, $mask\\t! div masked operation\" %}\n@@ -8871,1 +8959,1 @@\n-  format %{ \"vpdiv_masked $dst, $dst, $src2\\t! div masked operation\" %}\n+  format %{ \"vpdiv_masked $dst, $dst, $src2, $mask\\t! div masked operation\" %}\n@@ -8882,0 +8970,43 @@\n+instruct vrol_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (RotateLeftV (Binary dst shift) mask));\n+  match(Set dst (RotateRightV (Binary dst shift) mask));\n+  format %{ \"vprotate_imm_masked $dst, $dst, $shift, $mask\\t! rotate masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrol_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (RotateLeftV (Binary dst src2) mask));\n+  match(Set dst (RotateRightV (Binary dst src2) mask));\n+  format %{ \"vrotate_masked $dst, $dst, $src2, $mask\\t! rotate masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst (LShiftCntV shift)) mask));\n+  match(Set dst (LShiftVI (Binary dst (LShiftCntV shift)) mask));\n+  match(Set dst (LShiftVL (Binary dst (LShiftCntV shift)) mask));\n+  format %{ \"vplshift_imm_masked $dst, $dst, $shift, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -8886,1 +9017,1 @@\n-  format %{ \"vplshift_masked $dst, $dst, $src2\\t! lshift masked operation\" %}\n+  format %{ \"vplshift_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n@@ -8902,1 +9033,1 @@\n-  format %{ \"vplshift_masked $dst, $dst, $src2\\t! lshift masked operation\" %}\n+  format %{ \"vplshift_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n@@ -8913,0 +9044,15 @@\n+instruct vrshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (RShiftVI (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (RShiftVL (Binary dst (RShiftCntV shift)) mask));\n+  format %{ \"vprshift_imm_masked $dst, $dst, $shift, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -8917,1 +9063,1 @@\n-  format %{ \"vprshift_masked $dst, $dst, $src2\\t! rshift masked operation\" %}\n+  format %{ \"vprshift_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n@@ -8933,1 +9079,1 @@\n-  format %{ \"vprshift_masked $dst, $dst, $src2\\t! rshift masked operation\" %}\n+  format %{ \"vprshift_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n@@ -8944,0 +9090,15 @@\n+instruct vurshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (URShiftVI (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (URShiftVL (Binary dst (RShiftCntV shift)) mask));\n+  format %{ \"vpurshift_imm_masked $dst, $dst, $shift, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -8948,1 +9109,1 @@\n-  format %{ \"vpurshift_masked $dst, $dst, $src2\\t! urshift masked operation\" %}\n+  format %{ \"vpurshift_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n@@ -8964,1 +9125,1 @@\n-  format %{ \"vpurshift_masked $dst, $dst, $src2\\t! urshift masked operation\" %}\n+  format %{ \"vpurshift_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n@@ -8977,1 +9138,1 @@\n-  format %{ \"vpmax_masked $dst, $dst, $src2\\t! max masked operation\" %}\n+  format %{ \"vpmax_masked $dst, $dst, $src2, $mask\\t! max masked operation\" %}\n@@ -8990,1 +9151,1 @@\n-  format %{ \"vpmax_masked $dst, $dst, $src2\\t! max masked operation\" %}\n+  format %{ \"vpmax_masked $dst, $dst, $src2, $mask\\t! max masked operation\" %}\n@@ -9003,1 +9164,1 @@\n-  format %{ \"vpmin_masked $dst, $dst, $src2\\t! min masked operation\" %}\n+  format %{ \"vpmin_masked $dst, $dst, $src2, $mask\\t! min masked operation\" %}\n@@ -9016,1 +9177,1 @@\n-  format %{ \"vpmin_masked $dst, $dst, $src2\\t! min masked operation\" %}\n+  format %{ \"vpmin_masked $dst, $dst, $src2, $mask\\t! min masked operation\" %}\n@@ -9029,1 +9190,1 @@\n-  format %{ \"vprearrange_masked $dst, $dst, $src2\\t! rearrange masked operation\" %}\n+  format %{ \"vprearrange_masked $dst, $dst, $src2, $mask\\t! rearrange masked operation\" %}\n@@ -9088,1 +9249,1 @@\n-  format %{ \"vcmp_masked $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\\t! using $scratch as TEMP\" %}\n@@ -9139,2 +9300,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"mask_all_evexI $dst, $cnt \\t! mask all operation\" %}\n+  effect(TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $cnt \\t! using $tmp as TEMP\" %}\n@@ -9143,3 +9304,10 @@\n-    __ movq($tmp$$Register, $cnt$$constant);\n-    __ kmovql($dst$$KRegister, $tmp$$Register);\n-    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ movq($tmp$$Register, $cnt$$constant);\n+      __ kmovql($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ movq($tmp$$Register, $cnt$$constant);\n+      __ kmovwl($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n@@ -9152,2 +9320,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"mask_all_evexI $dst, $src \\t! mask all operation\" %}\n+  effect(TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $src \\t! using $tmp as TEMP\" %}\n@@ -9156,3 +9324,9 @@\n-    __ movslq($tmp$$Register, $src$$Register);\n-    __ kmovql($dst$$KRegister, $tmp$$Register);\n-    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ movslq($tmp$$Register, $src$$Register);\n+      __ kmovql($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ kmovwl($dst$$KRegister, $src$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n@@ -9169,2 +9343,33 @@\n-    __ kmovql($dst$$KRegister, $src$$Register);\n-    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ kmovql($dst$$KRegister, $src$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ kmovwl($dst$$KRegister, $src$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_not_immLT8(kReg dst, kReg src, rRegI rtmp, kReg ktmp, immI_M1 cnt) %{\n+  predicate(Matcher::vector_length(n) < 8 && VM_Version::supports_avx512dq());\n+  match(Set dst (XorVMask src (MaskAll cnt)));\n+  effect(TEMP_DEF dst, TEMP rtmp, TEMP ktmp);\n+  format %{ \"mask_not_LT8 $dst, $src, $cnt \\t!using $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    uint masklen = Matcher::vector_length(this);\n+    __ knot(masklen, $dst$$KRegister, $src$$KRegister, $ktmp$$KRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_not_imm(kReg dst, kReg src, immI_M1 cnt) %{\n+  predicate((Matcher::vector_length(n) == 8 && VM_Version::supports_avx512dq()) ||\n+            (Matcher::vector_length(n) == 16) ||\n+            (Matcher::vector_length(n) > 16 && VM_Version::supports_avx512bw()));\n+  match(Set dst (XorVMask src (MaskAll cnt)));\n+  format %{ \"mask_not $dst, $src, $cnt \\t! mask not operation\" %}\n+  ins_encode %{\n+    uint masklen = Matcher::vector_length(this);\n+    __ knot(masklen, $dst$$KRegister, $src$$KRegister);\n@@ -9181,1 +9386,1 @@\n-  format %{ \"mask_opers_evex $dst, $src1, $src2\\t!\" %}\n+  format %{ \"mask_opers_evex $dst, $src1, $src2\\t! using $kscratch as TEMP\" %}\n@@ -9187,0 +9392,1 @@\n+    masklen = (masklen < 16 && !VM_Version::supports_avx512dq()) ? 16 : masklen;\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":286,"deletions":80,"binary":false,"changes":366,"status":"modified"},{"patch":"@@ -2376,0 +2376,1 @@\n+  BasicType elem_bt = elem->array_element_basic_type();\n@@ -2377,4 +2378,1 @@\n-      \/\/ TODO: remove this condition once the backend is supported.\n-      \/\/ Workround to make tests pass on AVX-512\/SVE when predicate is not supported.\n-      \/\/ Could be removed once the backend is supported.\n-      Matcher::match_rule_supported_vector_masked(Op_StoreVectorMasked, MaxVectorSize, T_BOOLEAN)) {\n+      Matcher::match_rule_supported_vector_masked(Op_VectorLoadMask, length, elem_bt)) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1270,1 +1270,1 @@\n-      mask = gvn().transform(new VectorReinterpretNode(mask, elem_bt, from_mask_type, mem_elem_bt, to_mask_type));\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n@@ -1281,1 +1281,1 @@\n-      mask = gvn().transform(new VectorReinterpretNode(mask, elem_bt, from_mask_type, mem_elem_bt, to_mask_type));\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1390,2 +1390,0 @@\n-  BasicType  _dst_bt;\n-  BasicType  _src_bt;\n@@ -1397,11 +1395,3 @@\n-      : VectorNode(in, dst_vt), _src_vt(src_vt) {\n-     assert(!dst_vt->isa_vectmask() && !src_vt->isa_vectmask(), \"\");\n-     _src_bt = src_vt->element_basic_type();\n-     _dst_bt = dst_vt->element_basic_type();\n-     init_class_id(Class_VectorReinterpret);\n-  }\n-\n-  VectorReinterpretNode(Node* in, BasicType src_bt, const TypeVect* src_vt,\n-                        BasicType dst_bt, const TypeVect* dst_vt)\n-      : VectorNode(in, dst_vt), _src_vt(src_vt) {\n-     assert(dst_vt->isa_vectmask() && src_vt->isa_vectmask() || type2aelembytes(src_bt) >= type2aelembytes(dst_bt),\n+     : VectorNode(in, dst_vt), _src_vt(src_vt) {\n+     assert((!dst_vt->isa_vectmask() && !src_vt->isa_vectmask()) ||\n+            (type2aelembytes(src_vt->element_basic_type()) >= type2aelembytes(dst_vt->element_basic_type())),\n@@ -1409,2 +1399,0 @@\n-     _src_bt = src_bt;\n-     _dst_bt = dst_bt;\n@@ -1414,2 +1402,1 @@\n-  BasicType src_elem_type() { return _src_bt; }\n-  BasicType dst_elem_type() { return _dst_bt; }\n+  const TypeVect* src_type() { return _src_vt; }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":4,"deletions":17,"binary":false,"changes":21,"status":"modified"}]}