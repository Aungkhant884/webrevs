{"files":[{"patch":"@@ -1482,15 +1482,15 @@\n-generate(SpecialCases, [[\"ccmn\",   \"__ ccmn(zr, zr, 3u, Assembler::LE);\",                \"ccmn\\txzr, xzr, #3, LE\"],\n-                        [\"ccmnw\",  \"__ ccmnw(zr, zr, 5u, Assembler::EQ);\",               \"ccmn\\twzr, wzr, #5, EQ\"],\n-                        [\"ccmp\",   \"__ ccmp(zr, 1, 4u, Assembler::NE);\",                 \"ccmp\\txzr, 1, #4, NE\"],\n-                        [\"ccmpw\",  \"__ ccmpw(zr, 2, 2, Assembler::GT);\",                 \"ccmp\\twzr, 2, #2, GT\"],\n-                        [\"extr\",   \"__ extr(zr, zr, zr, 0);\",                            \"extr\\txzr, xzr, xzr, 0\"],\n-                        [\"stlxp\",  \"__ stlxp(r0, zr, zr, sp);\",                          \"stlxp\\tw0, xzr, xzr, [sp]\"],\n-                        [\"stlxpw\", \"__ stlxpw(r2, zr, zr, r3);\",                         \"stlxp\\tw2, wzr, wzr, [x3]\"],\n-                        [\"stxp\",   \"__ stxp(r4, zr, zr, r5);\",                           \"stxp\\tw4, xzr, xzr, [x5]\"],\n-                        [\"stxpw\",  \"__ stxpw(r6, zr, zr, sp);\",                          \"stxp\\tw6, wzr, wzr, [sp]\"],\n-                        [\"dup\",    \"__ dup(v0, __ T16B, zr);\",                           \"dup\\tv0.16b, wzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T1D, 0, zr);\",                         \"mov\\tv1.d[0], xzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T2S, 1, zr);\",                         \"mov\\tv1.s[1], wzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T4H, 2, zr);\",                         \"mov\\tv1.h[2], wzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T8B, 3, zr);\",                         \"mov\\tv1.b[3], wzr\"],\n-                        [\"ld1\",    \"__ ld1(v31, v0, __ T2D, Address(__ post(r1, r0)));\", \"ld1\\t{v31.2d, v0.2d}, [x1], x0\"],\n+generate(SpecialCases, [[\"ccmn\",    \"__ ccmn(zr, zr, 3u, Assembler::LE);\",                \"ccmn\\txzr, xzr, #3, LE\"],\n+                        [\"ccmnw\",   \"__ ccmnw(zr, zr, 5u, Assembler::EQ);\",               \"ccmn\\twzr, wzr, #5, EQ\"],\n+                        [\"ccmp\",    \"__ ccmp(zr, 1, 4u, Assembler::NE);\",                 \"ccmp\\txzr, 1, #4, NE\"],\n+                        [\"ccmpw\",   \"__ ccmpw(zr, 2, 2, Assembler::GT);\",                 \"ccmp\\twzr, 2, #2, GT\"],\n+                        [\"extr\",    \"__ extr(zr, zr, zr, 0);\",                            \"extr\\txzr, xzr, xzr, 0\"],\n+                        [\"stlxp\",   \"__ stlxp(r0, zr, zr, sp);\",                          \"stlxp\\tw0, xzr, xzr, [sp]\"],\n+                        [\"stlxpw\",  \"__ stlxpw(r2, zr, zr, r3);\",                         \"stlxp\\tw2, wzr, wzr, [x3]\"],\n+                        [\"stxp\",    \"__ stxp(r4, zr, zr, r5);\",                           \"stxp\\tw4, xzr, xzr, [x5]\"],\n+                        [\"stxpw\",   \"__ stxpw(r6, zr, zr, sp);\",                          \"stxp\\tw6, wzr, wzr, [sp]\"],\n+                        [\"dup\",     \"__ dup(v0, __ T16B, zr);\",                           \"dup\\tv0.16b, wzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T1D, 0, zr);\",                         \"mov\\tv1.d[0], xzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T2S, 1, zr);\",                         \"mov\\tv1.s[1], wzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T4H, 2, zr);\",                         \"mov\\tv1.h[2], wzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T8B, 3, zr);\",                         \"mov\\tv1.b[3], wzr\"],\n+                        [\"ld1\",     \"__ ld1(v31, v0, __ T2D, Address(__ post(r1, r0)));\", \"ld1\\t{v31.2d, v0.2d}, [x1], x0\"],\n@@ -1498,33 +1498,79 @@\n-                        [\"cpy\",    \"__ sve_cpy(z0, __ S, p0, v1);\",                      \"mov\\tz0.s, p0\/m, s1\"],\n-                        [\"inc\",    \"__ sve_inc(r0, __ S);\",                              \"incw\\tx0\"],\n-                        [\"dec\",    \"__ sve_dec(r1, __ H);\",                              \"dech\\tx1\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ B, z1, 7);\",                       \"lsl\\tz0.b, z1.b, #7\"],\n-                        [\"lsl\",    \"__ sve_lsl(z21, __ H, z1, 15);\",                     \"lsl\\tz21.h, z1.h, #15\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ S, z1, 31);\",                      \"lsl\\tz0.s, z1.s, #31\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ D, z1, 63);\",                      \"lsl\\tz0.d, z1.d, #63\"],\n-                        [\"lsr\",    \"__ sve_lsr(z0, __ B, z1, 7);\",                       \"lsr\\tz0.b, z1.b, #7\"],\n-                        [\"asr\",    \"__ sve_asr(z0, __ H, z11, 15);\",                     \"asr\\tz0.h, z11.h, #15\"],\n-                        [\"lsr\",    \"__ sve_lsr(z30, __ S, z1, 31);\",                     \"lsr\\tz30.s, z1.s, #31\"],\n-                        [\"asr\",    \"__ sve_asr(z0, __ D, z1, 63);\",                      \"asr\\tz0.d, z1.d, #63\"],\n-                        [\"addvl\",  \"__ sve_addvl(sp, r0, 31);\",                          \"addvl\\tsp, x0, #31\"],\n-                        [\"addpl\",  \"__ sve_addpl(r1, sp, -32);\",                         \"addpl\\tx1, sp, -32\"],\n-                        [\"cntp\",   \"__ sve_cntp(r8, __ B, p0, p1);\",                     \"cntp\\tx8, p0, p1.b\"],\n-                        [\"dup\",    \"__ sve_dup(z0, __ B, 127);\",                         \"dup\\tz0.b, 127\"],\n-                        [\"dup\",    \"__ sve_dup(z1, __ H, -128);\",                        \"dup\\tz1.h, -128\"],\n-                        [\"dup\",    \"__ sve_dup(z2, __ S, 32512);\",                       \"dup\\tz2.s, 32512\"],\n-                        [\"dup\",    \"__ sve_dup(z7, __ D, -32768);\",                      \"dup\\tz7.d, -32768\"],\n-                        [\"ld1b\",   \"__ sve_ld1b(z0, __ B, p0, Address(sp));\",            \"ld1b\\t{z0.b}, p0\/z, [sp]\"],\n-                        [\"ld1h\",   \"__ sve_ld1h(z10, __ H, p1, Address(sp, -8));\",       \"ld1h\\t{z10.h}, p1\/z, [sp, #-8, MUL VL]\"],\n-                        [\"ld1w\",   \"__ sve_ld1w(z20, __ S, p2, Address(r0, 7));\",        \"ld1w\\t{z20.s}, p2\/z, [x0, #7, MUL VL]\"],\n-                        [\"ld1b\",   \"__ sve_ld1b(z30, __ B, p3, Address(sp, r8));\",       \"ld1b\\t{z30.b}, p3\/z, [sp, x8]\"],\n-                        [\"ld1w\",   \"__ sve_ld1w(z0, __ S, p4, Address(sp, r28));\",       \"ld1w\\t{z0.s}, p4\/z, [sp, x28, LSL #2]\"],\n-                        [\"ld1d\",   \"__ sve_ld1d(z11, __ D, p5, Address(r0, r1));\",       \"ld1d\\t{z11.d}, p5\/z, [x0, x1, LSL #3]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z22, __ B, p6, Address(sp));\",           \"st1b\\t{z22.b}, p6, [sp]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z31, __ B, p7, Address(sp, -8));\",       \"st1b\\t{z31.b}, p7, [sp, #-8, MUL VL]\"],\n-                        [\"st1w\",   \"__ sve_st1w(z0, __ S, p1, Address(r0, 7));\",         \"st1w\\t{z0.s}, p1, [x0, #7, MUL VL]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z0, __ B, p2, Address(sp, r1));\",        \"st1b\\t{z0.b}, p2, [sp, x1]\"],\n-                        [\"st1h\",   \"__ sve_st1h(z0, __ H, p3, Address(sp, r8));\",        \"st1h\\t{z0.h}, p3, [sp, x8, LSL #1]\"],\n-                        [\"st1d\",   \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n-                        [\"ldr\",    \"__ sve_ldr(z0, Address(sp));\",                       \"ldr\\tz0, [sp]\"],\n-                        [\"ldr\",    \"__ sve_ldr(z31, Address(sp, -256));\",                \"ldr\\tz31, [sp, #-256, MUL VL]\"],\n-                        [\"str\",    \"__ sve_str(z8, Address(r8, 255));\",                  \"str\\tz8, [x8, #255, MUL VL]\"],\n+                        [\"cpy\",     \"__ sve_cpy(z0, __ S, p0, v1);\",                      \"mov\\tz0.s, p0\/m, s1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z0, __ B, p0, 127, true);\",               \"mov\\tz0.b, p0\/m, 127\"],\n+                        [\"cpy\",     \"__ sve_cpy(z1, __ H, p0, -128, true);\",              \"mov\\tz1.h, p0\/m, -128\"],\n+                        [\"cpy\",     \"__ sve_cpy(z2, __ S, p0, 32512, true);\",             \"mov\\tz2.s, p0\/m, 32512\"],\n+                        [\"cpy\",     \"__ sve_cpy(z5, __ D, p0, -32768, false);\",           \"mov\\tz5.d, p0\/z, -32768\"],\n+                        [\"cpy\",     \"__ sve_cpy(z10, __ B, p0, -1, false);\",              \"mov\\tz10.b, p0\/z, -1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z11, __ S, p0, -1, false);\",              \"mov\\tz11.s, p0\/z, -1\"],\n+                        [\"inc\",     \"__ sve_inc(r0, __ S);\",                              \"incw\\tx0\"],\n+                        [\"dec\",     \"__ sve_dec(r1, __ H);\",                              \"dech\\tx1\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, z1, 7);\",                       \"lsl\\tz0.b, z1.b, #7\"],\n+                        [\"lsl\",     \"__ sve_lsl(z21, __ H, z1, 15);\",                     \"lsl\\tz21.h, z1.h, #15\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ S, z1, 31);\",                      \"lsl\\tz0.s, z1.s, #31\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ D, z1, 63);\",                      \"lsl\\tz0.d, z1.d, #63\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, z1, 7);\",                       \"lsr\\tz0.b, z1.b, #7\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ H, z11, 15);\",                     \"asr\\tz0.h, z11.h, #15\"],\n+                        [\"lsr\",     \"__ sve_lsr(z30, __ S, z1, 31);\",                     \"lsr\\tz30.s, z1.s, #31\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ D, z1, 63);\",                      \"asr\\tz0.d, z1.d, #63\"],\n+                        [\"addvl\",   \"__ sve_addvl(sp, r0, 31);\",                          \"addvl\\tsp, x0, #31\"],\n+                        [\"addpl\",   \"__ sve_addpl(r1, sp, -32);\",                         \"addpl\\tx1, sp, -32\"],\n+                        [\"cntp\",    \"__ sve_cntp(r8, __ B, p0, p1);\",                     \"cntp\\tx8, p0, p1.b\"],\n+                        [\"dup\",     \"__ sve_dup(z0, __ B, 127);\",                         \"dup\\tz0.b, 127\"],\n+                        [\"dup\",     \"__ sve_dup(z1, __ H, -128);\",                        \"dup\\tz1.h, -128\"],\n+                        [\"dup\",     \"__ sve_dup(z2, __ S, 32512);\",                       \"dup\\tz2.s, 32512\"],\n+                        [\"dup\",     \"__ sve_dup(z7, __ D, -32768);\",                      \"dup\\tz7.d, -32768\"],\n+                        [\"dup\",     \"__ sve_dup(z10, __ B, -1);\",                         \"dup\\tz10.b, -1\"],\n+                        [\"dup\",     \"__ sve_dup(z11, __ S, -1);\",                         \"dup\\tz11.s, -1\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ B, p0, Address(sp));\",            \"ld1b\\t{z0.b}, p0\/z, [sp]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ H, p1, Address(sp));\",            \"ld1b\\t{z0.h}, p1\/z, [sp]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ S, p2, Address(sp, r8));\",        \"ld1b\\t{z0.s}, p2\/z, [sp, x8]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ D, p3, Address(sp, 7));\",         \"ld1b\\t{z0.d}, p3\/z, [sp, #7, MUL VL]\"],\n+                        [\"ld1h\",    \"__ sve_ld1h(z10, __ H, p1, Address(sp, -8));\",       \"ld1h\\t{z10.h}, p1\/z, [sp, #-8, MUL VL]\"],\n+                        [\"ld1w\",    \"__ sve_ld1w(z20, __ S, p2, Address(r0, 7));\",        \"ld1w\\t{z20.s}, p2\/z, [x0, #7, MUL VL]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z30, __ B, p3, Address(sp, r8));\",       \"ld1b\\t{z30.b}, p3\/z, [sp, x8]\"],\n+                        [\"ld1w\",    \"__ sve_ld1w(z0, __ S, p4, Address(sp, r28));\",       \"ld1w\\t{z0.s}, p4\/z, [sp, x28, LSL #2]\"],\n+                        [\"ld1d\",    \"__ sve_ld1d(z11, __ D, p5, Address(r0, r1));\",       \"ld1d\\t{z11.d}, p5\/z, [x0, x1, LSL #3]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z22, __ B, p6, Address(sp));\",           \"st1b\\t{z22.b}, p6, [sp]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z31, __ B, p7, Address(sp, -8));\",       \"st1b\\t{z31.b}, p7, [sp, #-8, MUL VL]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ H, p1, Address(sp));\",            \"st1b\\t{z0.h}, p1, [sp]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ S, p2, Address(sp, r8));\",        \"st1b\\t{z0.s}, p2, [sp, x8]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ D, p3, Address(sp));\",            \"st1b\\t{z0.d}, p3, [sp]\"],\n+                        [\"st1w\",    \"__ sve_st1w(z0, __ S, p1, Address(r0, 7));\",         \"st1w\\t{z0.s}, p1, [x0, #7, MUL VL]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ B, p2, Address(sp, r1));\",        \"st1b\\t{z0.b}, p2, [sp, x1]\"],\n+                        [\"st1h\",    \"__ sve_st1h(z0, __ H, p3, Address(sp, r8));\",        \"st1h\\t{z0.h}, p3, [sp, x8, LSL #1]\"],\n+                        [\"st1d\",    \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n+                        [\"ldr\",     \"__ sve_ldr(z0, Address(sp));\",                       \"ldr\\tz0, [sp]\"],\n+                        [\"ldr\",     \"__ sve_ldr(z31, Address(sp, -256));\",                \"ldr\\tz31, [sp, #-256, MUL VL]\"],\n+                        [\"str\",     \"__ sve_str(z8, Address(r8, 255));\",                  \"str\\tz8, [x8, #255, MUL VL]\"],\n+                        [\"sel\",     \"__ sve_sel(z0, __ B, p0, z1, z2);\",                  \"sel\\tz0.b, p0, z1.b, z2.b\"],\n+                        [\"sel\",     \"__ sve_sel(z4, __ D, p0, z5, z6);\",                  \"sel\\tz4.d, p0, z5.d, z6.d\"],\n+                        [\"cmpeq\",   \"__ sve_cmpeq(p1, __ B, p0, z0, z1);\",                \"cmpeq\\tp1.b, p0\/z, z0.b, z1.b\"],\n+                        [\"cmpne\",   \"__ sve_cmpne(p1, __ H, p0, z2, z3);\",                \"cmpne\\tp1.h, p0\/z, z2.h, z3.h\"],\n+                        [\"cmpge\",   \"__ sve_cmpge(p1, __ S, p2, z4, z5);\",                \"cmpge\\tp1.s, p2\/z, z4.s, z5.s\"],\n+                        [\"cmpgt\",   \"__ sve_cmpgt(p1, __ D, p3, z6, z7);\",                \"cmpgt\\tp1.d, p3\/z, z6.d, z7.d\"],\n+                        [\"cmple\",   \"__ sve_cmpge(p2, __ B, p0, z10, z11);\",              \"cmple\\tp2.b, p0\/z, z11.b, z10.b\"],\n+                        [\"cmplt\",   \"__ sve_cmpgt(p3, __ S, p0, z16, z17);\",              \"cmplt\\tp3.s, p0\/z, z17.s, z16.s\"],\n+                        [\"cmpeq\",   \"__ sve_cmpeq(p1, __ B, p4, z0, 15);\",                \"cmpeq\\tp1.b, p4\/z, z0.b, #15\"],\n+                        [\"cmpne\",   \"__ sve_cmpne(p1, __ H, p0, z2, -16);\",               \"cmpne\\tp1.h, p0\/z, z2.h, #-16\"],\n+                        [\"cmple\",   \"__ sve_cmple(p1, __ S, p1, z4, 0);\",                 \"cmple\\tp1.s, p1\/z, z4.s, #0\"],\n+                        [\"cmplt\",   \"__ sve_cmplt(p1, __ D, p2, z6, -1);\",                \"cmplt\\tp1.d, p2\/z, z6.d, #-1\"],\n+                        [\"cmpge\",   \"__ sve_cmpge(p1, __ S, p3, z4, 5);\",                 \"cmpge\\tp1.s, p3\/z, z4.s, #5\"],\n+                        [\"cmpgt\",   \"__ sve_cmpgt(p1, __ B, p4, z6, -2);\",                \"cmpgt\\tp1.b, p4\/z, z6.b, #-2\"],\n+                        [\"fcmeq\",   \"__ sve_fcmeq(p1, __ S, p0, z0, z1);\",                \"fcmeq\\tp1.s, p0\/z, z0.s, z1.s\"],\n+                        [\"fcmne\",   \"__ sve_fcmne(p1, __ D, p0, z2, z3);\",                \"fcmne\\tp1.d, p0\/z, z2.d, z3.d\"],\n+                        [\"fcmgt\",   \"__ sve_fcmgt(p1, __ S, p2, z4, z5);\",                \"fcmgt\\tp1.s, p2\/z, z4.s, z5.s\"],\n+                        [\"fcmge\",   \"__ sve_fcmge(p1, __ D, p3, z6, z7);\",                \"fcmge\\tp1.d, p3\/z, z6.d, z7.d\"],\n+                        [\"fcmlt\",   \"__ sve_fcmgt(p2, __ S, p0, z10, z11);\",              \"fcmlt\\tp2.s, p0\/z, z11.s, z10.s\"],\n+                        [\"fcmle\",   \"__ sve_fcmge(p3, __ D, p0, z16, z17);\",              \"fcmle\\tp3.d, p0\/z, z17.d, z16.d\"],\n+                        [\"uunpkhi\", \"__ sve_uunpkhi(z0, __ H, z1);\",                      \"uunpkhi\\tz0.h, z1.b\"],\n+                        [\"uunpklo\", \"__ sve_uunpklo(z4, __ S, z5);\",                      \"uunpklo\\tz4.s, z5.h\"],\n+                        [\"sunpkhi\", \"__ sve_sunpkhi(z6, __ D, z7);\",                      \"sunpkhi\\tz6.d, z7.s\"],\n+                        [\"sunpklo\", \"__ sve_sunpklo(z10, __ H, z11);\",                    \"sunpklo\\tz10.h, z11.b\"],\n+                        [\"whilelt\", \"__ sve_whilelt(p0, __ B, r1, r2);\",                  \"whilelt\\tp0.b, x1, x2\"],\n+                        [\"whilelt\", \"__ sve_whileltw(p1, __ H, r3, r4);\",                 \"whilelt\\tp1.h, w3, w4\"],\n+                        [\"whilele\", \"__ sve_whilele(p2, __ S, r5, r6);\",                  \"whilele\\tp2.s, x5, x6\"],\n+                        [\"whilele\", \"__ sve_whilelew(p3, __ D, r10, r11);\",               \"whilele\\tp3.d, w10, w11\"],\n+                        [\"whilelo\", \"__ sve_whilelo(p4, __ B, r1, r2);\",                  \"whilelo\\tp4.b, x1, x2\"],\n+                        [\"whilelo\", \"__ sve_whilelow(p0, __ H, r3, r4);\",                 \"whilelo\\tp0.h, w3, w4\"],\n+                        [\"whilels\", \"__ sve_whilels(p1, __ S, r5, r6);\",                  \"whilels\\tp1.s, x5, x6\"],\n+                        [\"whilels\", \"__ sve_whilelsw(p2, __ D, r10, r11);\",               \"whilels\\tp2.d, w10, w11\"],\n@@ -1599,0 +1645,2 @@\n+                       [\"uzp1\", \"ZZZ\"],\n+                       [\"uzp2\", \"ZZZ\"],\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64-asmtest.py","additions":96,"deletions":48,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -2490,0 +2490,4 @@\n+    if (bt == T_BOOLEAN) {\n+      \/\/ To support vector api load\/store mask.\n+      return MaxVectorSize \/ 8;\n+    }\n@@ -2517,1 +2521,1 @@\n-  if (UseSVE > 0 && 16 <= len && len <= 256) {\n+  if (UseSVE > 0 && 2 <= len && len <= 256) {\n@@ -16644,1 +16648,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 4);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 4);\n@@ -16655,1 +16659,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 8);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 8);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -104,0 +104,33 @@\n+  static inline uint vector_length(const MachNode* n) {\n+    const TypeVect* vt = n->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static inline uint vector_length(const MachNode* use, const MachOper* opnd) {\n+    int def_idx = use->operand_index(opnd);\n+    Node* def = use->in(def_idx);\n+    const TypeVect* vt = def->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static Assembler::SIMD_RegVariant elemBytes_to_regVariant(int esize) {\n+    switch(esize) {\n+      case 1:\n+        return Assembler::B;\n+      case 2:\n+        return Assembler::H;\n+      case 4:\n+        return Assembler::S;\n+      case 8:\n+        return Assembler::D;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+    return Assembler::INVALID;\n+  }\n+\n+  static Assembler::SIMD_RegVariant elemType_to_regVariant(BasicType bt) {\n+    return elemBytes_to_regVariant(type2aelembytes(bt));\n+  }\n+\n@@ -108,2 +141,3 @@\n-  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store,\n-                                   FloatRegister reg, PRegister pg, BasicType bt,\n+  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+                                   PRegister pg, BasicType mem_elem_bt,\n+                                   Assembler::SIMD_RegVariant vector_elem_size,\n@@ -112,2 +146,1 @@\n-    Assembler::SIMD_RegVariant type;\n-    int esize = type2aelembytes(bt);\n+    int esize = type2aelembytes(mem_elem_bt);\n@@ -119,1 +152,0 @@\n-        type = Assembler::B;\n@@ -123,1 +155,0 @@\n-        type = Assembler::H;\n@@ -127,1 +158,0 @@\n-        type = Assembler::S;\n@@ -131,1 +161,0 @@\n-        type = Assembler::D;\n@@ -137,1 +166,1 @@\n-      (masm.*insn)(reg, type, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n+      (masm.*insn)(reg, vector_elem_size, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n@@ -144,0 +173,30 @@\n+  static void sve_compare(C2_MacroAssembler masm, PRegister pd, BasicType bt,\n+                          PRegister pg, FloatRegister zn, FloatRegister zm, int cond) {\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (bt == T_FLOAT || bt == T_DOUBLE) {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    } else {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n@@ -165,1 +224,0 @@\n-      case Op_VectorBlend:\n@@ -175,1 +233,0 @@\n-      case Op_VectorLoadMask:\n@@ -177,1 +234,0 @@\n-      case Op_VectorMaskCmp:\n@@ -180,1 +236,0 @@\n-      case Op_VectorStoreMask:\n@@ -196,1 +251,0 @@\n-\n@@ -201,1 +255,1 @@\n-\/\/ Use predicated vector load\/store\n+\/\/ Unpredicated vector load\/store\n@@ -203,1 +257,2 @@\n-  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n@@ -209,0 +264,1 @@\n+    BasicType bt = vector_element_basic_type(this);\n@@ -210,1 +266,1 @@\n-                         vector_element_basic_type(this), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -217,1 +273,2 @@\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n@@ -223,0 +280,1 @@\n+    BasicType bt = vector_element_basic_type(this, $src);\n@@ -224,1 +282,1 @@\n-                         vector_element_basic_type(this, $src), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -230,0 +288,55 @@\n+\/\/ Predicated vector load\/store, based on the vector length of the node.\n+\/\/ Only load\/store values in the range of the memory_size. This is needed\n+\/\/ when the memory_size is lower than the hardware supported max vector size.\n+\/\/ And this might happen for Vector API mask vector load\/store.\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                       iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_LoadVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_LoadVector()->memory_size() != MaxVectorSize);\n+  match(Set dst (LoadVector mem));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_ldr $dst, $pTmp, $mem\\t # load vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                          iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_StoreVector()->memory_size() != MaxVectorSize);\n+  match(Set mem (StoreVector mem src));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_str $src, $pTmp, $mem\\t # store vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this, $src));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -860,1 +973,1 @@\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\"  %}\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n@@ -867,0 +980,255 @@\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n+            \"sve_cpy $dst, $pTmp, -1\\t # vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), -1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend\n+\n+instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) src3));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n+                 as_FloatRegister($src3$$reg), -1);\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend with compare\n+\n+instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n+                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src3, $src4\\t # vector cmp (sve)\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src3$$reg),\n+                as_FloatRegister($src4$$reg), (int)$cond$$constant);\n+    __ sve_sel(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector load mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskS(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to H)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to S)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskL(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 16);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector store mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 8);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (H to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (S to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(5 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (D to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ load\/store mask vector\n+\n+instruct vloadmask_loadV(vReg dst, vmemA mem) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # load vector mask (sve)\" %}\n+  ins_encode %{\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    Assembler::SIMD_RegVariant to_vect_size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n+                         T_BOOLEAN, to_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_neg(dst_reg, to_vect_size, ptrue, dst_reg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_vstoremask(vmemA mem, vReg src, vReg tmp, immI size) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= 2 &&\n+            n->as_StoreVector()->vect_type()->element_basic_type() == T_BOOLEAN);\n+  match(Set mem (StoreVector mem (VectorStoreMask src size)));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_neg $tmp, $src\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t # store vector mask (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant from_vect_size =\n+              elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_size, ptrue,\n+               as_FloatRegister($src$$reg));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n+                         ptrue, T_BOOLEAN, from_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":388,"deletions":20,"binary":false,"changes":408,"status":"modified"},{"patch":"@@ -91,0 +91,33 @@\n+  static inline uint vector_length(const MachNode* n) {\n+    const TypeVect* vt = n->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static inline uint vector_length(const MachNode* use, const MachOper* opnd) {\n+    int def_idx = use->operand_index(opnd);\n+    Node* def = use->in(def_idx);\n+    const TypeVect* vt = def->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static Assembler::SIMD_RegVariant elemBytes_to_regVariant(int esize) {\n+    switch(esize) {\n+      case 1:\n+        return Assembler::B;\n+      case 2:\n+        return Assembler::H;\n+      case 4:\n+        return Assembler::S;\n+      case 8:\n+        return Assembler::D;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+    return Assembler::INVALID;\n+  }\n+\n+  static Assembler::SIMD_RegVariant elemType_to_regVariant(BasicType bt) {\n+    return elemBytes_to_regVariant(type2aelembytes(bt));\n+  }\n+\n@@ -95,2 +128,3 @@\n-  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store,\n-                                   FloatRegister reg, PRegister pg, BasicType bt,\n+  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+                                   PRegister pg, BasicType mem_elem_bt,\n+                                   Assembler::SIMD_RegVariant vector_elem_size,\n@@ -99,2 +133,1 @@\n-    Assembler::SIMD_RegVariant type;\n-    int esize = type2aelembytes(bt);\n+    int esize = type2aelembytes(mem_elem_bt);\n@@ -106,1 +139,0 @@\n-        type = Assembler::B;\n@@ -110,1 +142,0 @@\n-        type = Assembler::H;\n@@ -114,1 +145,0 @@\n-        type = Assembler::S;\n@@ -118,1 +148,0 @@\n-        type = Assembler::D;\n@@ -124,1 +153,1 @@\n-      (masm.*insn)(reg, type, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n+      (masm.*insn)(reg, vector_elem_size, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n@@ -131,0 +160,30 @@\n+  static void sve_compare(C2_MacroAssembler masm, PRegister pd, BasicType bt,\n+                          PRegister pg, FloatRegister zn, FloatRegister zm, int cond) {\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (bt == T_FLOAT || bt == T_DOUBLE) {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    } else {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n@@ -152,1 +211,0 @@\n-      case Op_VectorBlend:\n@@ -162,1 +220,0 @@\n-      case Op_VectorLoadMask:\n@@ -164,1 +221,0 @@\n-      case Op_VectorMaskCmp:\n@@ -167,1 +223,0 @@\n-      case Op_VectorStoreMask:\n@@ -181,1 +236,0 @@\n-\n@@ -195,1 +249,1 @@\n-\/\/ Use predicated vector load\/store\n+\/\/ Unpredicated vector load\/store\n@@ -197,1 +251,2 @@\n-  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n@@ -203,0 +258,1 @@\n+    BasicType bt = vector_element_basic_type(this);\n@@ -204,1 +260,1 @@\n-                         vector_element_basic_type(this), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -211,1 +267,2 @@\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n@@ -217,0 +274,1 @@\n+    BasicType bt = vector_element_basic_type(this, $src);\n@@ -218,1 +276,1 @@\n-                         vector_element_basic_type(this, $src), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -224,0 +282,56 @@\n+\/\/ Predicated vector load\/store, based on the vector length of the node.\n+\/\/ Only load\/store values in the range of the memory_size. This is needed\n+\/\/ when the memory_size is lower than the hardware supported max vector size.\n+\/\/ And this might happen for Vector API mask vector load\/store.\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                       iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_LoadVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_LoadVector()->memory_size() != MaxVectorSize);\n+  match(Set dst (LoadVector mem));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_ldr $dst, $pTmp, $mem\\t # load vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                          iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_StoreVector()->memory_size() != MaxVectorSize);\n+  match(Set mem (StoreVector mem src));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_str $src, $pTmp, $mem\\t # store vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this, $src));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}dnl\n+\n@@ -525,1 +639,1 @@\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\"  %}\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n@@ -530,0 +644,255 @@\n+%}\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n+            \"sve_cpy $dst, $pTmp, -1\\t # vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), -1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend\n+\n+instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) src3));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n+                 as_FloatRegister($src3$$reg), -1);\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend with compare\n+\n+instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n+                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src3, $src4\\t # vector cmp (sve)\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src3$$reg),\n+                as_FloatRegister($src4$$reg), (int)$cond$$constant);\n+    __ sve_sel(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector load mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskS(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to H)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to S)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskL(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 16);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector store mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 8);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (H to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (S to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(5 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (D to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ load\/store mask vector\n+\n+instruct vloadmask_loadV(vReg dst, vmemA mem) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # load vector mask (sve)\" %}\n+  ins_encode %{\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    Assembler::SIMD_RegVariant to_vect_size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n+                         T_BOOLEAN, to_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_neg(dst_reg, to_vect_size, ptrue, dst_reg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_vstoremask(vmemA mem, vReg src, vReg tmp, immI size) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= 2 &&\n+            n->as_StoreVector()->vect_type()->element_basic_type() == T_BOOLEAN);\n+  match(Set mem (StoreVector mem (VectorStoreMask src size)));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_neg $tmp, $src\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t # store vector mask (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant from_vect_size =\n+              elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_size, ptrue,\n+               as_FloatRegister($src$$reg));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n+                         ptrue, T_BOOLEAN, from_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":389,"deletions":20,"binary":false,"changes":409,"status":"modified"},{"patch":"@@ -775,0 +775,6 @@\n+    __ sve_cpy(z0, __ B, p0, 127, true);               \/\/       mov     z0.b, p0\/m, 127\n+    __ sve_cpy(z1, __ H, p0, -128, true);              \/\/       mov     z1.h, p0\/m, -128\n+    __ sve_cpy(z2, __ S, p0, 32512, true);             \/\/       mov     z2.s, p0\/m, 32512\n+    __ sve_cpy(z5, __ D, p0, -32768, false);           \/\/       mov     z5.d, p0\/z, -32768\n+    __ sve_cpy(z10, __ B, p0, -1, false);              \/\/       mov     z10.b, p0\/z, -1\n+    __ sve_cpy(z11, __ S, p0, -1, false);              \/\/       mov     z11.s, p0\/z, -1\n@@ -792,0 +798,2 @@\n+    __ sve_dup(z10, __ B, -1);                         \/\/       dup     z10.b, -1\n+    __ sve_dup(z11, __ S, -1);                         \/\/       dup     z11.s, -1\n@@ -793,0 +801,3 @@\n+    __ sve_ld1b(z0, __ H, p1, Address(sp));            \/\/       ld1b    {z0.h}, p1\/z, [sp]\n+    __ sve_ld1b(z0, __ S, p2, Address(sp, r8));        \/\/       ld1b    {z0.s}, p2\/z, [sp, x8]\n+    __ sve_ld1b(z0, __ D, p3, Address(sp, 7));         \/\/       ld1b    {z0.d}, p3\/z, [sp, #7, MUL VL]\n@@ -800,0 +811,3 @@\n+    __ sve_st1b(z0, __ H, p1, Address(sp));            \/\/       st1b    {z0.h}, p1, [sp]\n+    __ sve_st1b(z0, __ S, p2, Address(sp, r8));        \/\/       st1b    {z0.s}, p2, [sp, x8]\n+    __ sve_st1b(z0, __ D, p3, Address(sp));            \/\/       st1b    {z0.d}, p3, [sp]\n@@ -807,0 +821,32 @@\n+    __ sve_sel(z0, __ B, p0, z1, z2);                  \/\/       sel     z0.b, p0, z1.b, z2.b\n+    __ sve_sel(z4, __ D, p0, z5, z6);                  \/\/       sel     z4.d, p0, z5.d, z6.d\n+    __ sve_cmpeq(p1, __ B, p0, z0, z1);                \/\/       cmpeq   p1.b, p0\/z, z0.b, z1.b\n+    __ sve_cmpne(p1, __ H, p0, z2, z3);                \/\/       cmpne   p1.h, p0\/z, z2.h, z3.h\n+    __ sve_cmpge(p1, __ S, p2, z4, z5);                \/\/       cmpge   p1.s, p2\/z, z4.s, z5.s\n+    __ sve_cmpgt(p1, __ D, p3, z6, z7);                \/\/       cmpgt   p1.d, p3\/z, z6.d, z7.d\n+    __ sve_cmpge(p2, __ B, p0, z10, z11);              \/\/       cmple   p2.b, p0\/z, z11.b, z10.b\n+    __ sve_cmpgt(p3, __ S, p0, z16, z17);              \/\/       cmplt   p3.s, p0\/z, z17.s, z16.s\n+    __ sve_cmpeq(p1, __ B, p4, z0, 15);                \/\/       cmpeq   p1.b, p4\/z, z0.b, #15\n+    __ sve_cmpne(p1, __ H, p0, z2, -16);               \/\/       cmpne   p1.h, p0\/z, z2.h, #-16\n+    __ sve_cmple(p1, __ S, p1, z4, 0);                 \/\/       cmple   p1.s, p1\/z, z4.s, #0\n+    __ sve_cmplt(p1, __ D, p2, z6, -1);                \/\/       cmplt   p1.d, p2\/z, z6.d, #-1\n+    __ sve_cmpge(p1, __ S, p3, z4, 5);                 \/\/       cmpge   p1.s, p3\/z, z4.s, #5\n+    __ sve_cmpgt(p1, __ B, p4, z6, -2);                \/\/       cmpgt   p1.b, p4\/z, z6.b, #-2\n+    __ sve_fcmeq(p1, __ S, p0, z0, z1);                \/\/       fcmeq   p1.s, p0\/z, z0.s, z1.s\n+    __ sve_fcmne(p1, __ D, p0, z2, z3);                \/\/       fcmne   p1.d, p0\/z, z2.d, z3.d\n+    __ sve_fcmgt(p1, __ S, p2, z4, z5);                \/\/       fcmgt   p1.s, p2\/z, z4.s, z5.s\n+    __ sve_fcmge(p1, __ D, p3, z6, z7);                \/\/       fcmge   p1.d, p3\/z, z6.d, z7.d\n+    __ sve_fcmgt(p2, __ S, p0, z10, z11);              \/\/       fcmlt   p2.s, p0\/z, z11.s, z10.s\n+    __ sve_fcmge(p3, __ D, p0, z16, z17);              \/\/       fcmle   p3.d, p0\/z, z17.d, z16.d\n+    __ sve_uunpkhi(z0, __ H, z1);                      \/\/       uunpkhi z0.h, z1.b\n+    __ sve_uunpklo(z4, __ S, z5);                      \/\/       uunpklo z4.s, z5.h\n+    __ sve_sunpkhi(z6, __ D, z7);                      \/\/       sunpkhi z6.d, z7.s\n+    __ sve_sunpklo(z10, __ H, z11);                    \/\/       sunpklo z10.h, z11.b\n+    __ sve_whilelt(p0, __ B, r1, r2);                  \/\/       whilelt p0.b, x1, x2\n+    __ sve_whileltw(p1, __ H, r3, r4);                 \/\/       whilelt p1.h, w3, w4\n+    __ sve_whilele(p2, __ S, r5, r6);                  \/\/       whilele p2.s, x5, x6\n+    __ sve_whilelew(p3, __ D, r10, r11);               \/\/       whilele p3.d, w10, w11\n+    __ sve_whilelo(p4, __ B, r1, r2);                  \/\/       whilelo p4.b, x1, x2\n+    __ sve_whilelow(p0, __ H, r3, r4);                 \/\/       whilelo p0.h, w3, w4\n+    __ sve_whilels(p1, __ S, r5, r6);                  \/\/       whilels p1.s, x5, x6\n+    __ sve_whilelsw(p2, __ D, r10, r11);               \/\/       whilels p2.d, w10, w11\n@@ -981,0 +1027,2 @@\n+    __ sve_uzp1(z9, __ D, z22, z11);                   \/\/       uzp1    z9.d, z22.d, z11.d\n+    __ sve_uzp2(z5, __ H, z30, z16);                   \/\/       uzp2    z5.h, z30.h, z16.h\n@@ -983,9 +1031,9 @@\n-    __ sve_andv(v9, __ D, p5, z11);                    \/\/       andv d9, p5, z11.d\n-    __ sve_orv(v5, __ H, p7, z16);                     \/\/       orv h5, p7, z16.h\n-    __ sve_eorv(v22, __ H, p3, z1);                    \/\/       eorv h22, p3, z1.h\n-    __ sve_smaxv(v8, __ D, p5, z16);                   \/\/       smaxv d8, p5, z16.d\n-    __ sve_sminv(v15, __ S, p1, z4);                   \/\/       sminv s15, p1, z4.s\n-    __ sve_fminv(v8, __ S, p1, z29);                   \/\/       fminv s8, p1, z29.s\n-    __ sve_fmaxv(v28, __ D, p4, z29);                  \/\/       fmaxv d28, p4, z29.d\n-    __ sve_fadda(v9, __ S, p3, z2);                    \/\/       fadda s9, p3, s9, z2.s\n-    __ sve_uaddv(v28, __ B, p0, z7);                   \/\/       uaddv d28, p0, z7.b\n+    __ sve_andv(v22, __ H, p3, z1);                    \/\/       andv h22, p3, z1.h\n+    __ sve_orv(v8, __ D, p5, z16);                     \/\/       orv d8, p5, z16.d\n+    __ sve_eorv(v15, __ S, p1, z4);                    \/\/       eorv s15, p1, z4.s\n+    __ sve_smaxv(v8, __ B, p1, z29);                   \/\/       smaxv b8, p1, z29.b\n+    __ sve_sminv(v28, __ D, p4, z29);                  \/\/       sminv d28, p4, z29.d\n+    __ sve_fminv(v9, __ S, p3, z2);                    \/\/       fminv s9, p3, z2.s\n+    __ sve_fmaxv(v28, __ S, p0, z7);                   \/\/       fmaxv s28, p0, z7.s\n+    __ sve_fadda(v26, __ S, p5, z17);                  \/\/       fadda s26, p5, s26, z17.s\n+    __ sve_uaddv(v8, __ D, p4, z21);                   \/\/       uaddv d8, p4, z21.d\n@@ -1010,7 +1058,7 @@\n-    0x14000000,     0x17ffffd7,     0x140002cd,     0x94000000,\n-    0x97ffffd4,     0x940002ca,     0x3400000a,     0x34fffa2a,\n-    0x340058ea,     0x35000008,     0x35fff9c8,     0x35005888,\n-    0xb400000b,     0xb4fff96b,     0xb400582b,     0xb500001d,\n-    0xb5fff91d,     0xb50057dd,     0x10000013,     0x10fff8b3,\n-    0x10005773,     0x90000013,     0x36300016,     0x3637f836,\n-    0x363056f6,     0x3758000c,     0x375ff7cc,     0x3758568c,\n+    0x14000000,     0x17ffffd7,     0x140002fd,     0x94000000,\n+    0x97ffffd4,     0x940002fa,     0x3400000a,     0x34fffa2a,\n+    0x34005eea,     0x35000008,     0x35fff9c8,     0x35005e88,\n+    0xb400000b,     0xb4fff96b,     0xb4005e2b,     0xb500001d,\n+    0xb5fff91d,     0xb5005ddd,     0x10000013,     0x10fff8b3,\n+    0x10005d73,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36305cf6,     0x3758000c,     0x375ff7cc,     0x37585c8c,\n@@ -1021,13 +1069,13 @@\n-    0x54005460,     0x54000001,     0x54fff541,     0x54005401,\n-    0x54000002,     0x54fff4e2,     0x540053a2,     0x54000002,\n-    0x54fff482,     0x54005342,     0x54000003,     0x54fff423,\n-    0x540052e3,     0x54000003,     0x54fff3c3,     0x54005283,\n-    0x54000004,     0x54fff364,     0x54005224,     0x54000005,\n-    0x54fff305,     0x540051c5,     0x54000006,     0x54fff2a6,\n-    0x54005166,     0x54000007,     0x54fff247,     0x54005107,\n-    0x54000008,     0x54fff1e8,     0x540050a8,     0x54000009,\n-    0x54fff189,     0x54005049,     0x5400000a,     0x54fff12a,\n-    0x54004fea,     0x5400000b,     0x54fff0cb,     0x54004f8b,\n-    0x5400000c,     0x54fff06c,     0x54004f2c,     0x5400000d,\n-    0x54fff00d,     0x54004ecd,     0x5400000e,     0x54ffefae,\n-    0x54004e6e,     0x5400000f,     0x54ffef4f,     0x54004e0f,\n+    0x54005a60,     0x54000001,     0x54fff541,     0x54005a01,\n+    0x54000002,     0x54fff4e2,     0x540059a2,     0x54000002,\n+    0x54fff482,     0x54005942,     0x54000003,     0x54fff423,\n+    0x540058e3,     0x54000003,     0x54fff3c3,     0x54005883,\n+    0x54000004,     0x54fff364,     0x54005824,     0x54000005,\n+    0x54fff305,     0x540057c5,     0x54000006,     0x54fff2a6,\n+    0x54005766,     0x54000007,     0x54fff247,     0x54005707,\n+    0x54000008,     0x54fff1e8,     0x540056a8,     0x54000009,\n+    0x54fff189,     0x54005649,     0x5400000a,     0x54fff12a,\n+    0x540055ea,     0x5400000b,     0x54fff0cb,     0x5400558b,\n+    0x5400000c,     0x54fff06c,     0x5400552c,     0x5400000d,\n+    0x54fff00d,     0x540054cd,     0x5400000e,     0x54ffefae,\n+    0x5400546e,     0x5400000f,     0x54ffef4f,     0x5400540f,\n@@ -1065,1 +1113,1 @@\n-    0xbd1b1869,     0x58003e5b,     0x1800000b,     0xf8945060,\n+    0xbd1b1869,     0x5800445b,     0x1800000b,     0xf8945060,\n@@ -1142,48 +1190,60 @@\n-    0x04b0e3e0,     0x0470e7e1,     0x042f9c20,     0x043f9c35,\n-    0x047f9c20,     0x04ff9c20,     0x04299420,     0x04319160,\n-    0x0461943e,     0x04a19020,     0x042053ff,     0x047f5401,\n-    0x25208028,     0x2538cfe0,     0x2578d001,     0x25b8efe2,\n-    0x25f8f007,     0xa400a3e0,     0xa4a8a7ea,     0xa547a814,\n-    0xa4084ffe,     0xa55c53e0,     0xa5e1540b,     0xe400fbf6,\n-    0xe408ffff,     0xe547e400,     0xe4014be0,     0xe4a84fe0,\n-    0xe5f15000,     0x858043e0,     0x85a043ff,     0xe59f5d08,\n-    0x1e601000,     0x1e603000,     0x1e621000,     0x1e623000,\n-    0x1e641000,     0x1e643000,     0x1e661000,     0x1e663000,\n-    0x1e681000,     0x1e683000,     0x1e6a1000,     0x1e6a3000,\n-    0x1e6c1000,     0x1e6c3000,     0x1e6e1000,     0x1e6e3000,\n-    0x1e701000,     0x1e703000,     0x1e721000,     0x1e723000,\n-    0x1e741000,     0x1e743000,     0x1e761000,     0x1e763000,\n-    0x1e781000,     0x1e783000,     0x1e7a1000,     0x1e7a3000,\n-    0x1e7c1000,     0x1e7c3000,     0x1e7e1000,     0x1e7e3000,\n-    0xf82d83a5,     0xf8380355,     0xf8381303,     0xf83a21f7,\n-    0xf8353303,     0xf8285299,     0xf8304051,     0xf8217300,\n-    0xf8246183,     0xf8bf815c,     0xf8ba0182,     0xf8b0103f,\n-    0xf8ad201d,     0xf8b3322c,     0xf8b6538d,     0xf8be403f,\n-    0xf8ba709c,     0xf8be60c4,     0xf8fe81fa,     0xf8e90188,\n-    0xf8e01034,     0xf8f82002,     0xf8e93358,     0xf8f0507e,\n-    0xf8ea4157,     0xf8e47050,     0xf8eb6148,     0xf86f8051,\n-    0xf86a018c,     0xf86f104d,     0xf8672354,     0xf8703044,\n-    0xf86451ec,     0xf87541f0,     0xf86b72f5,     0xf86c62fa,\n-    0xb83c816e,     0xb8380181,     0xb83f120a,     0xb8272062,\n-    0xb82d3233,     0xb8305023,     0xb82b40be,     0xb82873af,\n-    0xb83e6280,     0xb8a782f4,     0xb8bc0375,     0xb8b91025,\n-    0xb8b723f0,     0xb8a5312c,     0xb8bc53af,     0xb8b6427f,\n-    0xb8bf71c5,     0xb8b061ff,     0xb8fb8214,     0xb8ec012b,\n-    0xb8e6123e,     0xb8fb23dc,     0xb8e7328a,     0xb8ea5304,\n-    0xb8f142d1,     0xb8e371fd,     0xb8f66273,     0xb87681e2,\n-    0xb866020c,     0xb86b12ed,     0xb861227e,     0xb8653051,\n-    0xb87051b6,     0xb86a43b5,     0xb87b736c,     0xb86363e1,\n-    0xce312677,     0xce0e1b5b,     0xce7e8ed4,     0xce9ed858,\n-    0xce768151,     0xce718451,     0xcec08300,     0xce628ad9,\n-    0x04e30191,     0x04f0079d,     0x65dc0126,     0x65870887,\n-    0x658806c9,     0x0416b7db,     0x0440021a,     0x04d09903,\n-    0x04dabb55,     0x04138096,     0x04518071,     0x041008c1,\n-    0x0497bce9,     0x045eb4b6,     0x040813c8,     0x04ca0171,\n-    0x0481035c,     0x04dcadbc,     0x658098b0,     0x658d89ed,\n-    0x6586957a,     0x65879096,     0x65829233,     0x04ddac4e,\n-    0x6582b6e3,     0x6580a626,     0x6581b21b,     0x658dbc62,\n-    0x65819266,     0x65f8150c,     0x65b72151,     0x65b05db3,\n-    0x65f165c0,     0x04944ac8,     0x048f607b,     0x042430f4,\n-    0x04a83007,     0x046432d3,     0x04da3569,     0x04583e05,\n-    0x04592c36,     0x04c83608,     0x048a248f,     0x658727a8,\n-    0x65c633bc,     0x65982c49,     0x040120fc,\n+    0x05104fe0,     0x05505001,     0x05906fe2,     0x05d03005,\n+    0x05101fea,     0x05901feb,     0x04b0e3e0,     0x0470e7e1,\n+    0x042f9c20,     0x043f9c35,     0x047f9c20,     0x04ff9c20,\n+    0x04299420,     0x04319160,     0x0461943e,     0x04a19020,\n+    0x042053ff,     0x047f5401,     0x25208028,     0x2538cfe0,\n+    0x2578d001,     0x25b8efe2,     0x25f8f007,     0x2538dfea,\n+    0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,     0xa4484be0,\n+    0xa467afe0,     0xa4a8a7ea,     0xa547a814,     0xa4084ffe,\n+    0xa55c53e0,     0xa5e1540b,     0xe400fbf6,     0xe408ffff,\n+    0xe420e7e0,     0xe4484be0,     0xe460efe0,     0xe547e400,\n+    0xe4014be0,     0xe4a84fe0,     0xe5f15000,     0x858043e0,\n+    0x85a043ff,     0xe59f5d08,     0x0522c020,     0x05e6c0a4,\n+    0x2401a001,     0x2443a051,     0x24858881,     0x24c78cd1,\n+    0x240b8142,     0x24918213,     0x250f9001,     0x25508051,\n+    0x25802491,     0x25df28c1,     0x25850c81,     0x251e10d1,\n+    0x65816001,     0x65c36051,     0x65854891,     0x65c74cc1,\n+    0x658b4152,     0x65d14203,     0x05733820,     0x05b238a4,\n+    0x05f138e6,     0x0570396a,     0x25221420,     0x25640461,\n+    0x25a614b2,     0x25eb0553,     0x25221c24,     0x25640c60,\n+    0x25a61cb1,     0x25eb0d52,     0x1e601000,     0x1e603000,\n+    0x1e621000,     0x1e623000,     0x1e641000,     0x1e643000,\n+    0x1e661000,     0x1e663000,     0x1e681000,     0x1e683000,\n+    0x1e6a1000,     0x1e6a3000,     0x1e6c1000,     0x1e6c3000,\n+    0x1e6e1000,     0x1e6e3000,     0x1e701000,     0x1e703000,\n+    0x1e721000,     0x1e723000,     0x1e741000,     0x1e743000,\n+    0x1e761000,     0x1e763000,     0x1e781000,     0x1e783000,\n+    0x1e7a1000,     0x1e7a3000,     0x1e7c1000,     0x1e7c3000,\n+    0x1e7e1000,     0x1e7e3000,     0xf82d83a5,     0xf8380355,\n+    0xf8381303,     0xf83a21f7,     0xf8353303,     0xf8285299,\n+    0xf8304051,     0xf8217300,     0xf8246183,     0xf8bf815c,\n+    0xf8ba0182,     0xf8b0103f,     0xf8ad201d,     0xf8b3322c,\n+    0xf8b6538d,     0xf8be403f,     0xf8ba709c,     0xf8be60c4,\n+    0xf8fe81fa,     0xf8e90188,     0xf8e01034,     0xf8f82002,\n+    0xf8e93358,     0xf8f0507e,     0xf8ea4157,     0xf8e47050,\n+    0xf8eb6148,     0xf86f8051,     0xf86a018c,     0xf86f104d,\n+    0xf8672354,     0xf8703044,     0xf86451ec,     0xf87541f0,\n+    0xf86b72f5,     0xf86c62fa,     0xb83c816e,     0xb8380181,\n+    0xb83f120a,     0xb8272062,     0xb82d3233,     0xb8305023,\n+    0xb82b40be,     0xb82873af,     0xb83e6280,     0xb8a782f4,\n+    0xb8bc0375,     0xb8b91025,     0xb8b723f0,     0xb8a5312c,\n+    0xb8bc53af,     0xb8b6427f,     0xb8bf71c5,     0xb8b061ff,\n+    0xb8fb8214,     0xb8ec012b,     0xb8e6123e,     0xb8fb23dc,\n+    0xb8e7328a,     0xb8ea5304,     0xb8f142d1,     0xb8e371fd,\n+    0xb8f66273,     0xb87681e2,     0xb866020c,     0xb86b12ed,\n+    0xb861227e,     0xb8653051,     0xb87051b6,     0xb86a43b5,\n+    0xb87b736c,     0xb86363e1,     0xce312677,     0xce0e1b5b,\n+    0xce7e8ed4,     0xce9ed858,     0xce768151,     0xce718451,\n+    0xcec08300,     0xce628ad9,     0x04e30191,     0x04f0079d,\n+    0x65dc0126,     0x65870887,     0x658806c9,     0x0416b7db,\n+    0x0440021a,     0x04d09903,     0x04dabb55,     0x04138096,\n+    0x04518071,     0x041008c1,     0x0497bce9,     0x045eb4b6,\n+    0x040813c8,     0x04ca0171,     0x0481035c,     0x04dcadbc,\n+    0x658098b0,     0x658d89ed,     0x6586957a,     0x65879096,\n+    0x65829233,     0x04ddac4e,     0x6582b6e3,     0x6580a626,\n+    0x6581b21b,     0x658dbc62,     0x65819266,     0x65f8150c,\n+    0x65b72151,     0x65b05db3,     0x65f165c0,     0x04944ac8,\n+    0x048f607b,     0x042430f4,     0x04a83007,     0x046432d3,\n+    0x05eb6ac9,     0x05706fc5,     0x045a2c36,     0x04d83608,\n+    0x0499248f,     0x040827a8,     0x04ca33bc,     0x65872c49,\n+    0x658620fc,     0x6598363a,     0x04c132a8,\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.cpp","additions":138,"deletions":78,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -1538,1 +1538,1 @@\n-    B, H, S, D, Q\n+    B, H, S, D, Q, INVALID\n@@ -3198,1 +3198,1 @@\n-  void sve_dup(FloatRegister Zd, SIMD_RegVariant T, int imm8) {\n+  void sve_dup(FloatRegister Zd, SIMD_RegVariant T, int imm16) {\n@@ -3202,1 +3202,2 @@\n-    if (imm8 <= 127 && imm8 >= -128) {\n+    unsigned imm = imm16;\n+    if (imm16 <= 127 && imm16 >= -128) {\n@@ -3204,1 +3205,1 @@\n-    } else if (T != B && imm8 <= 32512 && imm8 >= -32768 && (imm8 & 0xff) == 0) {\n+    } else if (T != B && imm16 <= 32512 && imm16 >= -32768 && (imm16 & 0xff) == 0) {\n@@ -3206,1 +3207,1 @@\n-      imm8 = (imm8 >> 8);\n+      imm = (imm >> 8);\n@@ -3210,0 +3211,2 @@\n+    unsigned mask = (1U << 8) - 1;\n+    imm &= mask;\n@@ -3211,1 +3214,1 @@\n-    f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+    f(sh, 13), f(imm, 12, 5), rf(Zd, 0);\n@@ -3220,0 +3223,124 @@\n+   \/\/ SVE cpy immediate\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm16, bool isMerge) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    int sh = 0;\n+    unsigned imm = imm16;\n+    if (imm16 <= 127 && imm16 >= -128) {\n+      sh = 0;\n+    } else if (T != B && imm16 <= 32512 && imm16 >= -32768 && (imm16 & 0xff) == 0) {\n+      sh = 1;\n+      imm = (imm >> 8);\n+    } else {\n+      guarantee(false, \"invalid immediate\");\n+    }\n+    unsigned mask = (1U << 8) - 1;\n+    imm &= mask;\n+    int m = isMerge ? 1 : 0;\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b01, 21, 20);\n+    prf(Pg, 16), f(0b0, 15), f(m, 14), f(sh, 13), f(imm, 12, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE vector sel\n+  void sve_sel(FloatRegister Zd,\n+               SIMD_RegVariant T,\n+               PRegister Pg,\n+               FloatRegister Zn,\n+               FloatRegister Zm) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);\n+    f(0b11, 15, 14), prf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+\/\/ SVE compare vector\n+#define INSN(NAME, op, cond, fp)  \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pg, FloatRegister Zn, FloatRegister Zm)  { \\\n+    starti;                                                                                       \\\n+    if (fp == 0) {                                                                                \\\n+      assert(T != Q, \"invalid size\");                                                             \\\n+    } else {                                                                                      \\\n+      assert(T != B && T != Q, \"invalid size\");                                                   \\\n+    }                                                                                             \\\n+    f(op, 31, 24), f(T, 23, 22), f(0b0, 21), rf(Zm, 16), f((cond >> 1) & 0x7, 15, 13);            \\\n+    pgrf(Pg, 10), rf(Zn, 5), f(cond & 0x1, 4), prf(Pd, 0);                                        \\\n+  }\n+\n+  INSN(sve_cmpeq, 0b00100100, 0b1010, 0);\n+  INSN(sve_cmpne, 0b00100100, 0b1011, 0);\n+  INSN(sve_cmpge, 0b00100100, 0b1000, 0);\n+  INSN(sve_cmpgt, 0b00100100, 0b1001, 0);\n+  INSN(sve_fcmeq, 0b01100101, 0b0110, 1);\n+  INSN(sve_fcmne, 0b01100101, 0b0111, 1);\n+  INSN(sve_fcmgt, 0b01100101, 0b0101, 1);\n+  INSN(sve_fcmge, 0b01100101, 0b0100, 1);\n+#undef INSN\n+\n+\/\/ SVE compare vector with immediate\n+#define INSN(NAME, cond)  \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pg, FloatRegister Zn, int imm5) { \\\n+    starti;                                                                              \\\n+    assert(T != Q, \"invalid size\");                                                      \\\n+    if (imm5 > 15 || imm5 < -16) {                                                       \\\n+      guarantee(false, \"invalid immediate\");                                             \\\n+    }                                                                                    \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(0b0, 21), sf(imm5, 20, 16),                   \\\n+    f((cond >> 1) & 0x7, 15, 13), pgrf(Pg, 10), rf(Zn, 5), f(cond & 0x1, 4), prf(Pd, 0); \\\n+  }\n+\n+  INSN(sve_cmpeq, 0b1000);\n+  INSN(sve_cmpne, 0b1001);\n+  INSN(sve_cmpgt, 0b0001);\n+  INSN(sve_cmpge, 0b0000);\n+  INSN(sve_cmplt, 0b0010);\n+  INSN(sve_cmple, 0b0011);\n+#undef INSN\n+\n+\/\/ SVE unpack and extend\n+#define INSN(NAME, op) \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn) { \\\n+    starti;                                                          \\\n+    assert(T != B && T != Q, \"invalid size\");                        \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1100, 21, 18);          \\\n+    f(op, 17, 16), f(0b001110, 15, 10), rf(Zn, 5), rf(Zd, 0);        \\\n+  }\n+\n+  INSN(sve_uunpkhi, 0b11);\n+  INSN(sve_uunpklo, 0b10);\n+  INSN(sve_sunpkhi, 0b01);\n+  INSN(sve_sunpklo, 0b00);\n+#undef INSN\n+\n+\/\/ SVE vector uzp1,uzp2\n+#define INSN(NAME, op) \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn, FloatRegister Zm) { \\\n+    starti;                                                                            \\\n+    assert(T != Q, \"invalid size\");                                                    \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);                       \\\n+    f(0b01101, 15, 11), f(op, 10), rf(Zn, 5), rf(Zd, 0);                               \\\n+  }\n+\n+  INSN(sve_uzp1, 0b0);\n+  INSN(sve_uzp2, 0b1);\n+#undef INSN\n+\n+\/\/ SVE while[cond]\n+#define INSN(NAME, decode, sf)                                            \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, Register Rn, Register Rm) {  \\\n+    starti;                                                               \\\n+    assert(T != Q, \"invalid register variant\");                           \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(1, 21),                        \\\n+    zrf(Rm, 16), f(0, 15, 13), f(sf, 12), f(decode >> 1, 11, 10),         \\\n+    zrf(Rn, 5), f(decode & 0b1, 4), prf(Pd, 0);                           \\\n+  }\n+\n+  INSN(sve_whilelt,  0b010, 1);\n+  INSN(sve_whileltw, 0b010, 0);\n+  INSN(sve_whilele,  0b011, 1);\n+  INSN(sve_whilelew, 0b011, 0);\n+  INSN(sve_whilelo,  0b110, 1);\n+  INSN(sve_whilelow, 0b110, 0);\n+  INSN(sve_whilels,  0b111, 1);\n+  INSN(sve_whilelsw, 0b111, 0);\n+#undef INSN\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":133,"deletions":6,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -356,0 +356,3 @@\n+  if (!arch_supports_vector(Op_VectorLoadConst, num_elem, elem_bt, VecMaskNotUsed)) {\n+    return false;\n+  }\n@@ -387,1 +390,1 @@\n-    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(1));\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::ge));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -1062,0 +1062,1 @@\n+    assert((BoolTest::mask)predicate_node->get_int() == predicate, \"Unmatched predicates\");\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}