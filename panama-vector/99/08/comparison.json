{"files":[{"patch":"@@ -2461,0 +2461,7 @@\n+void Assembler::kmovbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x90, (0xC0 | encode));\n+}\n+\n@@ -2574,0 +2581,91 @@\n+void Assembler::knotbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n+void Assembler::korbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kandbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kanddl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n@@ -2621,0 +2719,21 @@\n+void Assembler::ktestdl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestwl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestbl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n@@ -2635,0 +2754,38 @@\n+void Assembler::kxnorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftlbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftrdl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrql(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n@@ -4115,18 +4272,0 @@\n-void Assembler::evpmovd2m(KRegister kdst, XMMRegister src, int vector_len) {\n-  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x39, (0xC0 | encode));\n-}\n-\n-void Assembler::evpmovq2m(KRegister kdst, XMMRegister src, int vector_len) {\n-  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x39, (0xC0 | encode));\n-}\n-\n@@ -7422,1 +7561,0 @@\n-  assert(VM_Version::supports_evex(), \"\");\n@@ -7424,0 +7562,1 @@\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -7434,0 +7573,112 @@\n+void Assembler::evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W1 EF \/r\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xDB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEB, (0xC0 | encode));\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEB);\n+  emit_operand(dst, src);\n+}\n+\n@@ -7978,6 +8229,10 @@\n-\/\/ duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL\n-void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(UseAVX >= 2, \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x58, (0xC0 | encode));\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFC, (0xC0 | encode));\n@@ -7986,3 +8241,1 @@\n-void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n-  assert(dst != xnoreg, \"sanity\");\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n@@ -7990,5 +8243,10 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n-  \/\/ swap src<->dst for encoding\n-  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int8(0x58);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFC);\n@@ -7998,13 +8256,1218 @@\n-\/\/ duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL\n-void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_rex_vex_w_reverted();\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x59, (0xC0 | encode));\n-}\n-\n-void Assembler::vpbroadcastq(XMMRegister dst, Address src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n-  assert(dst != xnoreg, \"sanity\");\n-  InstructionMark im(this);\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFD, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFD);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD4, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD4);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF9, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF9);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD5, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD5);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1C, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1D, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1E, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1F, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x12, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x10, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x38);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3C, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+\/\/ duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL\n+void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX >= 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+\/\/ duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL\n+void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::vpbroadcastq(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n@@ -9493,0 +10956,24 @@\n+void Assembler::evpmovq2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovd2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovw2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x29, (0xC0 | encode));\n+}\n+\n@@ -9501,0 +10988,31 @@\n+void Assembler::evpmovm2q(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2d(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2w(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2b(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":1564,"deletions":46,"binary":false,"changes":1610,"status":"modified"},{"patch":"@@ -1465,0 +1465,14 @@\n+  void kandbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kanddl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void korbl(KRegister dst, KRegister src1, KRegister src2);\n+  void korwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kordl(KRegister dst, KRegister src1, KRegister src2);\n+  void korql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void kxorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxordl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorql(KRegister dst, KRegister src1, KRegister src2);\n@@ -1467,0 +1481,1 @@\n+  void kmovbl(KRegister dst, KRegister src);\n@@ -1480,0 +1495,1 @@\n+  void knotbl(KRegister dst, KRegister src);\n@@ -1488,0 +1504,5 @@\n+  void kxnorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kshiftlbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrdl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrql(KRegister dst, KRegister src, int imm8);\n@@ -1492,0 +1513,3 @@\n+  void ktestdl(KRegister dst, KRegister src);\n+  void ktestwl(KRegister dst, KRegister src);\n+  void ktestbl(KRegister dst, KRegister src);\n@@ -2155,3 +2179,0 @@\n-  void evpmovd2m(KRegister kdst, XMMRegister src, int vector_len);\n-  void evpmovq2m(KRegister kdst, XMMRegister src, int vector_len);\n-\n@@ -2249,0 +2270,107 @@\n+  \/\/ Leaf level assembler routines for masked operations.\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+\n@@ -2367,1 +2495,0 @@\n-  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2380,3 +2507,0 @@\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n-\n@@ -2388,1 +2512,0 @@\n-  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2530,0 +2653,7 @@\n+  void evpmovw2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovd2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovq2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovm2b(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2w(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2d(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2q(XMMRegister dst, KRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":138,"deletions":8,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -3830,0 +3830,182 @@\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc,\n+                                    bool is_varshift) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AbsVB:\n+      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVS:\n+      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVI:\n+      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVL:\n+      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_VectorRearrange:\n+      evperm(eType, dst, mask, src2, src1, merge, vlen_enc); break;\n+    case Op_LShiftVS:\n+      evpsllw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVI:\n+      evpslld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVL:\n+      evpsllq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVS:\n+      evpsraw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVI:\n+      evpsrad(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVL:\n+      evpsraq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVS:\n+      evpsrlw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVI:\n+      evpsrld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVL:\n+      evpsrlq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, Address src2, bool merge, int vlen_enc) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AbsVB:\n+      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVS:\n+      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVI:\n+      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVL:\n+      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                                  KRegister src1, KRegister src2) {\n+  BasicType etype = T_ILLEGAL;\n+  switch(mask_len) {\n+    case 2:\n+    case 4:\n+    case 8:  etype = T_BYTE; break;\n+    case 16: etype = T_SHORT; break;\n+    case 32: etype = T_INT; break;\n+    case 64: etype = T_LONG; break;\n+    default: fatal(\"Unsupported type\"); break;\n+  }\n+  assert(etype != T_ILLEGAL, \"\");\n+  switch(ideal_opc) {\n+    case Op_AndVMask:\n+      kand(etype, dst, src1, src2); break;\n+    case Op_OrVMask:\n+      kor(etype, dst, src1, src2); break;\n+    case Op_XorVMask:\n+      kxor(etype, dst, src1, src2); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":182,"deletions":0,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -276,0 +276,11 @@\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                   bool merge, int vlen_enc, bool is_varshift = false);\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, Address src2,\n+                   bool merge, int vlen_enc);\n+\n+  void masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                 KRegister src1, KRegister src2);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -8208,0 +8208,296 @@\n+void MacroAssembler::kand(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kandbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kandwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kanddl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kandql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       korbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       korwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       korql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kxorbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kxorwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kxordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kxorql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evpermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evpermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src1, KRegister src2) {\n+   masklen = masklen < 8 ? 8 : masklen;\n+   ktest(masklen, src1, src2);\n+   setb(Assembler::notZero, dst);\n+   movzbl(dst, dst);\n+}\n+\n+void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch) {\n+  if (masklen < 8) {\n+    knotbl(kscratch, src2);\n+    kortestbl(src1, kscratch);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  } else {\n+    ktest(masklen, src1, src2);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  }\n+}\n+\n+void MacroAssembler::kortest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen) {\n+    case 8:\n+       kortestbl(src1, src2);\n+       break;\n+    case 16:\n+       kortestwl(src1, src2);\n+       break;\n+    case 32:\n+       kortestdl(src1, src2);\n+       break;\n+    case 64:\n+       kortestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+\n+void MacroAssembler::ktest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen)  {\n+    case 8:\n+       ktestbl(src1, src2);\n+       break;\n+    case 16:\n+       ktestwl(src1, src2);\n+       break;\n+    case 32:\n+       ktestdl(src1, src2);\n+       break;\n+    case 64:\n+       ktestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":296,"deletions":0,"binary":false,"changes":296,"status":"modified"},{"patch":"@@ -1331,0 +1331,69 @@\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpslld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrad(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n@@ -1620,1 +1689,21 @@\n-  \/\/ Data\n+  \/\/ AVX-512 mask operations.\n+  void kand(BasicType etype, KRegister dst, KRegister src1, KRegister src2);\n+  void kor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void kortest(uint masklen, KRegister src1, KRegister src2);\n+  void ktest(uint masklen, KRegister src1, KRegister src2);\n+\n+  void evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch);\n+  void anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":90,"deletions":1,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -4004,0 +4004,1 @@\n+    StubRoutines::x86::_vector_masked_cmp_bits = generate_vector_mask(\"vector_masked_cmp_bits\", 0x01010101);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -7584,0 +7584,1 @@\n+    StubRoutines::x86::_vector_masked_cmp_bits = generate_vector_mask(\"vector_masked_cmp_bits\", 0x0101010101010101);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+address StubRoutines::x86::_vector_masked_cmp_bits = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+  static address _vector_masked_cmp_bits;\n@@ -290,0 +291,3 @@\n+  static address vector_masked_cmp_bits() {\n+    return _vector_masked_cmp_bits;\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -887,0 +887,1 @@\n+  static bool supports_avx512bwdq()   { return (supports_evex() && supports_avx512bw() && supports_avx512dq()); }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1377,0 +1377,1 @@\n+  static address vector_masked_cmp_bits() { return StubRoutines::x86::vector_masked_cmp_bits(); }\n@@ -1805,0 +1806,2 @@\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n@@ -1806,1 +1809,1 @@\n-      if(bt == T_BYTE || bt == T_SHORT) {\n+      if(is_subword_type(bt)) {\n@@ -1817,0 +1820,5 @@\n+    case Op_MaskAll:\n+      if(!is_LP64 || !VM_Version::supports_avx512vlbw()) {\n+        return false;\n+      }\n+      break;\n@@ -1827,1 +1835,116 @@\n-  return false;\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+  \/\/ Needed for loadmask pattern which populates opmask register\n+  \/\/ consumed by masked instructions.\n+  if (!VM_Version::supports_avx512vlbw()) {\n+    return false;\n+  }\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_floating_point_type(bt)) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+      assert(!is_subword_type(bt) || VM_Version::supports_avx512bw(), \"\");\n+      return true;\n+\n+    case Op_VectorRearrange:\n+      assert(bt != T_SHORT || VM_Version::supports_avx512bw(), \"\");\n+      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      assert(bt != T_INT  || VM_Version::supports_avx512bw(), \"\");\n+      assert(bt != T_LONG || VM_Version::supports_avx512bw(), \"\");\n+      if (bt == T_BYTE && !VM_Version::supports_avx512dq()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+    case Op_MaskAll:\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      return true;\n+\n+    default:\n+      return false;\n+  }\n@@ -3317,0 +3440,1 @@\n+\n@@ -3318,0 +3442,70 @@\n+instruct reinterpret_mask(kReg dst) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n) == Matcher::vector_length(n->in(1))); \/\/ dst == src\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(125);\n+  format %{ \"vector_reinterpret $dst\\t!\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_W2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->as_VectorReinterpret()->src_elem_type() == T_SHORT &&\n+            n->as_VectorReinterpret()->dst_elem_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_W2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_SHORT);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2w($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_D2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            (n->as_VectorReinterpret()->src_elem_type() == T_INT ||\n+            n->as_VectorReinterpret()->src_elem_type() == T_FLOAT) &&\n+            n->as_VectorReinterpret()->dst_elem_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_D2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_INT);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2d($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_Q2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            (n->as_VectorReinterpret()->src_elem_type() == T_LONG ||\n+            n->as_VectorReinterpret()->src_elem_type() == T_DOUBLE) &&\n+            n->as_VectorReinterpret()->dst_elem_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_Q2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_LONG);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2q($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3320,1 +3514,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); \/\/ dst == src\n+  predicate(!n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); \/\/ dst == src\n@@ -3355,0 +3550,1 @@\n+            !n->bottom_type()->isa_vectmask() &&\n@@ -3370,0 +3566,1 @@\n+            !n->bottom_type()->isa_vectmask() &&\n@@ -3387,1 +3584,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); \/\/ src > dst\n+  predicate(!n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); \/\/ src > dst\n@@ -3589,1 +3787,1 @@\n-  predicate(Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n@@ -3614,1 +3812,1 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == 64);\n+  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n@@ -3617,1 +3815,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t! using $tmp and k2 as TEMP\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t! using $tmp and ktmp as TEMP\" %}\n@@ -3633,0 +3831,19 @@\n+instruct evgather_masked(vec dst, memory mem, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  predicate(VM_Version::supports_avx512vl());\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx mask)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp);\n+  format %{ \"load_vector_gather_masked $dst, $mem, $idx, $mask\\t! using $tmp and ktmp as TEMP\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"sanity\");\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    \/\/ Note: Since gather instruction partially updates the opmask register used\n+    \/\/ for predication hense moving mask operand to a temporary.\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3656,0 +3873,19 @@\n+instruct scatter_masked(memory mem, vec src, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  predicate(VM_Version::supports_avx512vl());\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx mask))));\n+  effect(TEMP tmp, TEMP ktmp);\n+  format %{ \"store_vector_scatter_masked $mem, $idx, $src, $mask\\t!\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(Matcher::vector_length_in_bytes(this, $src) >= 16, \"sanity\");\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    \/\/ Note: Since scatter instruction partially updates the opmask register used\n+    \/\/ for predication hense moving mask operand to a temporary.\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6911,1 +7147,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n@@ -6928,2 +7165,3 @@\n-instruct evcmpFD(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n@@ -6949,0 +7187,19 @@\n+instruct evcmpFD(kReg dst, vec src1, vec src2, immI8 cond) %{\n+  predicate(VM_Version::supports_avx512vl() &&\n+            is_floating_point_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1 T_FLOAT, T_DOUBLE\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vector_compare_evex $dst,$src1,$src2,$cond\\t!\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+    KRegister mask = k0; \/\/ The comparison itself is not being masked.\n+    if (Matcher::vector_element_basic_type(this, $src1) == T_FLOAT) {\n+      __ evcmpps($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    } else {\n+      __ evcmppd($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6950,1 +7207,1 @@\n-  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(!VM_Version::supports_avx512vl() &&\n@@ -7004,1 +7261,1 @@\n-instruct evcmp(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct vcmpu64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n@@ -7006,1 +7263,1 @@\n-            (VM_Version::supports_avx512vl() ||\n+            (!VM_Version::supports_avx512vl() &&\n@@ -7022,0 +7279,34 @@\n+    switch (src1_elem_bt) {\n+      case T_INT: {\n+        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      case T_LONG: {\n+        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct evcmp(kReg dst, vec src1, vec src2, immI8 cond) %{\n+  predicate(UseAVX > 2 &&\n+            VM_Version::supports_avx512vl() && \/\/ src1\n+            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vector_compared_evex $dst,$src1,$src2,$cond\\t!\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"required\");\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+    bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n@@ -7024,2 +7315,1 @@\n-        __ evpcmpb($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqub($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpb($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7029,2 +7319,1 @@\n-        __ evpcmpw($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquw($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpw($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7034,3 +7323,1 @@\n-        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n-\n+        __ evpcmpd($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7040,2 +7327,1 @@\n-        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpq($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7194,0 +7480,1 @@\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL &&\n@@ -7207,0 +7494,1 @@\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL &&\n@@ -7219,1 +7507,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == 64);\n+  predicate(Matcher::vector_length_in_bytes(n) == 64 &&\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL);\n@@ -7232,0 +7521,16 @@\n+\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+  predicate(n->in(2)->bottom_type()->isa_vectmask() &&\n+            (!is_subword_type(Matcher::vector_element_basic_type(n)) ||\n+             VM_Version::supports_avx512bw()));\n+  match(Set dst (VectorBlend (Binary src1 src2) mask));\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n+  effect(TEMP scratch);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ evpblend(elem_bt, $dst$$XMMRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7236,0 +7541,1 @@\n+  ins_cost(450);\n@@ -7251,0 +7557,1 @@\n+  ins_cost(450);\n@@ -7267,0 +7574,1 @@\n+  ins_cost(250);\n@@ -7281,0 +7589,1 @@\n+  ins_cost(450);\n@@ -7353,1 +7662,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7358,1 +7668,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_lt16 $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n@@ -7368,2 +7678,3 @@\n-instruct vptest_alltrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_alltrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7374,1 +7685,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_ge16  $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n@@ -7384,6 +7695,8 @@\n-instruct vptest_alltrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 64 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+instruct vptest_alltrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n->in(1)) < 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr, TEMP kscratch);\n+  format %{ \"vptest_alltrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7391,4 +7704,5 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::carrySet, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, $kscratch$$KRegister);\n@@ -7399,0 +7713,20 @@\n+\n+instruct vptest_alltrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n->in(1)) >= 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_alltrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n@@ -7400,1 +7734,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7405,1 +7740,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_lt16 $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n@@ -7415,2 +7750,3 @@\n-instruct vptest_anytrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_anytrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7421,1 +7757,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_ge16 $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7431,2 +7767,2 @@\n-instruct vptest_anytrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 64 &&\n+instruct vptest_anytrue_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n@@ -7434,3 +7770,3 @@\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7438,4 +7774,5 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::notZero, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint  masklen = Matcher::vector_length(this, $src1);\n+    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7447,1 +7784,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n@@ -7452,1 +7790,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t! using $vtmp as TEMP\" %}\n+  format %{ \"cmpvptest_anytrue_lt16 $src1,$src2\\t! using $vtmp as TEMP\" %}\n@@ -7460,2 +7798,3 @@\n-instruct cmpvptest_anytrue(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n+instruct cmpvptest_anytrue_ge16(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n@@ -7465,1 +7804,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_ge16 $src1,$src2\\t!\" %}\n@@ -7473,2 +7812,2 @@\n-instruct cmpvptest_anytrue_evex(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero, kReg ktmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 &&\n+instruct cmpvptest_anytrue_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n@@ -7477,2 +7816,1 @@\n-  effect(TEMP ktmp);\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_evex $src1,$src2\\t!\" %}\n@@ -7480,2 +7818,6 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    masklen = masklen < 8 ? 8 : masklen;\n+    __ ktest(masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7503,1 +7845,1 @@\n-instruct loadMask_evex(vec dst, vec src) %{\n+instruct loadMask_evex(kReg dst, vec src, rRegP scratch, kReg kscratch) %{\n@@ -7506,1 +7848,1 @@\n-  effect(TEMP dst);\n+  effect(TEMP scratch, TEMP kscratch);\n@@ -7509,4 +7851,3 @@\n-    int vlen_in_bytes = Matcher::vector_length_in_bytes(this);\n-    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n-\n-    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, false);\n+    int vlen_enc = vector_length_encoding(in(1));\n+    __ evpcmp(T_BYTE, $dst$$KRegister, k0, $src$$XMMRegister, ExternalAddress(vector_masked_cmp_bits()),\n+              Assembler::eq, vlen_enc, $scratch$$Register);\n@@ -7520,1 +7861,2 @@\n-  predicate(Matcher::vector_length(n) < 64 || VM_Version::supports_avx512vlbw());\n+  predicate((Matcher::vector_length(n) < 64 || VM_Version::supports_avx512vlbw()) &&\n+            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7537,1 +7879,2 @@\n-  predicate(Matcher::vector_length(n) <= 8);\n+  predicate(Matcher::vector_length(n) <= 8 &&\n+            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7562,13 +7905,0 @@\n-instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n-  predicate(VM_Version::supports_avx512bw());\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  ins_encode %{\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -7603,16 +7933,0 @@\n-instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n-  predicate(UseAVX > 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  ins_encode %{\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    if (!VM_Version::supports_avx512vl()) {\n-      src_vlen_enc = Assembler::AVX_512bit;\n-    }\n-    __ evpmovdb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -7650,0 +7964,30 @@\n+instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n+  predicate(VM_Version::supports_avx512bw() &&\n+            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  ins_encode %{\n+    int src_vlen_enc = vector_length_encoding(this, $src);\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  ins_encode %{\n+    int src_vlen_enc = vector_length_encoding(this, $src);\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    if (!VM_Version::supports_avx512vl()) {\n+      src_vlen_enc = Assembler::AVX_512bit;\n+    }\n+    __ evpmovdb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7651,1 +7995,1 @@\n-  predicate(UseAVX > 2);\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7666,0 +8010,25 @@\n+instruct vstoreMask_evex(vec dst, kReg mask, immI size) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vector_store_mask $dst,$mask\\t!\" %}\n+  ins_encode %{\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    __ evpmovm2b($dst$$XMMRegister, $mask$$KRegister, dst_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaskcast_evex(kReg dst) %{\n+  predicate((Matcher::vector_length(n) == Matcher::vector_length(n->in(1))) &&\n+            (Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast dst));\n+  ins_cost(0);\n+  format %{ \"vector_mask_cast $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(empty);\n+%}\n+\n@@ -8298,0 +8667,535 @@\n+\/\/ ---------------------------------- Vector Masked Operations ------------------------------------\n+\n+instruct vadd_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (AddVB (Binary dst src2) mask));\n+  match(Set dst (AddVS (Binary dst src2) mask));\n+  match(Set dst (AddVI (Binary dst src2) mask));\n+  match(Set dst (AddVL (Binary dst src2) mask));\n+  match(Set dst (AddVF (Binary dst src2) mask));\n+  match(Set dst (AddVD (Binary dst src2) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vadd_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (AddVB (Binary dst src2) mask));\n+  match(Set dst (AddVS (Binary dst src2) mask));\n+  match(Set dst (AddVI (Binary dst src2) mask));\n+  match(Set dst (AddVL (Binary dst src2) mask));\n+  match(Set dst (AddVF (Binary dst src2) mask));\n+  match(Set dst (AddVD (Binary dst src2) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (XorV (Binary dst src2) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (XorV (Binary dst src2) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (OrV (Binary dst src2) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (OrV (Binary dst src2) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (AndV (Binary dst src2) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (AndV (Binary dst src2) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (SubVB (Binary dst src2) mask));\n+  match(Set dst (SubVS (Binary dst src2) mask));\n+  match(Set dst (SubVI (Binary dst src2) mask));\n+  match(Set dst (SubVL (Binary dst src2) mask));\n+  match(Set dst (SubVF (Binary dst src2) mask));\n+  match(Set dst (SubVD (Binary dst src2) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (SubVB (Binary dst src2) mask));\n+  match(Set dst (SubVS (Binary dst src2) mask));\n+  match(Set dst (SubVI (Binary dst src2) mask));\n+  match(Set dst (SubVL (Binary dst src2) mask));\n+  match(Set dst (SubVF (Binary dst src2) mask));\n+  match(Set dst (SubVD (Binary dst src2) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MulVS (Binary dst src2) mask));\n+  match(Set dst (MulVI (Binary dst src2) mask));\n+  match(Set dst (MulVL (Binary dst src2) mask));\n+  match(Set dst (MulVF (Binary dst src2) mask));\n+  match(Set dst (MulVD (Binary dst src2) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MulVS (Binary dst src2) mask));\n+  match(Set dst (MulVI (Binary dst src2) mask));\n+  match(Set dst (MulVL (Binary dst src2) mask));\n+  match(Set dst (MulVF (Binary dst src2) mask));\n+  match(Set dst (MulVD (Binary dst src2) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (DivVF (Binary dst src2) mask));\n+  match(Set dst (DivVD (Binary dst src2) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (DivVF (Binary dst src2) mask));\n+  match(Set dst (DivVD (Binary dst src2) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MaxV (Binary dst src2) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MaxV (Binary dst src2) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MinV (Binary dst src2) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MinV (Binary dst src2) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrearrangev_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (VectorRearrange (Binary dst src2) mask));\n+  format %{ \"vprearrange_masked $dst, $dst, $src2\\t! rearrange masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, false, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vabs_masked(vec dst, kReg mask) %{\n+  match(Set dst (AbsVB dst mask));\n+  match(Set dst (AbsVS dst mask));\n+  match(Set dst (AbsVI dst mask));\n+  match(Set dst (AbsVL dst mask));\n+  format %{ \"vabs_masked $dst, $mask \\t! vabs masked operation\" %}\n+  ins_cost(100);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_reg_masked(vec dst, vec src2, vec src3, kReg mask) %{\n+  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_mem_masked(vec dst, vec src2, memory src3, kReg mask) %{\n+  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond mask)));\n+  effect(TEMP scratch);\n+  format %{ \"vcmp_masked $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n+    switch (src1_elem_bt) {\n+      case T_BYTE: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpb($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_SHORT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpw($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_INT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_LONG: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpq($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmpps($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmppd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt)); break;\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+#ifdef _LP64\n+instruct mask_all_evexI_imm(kReg dst, immI cnt, rRegL tmp) %{\n+  match(Set dst (MaskAll cnt));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $cnt \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    __ movq($tmp$$Register, $cnt$$constant);\n+    __ kmovql($dst$$KRegister, $tmp$$Register);\n+    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexI(kReg dst, rRegI src, rRegL tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    __ movslq($tmp$$Register, $src$$Register);\n+    __ kmovql($dst$$KRegister, $tmp$$Register);\n+    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    __ kmovql($dst$$KRegister, $src$$Register);\n+    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n+instruct mask_opers_evex(kReg dst, kReg src1, kReg src2, kReg kscratch) %{\n+  match(Set dst (AndVMask src1 src2));\n+  match(Set dst (OrVMask src1 src2));\n+  match(Set dst (XorVMask src1 src2));\n+  effect(TEMP kscratch);\n+  format %{ \"mask_opers_evex $dst, $src1, $src2\\t!\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this);\n+    __ masked_op(this->ideal_Opcode(), masklen, $dst$$KRegister, $src1$$KRegister, $src2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castMM(kReg dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":1005,"deletions":101,"binary":false,"changes":1106,"status":"modified"},{"patch":"@@ -2362,0 +2362,1 @@\n+         n->req() == 2 &&\n@@ -2369,1 +2370,1 @@\n-      return true;\n+      return n->req() == 2;\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -174,0 +174,1 @@\n+class VectorUnboxNode;\n@@ -175,0 +176,1 @@\n+class VectorReinterpretNode;\n@@ -709,0 +711,2 @@\n+        DEFINE_CLASS_ID(VectorUnbox, Vector, 1)\n+        DEFINE_CLASS_ID(VectorReinterpret, Vector, 2)\n@@ -934,0 +938,3 @@\n+  DEFINE_CLASS_QUERY(VectorMaskCmp)\n+  DEFINE_CLASS_QUERY(VectorUnbox)\n+  DEFINE_CLASS_QUERY(VectorReinterpret);\n@@ -938,1 +945,0 @@\n-  DEFINE_CLASS_QUERY(VectorMaskCmp)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -341,1 +341,4 @@\n-  if (is_mask && bt != T_BOOLEAN) {\n+  \/\/ If boxed mask value is present in a predicate register, it must be\n+  \/\/ spilled to a vector though a VectorStoreMaskOperation before actual StoreVector\n+  \/\/ operation to vector payload field.\n+  if (is_mask && (value->bottom_type()->isa_vectmask() || bt != T_BOOLEAN)) {\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -519,1 +519,1 @@\n-    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_klass));\n@@ -1244,1 +1244,1 @@\n-      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+      mask = gvn().transform(new VectorReinterpretNode(mask, elem_bt, from_mask_type, mem_elem_bt, to_mask_type));\n@@ -1255,1 +1255,1 @@\n-      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+      mask = gvn().transform(new VectorReinterpretNode(mask, elem_bt, from_mask_type, mem_elem_bt, to_mask_type));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -369,2 +369,2 @@\n-bool VectorNode::is_vshift_cnt(Node* n) {\n-  switch (n->Opcode()) {\n+bool VectorNode::is_vshift_cnt_opcode(int opc) {\n+  switch (opc) {\n@@ -379,0 +379,4 @@\n+bool VectorNode::is_vshift_cnt(Node* n) {\n+  return is_vshift_cnt_opcode(n->Opcode());\n+}\n+\n@@ -450,1 +454,1 @@\n-      if (Matcher::match_rule_supported_vector(Op_AndVMask, vlen, bt)) {\n+      if (Matcher::match_rule_supported_vector_masked(Op_AndVMask, vlen, bt)) {\n@@ -455,1 +459,1 @@\n-      if (Matcher::match_rule_supported_vector(Op_OrVMask, vlen, bt)) {\n+      if (Matcher::match_rule_supported_vector_masked(Op_OrVMask, vlen, bt)) {\n@@ -460,1 +464,1 @@\n-      if (Matcher::match_rule_supported_vector(Op_XorVMask, vlen, bt)) {\n+      if (Matcher::match_rule_supported_vector_masked(Op_XorVMask, vlen, bt)) {\n@@ -1076,1 +1080,1 @@\n-  if (out_bt == T_BOOLEAN) {\n+  if (!Matcher::has_predicated_vectors() && out_bt == T_BOOLEAN) {\n@@ -1079,0 +1083,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -70,4 +70,1 @@\n-    if (vect_type()->isa_vectmask()) {\n-      return Op_RegVectMask;\n-    }\n-    return Matcher::vector_ideal_reg(vect_type()->length_in_bytes());\n+    return type()->ideal_reg();\n@@ -85,0 +82,3 @@\n+\n+  static bool is_vshift_cnt_opcode(int opc);\n+\n@@ -1304,1 +1304,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  virtual  uint size_of() const { return sizeof(VectorMaskCmpNode); }\n@@ -1437,0 +1437,3 @@\n+  BasicType  _dst_bt;\n+  BasicType  _src_bt;\n+\n@@ -1438,1 +1441,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  uint size_of() const { return sizeof(VectorReinterpretNode); }\n@@ -1441,1 +1444,16 @@\n-      : VectorNode(in, dst_vt), _src_vt(src_vt) { }\n+      : VectorNode(in, dst_vt), _src_vt(src_vt) {\n+     assert(!dst_vt->isa_vectmask() && !src_vt->isa_vectmask(), \"\");\n+     _src_bt = src_vt->element_basic_type();\n+     _dst_bt = dst_vt->element_basic_type();\n+     init_class_id(Class_VectorReinterpret);\n+  }\n+\n+  VectorReinterpretNode(Node* in, BasicType src_bt, const TypeVect* src_vt,\n+                        BasicType dst_bt, const TypeVect* dst_vt)\n+      : VectorNode(in, dst_vt), _src_vt(src_vt) {\n+     assert(dst_vt->isa_vectmask() && src_vt->isa_vectmask() || type2aelembytes(src_bt) >= type2aelembytes(dst_bt),\n+            \"unsupported mask widening reinterpretation\");\n+     _src_bt = src_bt;\n+     _dst_bt = dst_bt;\n+     init_class_id(Class_VectorReinterpret);\n+  }\n@@ -1443,0 +1461,2 @@\n+  BasicType src_elem_type() { return _src_bt; }\n+  BasicType dst_elem_type() { return _dst_bt; }\n@@ -1575,0 +1595,1 @@\n+    init_class_id(Class_VectorUnbox);\n@@ -1604,1 +1625,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":28,"deletions":8,"binary":false,"changes":36,"status":"modified"}]}