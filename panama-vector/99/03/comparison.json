{"files":[{"patch":"@@ -2461,0 +2461,7 @@\n+void Assembler::kmovbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x90, (0xC0 | encode));\n+}\n+\n@@ -2574,0 +2581,84 @@\n+void Assembler::korbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kandbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kanddl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n@@ -2621,0 +2712,21 @@\n+void Assembler::ktestdl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestwl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestbl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n@@ -2635,0 +2747,23 @@\n+void Assembler::kxnorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftlbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n@@ -7422,1 +7557,0 @@\n-  assert(VM_Version::supports_evex(), \"\");\n@@ -7424,0 +7558,1 @@\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -7434,0 +7569,112 @@\n+void Assembler::evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W1 EF \/r\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xDB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEB, (0xC0 | encode));\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEB);\n+  emit_operand(dst, src);\n+}\n+\n@@ -7978,0 +8225,1212 @@\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFC, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFC);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFD, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFD);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD4, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD4);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF9, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF9);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD5, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD5);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1C, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1D, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1E, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1F, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evppermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evppermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evppermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evppermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evppermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evppermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evppermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evppermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x12, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x10, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x38);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3C, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":1460,"deletions":1,"binary":false,"changes":1461,"status":"modified"},{"patch":"@@ -1465,0 +1465,14 @@\n+  void kandbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kanddl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void korbl(KRegister dst, KRegister src1, KRegister src2);\n+  void korwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kordl(KRegister dst, KRegister src1, KRegister src2);\n+  void korql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void kxorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxordl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorql(KRegister dst, KRegister src1, KRegister src2);\n@@ -1467,0 +1481,1 @@\n+  void kmovbl(KRegister dst, KRegister src);\n@@ -1488,0 +1503,3 @@\n+  void kxnorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kshiftlbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrbl(KRegister dst, KRegister src, int imm8);\n@@ -1492,0 +1510,3 @@\n+  void ktestdl(KRegister dst, KRegister src);\n+  void ktestwl(KRegister dst, KRegister src);\n+  void ktestbl(KRegister dst, KRegister src);\n@@ -2249,0 +2270,107 @@\n+  \/\/ Leaf level assembler routines for masked operations.\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evppermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evppermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evppermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evppermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evppermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evppermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evppermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evppermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+\n@@ -2367,1 +2495,0 @@\n-  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2380,3 +2507,0 @@\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n-\n@@ -2388,1 +2512,0 @@\n-  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":128,"deletions":5,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -3830,0 +3830,182 @@\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc,\n+                                    bool is_varshift) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AbsVB:\n+      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVS:\n+      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVI:\n+      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVL:\n+      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_VectorRearrange:\n+      evpperm(eType, dst, mask, src2, src1, merge, vlen_enc); break;\n+    case Op_LShiftVS:\n+      evpsllw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVI:\n+      evpslld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVL:\n+      evpsllq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVS:\n+      evpsraw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVI:\n+      evpsrad(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVL:\n+      evpsraq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVS:\n+      evpsrlw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVI:\n+      evpsrld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVL:\n+      evpsrlq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, Address src2, bool merge, int vlen_enc) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AbsVB:\n+      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVS:\n+      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVI:\n+      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVL:\n+      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                                  KRegister src1, KRegister src2) {\n+  BasicType etype = T_ILLEGAL;\n+  switch(mask_len) {\n+    case 2:\n+    case 4:\n+    case 8:  etype = T_BYTE; break;\n+    case 16: etype = T_SHORT; break;\n+    case 32: etype = T_INT; break;\n+    case 64: etype = T_LONG; break;\n+    default: fatal(\"Unsupported type\"); break;\n+  }\n+  assert(etype != T_ILLEGAL, \"\");\n+  switch(ideal_opc) {\n+    case Op_AndVMask:\n+      kand(etype, dst, src1, src2); break;\n+    case Op_OrVMask:\n+      kor(etype, dst, src1, src2); break;\n+    case Op_XorVMask:\n+      kxor(etype, dst, src1, src2); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":182,"deletions":0,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -276,0 +276,11 @@\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                   bool merge, int vlen_enc, bool is_varshift = false);\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, Address src2,\n+                   bool merge, int vlen_enc);\n+\n+  void masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                 KRegister src1, KRegister src2);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -8208,0 +8208,304 @@\n+void MacroAssembler::kand(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kandbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kandwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kanddl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kandql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       korbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       korwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       korql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kxorbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kxorwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kxordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kxorql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evpperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evppermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evppermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evppermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evppermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evppermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evppermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evppermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evppermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch) {\n+  if (masklen < 8) {\n+    kxnorbl(kscratch, kscratch, kscratch);\n+    kshiftrbl(kscratch, kscratch, 8-masklen);\n+    ktestbl(kscratch, src);\n+    setb(Assembler::notZero, dst);\n+    movzbl(dst, dst);\n+  } else {\n+    ktest(masklen, src, src);\n+    setb(Assembler::notZero, dst);\n+    movzbl(dst, dst);\n+  }\n+}\n+\n+void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src, KRegister kscratch) {\n+  if (masklen < 8) {\n+    kxnorbl(kscratch, kscratch, kscratch);\n+    kshiftlbl(kscratch, kscratch, masklen);\n+    kortestbl(kscratch, src);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  } else {\n+    kortest(masklen, src, src);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  }\n+}\n+\n+void MacroAssembler::kortest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen) {\n+    case 8:\n+       kortestbl(src1, src2);\n+       break;\n+    case 16:\n+       kortestwl(src1, src2);\n+       break;\n+    case 32:\n+       kortestdl(src1, src2);\n+       break;\n+    case 64:\n+       kortestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+\n+void MacroAssembler::ktest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen)  {\n+    case 8:\n+       ktestbl(src1, src2);\n+       break;\n+    case 16:\n+       ktestwl(src1, src2);\n+       break;\n+    case 32:\n+       ktestdl(src1, src2);\n+       break;\n+    case 64:\n+       ktestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":304,"deletions":0,"binary":false,"changes":304,"status":"modified"},{"patch":"@@ -1331,0 +1331,69 @@\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpslld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrad(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n@@ -1620,1 +1689,21 @@\n-  \/\/ Data\n+  \/\/ AVX-512 mask operations.\n+  void kand(BasicType etype, KRegister dst, KRegister src1, KRegister src2);\n+  void kor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void kortest(uint masklen, KRegister src1, KRegister src2);\n+  void ktest(uint masklen, KRegister src1, KRegister src2);\n+\n+  void evpperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void alltrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n+  void anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":90,"deletions":1,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -183,1 +183,1 @@\n-      ret_value = VM_Version::supports_avx512vl();\n+      return true;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4004,0 +4004,1 @@\n+    StubRoutines::x86::_vector_masked_cmp_bits = generate_vector_mask(\"vector_masked_cmp_bits\", 0x01010101);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -7584,0 +7584,1 @@\n+    StubRoutines::x86::_vector_masked_cmp_bits = generate_vector_mask(\"vector_masked_cmp_bits\", 0x0101010101010101);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+address StubRoutines::x86::_vector_masked_cmp_bits = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+  static address _vector_masked_cmp_bits;\n@@ -290,0 +291,3 @@\n+  static address vector_masked_cmp_bits() {\n+    return _vector_masked_cmp_bits;\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -882,0 +882,1 @@\n+  static bool supports_avx512bwdq()   { return (supports_evex() && supports_avx512bw() && supports_avx512dq()); }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1410,0 +1410,1 @@\n+  static address vector_masked_cmp_bits() { return StubRoutines::x86::vector_masked_cmp_bits(); }\n@@ -1826,0 +1827,2 @@\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n@@ -1827,1 +1830,1 @@\n-      if(bt == T_BYTE || bt == T_SHORT) {\n+      if(is_subword_type(bt)) {\n@@ -1838,0 +1841,5 @@\n+    case Op_MaskAll:\n+      if(!VM_Version::supports_avx512vlbw()) {\n+        return false;\n+      }\n+      break;\n@@ -1848,1 +1856,108 @@\n-  return false;\n+  \/\/ Needed for loadmask pattern which populates opmask register\n+  \/\/ consumed by masked instructions.\n+  if (!VM_Version::supports_avx512bw()) {\n+    return false;\n+  }\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+     return false; \/\/ Implementation limitation\n+  }\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_floating_point_type(bt)) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+     if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+     }\n+     return true;\n+\n+    case Op_VectorRearrange:\n+      assert(bt != T_SHORT || VM_Version::supports_avx512bw(), \"\");\n+      if (bt == T_BYTE && VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+    case Op_MaskAll:\n+     return true;\n+\n+    default:\n+      return false;\n+  }\n@@ -3338,0 +3453,1 @@\n+\n@@ -3339,0 +3455,10 @@\n+instruct reinterpret_mask(kReg dst) %{\n+  predicate(UseAVX > 2 && vector_length(n) == vector_length(n->in(1))); \/\/ dst == src\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(125);\n+  format %{ \"vector_reinterpret $dst\\t!\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3610,1 +3736,1 @@\n-  predicate(vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && vector_length_in_bytes(n) <= 32);\n@@ -3635,1 +3761,1 @@\n-  predicate(vector_length_in_bytes(n) == 64);\n+  predicate(VM_Version::supports_avx512vl() || vector_length_in_bytes(n) == 64);\n@@ -3654,0 +3780,17 @@\n+instruct evgather_masked(vec dst, memory mem, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  predicate(VM_Version::supports_avx512vl());\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx mask)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp);\n+  format %{ \"load_vector_gather_masked $dst, $mem, $idx, $mask\\t! using $tmp and k2 as TEMP\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"sanity\");\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = vector_element_basic_type(this);\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3677,0 +3820,17 @@\n+instruct scatter_masked(memory mem, vec src, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  predicate(VM_Version::supports_avx512vl());\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx mask))));\n+  effect(TEMP tmp, TEMP ktmp);\n+  format %{ \"store_vector_scatter_masked $mem, $idx, $src, $mask\\t!\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType elem_bt = vector_element_basic_type(this, $src);\n+    assert(vector_length_in_bytes(this, $src) >= 16, \"sanity\");\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6885,1 +7045,2 @@\n-  predicate(vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n@@ -6902,2 +7063,3 @@\n-instruct evcmpFD(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n-  predicate(vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n@@ -6923,0 +7085,20 @@\n+instruct evcmpFD(kReg dst, vec src1, vec src2, immI8 cond, rRegP scratch) %{\n+  predicate(VM_Version::supports_avx512vl() &&\n+            is_floating_point_type(vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1 T_FLOAT, T_DOUBLE\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(TEMP scratch);\n+  format %{ \"vector_compare_evex $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+    KRegister mask = k0; \/\/ The comparison itself is not being masked.\n+    if (vector_element_basic_type(this, $src1) == T_FLOAT) {\n+      __ evcmpps($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    } else {\n+      __ evcmppd($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6924,1 +7106,1 @@\n-  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(!VM_Version::supports_avx512vl() &&\n@@ -6978,1 +7160,1 @@\n-instruct evcmp(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct vcmpu64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n@@ -6980,3 +7162,3 @@\n-            (VM_Version::supports_avx512vl() ||\n-             vector_length_in_bytes(n->in(1)->in(1)) == 64) && \/\/ src1\n-             is_integral_type(vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1\n+            !VM_Version::supports_avx512vl() &&\n+            vector_length_in_bytes(n->in(1)->in(1)) == 64 && \/\/ src1\n+            is_integral_type(vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1\n@@ -6996,0 +7178,35 @@\n+    switch (src1_elem_bt) {\n+      case T_INT: {\n+        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      case T_LONG: {\n+        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct evcmp(kReg dst, vec src1, vec src2, immI8 cond, rRegP scratch) %{\n+  predicate(UseAVX > 2 &&\n+            VM_Version::supports_avx512vl() && \/\/ src1\n+            is_integral_type(vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(TEMP scratch);\n+  format %{ \"vector_compared_evex $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"required\");\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+    bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+    BasicType src1_elem_bt = vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n@@ -6998,2 +7215,1 @@\n-        __ evpcmpb($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqub($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpb($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7003,2 +7219,1 @@\n-        __ evpcmpw($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquw($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpw($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7008,3 +7223,1 @@\n-        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n-\n+        __ evpcmpd($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7014,2 +7227,1 @@\n-        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpq($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7168,0 +7380,1 @@\n+            NULL == n->in(2)->bottom_type()->isa_vectmask() &&\n@@ -7181,0 +7394,1 @@\n+            NULL == n->in(2)->bottom_type()->isa_vectmask() &&\n@@ -7193,1 +7407,1 @@\n-  predicate(vector_length_in_bytes(n) == 64);\n+  predicate(vector_length_in_bytes(n) == 64 && NULL == n->in(2)->bottom_type()->isa_vectmask());\n@@ -7206,0 +7420,16 @@\n+\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+  predicate(n->in(2)->bottom_type()->isa_vectmask() &&\n+            (!is_subword_type(vector_element_basic_type(n)) ||\n+             VM_Version::supports_avx512bw()));\n+  match(Set dst (VectorBlend (Binary src1 src2) mask));\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n+  effect(TEMP scratch);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = vector_element_basic_type(this);\n+    __ evpblend(elem_bt, $dst$$XMMRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7210,0 +7440,1 @@\n+  ins_cost(450);\n@@ -7225,0 +7456,1 @@\n+  ins_cost(450);\n@@ -7241,0 +7473,1 @@\n+  ins_cost(250);\n@@ -7255,0 +7488,1 @@\n+  ins_cost(450);\n@@ -7327,1 +7561,2 @@\n-  predicate(vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7332,1 +7567,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_lt16 $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n@@ -7342,2 +7577,3 @@\n-instruct vptest_alltrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_alltrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7348,1 +7584,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_ge16  $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n@@ -7358,6 +7594,8 @@\n-instruct vptest_alltrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(vector_length_in_bytes(n->in(1)) == 64 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+instruct vptest_alltrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            vector_length(n->in(1)) < 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr, TEMP kscratch);\n+  format %{ \"vptest_alltrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7365,4 +7603,23 @@\n-    int vlen = vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::carrySet, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $kscratch$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vptest_alltrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            vector_length(n->in(1)) >= 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_alltrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, knoreg);\n@@ -7373,0 +7630,1 @@\n+\n@@ -7374,1 +7632,2 @@\n-  predicate(vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7379,1 +7638,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_lt16 $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n@@ -7389,2 +7648,3 @@\n-instruct vptest_anytrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_anytrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7395,1 +7655,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_ge16 $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7405,6 +7665,7 @@\n-instruct vptest_anytrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(vector_length_in_bytes(n->in(1)) == 64 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+instruct vptest_anytrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne &&\n+            vector_length(n->in(1)) < 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr, TEMP kscratch);\n+  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7412,4 +7673,22 @@\n-    int vlen = vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::notZero, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint  masklen = vector_length(this, $src1);\n+    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $kscratch$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vptest_anytrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne &&\n+            vector_length(n->in(1)) >= 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_anytrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint  masklen = vector_length(this, $src1);\n+    __ anytrue($dst$$Register, masklen, $src1$$KRegister, knoreg);\n@@ -7421,1 +7700,2 @@\n-  predicate(vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n@@ -7426,1 +7706,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t! using $vtmp as TEMP\" %}\n+  format %{ \"cmpvptest_anytrue_lt16 $src1,$src2\\t! using $vtmp as TEMP\" %}\n@@ -7434,2 +7714,3 @@\n-instruct cmpvptest_anytrue(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n-  predicate(vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n+instruct cmpvptest_anytrue_ge16(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n@@ -7439,1 +7720,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_ge16 $src1,$src2\\t!\" %}\n@@ -7447,3 +7728,4 @@\n-instruct cmpvptest_anytrue_evex(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero, kReg ktmp) %{\n-  predicate(vector_length_in_bytes(n->in(1)->in(1)) == 64 &&\n-            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne);\n+instruct cmpvptest_anytrue_lt8_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero, kReg ktmp) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne &&\n+            vector_length(n->in(1)->in(1)) < 8);\n@@ -7452,1 +7734,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_lt8_evex $src1,$src2\\t!\" %}\n@@ -7454,2 +7736,24 @@\n-    int vlen = vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = vector_length(this, $src1);\n+    __ kxnorbl($ktmp$$KRegister, $ktmp$$KRegister, $ktmp$$KRegister);\n+    __ kshiftrbl($ktmp$$KRegister, $ktmp$$KRegister, 8-masklen);\n+    __ kandbl($ktmp$$KRegister, $ktmp$$KRegister, $src1$$KRegister);\n+    __ ktestbl($ktmp$$KRegister, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct cmpvptest_anytrue_ge8_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne &&\n+            vector_length(n->in(1)->in(1)) >= 8);\n+  match(Set cr (CmpI (VectorTest src1 src2) zero));\n+  format %{ \"cmpvptest_anytrue_ge8_evex $src1,$src2\\t!\" %}\n+  ins_encode %{\n+    uint masklen = vector_length(this, $src1);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    __ ktest(masklen, $src1$$KRegister, $src1$$KRegister);\n@@ -7477,1 +7781,1 @@\n-instruct loadMask_evex(vec dst, vec src) %{\n+instruct loadMask_evex(kReg dst, vec src, rRegP scratch, kReg kscratch) %{\n@@ -7480,1 +7784,1 @@\n-  effect(TEMP dst);\n+  effect(TEMP scratch, TEMP kscratch);\n@@ -7483,4 +7787,4 @@\n-    int vlen_in_bytes = vector_length_in_bytes(this);\n-    BasicType elem_bt = vector_element_basic_type(this);\n-\n-    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, false);\n+    int vlen_in_bytes = vector_length_in_bytes(in(1));\n+    int vlen_enc = vector_length_encoding(vlen_in_bytes);\n+    __ evpcmp(T_BYTE, $dst$$KRegister, k0, $src$$XMMRegister, ExternalAddress(vector_masked_cmp_bits()),\n+              Assembler::eq, vlen_enc, $scratch$$Register);\n@@ -7494,1 +7798,2 @@\n-  predicate(vector_length(n) < 64 || VM_Version::supports_avx512vlbw());\n+  predicate((vector_length(n) < 64 || VM_Version::supports_avx512vlbw()) &&\n+            NULL == n->in(1)->bottom_type()->isa_vectmask());\n@@ -7511,1 +7816,2 @@\n-  predicate(vector_length(n) <= 8);\n+  predicate(vector_length(n) <= 8 &&\n+            NULL == n->in(1)->bottom_type()->isa_vectmask());\n@@ -7536,13 +7842,0 @@\n-instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n-  predicate(VM_Version::supports_avx512bw());\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  ins_encode %{\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -7577,16 +7870,0 @@\n-instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n-  predicate(UseAVX > 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  ins_encode %{\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    if (!VM_Version::supports_avx512vl()) {\n-      src_vlen_enc = Assembler::AVX_512bit;\n-    }\n-    __ evpmovdb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -7624,1 +7901,14 @@\n-instruct vstoreMask8B_evex(vec dst, vec src, immI_8 size) %{\n+instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n+  predicate(VM_Version::supports_avx512bw());\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  ins_encode %{\n+    int src_vlen_enc = vector_length_encoding(this, $src);\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n@@ -7628,0 +7918,16 @@\n+  ins_encode %{\n+    int src_vlen_enc = vector_length_encoding(this, $src);\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    if (!VM_Version::supports_avx512vl()) {\n+      src_vlen_enc = Assembler::AVX_512bit;\n+    }\n+    __ evpmovdb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vstoreMask8B_evex(vec dst, vec src, immI_8 size) %{\n+  predicate(UseAVX > 2 && NULL == n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n@@ -7640,1 +7946,15 @@\n-instruct vmaskcast(vec dst) %{\n+instruct vstoreMask_evex(vec dst, kReg mask, immI size, rRegP scratch) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP scratch);\n+  format %{ \"vector_store_mask $dst,$mask\\t!\" %}\n+  ins_encode %{\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+    __ evmovdqub($dst$$XMMRegister, $mask$$KRegister, ExternalAddress(vector_masked_cmp_bits()),\n+                  true, dst_vlen_enc, $scratch$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaskcast_evex(kReg dst) %{\n@@ -7652,3 +7972,15 @@\n-\/\/-------------------------------- Load Iota Indices ----------------------------------\n-\n-instruct loadIotaIndices(vec dst, immI_0 src, rRegP scratch) %{\n+instruct vmaskcast(vec dst) %{\n+  predicate((vector_length(n) == vector_length(n->in(1))) &&\n+            (vector_length_in_bytes(n) == vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast dst));\n+  ins_cost(0);\n+  format %{ \"vector_mask_cast $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(empty);\n+%}\n+\n+\/\/-------------------------------- Load Iota Indices ----------------------------------\n+\n+instruct loadIotaIndices(vec dst, immI_0 src, rRegP scratch) %{\n@@ -8272,0 +8604,564 @@\n+\/\/ ---------------------------------- Vector Masked Operations ------------------------------------\n+\n+instruct vadd_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (AddVB (Binary dst src2) mask));\n+  match(Set dst (AddVS (Binary dst src2) mask));\n+  match(Set dst (AddVI (Binary dst src2) mask));\n+  match(Set dst (AddVL (Binary dst src2) mask));\n+  match(Set dst (AddVF (Binary dst src2) mask));\n+  match(Set dst (AddVD (Binary dst src2) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vadd_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (AddVB (Binary dst src2) mask));\n+  match(Set dst (AddVS (Binary dst src2) mask));\n+  match(Set dst (AddVI (Binary dst src2) mask));\n+  match(Set dst (AddVL (Binary dst src2) mask));\n+  match(Set dst (AddVF (Binary dst src2) mask));\n+  match(Set dst (AddVD (Binary dst src2) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (XorV (Binary dst src2) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (XorV (Binary dst src2) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (OrV (Binary dst src2) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (OrV (Binary dst src2) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (AndV (Binary dst src2) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (AndV (Binary dst src2) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (SubVB (Binary dst src2) mask));\n+  match(Set dst (SubVS (Binary dst src2) mask));\n+  match(Set dst (SubVI (Binary dst src2) mask));\n+  match(Set dst (SubVL (Binary dst src2) mask));\n+  match(Set dst (SubVF (Binary dst src2) mask));\n+  match(Set dst (SubVD (Binary dst src2) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (SubVB (Binary dst src2) mask));\n+  match(Set dst (SubVS (Binary dst src2) mask));\n+  match(Set dst (SubVI (Binary dst src2) mask));\n+  match(Set dst (SubVL (Binary dst src2) mask));\n+  match(Set dst (SubVF (Binary dst src2) mask));\n+  match(Set dst (SubVD (Binary dst src2) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (MulVS (Binary dst src2) mask));\n+  match(Set dst (MulVI (Binary dst src2) mask));\n+  match(Set dst (MulVL (Binary dst src2) mask));\n+  match(Set dst (MulVF (Binary dst src2) mask));\n+  match(Set dst (MulVD (Binary dst src2) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (MulVS (Binary dst src2) mask));\n+  match(Set dst (MulVI (Binary dst src2) mask));\n+  match(Set dst (MulVL (Binary dst src2) mask));\n+  match(Set dst (MulVF (Binary dst src2) mask));\n+  match(Set dst (MulVD (Binary dst src2) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (DivVF (Binary dst src2) mask));\n+  match(Set dst (DivVD (Binary dst src2) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (DivVF (Binary dst src2) mask));\n+  match(Set dst (DivVD (Binary dst src2) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (MaxV (Binary dst src2) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (MaxV (Binary dst src2) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (MinV (Binary dst src2) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (MinV (Binary dst src2) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrearrangev_reg_masked(vec dst, vec src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (VectorRearrange (Binary dst src2) mask));\n+  format %{ \"vprearrange_masked $dst, $dst, $src2\\t! rearrange masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, false, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrearrangev_mem_masked(vec dst, memory src2, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (VectorRearrange (Binary dst src2) mask));\n+  format %{ \"vprearrange_masked $dst, $dst, $src2\\t! rearrange masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, false, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vabs_masked(vec dst, kReg mask) %{\n+  match(Set dst (AbsVB dst mask));\n+  match(Set dst (AbsVS dst mask));\n+  match(Set dst (AbsVI dst mask));\n+  match(Set dst (AbsVL dst mask));\n+  format %{ \"vabs_masked $dst, $mask \\t! vabs masked operation\" %}\n+  ins_cost(100);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_reg_masked(vec dst, vec src2, vec src3, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_mem_masked(vec dst, vec src2, memory src3, kReg mask) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+  predicate(UseAVX > 2);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond mask)));\n+  effect(TEMP scratch);\n+  format %{ \"vcmp_masked $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    BasicType src1_elem_bt = vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n+    switch (src1_elem_bt) {\n+      case T_BYTE: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpb($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_SHORT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpw($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_INT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_LONG: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpq($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmpps($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmppd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexI(kReg dst, rRegI src, vec xtmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP xtmp);\n+  format %{ \"mask_all_evexI $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(vector_length(this));\n+    __ evpbroadcastb($xtmp$$XMMRegister, $src$$Register, vlen_enc);\n+    __ evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+#ifdef _LP64\n+instruct mask_all_evexL(kReg dst, rRegL src, vec xtmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP xtmp);\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(vector_length(this));\n+    __ evpbroadcastb($xtmp$$XMMRegister, $src$$Register, vlen_enc);\n+    __ evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n+instruct mask_opers_evex(kReg dst, kReg src1, kReg src2, kReg kscratch) %{\n+  predicate(VM_Version::supports_avx512vlbwdq());\n+  match(Set dst (AndVMask src1 src2));\n+  match(Set dst (OrVMask src1 src2));\n+  match(Set dst (XorVMask src1 src2));\n+  effect(TEMP kscratch);\n+  format %{ \"mask_opers_evex $dst, $src1, $src2\\t!\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = vector_length(this);\n+    __ masked_op(this->ideal_Opcode(), masklen, $dst$$KRegister, $src1$$KRegister, $src2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castMM(kReg dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":1001,"deletions":105,"binary":false,"changes":1106,"status":"modified"},{"patch":"@@ -11285,1 +11285,1 @@\n-  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11301,1 +11301,1 @@\n-  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11317,1 +11317,1 @@\n-  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11333,1 +11333,1 @@\n-  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11349,1 +11349,1 @@\n-  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n@@ -11365,1 +11365,1 @@\n-  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n@@ -11381,1 +11381,1 @@\n-  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n@@ -11397,1 +11397,1 @@\n-  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n@@ -11580,1 +11580,1 @@\n-  predicate(UseAVX <= 2);\n+  predicate(!VM_Version::supports_avx512vlbw());\n@@ -11596,1 +11596,1 @@\n-  predicate(UseAVX > 2);\n+  predicate(VM_Version::supports_avx512vlbw());\n@@ -11613,1 +11613,1 @@\n-  predicate(UseAVX <= 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11629,1 +11629,1 @@\n-  predicate(UseAVX > 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11645,1 +11645,1 @@\n-  predicate(UseAVX <= 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11661,1 +11661,1 @@\n-  predicate(UseAVX > 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11677,1 +11677,1 @@\n-  predicate(UseAVX <= 2);\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n@@ -11693,1 +11693,1 @@\n-  predicate(UseAVX > 2);\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n@@ -11709,1 +11709,1 @@\n-  predicate(UseAVX <= 2);\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n@@ -11726,1 +11726,1 @@\n-  predicate(UseAVX > 2);\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n@@ -11743,1 +11743,1 @@\n-  predicate(UseAVX <= 2);\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n@@ -11757,1 +11757,1 @@\n-  predicate(UseAVX > 2);\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2359,0 +2359,1 @@\n+         n->req() == 2 &&\n@@ -2366,1 +2367,1 @@\n-      return true;\n+      return n->req() == 2;\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -174,0 +174,1 @@\n+class VectorUnboxNode;\n@@ -709,0 +710,1 @@\n+        DEFINE_CLASS_ID(VectorUnbox, Vector, 1)\n@@ -934,0 +936,2 @@\n+  DEFINE_CLASS_QUERY(VectorMaskCmp)\n+  DEFINE_CLASS_QUERY(VectorUnbox)\n@@ -938,1 +942,0 @@\n-  DEFINE_CLASS_QUERY(VectorMaskCmp)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -341,1 +341,4 @@\n-  if (is_mask && bt != T_BOOLEAN) {\n+  \/\/ If boxed mask value is present in a predicate register, it must be\n+  \/\/ spilled to a vector though a VectorStoreMaskOperation before actual StoreVector\n+  \/\/ operation to vector payload field.\n+  if (is_mask && (value->bottom_type()->isa_vectmask() || bt != T_BOOLEAN)) {\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -435,1 +435,1 @@\n-    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_klass));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -350,2 +350,2 @@\n-bool VectorNode::is_vshift_cnt(Node* n) {\n-  switch (n->Opcode()) {\n+bool VectorNode::is_vshift_cnt_opcode(int opc) {\n+  switch (opc) {\n@@ -360,0 +360,4 @@\n+bool VectorNode::is_vshift_cnt(Node* n) {\n+  return is_vshift_cnt_opcode(n->Opcode());\n+}\n+\n@@ -431,1 +435,1 @@\n-      if (Matcher::match_rule_supported_vector(Op_AndVMask, vlen, bt)) {\n+      if (Matcher::match_rule_supported_vector_masked(Op_AndVMask, vlen, bt)) {\n@@ -436,1 +440,1 @@\n-      if (Matcher::match_rule_supported_vector(Op_OrVMask, vlen, bt)) {\n+      if (Matcher::match_rule_supported_vector_masked(Op_OrVMask, vlen, bt)) {\n@@ -441,1 +445,1 @@\n-      if (Matcher::match_rule_supported_vector(Op_XorVMask, vlen, bt)) {\n+      if (Matcher::match_rule_supported_vector_masked(Op_XorVMask, vlen, bt)) {\n@@ -1047,1 +1051,1 @@\n-  if (out_bt == T_BOOLEAN) {\n+  if (!Matcher::has_predicated_vectors() && out_bt == T_BOOLEAN) {\n@@ -1050,0 +1054,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -70,4 +70,1 @@\n-    if (vect_type()->isa_vectmask()) {\n-      return Op_RegVectMask;\n-    }\n-    return Matcher::vector_ideal_reg(vect_type()->length_in_bytes());\n+    return type()->ideal_reg();\n@@ -85,0 +82,1 @@\n+  static bool is_vshift_cnt_opcode(int opc);\n@@ -1302,1 +1300,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  virtual  uint size_of() const { return sizeof(VectorMaskCmpNode); }\n@@ -1573,0 +1571,1 @@\n+    init_class_id(Class_VectorUnbox);\n@@ -1602,1 +1601,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"}]}