{"files":[{"patch":"@@ -2581,0 +2581,7 @@\n+void Assembler::knotbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n@@ -2767,0 +2774,15 @@\n+}\n+\n+void Assembler::kshiftrdl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrql(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1495,0 +1495,1 @@\n+  void knotbl(KRegister dst, KRegister src);\n@@ -1506,0 +1507,2 @@\n+  void kshiftrdl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrql(KRegister dst, KRegister src, int imm8);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -8444,12 +8444,5 @@\n-void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch) {\n-  if (masklen < 8) {\n-    kxnorbl(kscratch, kscratch, kscratch);\n-    kshiftrbl(kscratch, kscratch, 8-masklen);\n-    ktestbl(kscratch, src);\n-    setb(Assembler::notZero, dst);\n-    movzbl(dst, dst);\n-  } else {\n-    ktest(masklen, src, src);\n-    setb(Assembler::notZero, dst);\n-    movzbl(dst, dst);\n-  }\n+void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src1, KRegister src2) {\n+   masklen = masklen < 8 ? 8 : masklen;\n+   ktest(masklen, src1, src2);\n+   setb(Assembler::notZero, dst);\n+   movzbl(dst, dst);\n@@ -8458,1 +8451,1 @@\n-void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src, KRegister kscratch) {\n+void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch) {\n@@ -8460,3 +8453,2 @@\n-    kxnorbl(kscratch, kscratch, kscratch);\n-    kshiftlbl(kscratch, kscratch, masklen);\n-    kortestbl(kscratch, src);\n+    knotbl(kscratch, src2);\n+    kortestbl(src1, kscratch);\n@@ -8466,1 +8458,1 @@\n-    kortest(masklen, src, src);\n+    ktest(masklen, src1, src2);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":9,"deletions":17,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1708,1 +1708,1 @@\n-  void alltrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n+  void alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1842,1 +1842,1 @@\n-      if(!VM_Version::supports_avx512vlbw()) {\n+      if(!is_LP64 || !VM_Version::supports_avx512vlbw()) {\n@@ -1856,3 +1856,7 @@\n-  \/\/ Needed for loadmask pattern which populates opmask register\n-  \/\/ consumed by masked instructions.\n-  if (!VM_Version::supports_avx512bw()) {\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n@@ -1861,1 +1865,3 @@\n-  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+  \/\/ Needed for loadmask pattern which populates opmask register\n+  \/\/ consumed by masked instructions.\n+  if (!VM_Version::supports_avx512vlbw()) {\n@@ -1864,0 +1870,1 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1865,3 +1872,0 @@\n-  if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n-     return false; \/\/ Implementation limitation\n-  }\n@@ -1942,4 +1946,2 @@\n-     if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false;\n-     }\n-     return true;\n+      assert(!is_subword_type(bt) || VM_Version::supports_avx512bw(), \"\");\n+      return true;\n@@ -1958,0 +1960,5 @@\n+      assert(bt != T_INT  || VM_Version::supports_avx512bw(), \"\");\n+      assert(bt != T_LONG || VM_Version::supports_avx512bw(), \"\");\n+      if (bt == T_BYTE && VM_Version::supports_avx512dq()) {\n+        return false; \/\/ Implementation limitation\n+      }\n@@ -1959,1 +1966,2 @@\n-     return true;\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      return true;\n@@ -7605,1 +7613,1 @@\n-    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $kscratch$$KRegister);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, $kscratch$$KRegister);\n@@ -7623,1 +7631,1 @@\n-    __ alltrue($dst$$Register, masklen, $src1$$KRegister, knoreg);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, knoreg);\n@@ -7663,18 +7671,1 @@\n-instruct vptest_anytrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bwdq() &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne &&\n-            vector_length(n->in(1)) < 8);\n-  match(Set dst (VectorTest src1 src2));\n-  effect(KILL cr, TEMP kscratch);\n-  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n-  ins_encode %{\n-    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n-    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n-    uint  masklen = vector_length(this, $src1);\n-    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $kscratch$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vptest_anytrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+instruct vptest_anytrue_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n@@ -7682,2 +7673,1 @@\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne &&\n-            vector_length(n->in(1)) >= 8);\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n@@ -7686,1 +7676,1 @@\n-  format %{ \"vptest_anytrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7692,1 +7682,1 @@\n-    __ anytrue($dst$$Register, masklen, $src1$$KRegister, knoreg);\n+    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7726,20 +7716,1 @@\n-instruct cmpvptest_anytrue_lt8_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero, kReg ktmp) %{\n-  predicate(VM_Version::supports_avx512bwdq() &&\n-            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne &&\n-            vector_length(n->in(1)->in(1)) < 8);\n-  match(Set cr (CmpI (VectorTest src1 src2) zero));\n-  effect(TEMP ktmp);\n-  format %{ \"cmpvptest_anytrue_lt8_evex $src1,$src2\\t!\" %}\n-  ins_encode %{\n-    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n-    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n-    uint masklen = vector_length(this, $src1);\n-    __ kxnorbl($ktmp$$KRegister, $ktmp$$KRegister, $ktmp$$KRegister);\n-    __ kshiftrbl($ktmp$$KRegister, $ktmp$$KRegister, 8-masklen);\n-    __ ktestbl($ktmp$$KRegister, $src1$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpvptest_anytrue_ge8_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n+instruct cmpvptest_anytrue_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n@@ -7747,2 +7718,1 @@\n-            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne &&\n-            vector_length(n->in(1)->in(1)) >= 8);\n+            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne);\n@@ -7750,1 +7720,1 @@\n-  format %{ \"cmpvptest_anytrue_ge8_evex $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_evex $src1,$src2\\t!\" %}\n@@ -7756,1 +7726,2 @@\n-    __ ktest(masklen, $src1$$KRegister, $src1$$KRegister);\n+    masklen = masklen < 8 ? 8 : masklen;\n+    __ ktest(masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -9068,1 +9039,15 @@\n-instruct mask_all_evexI(kReg dst, rRegI src, vec xtmp) %{\n+#ifdef _LP64\n+instruct mask_all_evexI_imm(kReg dst, immI cnt, rRegL tmp) %{\n+  match(Set dst (MaskAll cnt));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $cnt \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vec_len = vector_length(this);\n+    __ movq($tmp$$Register, $cnt$$constant);\n+    __ kmovql($dst$$KRegister, $tmp$$Register);\n+    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexI(kReg dst, rRegI src, rRegL tmp) %{\n@@ -9070,1 +9055,1 @@\n-  effect(TEMP xtmp);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -9073,3 +9058,4 @@\n-    int vlen_enc = vector_length_encoding(vector_length(this));\n-    __ evpbroadcastb($xtmp$$XMMRegister, $src$$Register, vlen_enc);\n-    __ evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+    int vec_len = vector_length(this);\n+    __ movslq($tmp$$Register, $src$$Register);\n+    __ kmovql($dst$$KRegister, $tmp$$Register);\n+    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n@@ -9080,2 +9066,1 @@\n-#ifdef _LP64\n-instruct mask_all_evexL(kReg dst, rRegL src, vec xtmp) %{\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n@@ -9083,1 +9068,1 @@\n-  effect(TEMP xtmp);\n+  effect(TEMP_DEF dst);\n@@ -9086,3 +9071,3 @@\n-    int vlen_enc = vector_length_encoding(vector_length(this));\n-    __ evpbroadcastb($xtmp$$XMMRegister, $src$$Register, vlen_enc);\n-    __ evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+    int vec_len = vector_length(this);\n+    __ kmovql($dst$$KRegister, $src$$Register);\n+    __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n@@ -9095,1 +9080,0 @@\n-  predicate(VM_Version::supports_avx512vlbwdq());\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":57,"deletions":73,"binary":false,"changes":130,"status":"modified"}]}