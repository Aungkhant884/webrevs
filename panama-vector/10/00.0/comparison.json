{"files":[{"patch":"@@ -91,0 +91,7 @@\n+  else ifeq ($(call isTargetOs, windows), true)\n+    ifeq ($(call isTargetCpuBits, 64), true)\n+      ADLCFLAGS += -D_WIN64=1\n+    endif\n+    ifeq ($(HOTSPOT_TARGET_CPU_ARCH), aarch64)\n+      ADLCFLAGS += -DR18_RESERVED\n+    endif\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -19,0 +19,2 @@\n+        if self.number == 18:\n+            self.number = 17\n@@ -43,0 +45,2 @@\n+        if self.number == 18:\n+            self.number = 16\n@@ -60,0 +64,2 @@\n+        if self.number == 18:\n+            self.number = 15\n@@ -1485,1 +1491,1 @@\n-                        [\"st1d\",   \"__ sve_st1d(z0, __ D, p4, Address(r0, r18));\",       \"st1d\\t{z0.d}, p4, [x0, x18, LSL #3]\"],\n+                        [\"st1d\",   \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64-asmtest.py","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -117,2 +117,2 @@\n-reg_def R18     ( SOC, SOC, Op_RegI, 18, r18->as_VMReg()        );\n-reg_def R18_H   ( SOC, SOC, Op_RegI, 18, r18->as_VMReg()->next());\n+reg_def R18     ( SOC, SOC, Op_RegI, 18, r18_tls->as_VMReg()        );\n+reg_def R18_H   ( SOC, SOC, Op_RegI, 18, r18_tls->as_VMReg()->next());\n@@ -250,2 +250,2 @@\n-  reg_def V8   ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()          );\n-  reg_def V8_H ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next()  );\n+  reg_def V8   ( SOC, SOE, Op_RegF, 8, v8->as_VMReg()          );\n+  reg_def V8_H ( SOC, SOE, Op_RegF, 8, v8->as_VMReg()->next()  );\n@@ -259,2 +259,2 @@\n-  reg_def V9   ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()          );\n-  reg_def V9_H ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next()  );\n+  reg_def V9   ( SOC, SOE, Op_RegF, 9, v9->as_VMReg()          );\n+  reg_def V9_H ( SOC, SOE, Op_RegF, 9, v9->as_VMReg()->next()  );\n@@ -268,2 +268,2 @@\n-  reg_def V10   ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()          );\n-  reg_def V10_H ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next()  );\n+  reg_def V10   ( SOC, SOE, Op_RegF, 10, v10->as_VMReg()          );\n+  reg_def V10_H ( SOC, SOE, Op_RegF, 10, v10->as_VMReg()->next()  );\n@@ -277,2 +277,2 @@\n-  reg_def V11   ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()          );\n-  reg_def V11_H ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next()  );\n+  reg_def V11   ( SOC, SOE, Op_RegF, 11, v11->as_VMReg()          );\n+  reg_def V11_H ( SOC, SOE, Op_RegF, 11, v11->as_VMReg()->next()  );\n@@ -286,2 +286,2 @@\n-  reg_def V12   ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()          );\n-  reg_def V12_H ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next()  );\n+  reg_def V12   ( SOC, SOE, Op_RegF, 12, v12->as_VMReg()          );\n+  reg_def V12_H ( SOC, SOE, Op_RegF, 12, v12->as_VMReg()->next()  );\n@@ -295,2 +295,2 @@\n-  reg_def V13   ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()          );\n-  reg_def V13_H ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next()  );\n+  reg_def V13   ( SOC, SOE, Op_RegF, 13, v13->as_VMReg()          );\n+  reg_def V13_H ( SOC, SOE, Op_RegF, 13, v13->as_VMReg()->next()  );\n@@ -304,2 +304,2 @@\n-  reg_def V14   ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()          );\n-  reg_def V14_H ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next()  );\n+  reg_def V14   ( SOC, SOE, Op_RegF, 14, v14->as_VMReg()          );\n+  reg_def V14_H ( SOC, SOE, Op_RegF, 14, v14->as_VMReg()->next()  );\n@@ -313,2 +313,2 @@\n-  reg_def V15   ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()          );\n-  reg_def V15_H ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next()  );\n+  reg_def V15   ( SOC, SOE, Op_RegF, 15, v15->as_VMReg()          );\n+  reg_def V15_H ( SOC, SOE, Op_RegF, 15, v15->as_VMReg()->next()  );\n@@ -719,0 +719,4 @@\n+#ifdef R18_RESERVED\n+    \/\/ See comment in register_aarch64.hpp\n+    R18,                        \/\/ tls on Windows\n+#endif\n@@ -726,0 +730,4 @@\n+#ifdef R18_RESERVED\n+    \/\/ See comment in register_aarch64.hpp\n+    R18, R18_H,                 \/\/ tls on Windows, platform register on macOS\n+#endif\n@@ -1961,3 +1969,4 @@\n-    st->print(\"# touch polling page\\n\\t\");\n-    st->print(\"ldr rscratch1, [rthread],#polling_page_offset\\n\\t\");\n-    st->print(\"ldr zr, [rscratch1]\");\n+    st->print(\"# test polling word\\n\\t\");\n+    st->print(\"ldr  rscratch1, [rthread],#%d\\n\\t\", in_bytes(JavaThread::polling_word_offset()));\n+    st->print(\"cmp  sp, rscratch1\\n\\t\");\n+    st->print(\"bhi #slow_path\");\n@@ -1980,1 +1989,7 @@\n-    __ fetch_and_read_polling_page(rscratch1, relocInfo::poll_return_type);\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+    }\n+    __ relocate(relocInfo::poll_return_type);\n+    __ safepoint_poll(*code_stub, true \/* at_return *\/, false \/* acquire *\/, true \/* in_nmethod *\/);\n@@ -6341,1 +6356,1 @@\n-  max_instructions_per_bundle = 2;   \/\/ A53 = 2, A57 = 4\n+  max_instructions_per_bundle = 4;   \/\/ A53 = 2, A57 = 4\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":38,"deletions":23,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-    __ subw(r3, r13, r18, Assembler::ASR, 30);         \/\/       sub     w3, w13, w18, ASR #30\n+    __ subw(r3, r13, r17, Assembler::ASR, 30);         \/\/       sub     w3, w13, w17, ASR #30\n@@ -112,2 +112,2 @@\n-    __ orrw(r13, r18, r11, Assembler::LSR, 9);         \/\/       orr     w13, w18, w11, LSR #9\n-    __ eorw(r5, r5, r18, Assembler::LSR, 15);          \/\/       eor     w5, w5, w18, LSR #15\n+    __ orrw(r13, r17, r11, Assembler::LSR, 9);         \/\/       orr     w13, w17, w11, LSR #9\n+    __ eorw(r5, r5, r17, Assembler::LSR, 15);          \/\/       eor     w5, w5, w17, LSR #15\n@@ -138,1 +138,1 @@\n-    __ andsw(r7, r18, 1048576ull);                     \/\/       ands    w7, w18, #0x100000\n+    __ andsw(r7, r17, 1048576ull);                     \/\/       ands    w7, w17, #0x100000\n@@ -286,1 +286,1 @@\n-    __ ldxrw(r18, r0);                                 \/\/       ldxr    w18, [x0]\n+    __ ldxrw(r17, r0);                                 \/\/       ldxr    w17, [x0]\n@@ -314,1 +314,1 @@\n-    __ ldxpw(r18, r21, r13);                           \/\/       ldxp    w18, w21, [x13]\n+    __ ldxpw(r17, r21, r13);                           \/\/       ldxp    w17, w21, [x13]\n@@ -327,1 +327,1 @@\n-    __ ldrb(r18, Address(r23, -23));                   \/\/       ldrb    w18, [x23, -23]\n+    __ ldrb(r17, Address(r23, -23));                   \/\/       ldrb    w17, [x23, -23]\n@@ -378,1 +378,1 @@\n-    __ str(r10, Address(r18, r21, Address::sxtw(3)));  \/\/       str     x10, [x18, w21, sxtw #3]\n+    __ str(r10, Address(r17, r21, Address::sxtw(3)));  \/\/       str     x10, [x17, w21, sxtw #3]\n@@ -385,1 +385,1 @@\n-    __ ldrh(r9, Address(r4, r18, Address::uxtw(0)));   \/\/       ldrh    w9, [x4, w18, uxtw #0]\n+    __ ldrh(r9, Address(r4, r17, Address::uxtw(0)));   \/\/       ldrh    w9, [x4, w17, uxtw #0]\n@@ -442,1 +442,1 @@\n-    __ addw(r18, r24, r20, ext::uxtb, 2);              \/\/       add     w18, w24, w20, uxtb #2\n+    __ addw(r17, r24, r20, ext::uxtb, 2);              \/\/       add     w17, w24, w20, uxtb #2\n@@ -453,2 +453,2 @@\n-    __ ccmpw(r22, r18, 6u, Assembler::CC);             \/\/       ccmp    w22, w18, #6, CC\n-    __ ccmn(r18, r30, 14u, Assembler::VS);             \/\/       ccmn    x18, x30, #14, VS\n+    __ ccmpw(r22, r17, 6u, Assembler::CC);             \/\/       ccmp    w22, w17, #6, CC\n+    __ ccmn(r17, r30, 14u, Assembler::VS);             \/\/       ccmn    x17, x30, #14, VS\n@@ -479,1 +479,1 @@\n-    __ rbit(r18, r21);                                 \/\/       rbit    x18, x21\n+    __ rbit(r17, r21);                                 \/\/       rbit    x17, x21\n@@ -489,1 +489,1 @@\n-    __ lslvw(r3, r14, r18);                            \/\/       lslv    w3, w14, w18\n+    __ lslvw(r3, r14, r17);                            \/\/       lslv    w3, w14, w17\n@@ -494,1 +494,1 @@\n-    __ sdiv(r27, r16, r18);                            \/\/       sdiv    x27, x16, x18\n+    __ sdiv(r27, r16, r17);                            \/\/       sdiv    x27, x16, x17\n@@ -498,1 +498,1 @@\n-    __ rorv(r28, r2, r18);                             \/\/       rorv    x28, x2, x18\n+    __ rorv(r28, r2, r17);                             \/\/       rorv    x28, x2, x17\n@@ -543,1 +543,1 @@\n-    __ fsqrtd(v3, v18);                                \/\/       fsqrt   d3, d18\n+    __ fsqrtd(v3, v17);                                \/\/       fsqrt   d3, d17\n@@ -555,1 +555,1 @@\n-    __ fmovs(r18, v21);                                \/\/       fmov    w18, s21\n+    __ fmovs(r17, v21);                                \/\/       fmov    w17, s21\n@@ -557,1 +557,1 @@\n-    __ fmovs(v19, r18);                                \/\/       fmov    s19, w18\n+    __ fmovs(v19, r17);                                \/\/       fmov    s19, w17\n@@ -601,1 +601,1 @@\n-    __ ld2(v18, v19, __ T2D, Address(r10));            \/\/       ld2     {v18.2D, v19.2D}, [x10]\n+    __ ld2(v17, v18, __ T2D, Address(r10));            \/\/       ld2     {v17.2D, v18.2D}, [x10]\n@@ -603,1 +603,1 @@\n-    __ ld2r(v25, v26, __ T16B, Address(r18));          \/\/       ld2r    {v25.16B, v26.16B}, [x18]\n+    __ ld2r(v25, v26, __ T16B, Address(r17));          \/\/       ld2r    {v25.16B, v26.16B}, [x17]\n@@ -605,1 +605,1 @@\n-    __ ld2r(v16, v17, __ T2D, Address(__ post(r18, r9))); \/\/    ld2r    {v16.2D, v17.2D}, [x18], x9\n+    __ ld2r(v16, v17, __ T2D, Address(__ post(r17, r9))); \/\/    ld2r    {v16.2D, v17.2D}, [x17], x9\n@@ -613,1 +613,1 @@\n-    __ ld4r(v24, v25, v26, v27, __ T8B, Address(r18)); \/\/       ld4r    {v24.8B, v25.8B, v26.8B, v27.8B}, [x18]\n+    __ ld4r(v24, v25, v26, v27, __ T8B, Address(r17)); \/\/       ld4r    {v24.8B, v25.8B, v26.8B, v27.8B}, [x17]\n@@ -701,1 +701,1 @@\n-    __ mlsv(v18, __ T2S, v19, v20);                    \/\/       mls     v18.2S, v19.2S, v20.2S\n+    __ mlsv(v17, __ T2S, v18, v19);                    \/\/       mls     v17.2S, v18.2S, v19.2S\n@@ -710,1 +710,1 @@\n-    __ maxv(v18, __ T16B, v19, v20);                   \/\/       smax    v18.16B, v19.16B, v20.16B\n+    __ maxv(v17, __ T16B, v18, v19);                   \/\/       smax    v17.16B, v18.16B, v19.16B\n@@ -723,1 +723,1 @@\n-    __ minv(v18, __ T4S, v19, v20);                    \/\/       smin    v18.4S, v19.4S, v20.4S\n+    __ minv(v17, __ T4S, v18, v19);                    \/\/       smin    v17.4S, v18.4S, v19.4S\n@@ -725,1 +725,1 @@\n-    __ fmin(v18, __ T4S, v19, v20);                    \/\/       fmin    v18.4S, v19.4S, v20.4S\n+    __ fmin(v17, __ T4S, v18, v19);                    \/\/       fmin    v17.4S, v18.4S, v19.4S\n@@ -747,1 +747,1 @@\n-    __ cmge(v18, __ T8B, v19, v20);                    \/\/       cmge    v18.8B, v19.8B, v20.8B\n+    __ cmge(v17, __ T8B, v18, v19);                    \/\/       cmge    v17.8B, v18.8B, v19.8B\n@@ -756,1 +756,1 @@\n-    __ fcmge(v18, __ T2D, v19, v20);                   \/\/       fcmge   v18.2D, v19.2D, v20.2D\n+    __ fcmge(v17, __ T2D, v18, v19);                   \/\/       fcmge   v17.2D, v18.2D, v19.2D\n@@ -809,1 +809,1 @@\n-    __ sve_st1d(z0, __ D, p4, Address(r0, r18));       \/\/       st1d    {z0.d}, p4, [x0, x18, LSL #3]\n+    __ sve_st1d(z0, __ D, p4, Address(r0, r17));       \/\/       st1d    {z0.d}, p4, [x0, x17, LSL #3]\n@@ -856,1 +856,1 @@\n-    __ ldumin(Assembler::xword, r12, r18, sp);         \/\/       ldumin  x12, x18, [sp]\n+    __ ldumin(Assembler::xword, r12, r16, sp);         \/\/       ldumin  x12, x16, [sp]\n@@ -866,1 +866,1 @@\n-    __ ldsmaxa(Assembler::xword, r18, r9, r8);         \/\/       ldsmaxa x18, x9, [x8]\n+    __ ldsmaxa(Assembler::xword, r16, r9, r8);         \/\/       ldsmaxa x16, x9, [x8]\n@@ -874,1 +874,1 @@\n-    __ ldeoral(Assembler::xword, r10, r4, r18);        \/\/       ldeoral x10, x4, [x18]\n+    __ ldeoral(Assembler::xword, r10, r4, r15);        \/\/       ldeoral x10, x4, [x15]\n@@ -884,2 +884,2 @@\n-    __ ldbicl(Assembler::xword, r18, r21, r16);        \/\/       ldclrl  x18, x21, [x16]\n-    __ ldeorl(Assembler::xword, r18, r11, r21);        \/\/       ldeorl  x18, x11, [x21]\n+    __ ldbicl(Assembler::xword, r16, r21, r16);        \/\/       ldclrl  x16, x21, [x16]\n+    __ ldeorl(Assembler::xword, r16, r11, r21);        \/\/       ldeorl  x16, x11, [x21]\n@@ -908,2 +908,2 @@\n-    __ ldorra(Assembler::word, r14, r18, sp);          \/\/       ldseta  w14, w18, [sp]\n-    __ ldsmina(Assembler::word, r18, r27, r20);        \/\/       ldsmina w18, w27, [x20]\n+    __ ldorra(Assembler::word, r14, r16, sp);          \/\/       ldseta  w14, w16, [sp]\n+    __ ldsmina(Assembler::word, r16, r27, r20);        \/\/       ldsmina w16, w27, [x20]\n@@ -938,1 +938,1 @@\n-    __ sve_sub(z18, __ S, z10, z22);                   \/\/       sub     z18.s, z10.s, z22.s\n+    __ sve_sub(z17, __ S, z10, z22);                   \/\/       sub     z17.s, z10.s, z22.s\n@@ -943,1 +943,1 @@\n-    __ sve_add(z9, __ B, p7, z18);                     \/\/       add     z9.b, p7\/m, z9.b, z18.b\n+    __ sve_add(z9, __ B, p7, z17);                     \/\/       add     z9.b, p7\/m, z9.b, z17.b\n@@ -970,1 +970,1 @@\n-    __ sve_mla(z18, __ B, p0, z10, z23);               \/\/       mla     z18.b, p0\/m, z10.b, z23.b\n+    __ sve_mla(z17, __ B, p0, z10, z23);               \/\/       mla     z17.b, p0\/m, z10.b, z23.b\n@@ -977,1 +977,1 @@\n-    __ sve_andv(v3, __ S, p3, z18);                    \/\/       andv s3, p3, z18.s\n+    __ sve_andv(v3, __ S, p3, z17);                    \/\/       andv s3, p3, z17.s\n@@ -995,1 +995,1 @@\n-    0x0b9b3ec9,     0x4b9279a3,     0x2b88474e,     0x6b8c56c0,\n+    0x0b9b3ec9,     0x4b9179a3,     0x2b88474e,     0x6b8c56c0,\n@@ -997,1 +997,1 @@\n-    0x0a5d4a19,     0x2a4b264d,     0x4a523ca5,     0x6a9b6ae2,\n+    0x0a5d4a19,     0x2a4b262d,     0x4a513ca5,     0x6a9b6ae2,\n@@ -1002,1 +1002,1 @@\n-    0x120cb166,     0x321764bc,     0x52174681,     0x720c0247,\n+    0x120cb166,     0x321764bc,     0x52174681,     0x720c0227,\n@@ -1033,1 +1033,1 @@\n-    0x880bfcd0,     0x885f7c12,     0x885ffd44,     0x889ffed8,\n+    0x880bfcd0,     0x885f7c11,     0x885ffd44,     0x889ffed8,\n@@ -1038,1 +1038,1 @@\n-    0xc82cb5f0,     0x887f55b2,     0x887ff90b,     0x88382c2d,\n+    0xc82cb5f0,     0x887f55b1,     0x887ff90b,     0x88382c2d,\n@@ -1040,1 +1040,1 @@\n-    0x781ce322,     0xf850f044,     0xb85e129e,     0x385e92f2,\n+    0x781ce322,     0xf850f044,     0xb85e129e,     0x385e92f1,\n@@ -1051,1 +1051,1 @@\n-    0xbc1df408,     0xf835da4a,     0xb836d9a4,     0x3833580d,\n+    0xbc1df408,     0xf835da2a,     0xb836d9a4,     0x3833580d,\n@@ -1053,1 +1053,1 @@\n-    0x78724889,     0x38a7789b,     0x78beca2f,     0x78f6c810,\n+    0x78714889,     0x38a7789b,     0x78beca2f,     0x78f6c810,\n@@ -1062,1 +1062,1 @@\n-    0xba1e030c,     0xda0f0320,     0xfa030301,     0x0b340b12,\n+    0xba1e030c,     0xda0f0320,     0xfa030301,     0x0b340b11,\n@@ -1065,1 +1065,1 @@\n-    0x7a5232c6,     0xba5e624e,     0xfa53814c,     0x3a52d8c2,\n+    0x7a5132c6,     0xba5e622e,     0xfa53814c,     0x3a52d8c2,\n@@ -1070,1 +1070,1 @@\n-    0xdac002b2,     0xdac0061d,     0xdac00a95,     0xdac00e66,\n+    0xdac002b1,     0xdac0061d,     0xdac00a95,     0xdac00e66,\n@@ -1072,3 +1072,3 @@\n-    0x1ad221c3,     0x1ad825e7,     0x1ad92a3c,     0x1adc2f42,\n-    0x9ada0b25,     0x9ad20e1b,     0x9acc22a6,     0x9acc2480,\n-    0x9adc2a3b,     0x9ad22c5c,     0x9bce7dea,     0x9b597c6e,\n+    0x1ad121c3,     0x1ad825e7,     0x1ad92a3c,     0x1adc2f42,\n+    0x9ada0b25,     0x9ad10e1b,     0x9acc22a6,     0x9acc2480,\n+    0x9adc2a3b,     0x9ad12c5c,     0x9bce7dea,     0x9b597c6e,\n@@ -1083,1 +1083,1 @@\n-    0x1e60c027,     0x1e61400b,     0x1e61c243,     0x1e6240dc,\n+    0x1e60c027,     0x1e61400b,     0x1e61c223,     0x1e6240dc,\n@@ -1086,1 +1086,1 @@\n-    0x1e2602b2,     0x9e660299,     0x1e270253,     0x9e6703a2,\n+    0x1e2602b1,     0x9e660299,     0x1e270233,     0x9e6703a2,\n@@ -1094,2 +1094,2 @@\n-    0x4ddfcaf8,     0x0dd9ccaa,     0x4c408d52,     0x0cdf85ec,\n-    0x4d60c259,     0x0dffcbc1,     0x4de9ce50,     0x4cc24999,\n+    0x4ddfcaf8,     0x0dd9ccaa,     0x4c408d51,     0x0cdf85ec,\n+    0x4d60c239,     0x0dffcbc1,     0x4de9ce30,     0x4cc24999,\n@@ -1097,1 +1097,1 @@\n-    0x4cdf07b1,     0x0cc000fb,     0x0d60e258,     0x0dffe740,\n+    0x4cdf07b1,     0x0cc000fb,     0x0d60e238,     0x0dffe740,\n@@ -1118,1 +1118,1 @@\n-    0x2eb49672,     0x6ebe97bc,     0x0ebbcf59,     0x4eabcd49,\n+    0x2eb39651,     0x6ebe97bc,     0x0ebbcf59,     0x4eabcd49,\n@@ -1120,1 +1120,1 @@\n-    0x0e2c656a,     0x4e346672,     0x0e7a6738,     0x4e7766d5,\n+    0x0e2c656a,     0x4e336651,     0x0e7a6738,     0x4e7766d5,\n@@ -1123,2 +1123,2 @@\n-    0x4e676cc5,     0x0eb66eb4,     0x4eb46e72,     0x0eb1f60f,\n-    0x4eb4f672,     0x4efff7dd,     0x2e3c8f7a,     0x6e3e8fbc,\n+    0x4e676cc5,     0x0eb66eb4,     0x4eb36e51,     0x0eb1f60f,\n+    0x4eb3f651,     0x4efff7dd,     0x2e3c8f7a,     0x6e3e8fbc,\n@@ -1129,1 +1129,1 @@\n-    0x6eaae528,     0x6ee0e7fe,     0x0e343e72,     0x4e2c3d6a,\n+    0x6eaae528,     0x6ee0e7fe,     0x0e333e51,     0x4e2c3d6a,\n@@ -1131,1 +1131,1 @@\n-    0x4ee53c83,     0x2e2ae528,     0x6e38e6f6,     0x6e74e672,\n+    0x4ee53c83,     0x2e2ae528,     0x6e38e6f6,     0x6e73e651,\n@@ -1144,1 +1144,1 @@\n-    0xe5f25000,     0x858043e0,     0x85a043ff,     0xe59f5d08,\n+    0xe5f15000,     0x858043e0,     0x85a043ff,     0xe59f5d08,\n@@ -1154,1 +1154,1 @@\n-    0xf8383064,     0xf82c539f,     0xf82a405a,     0xf82c73f2,\n+    0xf8383064,     0xf82c539f,     0xf82a405a,     0xf82c73f0,\n@@ -1156,1 +1156,1 @@\n-    0xf8a1239a,     0xf8a4309e,     0xf8a6535e,     0xf8b24109,\n+    0xf8a1239a,     0xf8a4309e,     0xf8a6535e,     0xf8b04109,\n@@ -1158,1 +1158,1 @@\n-    0xf8e312ea,     0xf8ea2244,     0xf8e2310b,     0xf8ea522f,\n+    0xf8e312ea,     0xf8ea21e4,     0xf8e2310b,     0xf8ea522f,\n@@ -1160,1 +1160,1 @@\n-    0xf8620184,     0xf8721215,     0xf87222ab,     0xf877334c,\n+    0xf8620184,     0xf8701215,     0xf87022ab,     0xf877334c,\n@@ -1165,1 +1165,1 @@\n-    0xb8b320bf,     0xb8ae33f2,     0xb8b2529b,     0xb8b0416c,\n+    0xb8b320bf,     0xb8ae33f0,     0xb8b0529b,     0xb8b0416c,\n@@ -1171,2 +1171,2 @@\n-    0x04fe0058,     0x04b60552,     0x65c00222,     0x65c20ad9,\n-    0x65db046c,     0x0416b35c,     0x04001e49,     0x045085e4,\n+    0x04fe0058,     0x04b60551,     0x65c00222,     0x65c20ad9,\n+    0x65db046c,     0x0416b35c,     0x04001e29,     0x045085e4,\n@@ -1179,2 +1179,2 @@\n-    0x65f8750c,     0x04174152,     0x04107db3,     0x042e30e0,\n-    0x04aa3119,     0x047b32d4,     0x049a2e43,     0x04182787,\n+    0x65f8750c,     0x04174151,     0x04107db3,     0x042e30e0,\n+    0x04aa3119,     0x047b32d4,     0x049a2e23,     0x04182787,\n@@ -1530,0 +1530,4 @@\n+\n+address Assembler::locate_next_instruction(address inst) {\n+  return inst + Assembler::instruction_size;\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.cpp","additions":80,"deletions":76,"binary":false,"changes":156,"status":"modified"},{"patch":"@@ -31,0 +31,13 @@\n+#ifdef __GNUC__\n+\n+\/\/ __nop needs volatile so that compiler doesn't optimize it away\n+#define NOP() asm volatile (\"nop\");\n+\n+#elif defined(_MSC_VER)\n+\n+\/\/ Use MSVC instrinsic: https:\/\/docs.microsoft.com\/en-us\/cpp\/intrinsics\/arm64-intrinsics?view=vs-2019#I\n+#define NOP() __nop();\n+\n+#endif\n+\n+\n@@ -404,3 +417,1 @@\n-#ifdef ASSERT\n-    : _base(r), _index(noreg), _offset(in_bytes(disp)), _mode(base_plus_offset), _target(0) { }\n-#endif\n+    : Address(r, in_bytes(disp)) { }\n@@ -645,1 +656,1 @@\n-      asm volatile (\"nop\");\n+      NOP();\n@@ -685,0 +696,2 @@\n+  static address locate_next_instruction(address inst);\n+\n@@ -1590,0 +1603,5 @@\n+#ifdef _WIN64\n+\/\/ In MSVC, `mvn` is defined as a macro and it affects compilation\n+#undef mvn\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1105,1 +1105,1 @@\n-    const Register send = r17, dend = r18;\n+    const Register send = r17, dend = r16;\n@@ -1295,0 +1295,2 @@\n+    RegSet clobbered\n+      = MacroAssembler::call_clobbered_registers() - rscratch1;\n@@ -1297,2 +1299,3 @@\n-    for (Register r = r3; r <= r18; r++)\n-      if (r != rscratch1) __ mov(r, rscratch1);\n+    for (RegSetIterator it = clobbered.begin(); *it != noreg; ++it) {\n+      __ mov(*it, rscratch1);\n+    }\n@@ -1300,0 +1303,1 @@\n+\n@@ -1732,1 +1736,2 @@\n-    \/\/ Registers used as temps (r18, r19, r20 are save-on-entry)\n+    \/\/ Registers used as temps (r19, r20, r21, r22 are save-on-entry)\n+    const Register copied_oop  = r22;       \/\/ actual oop copied\n@@ -1735,1 +1740,0 @@\n-    const Register copied_oop  = r18;       \/\/ actual oop copied\n@@ -1772,2 +1776,1 @@\n-\n-    __ push(RegSet::of(r18, r19, r20, r21), sp);\n+    __ push(RegSet::of(r19, r20, r21, r22), sp);\n@@ -1842,1 +1845,1 @@\n-    __ pop(RegSet::of(r18, r19, r20, r21), sp);\n+    __ pop(RegSet::of(r19, r20, r21, r22), sp);\n@@ -2019,1 +2022,1 @@\n-    const Register lh                = r18; \/\/ layout helper\n+    const Register lh                = r15; \/\/ layout helper\n@@ -2090,1 +2093,1 @@\n-    const Register r18_elsize = lh; \/\/ element size\n+    const Register r15_elsize = lh; \/\/ element size\n@@ -2111,2 +2114,2 @@\n-    __ tbnz(r18_elsize, 1, L_copy_ints);\n-    __ tbnz(r18_elsize, 0, L_copy_shorts);\n+    __ tbnz(r15_elsize, 1, L_copy_ints);\n+    __ tbnz(r15_elsize, 0, L_copy_shorts);\n@@ -2125,1 +2128,1 @@\n-    __ tbnz(r18_elsize, 0, L_copy_longs);\n+    __ tbnz(r15_elsize, 0, L_copy_longs);\n@@ -2136,2 +2139,2 @@\n-      __ andw(lh, lh, Klass::_lh_log2_element_size_mask); \/\/ lh -> r18_elsize\n-      __ cmpw(r18_elsize, LogBytesPerLong);\n+      __ andw(lh, lh, Klass::_lh_log2_element_size_mask); \/\/ lh -> r15_elsize\n+      __ cmpw(r15_elsize, LogBytesPerLong);\n@@ -2155,2 +2158,2 @@\n-    __ load_klass(r18, dst);\n-    __ cmp(scratch_src_klass, r18); \/\/ usual case is exact equality\n+    __ load_klass(r15, dst);\n+    __ cmp(scratch_src_klass, r15); \/\/ usual case is exact equality\n@@ -2172,1 +2175,1 @@\n-    \/\/ live at this point:  scratch_src_klass, scratch_length, r18 (dst_klass)\n+    \/\/ live at this point:  scratch_src_klass, scratch_length, r15 (dst_klass)\n@@ -2175,1 +2178,1 @@\n-      __ ldrw(rscratch1, Address(r18, lh_offset));\n+      __ ldrw(rscratch1, Address(r15, lh_offset));\n@@ -2182,1 +2185,1 @@\n-                             r18, L_failed);\n+                             r15, L_failed);\n@@ -5079,2 +5082,2 @@\n-      Register reg = c_rarg0;\n-      Pa_base = reg;       \/\/ Argument registers\n+      RegSetIterator regs = (RegSet::range(r0, r26) - r18_tls).begin();\n+      Pa_base = *regs;       \/\/ Argument registers\n@@ -5084,5 +5087,5 @@\n-        Pb_base = ++reg;\n-      Pn_base = ++reg;\n-      Rlen= ++reg;\n-      inv = ++reg;\n-      Pm_base = ++reg;\n+        Pb_base = *++regs;\n+      Pn_base = *++regs;\n+      Rlen= *++regs;\n+      inv = *++regs;\n+      Pm_base = *++regs;\n@@ -5091,4 +5094,4 @@\n-      Ra =  ++reg;        \/\/ The current digit of a, b, n, and m.\n-      Rb =  ++reg;\n-      Rm =  ++reg;\n-      Rn =  ++reg;\n+      Ra =  *++regs;        \/\/ The current digit of a, b, n, and m.\n+      Rb =  *++regs;\n+      Rm =  *++regs;\n+      Rn =  *++regs;\n@@ -5096,4 +5099,4 @@\n-      Pa =  ++reg;        \/\/ Pointers to the current\/next digit of a, b, n, and m.\n-      Pb =  ++reg;\n-      Pm =  ++reg;\n-      Pn =  ++reg;\n+      Pa =  *++regs;        \/\/ Pointers to the current\/next digit of a, b, n, and m.\n+      Pb =  *++regs;\n+      Pm =  *++regs;\n+      Pn =  *++regs;\n@@ -5101,3 +5104,3 @@\n-      t0 =  ++reg;        \/\/ Three registers which form a\n-      t1 =  ++reg;        \/\/ triple-precision accumuator.\n-      t2 =  ++reg;\n+      t0 =  *++regs;        \/\/ Three registers which form a\n+      t1 =  *++regs;        \/\/ triple-precision accumuator.\n+      t2 =  *++regs;\n@@ -5105,2 +5108,2 @@\n-      Ri =  ++reg;        \/\/ Inner and outer loop indexes.\n-      Rj =  ++reg;\n+      Ri =  *++regs;        \/\/ Inner and outer loop indexes.\n+      Rj =  *++regs;\n@@ -5108,4 +5111,4 @@\n-      Rhi_ab = ++reg;     \/\/ Product registers: low and high parts\n-      Rlo_ab = ++reg;     \/\/ of a*b and m*n.\n-      Rhi_mn = ++reg;\n-      Rlo_mn = ++reg;\n+      Rhi_ab = *++regs;     \/\/ Product registers: low and high parts\n+      Rlo_ab = *++regs;     \/\/ of a*b and m*n.\n+      Rhi_mn = *++regs;\n+      Rlo_mn = *++regs;\n@@ -5114,1 +5117,1 @@\n-      _toSave = RegSet::range(r19, reg) + Pm_base;\n+      _toSave = RegSet::range(r19, *regs) + Pm_base;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":48,"deletions":45,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -1430,1 +1430,1 @@\n-    int bang_end = JavaThread::stack_shadow_zone_size();\n+    int bang_end = StackOverflow::stack_shadow_zone_size();\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2709,0 +2709,32 @@\n+void Assembler::evmovdqu(XMMRegister dst, KRegister mask, Address src, int vector_len, int type) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  assert(type == T_BYTE || type == T_SHORT || type == T_CHAR || type == T_INT || type == T_LONG, \"\");\n+  InstructionMark im(this);\n+  bool wide = type == T_SHORT || type == T_CHAR || type == T_LONG;\n+  int prefix = (type == T_BYTE ||  type == T_SHORT || type == T_CHAR) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ wide, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(src, 0, dst->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmovdqu(Address dst, KRegister mask, XMMRegister src, int vector_len, int type) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  assert(src != xnoreg, \"sanity\");\n+  assert(type == T_BYTE || type == T_SHORT || type == T_CHAR || type == T_INT || type == T_LONG, \"\");\n+  InstructionMark im(this);\n+  bool wide = type == T_SHORT || type == T_CHAR || type == T_LONG;\n+  int prefix = (type == T_BYTE ||  type == T_SHORT || type == T_CHAR) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ wide, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(dst, 0, src->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x7F);\n+  emit_operand(src, dst);\n+}\n+\n@@ -9127,0 +9159,7 @@\n+void Assembler::shrxq(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF7, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -259,0 +259,13 @@\n+  \/\/ The following overloads are used in connection with the\n+  \/\/ ByteSize type (see sizes.hpp).  They simplify the use of\n+  \/\/ ByteSize'd arguments in assembly code.\n+\n+  Address(Register base, ByteSize disp)\n+    : Address(base, in_bytes(disp)) {}\n+\n+  Address(Register base, Register index, ScaleFactor scale, ByteSize disp)\n+    : Address(base, index, scale, in_bytes(disp)) {}\n+\n+  Address(Register base, RegisterOrConstant index, ScaleFactor scale, ByteSize disp)\n+    : Address(base, index, scale, in_bytes(disp)) {}\n+\n@@ -279,45 +292,0 @@\n-  \/\/ The following two overloads are used in connection with the\n-  \/\/ ByteSize type (see sizes.hpp).  They simplify the use of\n-  \/\/ ByteSize'd arguments in assembly code. Note that their equivalent\n-  \/\/ for the optimized build are the member functions with int disp\n-  \/\/ argument since ByteSize is mapped to an int type in that case.\n-  \/\/\n-  \/\/ Note: DO NOT introduce similar overloaded functions for WordSize\n-  \/\/ arguments as in the optimized mode, both ByteSize and WordSize\n-  \/\/ are mapped to the same type and thus the compiler cannot make a\n-  \/\/ distinction anymore (=> compiler errors).\n-\n-#ifdef ASSERT\n-  Address(Register base, ByteSize disp)\n-    : _base(base),\n-      _index(noreg),\n-      _xmmindex(xnoreg),\n-      _scale(no_scale),\n-      _disp(in_bytes(disp)),\n-      _isxmmindex(false){\n-  }\n-\n-  Address(Register base, Register index, ScaleFactor scale, ByteSize disp)\n-    : _base(base),\n-      _index(index),\n-      _xmmindex(xnoreg),\n-      _scale(scale),\n-      _disp(in_bytes(disp)),\n-      _isxmmindex(false){\n-    assert(!index->is_valid() == (scale == Address::no_scale),\n-           \"inconsistent address\");\n-  }\n-  Address(Register base, RegisterOrConstant index, ScaleFactor scale, ByteSize disp)\n-    : _base (base),\n-      _index(index.register_or_noreg()),\n-      _xmmindex(xnoreg),\n-      _scale(scale),\n-      _disp (in_bytes(disp) + (index.constant_or_zero() * scale_size(scale))),\n-      _isxmmindex(false) {\n-    if (!index.is_register())  scale = Address::no_scale;\n-    assert(!_index->is_valid() == (scale == Address::no_scale),\n-           \"inconsistent address\");\n-  }\n-\n-#endif \/\/ ASSERT\n-\n@@ -875,1 +843,0 @@\n-  void decq(Register dst);\n@@ -960,0 +927,1 @@\n+  void decq(Register dst);\n@@ -1596,0 +1564,4 @@\n+  \/\/ Generic move instructions.\n+  void evmovdqu(Address dst, KRegister mask, XMMRegister src, int vector_len, int type);\n+  void evmovdqu(XMMRegister dst, KRegister mask, Address src, int vector_len, int type);\n+\n@@ -2161,0 +2133,2 @@\n+  void shrxq(Register dst, Register src1, Register src2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":20,"deletions":46,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -3128,1 +3128,1 @@\n-\/\/   @HotSpotIntrinsicCandidate\n+\/\/   @IntrinsicCandidate\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1062,1 +1062,1 @@\n-  for (int i = 1; i < ((int)JavaThread::stack_shadow_zone_size() \/ os::vm_page_size()); i++) {\n+  for (int i = 1; i < ((int)StackOverflow::stack_shadow_zone_size() \/ os::vm_page_size()); i++) {\n@@ -1160,1 +1160,1 @@\n-  jccb(Assembler::notZero, try_revoke_bias);\n+  jcc(Assembler::notZero, try_revoke_bias);\n@@ -2763,1 +2763,1 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path, Register thread_reg, Register temp_reg) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, Register thread_reg, bool at_return, bool in_nmethod) {\n@@ -2765,5 +2765,6 @@\n-  assert(thread_reg == r15_thread, \"should be\");\n-#else\n-  if (thread_reg == noreg) {\n-    thread_reg = temp_reg;\n-    get_thread(thread_reg);\n+  if (at_return) {\n+    \/\/ Note that when in_nmethod is set, the stack pointer is incremented before the poll. Therefore,\n+    \/\/ we may safely use rsp instead to perform the stack watermark check.\n+    cmpq(Address(thread_reg, Thread::polling_word_offset()), in_nmethod ? rsp : rbp);\n+    jcc(Assembler::above, slow_path);\n+    return;\n@@ -2772,1 +2773,1 @@\n-  testb(Address(thread_reg, Thread::polling_page_offset()), SafepointMechanism::poll_bit());\n+  testb(Address(thread_reg, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -5306,1 +5307,1 @@\n-   \/\/@HotSpotIntrinsicCandidate\n+   \/\/@IntrinsicCandidate\n@@ -7683,1 +7684,1 @@\n-\/\/   @HotSpotIntrinsicCandidate\n+\/\/   @IntrinsicCandidate\n@@ -7899,1 +7900,1 @@\n-\/\/   @HotSpotIntrinsicCandidate\n+\/\/   @IntrinsicCandidate\n@@ -8136,0 +8137,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":14,"deletions":12,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -650,3 +650,1 @@\n-  \/\/ If thread_reg is != noreg the code assumes the register passed contains\n-  \/\/ the thread (required on 64 bit).\n-  void safepoint_poll(Label& slow_path, Register thread_reg, Register temp_reg);\n+  void safepoint_poll(Label& slow_path, Register thread_reg, bool at_return, bool in_nmethod);\n@@ -1804,0 +1802,29 @@\n+\n+#if COMPILER2_OR_JVMCI\n+  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                    Register to, Register count, int shift,\n+                                    Register index, Register temp,\n+                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                             Register to, Register start_index, Register end_index,\n+                                             Register count, int shift, Register temp,\n+                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0,\n+                         bool use64byteVector = false);\n+\n+  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0);\n+\n+  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  int shift = Address::times_1, int offset = 0);\n+\n+  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  bool conjoint, int shift = Address::times_1, int offset = 0,\n+                  bool use64byteVector = false);\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":30,"deletions":3,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -1193,18 +1193,2 @@\n-      \/\/ Copy 64-bytes per iteration\n-      if (UseAVX > 2) {\n-        Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;\n-\n-        __ BIND(L_copy_bytes);\n-        __ cmpptr(qword_count, (-1 * AVX3Threshold \/ 8));\n-        __ jccb(Assembler::less, L_above_threshold);\n-        __ jmpb(L_below_threshold);\n-\n-        __ bind(L_loop_avx512);\n-        __ evmovdqul(xmm0, Address(end_from, qword_count, Address::times_8, -56), Assembler::AVX_512bit);\n-        __ evmovdqul(Address(end_to, qword_count, Address::times_8, -56), xmm0, Assembler::AVX_512bit);\n-        __ bind(L_above_threshold);\n-        __ addptr(qword_count, 8);\n-        __ jcc(Assembler::lessEqual, L_loop_avx512);\n-        __ jmpb(L_32_byte_head);\n-\n-        __ bind(L_loop_avx2);\n+      __ BIND(L_loop);\n+      if (UseAVX >= 2) {\n@@ -1215,29 +1199,8 @@\n-        __ bind(L_below_threshold);\n-        __ addptr(qword_count, 8);\n-        __ jcc(Assembler::lessEqual, L_loop_avx2);\n-\n-        __ bind(L_32_byte_head);\n-        __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-        __ jccb(Assembler::greater, L_end);\n-        __ BIND(L_loop);\n-        if (UseAVX == 2) {\n-          __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-          __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-          __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-          __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-        } else {\n-          __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-          __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-          __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-          __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-        }\n-\n-        __ BIND(L_copy_bytes);\n-        __ addptr(qword_count, 8);\n-        __ jcc(Assembler::lessEqual, L_loop);\n-        __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-        __ jccb(Assembler::greater, L_end);\n+        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n+        __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n+        __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n@@ -1246,0 +1209,6 @@\n+\n+      __ BIND(L_copy_bytes);\n+      __ addptr(qword_count, 8);\n+      __ jcc(Assembler::lessEqual, L_loop);\n+      __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n+      __ jccb(Assembler::greater, L_end);\n@@ -1301,18 +1270,2 @@\n-      \/\/ Copy 64-bytes per iteration\n-      if (UseAVX > 2) {\n-        Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;\n-\n-        __ BIND(L_copy_bytes);\n-        __ cmpptr(qword_count, (AVX3Threshold \/ 8));\n-        __ jccb(Assembler::greater, L_above_threshold);\n-        __ jmpb(L_below_threshold);\n-\n-        __ BIND(L_loop_avx512);\n-        __ evmovdqul(xmm0, Address(from, qword_count, Address::times_8, 0), Assembler::AVX_512bit);\n-        __ evmovdqul(Address(dest, qword_count, Address::times_8, 0), xmm0, Assembler::AVX_512bit);\n-        __ bind(L_above_threshold);\n-        __ subptr(qword_count, 8);\n-        __ jcc(Assembler::greaterEqual, L_loop_avx512);\n-        __ jmpb(L_32_byte_head);\n-\n-        __ bind(L_loop_avx2);\n+      __ BIND(L_loop);\n+      if (UseAVX >= 2) {\n@@ -1321,9 +1274,2 @@\n-        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8, 0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm1);\n-        __ bind(L_below_threshold);\n-        __ subptr(qword_count, 8);\n-        __ jcc(Assembler::greaterEqual, L_loop_avx2);\n-\n-        __ bind(L_32_byte_head);\n-        __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-        __ jccb(Assembler::less, L_end);\n+        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+        __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n@@ -1331,16 +1277,9 @@\n-        __ BIND(L_loop);\n-        if (UseAVX == 2) {\n-          __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-          __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-          __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-          __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-        } else {\n-          __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-          __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-          __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-          __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-          __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-          __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-          __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-          __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-        }\n+        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n+        __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n+        __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n+        __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n+        __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n+        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n+        __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n+        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n+      }\n@@ -1348,3 +1287,3 @@\n-        __ BIND(L_copy_bytes);\n-        __ subptr(qword_count, 8);\n-        __ jcc(Assembler::greaterEqual, L_loop);\n+      __ BIND(L_copy_bytes);\n+      __ subptr(qword_count, 8);\n+      __ jcc(Assembler::greaterEqual, L_loop);\n@@ -1352,3 +1291,2 @@\n-        __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-        __ jccb(Assembler::less, L_end);\n-      }\n+      __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n+      __ jccb(Assembler::less, L_end);\n@@ -1392,0 +1330,438 @@\n+#ifndef PRODUCT\n+    int& get_profile_ctr(int shift) {\n+      if ( 0 == shift)\n+        return SharedRuntime::_jbyte_array_copy_ctr;\n+      else if(1 == shift)\n+        return SharedRuntime::_jshort_array_copy_ctr;\n+      else if(2 == shift)\n+        return SharedRuntime::_jint_array_copy_ctr;\n+      else\n+        return SharedRuntime::_jlong_array_copy_ctr;\n+    }\n+#endif\n+\n+  void setup_argument_regs(BasicType type) {\n+    if (type == T_BYTE || type == T_SHORT) {\n+      setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                        \/\/ r9 and r10 may be used to save non-volatile registers\n+    } else {\n+      setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                     \/\/ r9 is used to save r15_thread\n+    }\n+  }\n+\n+  void restore_argument_regs(BasicType type) {\n+    if (type == T_BYTE || type == T_SHORT) {\n+      restore_arg_regs();\n+    } else {\n+      restore_arg_regs_using_thread();\n+    }\n+  }\n+\n+#if COMPILER2_OR_JVMCI\n+  \/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n+  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+  \/\/   default configuration.\n+  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+  \/\/   copy performs better.\n+  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+  \/\/   64 byte vector registers (ZMMs).\n+\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n+  \/\/\n+\n+  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             bool aligned, bool is_oop, bool dest_uninitialized) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    bool use64byteVector = MaxVectorSize > 32 && AVX3Threshold == 0;\n+    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+    const Register from        = rdi;  \/\/ source array address\n+    const Register to          = rsi;  \/\/ destination array address\n+    const Register count       = rdx;  \/\/ elements count\n+    const Register temp1       = r8;\n+    const Register temp2       = r11;\n+    const Register temp3       = rax;\n+    const Register temp4       = rcx;\n+    \/\/ End pointers are inclusive, and if count is not zero they point\n+    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+    setup_argument_regs(type);\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+    if (aligned) {\n+      decorators |= ARRAYCOPY_ALIGNED;\n+    }\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+    {\n+      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n+      int loop_size[]        = { 192,     96,       48,      24};\n+      int threshold[]        = { 4096,    2048,     1024,    512};\n+\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+\n+      \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n+      \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n+      __ mov64(temp4, 0);\n+      __ movq(temp1, count);\n+\n+      \/\/ Zero length check.\n+      __ BIND(L_tail);\n+      __ cmpq(temp1, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      \/\/ Special cases using 32 byte [masked] vector copy operations.\n+      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n+\n+      \/\/ PRE-MAIN-POST loop for aligned copy.\n+      __ BIND(L_entry);\n+\n+      if (AVX3Threshold != 0) {\n+        __ cmpq(count, threshold[shift]);\n+        if (MaxVectorSize == 64) {\n+          \/\/ Copy using 64 byte vectors.\n+          __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+        } else {\n+          assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n+          \/\/ REP MOVS offer a faster copy path.\n+          __ jcc(Assembler::greaterEqual, L_repmovs);\n+        }\n+      }\n+\n+      if (MaxVectorSize < 64  || AVX3Threshold != 0) {\n+        \/\/ Partial copy to make dst address 32 byte aligned.\n+        __ movq(temp2, to);\n+        __ andq(temp2, 31);\n+        __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+        __ negptr(temp2);\n+        __ addq(temp2, 32);\n+        if (shift) {\n+          __ shrq(temp2, shift);\n+        }\n+        __ movq(temp3, temp2);\n+        __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail);\n+\n+        __ BIND(L_main_pre_loop);\n+        __ subq(temp1, loop_size[shift]);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+        __ BIND(L_main_loop);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n+           __ addptr(temp4, loop_size[shift]);\n+           __ subq(temp1, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop);\n+\n+        __ addq(temp1, loop_size[shift]);\n+\n+        \/\/ Tail loop.\n+        __ jmp(L_tail);\n+\n+        __ BIND(L_repmovs);\n+          __ movq(temp2, temp1);\n+          \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n+          __ movq(temp3, to);\n+          __ movq(to,  from);\n+          __ movq(from, temp3);\n+          \/\/ Save to\/from for restoration post rep_mov.\n+          __ movq(temp1, to);\n+          __ movq(temp3, from);\n+          if(shift < 3) {\n+            __ shrq(temp2, 3-shift);     \/\/ quad word count\n+          }\n+          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n+          __ rep_mov();\n+          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n+          if(shift) {\n+            __ shrq(temp2, shift);       \/\/ type specific count.\n+          }\n+          \/\/ Restore original addresses in to\/from.\n+          __ movq(to, temp3);\n+          __ movq(from, temp1);\n+          __ movq(temp4, temp2);\n+          __ movq(temp1, count);\n+          __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n+          __ jmp(L_tail);\n+      }\n+\n+      if (MaxVectorSize > 32) {\n+        __ BIND(L_pre_main_post_64);\n+        \/\/ Partial copy to make dst address 64 byte aligned.\n+        __ movq(temp2, to);\n+        __ andq(temp2, 63);\n+        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+        __ negptr(temp2);\n+        __ addq(temp2, 64);\n+        if (shift) {\n+          __ shrq(temp2, shift);\n+        }\n+        __ movq(temp3, temp2);\n+        __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail64);\n+\n+        __ BIND(L_main_pre_loop_64bytes);\n+        __ subq(temp1, loop_size[shift]);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at\n+        \/\/ 64 byte copy granularity.\n+        __ BIND(L_main_loop_64bytes);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n+           __ addptr(temp4, loop_size[shift]);\n+           __ subq(temp1, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+        __ addq(temp1, loop_size[shift]);\n+        \/\/ Zero length check.\n+        __ jcc(Assembler::lessEqual, L_exit);\n+\n+        __ BIND(L_tail64);\n+\n+        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+        use64byteVector = true;\n+        __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                        temp4, temp3, use64byteVector, L_entry, L_exit);\n+      }\n+      __ BIND(L_exit);\n+    }\n+\n+    address ucme_exit_pc = __ pc();\n+    \/\/ When called from generic_arraycopy r11 contains specific values\n+    \/\/ used during arraycopy epilogue, re-initializing r11.\n+    if (is_oop) {\n+      __ movq(r11, shift == 3 ? count : to);\n+    }\n+    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+    restore_argument_regs(type);\n+    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+    return start;\n+  }\n+\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/\n+  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             address nooverlap_target, bool aligned, bool is_oop,\n+                                             bool dest_uninitialized) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    bool use64byteVector = MaxVectorSize > 32 && AVX3Threshold == 0;\n+\n+    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+    const Register from        = rdi;  \/\/ source array address\n+    const Register to          = rsi;  \/\/ destination array address\n+    const Register count       = rdx;  \/\/ elements count\n+    const Register temp1       = r8;\n+    const Register temp2       = rcx;\n+    const Register temp3       = r11;\n+    const Register temp4       = rax;\n+    \/\/ End pointers are inclusive, and if count is not zero they point\n+    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+\n+    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+    setup_argument_regs(type);\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+    if (aligned) {\n+      decorators |= ARRAYCOPY_ALIGNED;\n+    }\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+    {\n+      \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n+      int loop_size[]   = { 192,     96,       48,      24};\n+      int threshold[]   = { 4096,    2048,     1024,    512};\n+\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+\n+      \/\/ temp1 holds remaining count.\n+      __ movq(temp1, count);\n+\n+      \/\/ Zero length check.\n+      __ BIND(L_tail);\n+      __ cmpq(temp1, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ mov64(temp2, 0);\n+      __ movq(temp3, temp1);\n+      \/\/ Special cases using 32 byte [masked] vector copy operations.\n+      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                               temp4, use64byteVector, L_entry, L_exit);\n+\n+      \/\/ PRE-MAIN-POST loop for aligned copy.\n+      __ BIND(L_entry);\n+\n+      if (MaxVectorSize > 32 && AVX3Threshold != 0) {\n+        __ cmpq(temp1, threshold[shift]);\n+        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      }\n+\n+      if (MaxVectorSize < 64  || AVX3Threshold != 0) {\n+        \/\/ Partial copy to make dst address 32 byte aligned.\n+        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+        __ andq(temp2, 31);\n+        __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+        if (shift) {\n+          __ shrq(temp2, shift);\n+        }\n+        __ subq(temp1, temp2);\n+        __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail);\n+\n+        __ BIND(L_main_pre_loop);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+        __ BIND(L_main_loop);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n+           __ subptr(temp1, loop_size[shift]);\n+           __ cmpq(temp1, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop);\n+\n+        \/\/ Tail loop.\n+        __ jmp(L_tail);\n+      }\n+\n+      if (MaxVectorSize > 32) {\n+        __ BIND(L_pre_main_post_64);\n+        \/\/ Partial copy to make dst address 64 byte aligned.\n+        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+        __ andq(temp2, 63);\n+        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+        if (shift) {\n+          __ shrq(temp2, shift);\n+        }\n+        __ subq(temp1, temp2);\n+        __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail64);\n+\n+        __ BIND(L_main_pre_loop_64bytes);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at\n+        \/\/ 64 byte copy granularity.\n+        __ BIND(L_main_loop_64bytes);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n+           __ subq(temp1, loop_size[shift]);\n+           __ cmpq(temp1, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+        \/\/ Zero length check.\n+        __ cmpq(temp1, 0);\n+        __ jcc(Assembler::lessEqual, L_exit);\n+\n+        __ BIND(L_tail64);\n+\n+        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+        use64byteVector = true;\n+        __ mov64(temp2, 0);\n+        __ movq(temp3, temp1);\n+        __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                                 temp4, use64byteVector, L_entry, L_exit);\n+      }\n+      __ BIND(L_exit);\n+    }\n+    address ucme_exit_pc = __ pc();\n+    \/\/ When called from generic_arraycopy r11 contains specific values\n+    \/\/ used during arraycopy epilogue, re-initializing r11.\n+    if(is_oop) {\n+      __ movq(r11, count);\n+    }\n+    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+    restore_argument_regs(type);\n+    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+    return start;\n+  }\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+\n@@ -1412,0 +1788,6 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n+                                                 aligned, false, false);\n+    }\n+#endif\n@@ -1522,0 +1904,6 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n+                                                 nooverlap_target, aligned, false, false);\n+    }\n+#endif\n@@ -1627,0 +2015,7 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n+                                                 aligned, false, false);\n+    }\n+#endif\n+\n@@ -1751,0 +2146,6 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n+                                                 nooverlap_target, aligned, false, false);\n+    }\n+#endif\n@@ -1849,0 +2250,7 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n+                                                 aligned, is_oop, dest_uninitialized);\n+    }\n+#endif\n+\n@@ -1953,0 +2361,6 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n+                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n+    }\n+#endif\n@@ -2060,0 +2474,6 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n+                                                 aligned, is_oop, dest_uninitialized);\n+    }\n+#endif\n@@ -2164,0 +2584,6 @@\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n+                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n+    }\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":522,"deletions":96,"binary":false,"changes":618,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  code_size2 = 35300 LP64_ONLY(+11400)          \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 35300 LP64_ONLY(+25000)          \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -935,2 +935,2 @@\n-    st->print_cr(\"movq    rscratch1, poll_offset[r15_thread] #polling_page_address\\n\\t\"\n-                 \"testl   rax, [rscratch1]\\t\"\n+    st->print_cr(\"cmpq    poll_offset[r15_thread], rsp\\n\\t\"\n+                 \"ja      #safepoint_stub\\t\"\n@@ -983,1 +983,5 @@\n-    __ movq(rscratch1, Address(r15_thread, Thread::polling_page_offset()));\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+    }\n@@ -985,1 +989,1 @@\n-    __ testl(rax, Address(rscratch1, 0));\n+    __ safepoint_poll(*code_stub, r15_thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n@@ -2942,0 +2946,10 @@\n+operand immU7()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 0x7F));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4283,10 +4297,0 @@\n-\/\/ Long ALU reg operation using big decoder\n-pipe_class ialu_reg_long_fat(rRegL dst)\n-%{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    D0     : S0(2);     \/\/ big decoder only; twice\n-    ALU    : S3(2);     \/\/ any 2 alus\n-%}\n-\n@@ -4303,10 +4307,0 @@\n-\/\/ Long ALU reg-reg operation\n-pipe_class ialu_reg_reg_long(rRegL dst, rRegL src)\n-%{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    src    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    ALU    : S3(2);     \/\/ both alus\n-%}\n-\n@@ -4323,10 +4317,0 @@\n-\/\/ Long ALU reg-reg operation\n-pipe_class ialu_reg_reg_long_fat(rRegL dst, rRegL src)\n-%{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    src    : S3(read);\n-    D0     : S0(2);     \/\/ big decoder only; twice\n-    ALU    : S3(2);     \/\/ both alus\n-%}\n-\n@@ -11830,1 +11814,1 @@\n-instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU8 imm, immI_0 zero)\n+instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":19,"deletions":35,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -1182,2 +1182,1 @@\n-      \/\/ supported. Clear the _java_mirror within the archived class.\n-      k->clear_java_mirror_handle();\n+      \/\/ supported.\n@@ -1248,0 +1247,2 @@\n+    set_signers(archived_mirror, NULL);\n+    set_source_file(archived_mirror, NULL);\n@@ -2440,1 +2441,1 @@\n-  vframeStream st(thread);\n+  vframeStream st(thread, false \/* stop_at_java_call_stub *\/, false \/* process_frames *\/);\n@@ -2443,1 +2444,1 @@\n-  RegisterMap map(thread, false);\n+  RegisterMap map(thread, false \/* update *\/, false \/* process_frames *\/);\n@@ -2585,1 +2586,1 @@\n-  vframeStream st(THREAD);\n+  vframeStream st(THREAD, false \/* stop_at_java_call_stub *\/, false \/* process_frames *\/);\n@@ -4819,0 +4820,21 @@\n+\/\/ java_lang_InternalError\n+int java_lang_InternalError::_during_unsafe_access_offset;\n+\n+void java_lang_InternalError::set_during_unsafe_access(oop internal_error) {\n+  internal_error->bool_field_put(_during_unsafe_access_offset, true);\n+}\n+\n+jboolean java_lang_InternalError::during_unsafe_access(oop internal_error) {\n+  return internal_error->bool_field(_during_unsafe_access_offset);\n+}\n+\n+void java_lang_InternalError::compute_offsets() {\n+  INTERNALERROR_INJECTED_FIELDS(INJECTED_FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void java_lang_InternalError::serialize_offsets(SerializeClosure* f) {\n+  INTERNALERROR_INJECTED_FIELDS(INJECTED_FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":27,"deletions":5,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+  f(java_lang_InternalError) \\\n@@ -1673,0 +1674,16 @@\n+\n+\/\/ Interface to java.lang.InternalError objects\n+\n+#define INTERNALERROR_INJECTED_FIELDS(macro)                      \\\n+  macro(java_lang_InternalError, during_unsafe_access, bool_signature, false)\n+\n+class java_lang_InternalError : AllStatic {\n+ private:\n+  static int _during_unsafe_access_offset;\n+ public:\n+  static jboolean during_unsafe_access(oop internal_error);\n+  static void set_during_unsafe_access(oop internal_error);\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+};\n+\n@@ -1712,1 +1729,3 @@\n-  MODULE_INJECTED_FIELDS(macro)\n+  MODULE_INJECTED_FIELDS(macro)             \\\n+  INTERNALERROR_INJECTED_FIELDS(macro)\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -129,0 +129,1 @@\n+  do_klass(InternalError_klass,                         java_lang_InternalError                               ) \\\n@@ -633,1 +634,0 @@\n-protected:\n@@ -636,0 +636,1 @@\n+protected:\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-\/\/ Intrinsic methods are marked by the jdk.internal.HotSpotIntrinsicCandidate\n+\/\/ Intrinsic methods are marked by the jdk.internal.vm.annotation.IntrinsicCandidate\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -295,0 +295,4 @@\n+  \/* used by CDS *\/                                                                               \\\n+  template(jdk_internal_misc_CDS, \"jdk\/internal\/misc\/CDS\")                                        \\\n+  template(generateLambdaFormHolderClasses, \"generateLambdaFormHolderClasses\")                    \\\n+  template(generateLambdaFormHolderClasses_signature, \"([Ljava\/lang\/String;)[Ljava\/lang\/Object;\") \\\n@@ -297,1 +301,0 @@\n-  template(jdk_internal_HotSpotIntrinsicCandidate_signature, \"Ljdk\/internal\/HotSpotIntrinsicCandidate;\") \\\n@@ -301,0 +304,1 @@\n+  template(jdk_internal_vm_annotation_IntrinsicCandidate_signature, \"Ljdk\/internal\/vm\/annotation\/IntrinsicCandidate;\") \\\n@@ -302,1 +306,0 @@\n-                                                                                                  \\\n@@ -474,0 +477,1 @@\n+  template(during_unsafe_access_name,                 \"during_unsafe_access\")                     \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -52,0 +52,8 @@\n+  product(bool, StressIGVN, false, DIAGNOSTIC,                              \\\n+          \"Randomize worklist traversal in IGVN\")                           \\\n+                                                                            \\\n+  product(uint, StressSeed, 0, DIAGNOSTIC,                                  \\\n+          \"Seed for randomized stress testing (if unset, a random one is \"  \\\n+          \"generated)\")                                                     \\\n+          range(0, max_juint)                                               \\\n+                                                                            \\\n@@ -100,0 +108,4 @@\n+  notproduct(uintx, PrintIdealIndentThreshold, 0,                           \\\n+          \"A depth threshold of ideal graph. Indentation is disabled \"      \\\n+          \"when users attempt to dump an ideal graph deeper than it.\")      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -432,1 +432,1 @@\n-      map->disconnect_inputs(NULL, C);\n+      map->disconnect_inputs(C);\n@@ -459,1 +459,0 @@\n-    C->set_has_loops(C->has_loops() || _inline_cg->method()->has_loops());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -319,2 +319,8 @@\n-bool RegionNode::is_unreachable_region(PhaseGVN *phase) const {\n-  assert(req() == 2, \"\");\n+bool RegionNode::is_unreachable_region(const PhaseGVN* phase) {\n+  Node* top = phase->C->top();\n+  assert(req() == 2 || (req() == 3 && in(1) != NULL && in(2) == top), \"sanity check arguments\");\n+  if (_is_unreachable_region) {\n+    \/\/ Return cached result from previous evaluation which should still be valid\n+    assert(is_unreachable_from_root(phase), \"walk the graph again and check if its indeed unreachable\");\n+    return true;\n+  }\n@@ -324,0 +330,11 @@\n+  if (is_possible_unsafe_loop(phase)) {\n+    \/\/ If we have a possible unsafe loop, check if the region node is actually unreachable from root.\n+    if (is_unreachable_from_root(phase)) {\n+      _is_unreachable_region = true;\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool RegionNode::is_possible_unsafe_loop(const PhaseGVN* phase) const {\n@@ -327,4 +344,5 @@\n-    Node* phi = raw_out(i);\n-    if (phi != NULL && phi->is_Phi()) {\n-      assert(phase->eqv(phi->in(0), this) && phi->req() == 2, \"\");\n-      if (phi->outcnt() == 0)\n+    Node* n = raw_out(i);\n+    if (n != NULL && n->is_Phi()) {\n+      PhiNode* phi = n->as_Phi();\n+      assert(phase->eqv(phi->in(0), this), \"sanity check phi\");\n+      if (phi->outcnt() == 0) {\n@@ -332,0 +350,1 @@\n+      }\n@@ -336,1 +355,1 @@\n-        if (u != NULL && (u->is_Phi() || u->is_CFG()))\n+        if (u != NULL && (u->is_Phi() || u->is_CFG())) {\n@@ -338,0 +357,1 @@\n+        }\n@@ -340,1 +360,1 @@\n-      if (phi->as_Phi()->simple_data_loop_check(phi->in(1)) >= PhiNode::Unsafe)\n+      if (phi->as_Phi()->simple_data_loop_check(phi->in(1)) >= PhiNode::Unsafe) {\n@@ -342,0 +362,1 @@\n+      }\n@@ -344,1 +365,1 @@\n-  if (i >= max)\n+  if (i >= max) {\n@@ -346,0 +367,3 @@\n+  }\n+  return true;\n+}\n@@ -347,1 +371,1 @@\n-  \/\/ Unsafe case - check if the Region node is reachable from root.\n+bool RegionNode::is_unreachable_from_root(const PhaseGVN* phase) const {\n@@ -349,1 +373,0 @@\n-\n@@ -371,1 +394,0 @@\n-\n@@ -552,1 +574,1 @@\n-            \/\/ Eagerly replace phis with top to avoid phis copies generation.\n+            \/\/ Eagerly replace phis with top to avoid regionless phis.\n@@ -593,1 +615,1 @@\n-        \/\/ Kill phis here to avoid it. PhiNode::is_copy() will be always false.\n+        \/\/ Kill phis here to avoid it.\n@@ -608,1 +630,1 @@\n-          \/\/ Eagerly replace phis to avoid copies generation.\n+          \/\/ Eagerly replace phis to avoid regionless phis.\n@@ -822,1 +844,2 @@\n-             cmp2->Opcode() == Op_CmpP || cmp2->Opcode() == Op_CmpN) {\n+             cmp2->Opcode() == Op_CmpP || cmp2->Opcode() == Op_CmpN ||\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n@@ -824,0 +847,1 @@\n+    \/\/ SubTypeCheck is not commutative\n@@ -1381,1 +1405,0 @@\n-  if (r == NULL)  return in(1);         \/\/ Already degraded to a Copy\n@@ -1865,4 +1888,1 @@\n-  \/\/ The next should never happen after 6297035 fix.\n-  if( is_copy() )               \/\/ Already degraded to a Copy ?\n-    return NULL;                \/\/ No change\n-\n+  assert(r != NULL && r->is_Region(), \"this phi must have a region\");\n@@ -1935,10 +1955,1 @@\n-      \/\/ First, take the short cut when we know it is a loop and\n-      \/\/ the EntryControl data path is dead.\n-      \/\/ Loop node may have only one input because entry path\n-      \/\/ is removed in PhaseIdealLoop::Dominators().\n-      assert(!r->is_Loop() || r->req() <= 3, \"Loop node should have 3 or less inputs\");\n-      bool is_loop = (r->is_Loop() && r->req() == 3);\n-      \/\/ Then, check if there is a data loop when phi references itself directly\n-      \/\/ or through other data nodes.\n-      if ((is_loop && !uin->eqv_uncast(in(LoopNode::EntryControl))) ||\n-          (!is_loop && is_unsafe_data_reference(uin))) {\n+      if (is_data_loop(r->as_Region(), uin, phase)) {\n@@ -2424,0 +2435,16 @@\n+bool PhiNode::is_data_loop(RegionNode* r, Node* uin, const PhaseGVN* phase) {\n+  \/\/ First, take the short cut when we know it is a loop and the EntryControl data path is dead.\n+  \/\/ The loop node may only have one input because the entry path was removed in PhaseIdealLoop::Dominators().\n+  \/\/ Then, check if there is a data loop when the phi references itself directly or through other data nodes.\n+  assert(!r->is_Loop() || r->req() <= 3, \"Loop node should have 3 or less inputs\");\n+  const bool is_loop = (r->is_Loop() && r->req() == 3);\n+  const Node* top = phase->C->top();\n+  if (is_loop) {\n+    return !uin->eqv_uncast(in(LoopNode::EntryControl));\n+  } else {\n+    \/\/ We have a data loop either with an unsafe data reference or if a region is unreachable.\n+    return is_unsafe_data_reference(uin)\n+           || (r->req() == 3 && (r->in(1) != top && r->in(2) == top && r->is_unreachable_region(phase)));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":58,"deletions":31,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -528,0 +529,1 @@\n+                  _stress_seed(0),\n@@ -733,0 +735,12 @@\n+  \/\/ If LCM, GCM, or IGVN are randomized for stress testing, seed\n+  \/\/ random number generation and log the seed for repeatability.\n+  if (StressLCM || StressGCM || StressIGVN) {\n+    _stress_seed = FLAG_IS_DEFAULT(StressSeed) ?\n+      static_cast<uint>(Ticks::now().nanoseconds()) : StressSeed;\n+    if (_log != NULL) {\n+      _log->elem(\"stress_test seed='%u'\", _stress_seed);\n+    } else if (FLAG_IS_DEFAULT(StressSeed)) {\n+      tty->print_cr(\"Warning:  set +LogCompilation to log the seed.\");\n+    }\n+  }\n+\n@@ -815,0 +829,1 @@\n+    _stress_seed(0),\n@@ -913,1 +928,1 @@\n-  set_has_loops(has_method() && method()->has_loops()); \/\/ first approximation\n+  set_has_loops(false); \/\/ first approximation\n@@ -1013,0 +1028,1 @@\n+  _exception_backedge = false;\n@@ -2803,1 +2819,1 @@\n-        mem->disconnect_inputs(NULL, this);\n+        mem->disconnect_inputs(this);\n@@ -3086,1 +3102,1 @@\n-            addp->disconnect_inputs(NULL, this);\n+            addp->disconnect_inputs(this);\n@@ -3159,1 +3175,1 @@\n-        in1->disconnect_inputs(NULL, this);\n+        in1->disconnect_inputs(this);\n@@ -3164,1 +3180,1 @@\n-        n->disconnect_inputs(NULL, this);\n+        n->disconnect_inputs(this);\n@@ -3242,1 +3258,1 @@\n-          in1->disconnect_inputs(NULL, this);\n+          in1->disconnect_inputs(this);\n@@ -3245,1 +3261,1 @@\n-          in2->disconnect_inputs(NULL, this);\n+          in2->disconnect_inputs(this);\n@@ -3276,1 +3292,1 @@\n-      in1->disconnect_inputs(NULL, this);\n+      in1->disconnect_inputs(this);\n@@ -3444,1 +3460,1 @@\n-        in2->disconnect_inputs(NULL, this);\n+        in2->disconnect_inputs(this);\n@@ -3476,1 +3492,1 @@\n-          m->disconnect_inputs(NULL, this);\n+          m->disconnect_inputs(this);\n@@ -3619,1 +3635,1 @@\n-          in->disconnect_inputs(NULL, this);\n+          in->disconnect_inputs(this);\n@@ -4486,2 +4502,7 @@\n-\/\/ Auxiliary method to support randomized stressing\/fuzzing.\n-\/\/\n+\/\/ Auxiliary methods to support randomized stressing\/fuzzing.\n+\n+int Compile::random() {\n+  _stress_seed = os::next_random(_stress_seed);\n+  return static_cast<int>(_stress_seed);\n+}\n+\n@@ -4518,1 +4539,1 @@\n-  return (os::random() & RANDOMIZED_DOMAIN_MASK) < (RANDOMIZED_DOMAIN \/ count);\n+  return (random() & RANDOMIZED_DOMAIN_MASK) < (RANDOMIZED_DOMAIN \/ count);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":35,"deletions":14,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -72,1 +72,0 @@\n-class PhaseCCP_DCE;\n@@ -304,0 +303,1 @@\n+  uint                  _stress_seed;           \/\/ Seed for stress testing\n@@ -434,0 +434,1 @@\n+  DEBUG_ONLY(bool _exception_backedge;)\n@@ -1154,2 +1155,3 @@\n-  \/\/ Auxiliary method for randomized fuzzing\/stressing\n-  static bool randomized_select(int count);\n+  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  int random();\n+  bool randomized_select(int count);\n@@ -1182,0 +1184,2 @@\n+  void set_exception_backedge() { _exception_backedge = true; }\n+  bool has_exception_backedge() const { return _exception_backedge; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -672,1 +672,0 @@\n-    C->set_has_loops(C->has_loops() || cg->method()->has_loops());\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -452,1 +452,1 @@\n-      in->disconnect_inputs(NULL, C);\n+      in->disconnect_inputs(C);\n@@ -635,1 +635,1 @@\n-         ((StressLCM && Compile::randomized_select(cand_cnt)) ||\n+         ((StressLCM && C->randomized_select(cand_cnt)) ||\n@@ -1373,1 +1373,1 @@\n-    block->get_node(beg)->disconnect_inputs(NULL, C);\n+    block->get_node(beg)->disconnect_inputs(C);\n@@ -1386,1 +1386,1 @@\n-        n->disconnect_inputs(NULL, C);\n+        n->disconnect_inputs(C);\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4584,2 +4584,1 @@\n-  if (phi_base != NULL && !phi_base->is_copy()) {\n-    \/\/ do not examine phi if degraded to a copy\n+  if (phi_base != NULL) {\n@@ -4900,4 +4899,0 @@\n-  while (n->is_Phi() && (n = n->as_Phi()->is_copy()) != NULL) {\n-    if (mem == n)  return true;\n-    if (n == NULL)  break;\n-  }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -452,2 +452,1 @@\n-  \/\/ Return the number of edges between 'n' and 'this'\n-  int  disconnect_inputs(Node *n, Compile *c);\n+  void disconnect_inputs(Compile* C);\n@@ -523,1 +522,1 @@\n-    disconnect_inputs(NULL, c);\n+    disconnect_inputs(c);\n@@ -614,1 +613,1 @@\n-  \/\/ so that it's values fits into 16 bits.\n+  \/\/ so that its values fit into 32 bits.\n@@ -728,1 +727,1 @@\n-    _max_classes  = ClassMask_Halt\n+    _max_classes  = ClassMask_Opaque1\n@@ -1131,0 +1130,6 @@\n+  int _indent;\n+\n+ public:\n+  void set_indent(int indent) { _indent = indent; }\n+\n+ private:\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -227,0 +227,66 @@\n+volatile int C2SafepointPollStubTable::_stub_size = 0;\n+\n+Label& C2SafepointPollStubTable::add_safepoint(uintptr_t safepoint_offset) {\n+  C2SafepointPollStub* entry = new (Compile::current()->comp_arena()) C2SafepointPollStub(safepoint_offset);\n+  _safepoints.append(entry);\n+  return entry->_stub_label;\n+}\n+\n+void C2SafepointPollStubTable::emit(CodeBuffer& cb) {\n+  MacroAssembler masm(&cb);\n+  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+\n+    C2SafepointPollStub* entry = _safepoints.at(i);\n+    emit_stub(masm, entry);\n+  }\n+}\n+\n+int C2SafepointPollStubTable::stub_size_lazy() const {\n+  int size = Atomic::load(&_stub_size);\n+\n+  if (size != 0) {\n+    return size;\n+  }\n+\n+  Compile* const C = Compile::current();\n+  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n+  CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n+  MacroAssembler masm(&cb);\n+  C2SafepointPollStub* entry = _safepoints.at(0);\n+  emit_stub(masm, entry);\n+  size += cb.insts_size();\n+\n+  Atomic::store(&_stub_size, size);\n+\n+  return size;\n+}\n+\n+int C2SafepointPollStubTable::estimate_stub_size() const {\n+  if (_safepoints.length() == 0) {\n+    return 0;\n+  }\n+\n+  int result = stub_size_lazy() * _safepoints.length();\n+\n+#ifdef ASSERT\n+  Compile* const C = Compile::current();\n+  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n+  int size = 0;\n+\n+  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n+    CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n+    MacroAssembler masm(&cb);\n+    C2SafepointPollStub* entry = _safepoints.at(i);\n+    emit_stub(masm, entry);\n+    size += cb.insts_size();\n+  }\n+  assert(size == result, \"stubs should not have variable size\");\n+#endif\n+\n+  return result;\n+}\n@@ -1242,0 +1308,1 @@\n+  stub_req += safepoint_poll_table()->estimate_stub_size();\n@@ -1744,0 +1811,4 @@\n+  \/\/ Fill in stubs for calling the runtime from safepoint polls.\n+  safepoint_poll_table()->emit(*cb);\n+  if (C->failing())  return;\n+\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -453,1 +453,1 @@\n-    vec_unbox->disconnect_inputs(NULL, C);\n+    vec_unbox->disconnect_inputs(C);\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1105,4 +1105,1 @@\n-  int shiftLOpc;\n-  int shiftROpc;\n-  Node* shiftLCnt = NULL;\n-  Node* shiftRCnt = NULL;\n+  assert(bt == T_INT || bt == T_LONG, \"sanity\");\n@@ -1111,0 +1108,4 @@\n+  int shift_mask = (bt == T_INT) ? 0x1F : 0x3F;\n+  int shiftLOpc = (bt == T_INT) ? Op_LShiftI : Op_LShiftL;\n+  int shiftROpc = (bt == T_INT) ? Op_URShiftI: Op_URShiftL;\n+\n@@ -1113,1 +1114,3 @@\n-  if (cnt->is_Con()) {\n+  Node* shiftRCnt = NULL;\n+  Node* shiftLCnt = NULL;\n+  if (cnt->is_Con() && cnt->bottom_type()->isa_int()) {\n@@ -1115,13 +1118,3 @@\n-    if (bt == T_INT) {\n-      int shift = cnt->get_int() & 31;\n-      shiftRCnt = phase->intcon(shift);\n-      shiftLCnt = phase->intcon(32 - shift);\n-      shiftLOpc = Op_LShiftI;\n-      shiftROpc = Op_URShiftI;\n-    } else {\n-      int shift = cnt->get_int() & 63;\n-      shiftRCnt = phase->intcon(shift);\n-      shiftLCnt = phase->intcon(64 - shift);\n-      shiftLOpc = Op_LShiftL;\n-      shiftROpc = Op_URShiftL;\n-    }\n+    int shift = cnt->get_int() & shift_mask;\n+    shiftRCnt = phase->intcon(shift);\n+    shiftLCnt = phase->intcon(shift_mask + 1 - shift);\n@@ -1132,6 +1125,2 @@\n-    if (bt == T_INT) {\n-      shiftRCnt = phase->transform(new AndINode(cnt, phase->intcon(31)));\n-      shiftLCnt = phase->transform(new SubINode(phase->intcon(32), shiftRCnt));\n-      shiftLOpc = Op_LShiftI;\n-      shiftROpc = Op_URShiftI;\n-    } else {\n+    if (bt == T_LONG) {\n+      \/\/ Shift count vector for Rotate vector has long elements too.\n@@ -1140,4 +1129,2 @@\n-      shiftRCnt = phase->transform(new AndINode(cnt, phase->intcon(63)));\n-      shiftLCnt = phase->transform(new SubINode(phase->intcon(64), shiftRCnt));\n-      shiftLOpc = Op_LShiftL;\n-      shiftROpc = Op_URShiftL;\n+    shiftRCnt = phase->transform(new AndINode(cnt, phase->intcon(shift_mask)));\n+    shiftLCnt = phase->transform(new SubINode(phase->intcon(shift_mask + 1), shiftRCnt));\n@@ -1178,83 +1165,0 @@\n-Node* OrVNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n-  int lopcode = in(1)->Opcode();\n-  int ropcode = in(2)->Opcode();\n-  const TypeVect* vt = bottom_type()->is_vect();\n-  int vec_len = vt->length();\n-  BasicType bt = vt->element_basic_type();\n-\n-  \/\/ Vector Rotate operations inferencing, this will be useful when vector\n-  \/\/ operations are created via non-SLP route i.e. (VectorAPI).\n-  if (Matcher::match_rule_supported_vector(Op_RotateLeftV, vec_len, bt) &&\n-      ((ropcode == Op_LShiftVI && lopcode == Op_URShiftVI) ||\n-       (ropcode == Op_LShiftVL && lopcode == Op_URShiftVL)) &&\n-      in(1)->in(1) == in(2)->in(1)) {\n-    assert(Op_RShiftCntV == in(1)->in(2)->Opcode(), \"LShiftCntV operand expected\");\n-    assert(Op_LShiftCntV == in(2)->in(2)->Opcode(), \"RShiftCntV operand expected\");\n-    Node* lshift = in(1)->in(2)->in(1);\n-    Node* rshift = in(2)->in(2)->in(1);\n-    int mod_val = bt == T_LONG ? 64 : 32;\n-    int shift_mask = bt == T_LONG ? 0x3F : 0x1F;\n-    \/\/ val >> norm_con_shift | val << (32 - norm_con_shift) => rotate_right val ,\n-    \/\/ norm_con_shift\n-    if (lshift->is_Con() && rshift->is_Con() &&\n-        ((lshift->get_int() & shift_mask) ==\n-         (mod_val - (rshift->get_int() & shift_mask)))) {\n-      return new RotateRightVNode(\n-          in(1)->in(1), phase->intcon(lshift->get_int() & shift_mask), vt);\n-    }\n-    if (lshift->Opcode() == Op_AndI && rshift->Opcode() == Op_AndI &&\n-        lshift->in(2)->is_Con() && rshift->in(2)->is_Con() &&\n-        lshift->in(2)->get_int() == (mod_val - 1) &&\n-        rshift->in(2)->get_int() == (mod_val - 1)) {\n-      lshift = lshift->in(1);\n-      rshift = rshift->in(1);\n-      \/\/ val << var_shift | val >> (0\/32 - var_shift) => rotate_left val ,\n-      \/\/ var_shift\n-      if (lshift->Opcode() == Op_SubI && lshift->in(2) == rshift &&\n-          lshift->in(1)->is_Con() &&\n-          (lshift->in(1)->get_int() == 0 ||\n-           lshift->in(1)->get_int() == mod_val)) {\n-        Node* rotate_cnt = phase->transform(new ReplicateINode(rshift, vt));\n-        return new RotateLeftVNode(in(1)->in(1), rotate_cnt, vt);\n-      }\n-    }\n-  }\n-\n-  if (Matcher::match_rule_supported_vector(Op_RotateRightV, vec_len, bt) &&\n-      ((ropcode == Op_URShiftVI && lopcode == Op_LShiftVI) ||\n-       (ropcode == Op_URShiftVL && lopcode == Op_LShiftVL)) &&\n-      in(1)->in(1) == in(2)->in(1)) {\n-    assert(Op_LShiftCntV == in(1)->in(2)->Opcode(), \"RShiftCntV operand expected\");\n-    assert(Op_RShiftCntV == in(2)->in(2)->Opcode(), \"LShiftCntV operand expected\");\n-    Node* rshift = in(1)->in(2)->in(1);\n-    Node* lshift = in(2)->in(2)->in(1);\n-    int mod_val = bt == T_LONG ? 64 : 32;\n-    int shift_mask = bt == T_LONG ? 0x3F : 0x1F;\n-    \/\/ val << norm_con_shift | val >> (32 - norm_con_shift) => rotate_left val\n-    \/\/ , norm_con_shift\n-    if (rshift->is_Con() && lshift->is_Con() &&\n-        ((rshift->get_int() & shift_mask) ==\n-         (mod_val - (lshift->get_int() & shift_mask)))) {\n-      return new RotateLeftVNode(\n-          in(1)->in(1), phase->intcon(rshift->get_int() & shift_mask), vt);\n-    }\n-    if (lshift->Opcode() == Op_AndI && rshift->Opcode() == Op_AndI &&\n-        lshift->in(2)->is_Con() && rshift->in(2)->is_Con() &&\n-        rshift->in(2)->get_int() == (mod_val - 1) &&\n-        lshift->in(2)->get_int() == (mod_val - 1)) {\n-      rshift = rshift->in(1);\n-      lshift = lshift->in(1);\n-      \/\/ val >> var_shift | val << (0\/32 - var_shift) => rotate_right val ,\n-      \/\/ var_shift\n-      if (rshift->Opcode() == Op_SubI && rshift->in(2) == lshift &&\n-          rshift->in(1)->is_Con() &&\n-          (rshift->in(1)->get_int() == 0 ||\n-           rshift->in(1)->get_int() == mod_val)) {\n-        Node* rotate_cnt = phase->transform(new ReplicateINode(lshift, vt));\n-        return new RotateRightVNode(in(1)->in(1), rotate_cnt, vt);\n-      }\n-    }\n-  }\n-  return NULL;\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":15,"deletions":111,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -649,1 +649,0 @@\n-  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -44,2 +44,1 @@\n-#include \"runtime\/flags\/jvmFlagConstraintList.hpp\"\n-#include \"runtime\/flags\/jvmFlagRangeList.hpp\"\n+#include \"runtime\/flags\/jvmFlagAccess.hpp\"\n@@ -557,0 +556,1 @@\n+  { \"Debugging\",                     JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },\n@@ -880,1 +880,1 @@\n-  if (JVMFlag::boolAtPut(flag, &value, origin) == JVMFlag::SUCCESS) {\n+  if (JVMFlagAccess::boolAtPut(flag, &value, origin) == JVMFlag::SUCCESS) {\n@@ -895,1 +895,1 @@\n-  if (JVMFlag::doubleAtPut(flag, &v, origin) == JVMFlag::SUCCESS) {\n+  if (JVMFlagAccess::doubleAtPut(flag, &v, origin) == JVMFlag::SUCCESS) {\n@@ -927,1 +927,1 @@\n-    return JVMFlag::intAtPut(flag, &int_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::intAtPut(flag, &int_v, origin) == JVMFlag::SUCCESS;\n@@ -930,1 +930,1 @@\n-    return JVMFlag::uintAtPut(flag, &uint_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::uintAtPut(flag, &uint_v, origin) == JVMFlag::SUCCESS;\n@@ -936,1 +936,1 @@\n-    return JVMFlag::intxAtPut(flag, &intx_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::intxAtPut(flag, &intx_v, origin) == JVMFlag::SUCCESS;\n@@ -939,1 +939,1 @@\n-    return JVMFlag::uintxAtPut(flag, &uintx_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::uintxAtPut(flag, &uintx_v, origin) == JVMFlag::SUCCESS;\n@@ -942,1 +942,1 @@\n-    return JVMFlag::uint64_tAtPut(flag, &uint64_t_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::uint64_tAtPut(flag, &uint64_t_v, origin) == JVMFlag::SUCCESS;\n@@ -945,1 +945,1 @@\n-    return JVMFlag::size_tAtPut(flag, &size_t_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::size_tAtPut(flag, &size_t_v, origin) == JVMFlag::SUCCESS;\n@@ -948,1 +948,1 @@\n-    return JVMFlag::doubleAtPut(flag, &double_v, origin) == JVMFlag::SUCCESS;\n+    return JVMFlagAccess::doubleAtPut(flag, &double_v, origin) == JVMFlag::SUCCESS;\n@@ -955,1 +955,1 @@\n-  if (JVMFlag::ccstrAtPut(flag, &value, origin) != JVMFlag::SUCCESS) return false;\n+  if (JVMFlagAccess::ccstrAtPut(flag, &value, origin) != JVMFlag::SUCCESS) return false;\n@@ -963,1 +963,1 @@\n-  if (JVMFlag::ccstrAt(flag, &old_value) != JVMFlag::SUCCESS) return false;\n+  if (JVMFlagAccess::ccstrAt(flag, &old_value) != JVMFlag::SUCCESS) return false;\n@@ -980,1 +980,1 @@\n-  (void) JVMFlag::ccstrAtPut(flag, &value, origin);\n+  (void) JVMFlagAccess::ccstrAtPut(flag, &value, origin);\n@@ -1358,1 +1358,1 @@\n-                  fuzzy_matched->_name,\n+                  fuzzy_matched->name(),\n@@ -3235,1 +3235,0 @@\n-  NOT_PRODUCT(UNSUPPORTED_OPTION(TraceProfileInterpreter));\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -164,0 +165,7 @@\n+  if (exec_mode == Unpack_exception) {\n+    \/\/ When we get here, a callee has thrown an exception into a deoptimized\n+    \/\/ frame. That throw might have deferred stack watermark checking until\n+    \/\/ after unwinding. So we deal with such deferred requests here.\n+    StackWatermarkSet::after_unwind(thread);\n+  }\n+\n@@ -258,0 +266,4 @@\n+  \/\/ When we get here we are about to unwind the deoptee frame. In order to\n+  \/\/ catch not yet safe to use frames, the following stack watermark barrier\n+  \/\/ poll will make such frames safe to use.\n+  StackWatermarkSet::before_unwind(thread);\n@@ -1527,1 +1539,1 @@\n-    StackFrameStream sfs(thread, true);\n+    StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n@@ -1674,0 +1686,1 @@\n+      \/\/ Only metaspace OOM is expected. No Java code executed.\n@@ -1692,1 +1705,16 @@\n-    Klass* tk = constant_pool->klass_at_ignore_error(index, CHECK);\n+    Klass* tk = constant_pool->klass_at_ignore_error(index, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      \/\/ Exception happened during classloading. We ignore the exception here, since it\n+      \/\/ is going to be rethrown since the current activation is going to be deoptimized and\n+      \/\/ the interpreter will re-execute the bytecode.\n+      \/\/ Do not clear probable Async Exceptions.\n+      CLEAR_PENDING_NONASYNC_EXCEPTION;\n+      \/\/ Class loading called java code which may have caused a stack\n+      \/\/ overflow. If the exception was thrown right before the return\n+      \/\/ to the runtime the stack is no longer guarded. Reguard the\n+      \/\/ stack otherwise if we return to the uncommon trap blob and the\n+      \/\/ stack bang causes a stack overflow we crash.\n+      JavaThread* jt = THREAD->as_Java_thread();\n+      bool guard_pages_enabled = jt->stack_overflow_state()->reguard_stack_if_needed();\n+      assert(guard_pages_enabled, \"stack banging in uncommon trap blob may cause crash\");\n+    }\n@@ -1700,21 +1728,0 @@\n-\n-void Deoptimization::load_class_by_index(const constantPoolHandle& constant_pool, int index) {\n-  EXCEPTION_MARK;\n-  load_class_by_index(constant_pool, index, THREAD);\n-  if (HAS_PENDING_EXCEPTION) {\n-    \/\/ Exception happened during classloading. We ignore the exception here, since it\n-    \/\/ is going to be rethrown since the current activation is going to be deoptimized and\n-    \/\/ the interpreter will re-execute the bytecode.\n-    CLEAR_PENDING_EXCEPTION;\n-    \/\/ Class loading called java code which may have caused a stack\n-    \/\/ overflow. If the exception was thrown right before the return\n-    \/\/ to the runtime the stack is no longer guarded. Reguard the\n-    \/\/ stack otherwise if we return to the uncommon trap blob and the\n-    \/\/ stack bang causes a stack overflow we crash.\n-    JavaThread* thread = THREAD->as_Java_thread();\n-    bool guard_pages_enabled = thread->stack_guards_enabled();\n-    if (!guard_pages_enabled) guard_pages_enabled = thread->reguard_stack();\n-    assert(guard_pages_enabled, \"stack banging in uncommon trap blob may cause crash\");\n-  }\n-}\n-\n@@ -1982,1 +1989,1 @@\n-      load_class_by_index(constants, unloaded_class_index);\n+      load_class_by_index(constants, unloaded_class_index, THREAD);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":31,"deletions":24,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -101,2 +101,6 @@\n-        \/\/ saved long to the int that the JVM wants.\n-        value.noop =  (narrowOop) *(julong*) value_addr;\n+        \/\/ saved long to the int that the JVM wants.  We can't just\n+        \/\/ use narrow_oop_cast directly, because we don't know what\n+        \/\/ the high bits of the value might be.\n+        static_assert(sizeof(narrowOop) == sizeof(juint), \"size mismatch\");\n+        juint narrow_value = (juint) *(julong*)value_addr;\n+        value.noop = CompressedOops::narrow_oop_cast(narrow_value);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -299,0 +299,1 @@\n+  AOT_ONLY(nonstatic_field(MethodCounters,     _method,                                       Method*))                              \\\n@@ -742,1 +743,1 @@\n-  nonstatic_field(NamedThread,                 _processed_thread,                             JavaThread*)                           \\\n+  nonstatic_field(NamedThread,                 _processed_thread,                             Thread*)                               \\\n@@ -1011,1 +1012,1 @@\n-  nonstatic_field(JVMFlag,                     _type,                                         const char*)                           \\\n+  nonstatic_field(JVMFlag,                     _type,                                         int)                                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-import jdk.internal.HotSpotIntrinsicCandidate;\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -127,1 +127,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -142,1 +142,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -155,1 +155,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -169,1 +169,1 @@\n-    \/\/FIXME @HotSpotIntrinsicCandidate\n+    \/\/FIXME @IntrinsicCandidate\n@@ -181,1 +181,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -197,1 +197,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -213,1 +213,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -225,1 +225,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -237,1 +237,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -253,1 +253,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -271,1 +271,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -288,1 +288,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -306,1 +306,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -324,1 +324,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -338,1 +338,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -354,1 +354,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -373,1 +373,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -393,1 +393,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -411,1 +411,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -431,1 +431,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n@@ -446,1 +446,1 @@\n-    @HotSpotIntrinsicCandidate\n+    @IntrinsicCandidate\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -143,1 +143,0 @@\n-        jdk.jfr,\n@@ -236,0 +235,1 @@\n+        jdk.jfr,\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}