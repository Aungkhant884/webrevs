{"files":[{"patch":"@@ -141,0 +141,1 @@\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/$(HOTSPOT_TARGET_CPU_ARCH)_neon.ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1,0 +1,1 @@\n+import os\n@@ -2,0 +3,2 @@\n+import subprocess\n+import sys\n@@ -132,0 +135,2 @@\n+              'b' : FloatRegister,\n+              'h' : FloatRegister,\n@@ -201,1 +206,1 @@\n-            self.asmRegPrefix = [\"d\", \"s\"][self.isWord] \n+            self.asmRegPrefix = [\"d\", \"s\"][self.isWord]\n@@ -205,1 +210,1 @@\n-       \n+\n@@ -208,1 +213,1 @@\n-            \n+\n@@ -210,1 +215,1 @@\n-        return (self._name+mode if (mode == 'b' or mode == 'h') \n+        return (self._name+mode if (mode == 'b' or mode == 'h')\n@@ -223,1 +228,1 @@\n-                + ('%s, %s, %s' \n+                + ('%s, %s, %s'\n@@ -226,1 +231,1 @@\n-                \n+\n@@ -230,1 +235,1 @@\n-                + ('%s, %s, %s' \n+                + ('%s, %s, %s'\n@@ -233,1 +238,1 @@\n-                \n+\n@@ -244,1 +249,1 @@\n-                \n+\n@@ -249,1 +254,1 @@\n-                \n+\n@@ -264,1 +269,1 @@\n-                + ('%s, %s' \n+                + ('%s, %s'\n@@ -267,1 +272,1 @@\n-                \n+\n@@ -274,1 +279,1 @@\n-        \n+\n@@ -304,1 +309,1 @@\n-        \n+\n@@ -306,1 +311,1 @@\n-        return ('%s, Assembler::%s, %s);' \n+        return ('%s, Assembler::%s, %s);'\n@@ -317,1 +322,1 @@\n-    \n+\n@@ -319,1 +324,1 @@\n-        return ('%s);' \n+        return ('%s);'\n@@ -335,1 +340,1 @@\n-                + (\", ext::\" + AddSubExtendedOp.optNames[self.option] \n+                + (\", ext::\" + AddSubExtendedOp.optNames[self.option]\n@@ -337,1 +342,1 @@\n-                \n+\n@@ -340,1 +345,1 @@\n-                + (\", \" + AddSubExtendedOp.optNames[self.option] \n+                + (\", \" + AddSubExtendedOp.optNames[self.option]\n@@ -347,1 +352,1 @@\n-    \n+\n@@ -352,16 +357,16 @@\n-         = [0x1, 0x3f, 0x1f0, 0x7e0, \n-            0x1c00, 0x3ff0, 0x8000, 0x1e000, \n-            0x3e000, 0x78000, 0xe0000, 0x100000, \n-            0x1fffe0, 0x3fe000, 0x780000, 0x7ffff8, \n-            0xff8000, 0x1800180, 0x1fffc00, 0x3c003c0, \n-            0x3ffff00, 0x7c00000, 0x7fffe00, 0xf000f00, \n-            0xfffe000, 0x18181818, 0x1ffc0000, 0x1ffffffe, \n-            0x3f003f00, 0x3fffe000, 0x60006000, 0x7f807f80, \n-            0x7ffffc00, 0x800001ff, 0x803fffff, 0x9f9f9f9f, \n-            0xc0000fff, 0xc0c0c0c0, 0xe0000000, 0xe003e003, \n-            0xe3ffffff, 0xf0000fff, 0xf0f0f0f0, 0xf80000ff, \n-            0xf83ff83f, 0xfc00007f, 0xfc1fffff, 0xfe0001ff, \n-            0xfe3fffff, 0xff003fff, 0xff800003, 0xff87ff87, \n-            0xffc00fff, 0xffe0000f, 0xffefffef, 0xfff1fff1, \n-            0xfff83fff, 0xfffc0fff, 0xfffe0fff, 0xffff3fff, \n-            0xffffc007, 0xffffe1ff, 0xfffff80f, 0xfffffe07, \n+         = [0x1, 0x3f, 0x1f0, 0x7e0,\n+            0x1c00, 0x3ff0, 0x8000, 0x1e000,\n+            0x3e000, 0x78000, 0xe0000, 0x100000,\n+            0x1fffe0, 0x3fe000, 0x780000, 0x7ffff8,\n+            0xff8000, 0x1800180, 0x1fffc00, 0x3c003c0,\n+            0x3ffff00, 0x7c00000, 0x7fffe00, 0xf000f00,\n+            0xfffe000, 0x18181818, 0x1ffc0000, 0x1ffffffe,\n+            0x3f003f00, 0x3fffe000, 0x60006000, 0x7f807f80,\n+            0x7ffffc00, 0x800001ff, 0x803fffff, 0x9f9f9f9f,\n+            0xc0000fff, 0xc0c0c0c0, 0xe0000000, 0xe003e003,\n+            0xe3ffffff, 0xf0000fff, 0xf0f0f0f0, 0xf80000ff,\n+            0xf83ff83f, 0xfc00007f, 0xfc1fffff, 0xfe0001ff,\n+            0xfe3fffff, 0xff003fff, 0xff800003, 0xff87ff87,\n+            0xffc00fff, 0xffe0000f, 0xffefffef, 0xfff1fff1,\n+            0xfff83fff, 0xfffc0fff, 0xfffe0fff, 0xffff3fff,\n+            0xffffc007, 0xffffe1ff, 0xfffff80f, 0xfffffe07,\n@@ -371,15 +376,15 @@\n-         = [0x1, 0x1f80, 0x3fff0, 0x3ffffc, \n-            0x3fe0000, 0x1ffc0000, 0xf8000000, 0x3ffffc000, \n-            0xffffffe00, 0x3ffffff800, 0xffffc00000, 0x3f000000000, \n-            0x7fffffff800, 0x1fe000001fe0, 0x3ffffff80000, 0xc00000000000, \n-            0x1ffc000000000, 0x3ffff0003ffff, 0x7ffffffe00000, 0xfffffffffc000, \n-            0x1ffffffffffc00, 0x3fffffffffff00, 0x7ffffffffffc00, 0xffffffffff8000, \n-            0x1ffffffff800000, 0x3fffffc03fffffc, 0x7fffc0000000000, 0xff80ff80ff80ff8, \n-            0x1c00000000000000, 0x1fffffffffff0000, 0x3fffff803fffff80, 0x7fc000007fc00000, \n-            0x8000000000000000, 0x803fffff803fffff, 0xc000007fc000007f, 0xe00000000000ffff, \n-            0xe3ffffffffffffff, 0xf007f007f007f007, 0xf80003ffffffffff, 0xfc000003fc000003, \n-            0xfe000000007fffff, 0xff00000000007fff, 0xff800000000003ff, 0xffc00000000000ff, \n-            0xffe00000000003ff, 0xfff0000000003fff, 0xfff80000001fffff, 0xfffc0000fffc0000, \n-            0xfffe003fffffffff, 0xffff3fffffffffff, 0xffffc0000007ffff, 0xffffe01fffffe01f, \n-            0xfffff800000007ff, 0xfffffc0fffffffff, 0xffffff00003fffff, 0xffffffc0000007ff, \n-            0xfffffff0000001ff, 0xfffffffc00003fff, 0xffffffff07ffffff, 0xffffffffe003ffff, \n+         = [0x1, 0x1f80, 0x3fff0, 0x3ffffc,\n+            0x3fe0000, 0x1ffc0000, 0xf8000000, 0x3ffffc000,\n+            0xffffffe00, 0x3ffffff800, 0xffffc00000, 0x3f000000000,\n+            0x7fffffff800, 0x1fe000001fe0, 0x3ffffff80000, 0xc00000000000,\n+            0x1ffc000000000, 0x3ffff0003ffff, 0x7ffffffe00000, 0xfffffffffc000,\n+            0x1ffffffffffc00, 0x3fffffffffff00, 0x7ffffffffffc00, 0xffffffffff8000,\n+            0x1ffffffff800000, 0x3fffffc03fffffc, 0x7fffc0000000000, 0xff80ff80ff80ff8,\n+            0x1c00000000000000, 0x1fffffffffff0000, 0x3fffff803fffff80, 0x7fc000007fc00000,\n+            0x8000000000000000, 0x803fffff803fffff, 0xc000007fc000007f, 0xe00000000000ffff,\n+            0xe3ffffffffffffff, 0xf007f007f007f007, 0xf80003ffffffffff, 0xfc000003fc000003,\n+            0xfe000000007fffff, 0xff00000000007fff, 0xff800000000003ff, 0xffc00000000000ff,\n+            0xffe00000000003ff, 0xfff0000000003fff, 0xfff80000001fffff, 0xfffc0000fffc0000,\n+            0xfffe003fffffffff, 0xffff3fffffffffff, 0xffffc0000007ffff, 0xffffe01fffffe01f,\n+            0xfffff800000007ff, 0xfffffc0fffffffff, 0xffffff00003fffff, 0xffffffc0000007ff,\n+            0xfffffff0000001ff, 0xfffffffc00003fff, 0xffffffff07ffffff, 0xffffffffe003ffff,\n@@ -392,4 +397,3 @@\n-              \tif self.isWord \\\n-              else \\\n-              \tself.immediates[random.randint(0, len(self.immediates)-1)]\n-              \n+              if self.isWord else \\\n+              self.immediates[random.randint(0, len(self.immediates)-1)]\n+\n@@ -397,1 +401,1 @@\n-                  \n+\n@@ -404,1 +408,1 @@\n-    \n+\n@@ -425,1 +429,1 @@\n-    \n+\n@@ -427,1 +431,1 @@\n-        if self.name() == \"adrp\": \n+        if self.name() == \"adrp\":\n@@ -437,1 +441,1 @@\n-    \n+\n@@ -441,1 +445,1 @@\n-        return (super(RegAndAbsOp, self).cstr() \n+        return (super(RegAndAbsOp, self).cstr()\n@@ -449,1 +453,1 @@\n-    \n+\n@@ -456,1 +460,1 @@\n-                + (\"%s, #%s, %s\" \n+                + (\"%s, #%s, %s\"\n@@ -465,1 +469,1 @@\n-    \n+\n@@ -475,2 +479,2 @@\n-                + (\"%s, #%s, lsl %s\" \n-                   % (self.reg.astr(self.asmRegPrefix), \n+                + (\"%s, #%s, lsl %s\"\n+                   % (self.reg.astr(self.asmRegPrefix),\n@@ -489,1 +493,1 @@\n-    \n+\n@@ -516,1 +520,1 @@\n-    \n+\n@@ -520,1 +524,1 @@\n-    \n+\n@@ -525,1 +529,1 @@\n-        \n+\n@@ -533,1 +537,1 @@\n-        \n+\n@@ -536,1 +540,1 @@\n-        \n+\n@@ -545,0 +549,2 @@\n+    def astr(self):\n+        return self.aname();\n@@ -576,1 +582,1 @@\n-        return (super(ConditionalCompareOp, self).cstr() + \", \" \n+        return (super(ConditionalCompareOp, self).cstr() + \", \"\n@@ -580,1 +586,1 @@\n-        return (super(ConditionalCompareOp, self).astr() + \n+        return (super(ConditionalCompareOp, self).astr() +\n@@ -599,2 +605,2 @@\n-        return (Instruction.astr(self) \n-                + self.reg.astr(self.asmRegPrefix) \n+        return (Instruction.astr(self)\n+                + self.reg.astr(self.asmRegPrefix)\n@@ -606,1 +612,1 @@\n-    \n+\n@@ -611,1 +617,1 @@\n-    \n+\n@@ -616,1 +622,1 @@\n-    \n+\n@@ -623,3 +629,3 @@\n-        return (Instruction.astr(self) \n-                + self.reg[0].astr(self.asmRegPrefix) \n-                + \", \" + self.reg[1].astr(midPrefix) \n+        return (Instruction.astr(self)\n+                + self.reg[0].astr(self.asmRegPrefix)\n+                + \", \" + self.reg[1].astr(midPrefix)\n@@ -641,2 +647,2 @@\n-        return (ThreeRegInstruction.astr(self) \n-                + \", \" + conditionCodes[self.cond])    \n+        return (ThreeRegInstruction.astr(self)\n+                + \", \" + conditionCodes[self.cond])\n@@ -654,1 +660,1 @@\n-        prefix = ('x' if (self.mode == 'x') \n+        prefix = ('x' if (self.mode == 'x')\n@@ -701,1 +707,1 @@\n-    \n+\n@@ -704,1 +710,1 @@\n-    kinds = [\"base_plus_unscaled_offset\", \"pre\", \"post\", \"base_plus_reg\", \n+    kinds = [\"base_plus_unscaled_offset\", \"pre\", \"post\", \"base_plus_reg\",\n@@ -711,1 +717,1 @@\n-    \n+\n@@ -741,1 +747,1 @@\n-            Address.base_plus_scaled_offset: \n+            Address.base_plus_scaled_offset:\n@@ -761,1 +767,1 @@\n-                % (self.base.astr(prefix), self.index.astr(extend_prefix), \n+                % (self.base.astr(prefix), self.index.astr(extend_prefix),\n@@ -770,1 +776,1 @@\n-        \n+\n@@ -825,1 +831,1 @@\n-     \n+\n@@ -830,1 +836,1 @@\n-          \n+\n@@ -832,1 +838,1 @@\n-          self.reg = [OperandFactory.create(self.mode).generate() \n+          self.reg = [OperandFactory.create(self.mode).generate()\n@@ -849,2 +855,2 @@\n-              % (self.asmname, \n-                 self.reg[0].astr(self.asmRegPrefix), \n+              % (self.asmname,\n+                 self.reg[0].astr(self.asmRegPrefix),\n@@ -878,1 +884,1 @@\n-        self.reg = [OperandFactory.create(self.modes[i]).generate() \n+        self.reg = [OperandFactory.create(self.modes[i]).generate()\n@@ -887,1 +893,1 @@\n-    \n+\n@@ -988,1 +994,1 @@\n-class LdStSIMDOp(Instruction):\n+class LdStNEONOp(Instruction):\n@@ -1007,1 +1013,1 @@\n-        buf = super(LdStSIMDOp, self).cstr() + str(self._firstSIMDreg)\n+        buf = super(LdStNEONOp, self).cstr() + str(self._firstSIMDreg)\n@@ -1025,0 +1031,51 @@\n+class NEONReduceInstruction(Instruction):\n+    def __init__(self, args):\n+        self._name, self.insname, self.arrangement = args\n+\n+    def generate(self):\n+        current = FloatRegister().generate()\n+        self.dstSIMDreg = current\n+        self.srcSIMDreg = current.nextReg()\n+        return self\n+\n+    def cstr(self):\n+        buf = Instruction.cstr(self) + str(self.dstSIMDreg)\n+        buf = '%s, __ T%s, %s);' % (buf, self.arrangement, self.srcSIMDreg)\n+        return buf\n+\n+    def astr(self):\n+        buf = '%s\\t%s' % (self.insname, self.dstSIMDreg.astr(self.arrangement[-1].lower()))\n+        buf = '%s, %s.%s' % (buf, self.srcSIMDreg, self.arrangement)\n+        return buf\n+\n+    def aname(self):\n+        return self._name\n+\n+class CommonNEONInstruction(Instruction):\n+    def __init__(self, args):\n+        self._name, self.insname, self.arrangement = args\n+\n+    def generate(self):\n+        self._firstSIMDreg = FloatRegister().generate()\n+        return self\n+\n+    def cstr(self):\n+        buf = Instruction.cstr(self) + str(self._firstSIMDreg)\n+        buf = '%s, __ T%s' % (buf, self.arrangement)\n+        current = self._firstSIMDreg\n+        for cnt in range(1, self.numRegs):\n+            buf = '%s, %s' % (buf, current.nextReg())\n+            current = current.nextReg()\n+        return '%s);' % (buf)\n+\n+    def astr(self):\n+        buf = '%s\\t%s.%s' % (self.insname, self._firstSIMDreg, self.arrangement)\n+        current = self._firstSIMDreg\n+        for cnt in range(1, self.numRegs):\n+            buf = '%s, %s.%s' % (buf, current.nextReg(), self.arrangement)\n+            current = current.nextReg()\n+        return buf\n+\n+    def aname(self):\n+        return self._name\n+\n@@ -1100,0 +1157,6 @@\n+class TwoRegNEONOp(CommonNEONInstruction):\n+    numRegs = 2\n+\n+class ThreeRegNEONOp(TwoRegNEONOp):\n+    numRegs = 3\n+\n@@ -1132,0 +1195,1 @@\n+# To minimize the changes of assembler test code\n@@ -1142,1 +1206,1 @@\n-generate (ArithOp, \n+generate (ArithOp,\n@@ -1146,2 +1210,2 @@\n-            \"andw\", \"orrw\", \"eorw\", \"andsw\", \n-            \"bic\", \"orn\", \"eon\", \"bics\", \n+            \"andw\", \"orrw\", \"eorw\", \"andsw\",\n+            \"bic\", \"orn\", \"eon\", \"bics\",\n@@ -1150,1 +1214,1 @@\n-generate (AddSubImmOp, \n+generate (AddSubImmOp,\n@@ -1153,1 +1217,1 @@\n-generate (LogicalImmOp, \n+generate (LogicalImmOp,\n@@ -1194,1 +1258,1 @@\n-    print \"\\n\/\/ \" + Address.kindToStr(kind),\n+    sys.stdout.write(\"\\n\/\/ \" + Address.kindToStr(kind))\n@@ -1196,2 +1260,2 @@\n-        generate (LoadStoreOp, \n-                  [[\"str\", \"str\", kind, \"x\"], [\"str\", \"str\", kind, \"w\"], \n+        generate (LoadStoreOp,\n+                  [[\"str\", \"str\", kind, \"x\"], [\"str\", \"str\", kind, \"w\"],\n@@ -1199,1 +1263,1 @@\n-                   [\"ldr\", \"ldr\", kind, \"x\"], [\"ldr\", \"ldr\", kind, \"w\"], \n+                   [\"ldr\", \"ldr\", kind, \"x\"], [\"ldr\", \"ldr\", kind, \"w\"],\n@@ -1201,1 +1265,1 @@\n-                   [\"ldrsb\", \"ldrsb\", kind, \"x\"], [\"ldrsh\", \"ldrsh\", kind, \"x\"], \n+                   [\"ldrsb\", \"ldrsb\", kind, \"x\"], [\"ldrsh\", \"ldrsh\", kind, \"x\"],\n@@ -1203,2 +1267,2 @@\n-                   [\"ldr\", \"ldr\", kind, \"d\"], [\"ldr\", \"ldr\", kind, \"s\"], \n-                   [\"str\", \"str\", kind, \"d\"], [\"str\", \"str\", kind, \"s\"], \n+                   [\"ldr\", \"ldr\", kind, \"d\"], [\"ldr\", \"ldr\", kind, \"s\"],\n+                   [\"str\", \"str\", kind, \"d\"], [\"str\", \"str\", kind, \"s\"],\n@@ -1207,1 +1271,1 @@\n-        generate (LoadStoreOp, \n+        generate (LoadStoreOp,\n@@ -1209,1 +1273,1 @@\n-        \n+\n@@ -1213,1 +1277,1 @@\n-    generate (LoadStoreOp, \n+    generate (LoadStoreOp,\n@@ -1222,1 +1286,1 @@\n-generate(ConditionalSelectOp, \n+generate(ConditionalSelectOp,\n@@ -1225,2 +1289,2 @@\n-generate(TwoRegOp, \n-         [\"rbitw\", \"rev16w\", \"revw\", \"clzw\", \"clsw\", \"rbit\", \n+generate(TwoRegOp,\n+         [\"rbitw\", \"rev16w\", \"revw\", \"clzw\", \"clsw\", \"rbit\",\n@@ -1228,2 +1292,2 @@\n-generate(ThreeRegOp, \n-         [\"udivw\", \"sdivw\", \"lslvw\", \"lsrvw\", \"asrvw\", \"rorvw\", \"udiv\", \"sdiv\", \n+generate(ThreeRegOp,\n+         [\"udivw\", \"sdivw\", \"lslvw\", \"lsrvw\", \"asrvw\", \"rorvw\", \"udiv\", \"sdiv\",\n@@ -1231,1 +1295,1 @@\n-generate(FourRegMulOp, \n+generate(FourRegMulOp,\n@@ -1234,2 +1298,2 @@\n-generate(ThreeRegFloatOp, \n-         [[\"fmuls\", \"sss\"], [\"fdivs\", \"sss\"], [\"fadds\", \"sss\"], [\"fsubs\", \"sss\"], \n+generate(ThreeRegFloatOp,\n+         [[\"fmuls\", \"sss\"], [\"fdivs\", \"sss\"], [\"fadds\", \"sss\"], [\"fsubs\", \"sss\"],\n@@ -1237,1 +1301,1 @@\n-          [\"fmuld\", \"ddd\"], [\"fdivd\", \"ddd\"], [\"faddd\", \"ddd\"], [\"fsubd\", \"ddd\"], \n+          [\"fmuld\", \"ddd\"], [\"fdivd\", \"ddd\"], [\"faddd\", \"ddd\"], [\"fsubd\", \"ddd\"],\n@@ -1240,2 +1304,2 @@\n-generate(FourRegFloatOp, \n-         [[\"fmadds\", \"ssss\"], [\"fmsubs\", \"ssss\"], [\"fnmadds\", \"ssss\"], [\"fnmadds\", \"ssss\"], \n+generate(FourRegFloatOp,\n+         [[\"fmadds\", \"ssss\"], [\"fmsubs\", \"ssss\"], [\"fnmadds\", \"ssss\"], [\"fnmadds\", \"ssss\"],\n@@ -1244,2 +1308,2 @@\n-generate(TwoRegFloatOp, \n-         [[\"fmovs\", \"ss\"], [\"fabss\", \"ss\"], [\"fnegs\", \"ss\"], [\"fsqrts\", \"ss\"], \n+generate(TwoRegFloatOp,\n+         [[\"fmovs\", \"ss\"], [\"fabss\", \"ss\"], [\"fnegs\", \"ss\"], [\"fsqrts\", \"ss\"],\n@@ -1247,1 +1311,1 @@\n-          [\"fmovd\", \"dd\"], [\"fabsd\", \"dd\"], [\"fnegd\", \"dd\"], [\"fsqrtd\", \"dd\"], \n+          [\"fmovd\", \"dd\"], [\"fabsd\", \"dd\"], [\"fnegd\", \"dd\"], [\"fsqrtd\", \"dd\"],\n@@ -1258,1 +1322,1 @@\n-generate(TwoRegFloatOp, [[\"fcmps\", \"ss\"], [\"fcmpd\", \"dd\"], \n+generate(TwoRegFloatOp, [[\"fcmps\", \"ss\"], [\"fcmpd\", \"dd\"],\n@@ -1263,1 +1327,1 @@\n-                                [\"ldpsw\", \"ldpsw\", kind, \"x\"], \n+                                [\"ldpsw\", \"ldpsw\", kind, \"x\"],\n@@ -1269,1 +1333,1 @@\n-generate(LdStSIMDOp, [[\"ld1\",  1, \"8B\",  Address.base_only],\n+generate(LdStNEONOp, [[\"ld1\",  1, \"8B\",  Address.base_only],\n@@ -1293,0 +1357,87 @@\n+generate(NEONReduceInstruction,\n+         [[\"addv\", \"addv\", \"8B\"], [\"addv\", \"addv\", \"16B\"],\n+          [\"addv\", \"addv\", \"4H\"], [\"addv\", \"addv\", \"8H\"],\n+          [\"addv\", \"addv\", \"4S\"],\n+          [\"smaxv\", \"smaxv\", \"8B\"], [\"smaxv\", \"smaxv\", \"16B\"],\n+          [\"smaxv\", \"smaxv\", \"4H\"], [\"smaxv\", \"smaxv\", \"8H\"],\n+          [\"smaxv\", \"smaxv\", \"4S\"], [\"fmaxv\", \"fmaxv\", \"4S\"],\n+          [\"sminv\", \"sminv\", \"8B\"], [\"sminv\", \"sminv\", \"16B\"],\n+          [\"sminv\", \"sminv\", \"4H\"], [\"sminv\", \"sminv\", \"8H\"],\n+          [\"sminv\", \"sminv\", \"4S\"], [\"fminv\", \"fminv\", \"4S\"],\n+          ])\n+\n+generate(TwoRegNEONOp,\n+         [[\"absr\", \"abs\", \"8B\"], [\"absr\", \"abs\", \"16B\"],\n+          [\"absr\", \"abs\", \"4H\"], [\"absr\", \"abs\", \"8H\"],\n+          [\"absr\", \"abs\", \"2S\"], [\"absr\", \"abs\", \"4S\"],\n+          [\"absr\", \"abs\", \"2D\"],\n+          [\"fabs\", \"fabs\", \"2S\"], [\"fabs\", \"fabs\", \"4S\"],\n+          [\"fabs\", \"fabs\", \"2D\"],\n+          [\"fneg\", \"fneg\", \"2S\"], [\"fneg\", \"fneg\", \"4S\"],\n+          [\"fneg\", \"fneg\", \"2D\"],\n+          [\"fsqrt\", \"fsqrt\", \"2S\"], [\"fsqrt\", \"fsqrt\", \"4S\"],\n+          [\"fsqrt\", \"fsqrt\", \"2D\"],\n+          [\"notr\", \"not\", \"8B\"], [\"notr\", \"not\", \"16B\"],\n+          ])\n+\n+generate(ThreeRegNEONOp,\n+         [[\"andr\", \"and\", \"8B\"], [\"andr\", \"and\", \"16B\"],\n+          [\"orr\", \"orr\", \"8B\"], [\"orr\", \"orr\", \"16B\"],\n+          [\"eor\", \"eor\", \"8B\"], [\"eor\", \"eor\", \"16B\"],\n+          [\"addv\", \"add\", \"8B\"], [\"addv\", \"add\", \"16B\"],\n+          [\"addv\", \"add\", \"4H\"], [\"addv\", \"add\", \"8H\"],\n+          [\"addv\", \"add\", \"2S\"], [\"addv\", \"add\", \"4S\"],\n+          [\"addv\", \"add\", \"2D\"],\n+          [\"fadd\", \"fadd\", \"2S\"], [\"fadd\", \"fadd\", \"4S\"],\n+          [\"fadd\", \"fadd\", \"2D\"],\n+          [\"subv\", \"sub\", \"8B\"], [\"subv\", \"sub\", \"16B\"],\n+          [\"subv\", \"sub\", \"4H\"], [\"subv\", \"sub\", \"8H\"],\n+          [\"subv\", \"sub\", \"2S\"], [\"subv\", \"sub\", \"4S\"],\n+          [\"subv\", \"sub\", \"2D\"],\n+          [\"fsub\", \"fsub\", \"2S\"], [\"fsub\", \"fsub\", \"4S\"],\n+          [\"fsub\", \"fsub\", \"2D\"],\n+          [\"mulv\", \"mul\", \"8B\"], [\"mulv\", \"mul\", \"16B\"],\n+          [\"mulv\", \"mul\", \"4H\"], [\"mulv\", \"mul\", \"8H\"],\n+          [\"mulv\", \"mul\", \"2S\"], [\"mulv\", \"mul\", \"4S\"],\n+          [\"fmul\", \"fmul\", \"2S\"], [\"fmul\", \"fmul\", \"4S\"],\n+          [\"fmul\", \"fmul\", \"2D\"],\n+          [\"mlav\", \"mla\", \"4H\"], [\"mlav\", \"mla\", \"8H\"],\n+          [\"mlav\", \"mla\", \"2S\"], [\"mlav\", \"mla\", \"4S\"],\n+          [\"fmla\", \"fmla\", \"2S\"], [\"fmla\", \"fmla\", \"4S\"],\n+          [\"fmla\", \"fmla\", \"2D\"],\n+          [\"mlsv\", \"mls\", \"4H\"], [\"mlsv\", \"mls\", \"8H\"],\n+          [\"mlsv\", \"mls\", \"2S\"], [\"mlsv\", \"mls\", \"4S\"],\n+          [\"fmls\", \"fmls\", \"2S\"], [\"fmls\", \"fmls\", \"4S\"],\n+          [\"fmls\", \"fmls\", \"2D\"],\n+          [\"fdiv\", \"fdiv\", \"2S\"], [\"fdiv\", \"fdiv\", \"4S\"],\n+          [\"fdiv\", \"fdiv\", \"2D\"],\n+          [\"maxv\", \"smax\", \"8B\"], [\"maxv\", \"smax\", \"16B\"],\n+          [\"maxv\", \"smax\", \"4H\"], [\"maxv\", \"smax\", \"8H\"],\n+          [\"maxv\", \"smax\", \"2S\"], [\"maxv\", \"smax\", \"4S\"],\n+          [\"fmax\", \"fmax\", \"2S\"], [\"fmax\", \"fmax\", \"4S\"],\n+          [\"fmax\", \"fmax\", \"2D\"],\n+          [\"minv\", \"smin\", \"8B\"], [\"minv\", \"smin\", \"16B\"],\n+          [\"minv\", \"smin\", \"4H\"], [\"minv\", \"smin\", \"8H\"],\n+          [\"minv\", \"smin\", \"2S\"], [\"minv\", \"smin\", \"4S\"],\n+          [\"fmin\", \"fmin\", \"2S\"], [\"fmin\", \"fmin\", \"4S\"],\n+          [\"fmin\", \"fmin\", \"2D\"],\n+          [\"cmeq\", \"cmeq\", \"8B\"], [\"cmeq\", \"cmeq\", \"16B\"],\n+          [\"cmeq\", \"cmeq\", \"4H\"], [\"cmeq\", \"cmeq\", \"8H\"],\n+          [\"cmeq\", \"cmeq\", \"2S\"], [\"cmeq\", \"cmeq\", \"4S\"],\n+          [\"cmeq\", \"cmeq\", \"2D\"],\n+          [\"fcmeq\", \"fcmeq\", \"2S\"], [\"fcmeq\", \"fcmeq\", \"4S\"],\n+          [\"fcmeq\", \"fcmeq\", \"2D\"],\n+          [\"cmgt\", \"cmgt\", \"8B\"], [\"cmgt\", \"cmgt\", \"16B\"],\n+          [\"cmgt\", \"cmgt\", \"4H\"], [\"cmgt\", \"cmgt\", \"8H\"],\n+          [\"cmgt\", \"cmgt\", \"2S\"], [\"cmgt\", \"cmgt\", \"4S\"],\n+          [\"cmgt\", \"cmgt\", \"2D\"],\n+          [\"fcmgt\", \"fcmgt\", \"2S\"], [\"fcmgt\", \"fcmgt\", \"4S\"],\n+          [\"fcmgt\", \"fcmgt\", \"2D\"],\n+          [\"cmge\", \"cmge\", \"8B\"], [\"cmge\", \"cmge\", \"16B\"],\n+          [\"cmge\", \"cmge\", \"4H\"], [\"cmge\", \"cmge\", \"8H\"],\n+          [\"cmge\", \"cmge\", \"2S\"], [\"cmge\", \"cmge\", \"4S\"],\n+          [\"cmge\", \"cmge\", \"2D\"],\n+          [\"fcmge\", \"fcmge\", \"2S\"], [\"fcmge\", \"fcmge\", \"4S\"],\n+          [\"fcmge\", \"fcmge\", \"2D\"],\n+          ])\n+\n@@ -1347,3 +1498,3 @@\n-for float in (\"2.0\", \"2.125\", \"4.0\", \"4.25\", \"8.0\", \"8.5\", \"16.0\", \"17.0\", \"0.125\", \n-              \"0.1328125\", \"0.25\", \"0.265625\", \"0.5\", \"0.53125\", \"1.0\", \"1.0625\", \n-              \"-2.0\", \"-2.125\", \"-4.0\", \"-4.25\", \"-8.0\", \"-8.5\", \"-16.0\", \"-17.0\", \n+for float in (\"2.0\", \"2.125\", \"4.0\", \"4.25\", \"8.0\", \"8.5\", \"16.0\", \"17.0\", \"0.125\",\n+              \"0.1328125\", \"0.25\", \"0.265625\", \"0.5\", \"0.53125\", \"1.0\", \"1.0625\",\n+              \"-2.0\", \"-2.125\", \"-4.0\", \"-4.25\", \"-8.0\", \"-8.5\", \"-16.0\", \"-17.0\",\n@@ -1417,3 +1568,0 @@\n-import subprocess\n-import sys\n-\n@@ -1424,3 +1572,1 @@\n-print \"\/*\",\n-sys.stdout.flush()\n-subprocess.check_call([AARCH64_OBJDUMP, \"-d\", \"aarch64ops.o\"])\n+print \"\/*\"\n@@ -1447,0 +1593,1 @@\n+infile.close()\n@@ -1448,0 +1595,2 @@\n+for f in [\"aarch64ops.s\", \"aarch64ops.o\", \"aarch64ops.bin\"]:\n+    os.remove(f)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64-asmtest.py","additions":291,"deletions":142,"binary":false,"changes":433,"status":"modified"},{"patch":"@@ -2413,0 +2413,6 @@\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+      if (vlen < 4) {\n+        return false;\n+      }\n+      break;\n@@ -2424,0 +2430,4 @@\n+bool Matcher::supports_vector_variable_shifts(void) {\n+  return true;\n+}\n+\n@@ -2469,2 +2479,2 @@\n-  } else {\n-    \/\/  For the moment limit the vector size to 8 bytes with NEON.\n+  } else { \/\/ NEON\n+    \/\/ Limit the vector size to 8 bytes\n@@ -2472,0 +2482,7 @@\n+    if (bt == T_BYTE) {\n+      \/\/ To support vector api shuffle\/rearrange.\n+      size = 4;\n+    } else if (bt == T_BOOLEAN) {\n+      \/\/ To support vector api load\/store mask.\n+      size = 2;\n+    }\n@@ -2473,1 +2490,1 @@\n-    return size;\n+    return MIN2(size,max_size);\n@@ -2492,0 +2509,3 @@\n+    \/\/ For 16-bit\/32-bit mask vector, reuse VecD.\n+    case  2:\n+    case  4:\n@@ -3134,0 +3154,6 @@\n+  enc_class aarch64_enc_ldrvH(vecD dst, memory mem) %{\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, MacroAssembler::H,\n+       $mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+\n@@ -3152,0 +3178,6 @@\n+  enc_class aarch64_enc_strvH(vecD src, memory mem) %{\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, MacroAssembler::H,\n+       $mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+\n@@ -4255,0 +4287,20 @@\n+operand immI_2()\n+%{\n+  predicate(n->get_int() == 2);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_4()\n+%{\n+  predicate(n->get_int() == 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -11225,0 +11277,1 @@\n+\/\/ This section is generated from aarch64_ad.m4\n@@ -16831,0 +16884,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -16850,0 +16904,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -16868,0 +16923,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -16887,0 +16943,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -17968,2 +18025,1 @@\n-    __ fabs(as_FloatRegister($dst$$reg), __ T2S,\n-            as_FloatRegister($src$$reg));\n+    __ fabs(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg));\n@@ -17981,2 +18037,1 @@\n-    __ fabs(as_FloatRegister($dst$$reg), __ T4S,\n-            as_FloatRegister($src$$reg));\n+    __ fabs(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg));\n@@ -17994,2 +18049,1 @@\n-    __ fabs(as_FloatRegister($dst$$reg), __ T2D,\n-            as_FloatRegister($src$$reg));\n+    __ fabs(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg));\n@@ -18136,1 +18190,2 @@\n-  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  predicate(n->as_Vector()->length_in_bytes() == 4 ||\n+            n->as_Vector()->length_in_bytes() == 8);\n@@ -18960,6 +19015,6 @@\n-     __ cnt(as_FloatRegister($dst$$reg), __ T16B,\n-            as_FloatRegister($src$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n-               as_FloatRegister($dst$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n-               as_FloatRegister($dst$$reg));\n+    __ cnt(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n+              as_FloatRegister($dst$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n+              as_FloatRegister($dst$$reg));\n@@ -18979,6 +19034,6 @@\n-     __ cnt(as_FloatRegister($dst$$reg), __ T8B,\n-            as_FloatRegister($src$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T8B,\n-               as_FloatRegister($dst$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T4H,\n-               as_FloatRegister($dst$$reg));\n+    __ cnt(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T8B,\n+              as_FloatRegister($dst$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T4H,\n+              as_FloatRegister($dst$$reg));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":77,"deletions":22,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -593,1 +593,1 @@\n-\/\/ LdStSIMDOp\n+\/\/ LdStNEONOp\n@@ -617,0 +617,141 @@\n+\/\/ NEONReduceInstruction\n+    __ addv(v22, __ T8B, v23);                         \/\/       addv    b22, v23.8B\n+    __ addv(v27, __ T16B, v28);                        \/\/       addv    b27, v28.16B\n+    __ addv(v4, __ T4H, v5);                           \/\/       addv    h4, v5.4H\n+    __ addv(v7, __ T8H, v8);                           \/\/       addv    h7, v8.8H\n+    __ addv(v6, __ T4S, v7);                           \/\/       addv    s6, v7.4S\n+    __ smaxv(v1, __ T8B, v2);                          \/\/       smaxv   b1, v2.8B\n+    __ smaxv(v26, __ T16B, v27);                       \/\/       smaxv   b26, v27.16B\n+    __ smaxv(v15, __ T4H, v16);                        \/\/       smaxv   h15, v16.4H\n+    __ smaxv(v2, __ T8H, v3);                          \/\/       smaxv   h2, v3.8H\n+    __ smaxv(v13, __ T4S, v14);                        \/\/       smaxv   s13, v14.4S\n+    __ fmaxv(v13, __ T4S, v14);                        \/\/       fmaxv   s13, v14.4S\n+    __ sminv(v24, __ T8B, v25);                        \/\/       sminv   b24, v25.8B\n+    __ sminv(v23, __ T16B, v24);                       \/\/       sminv   b23, v24.16B\n+    __ sminv(v4, __ T4H, v5);                          \/\/       sminv   h4, v5.4H\n+    __ sminv(v19, __ T8H, v20);                        \/\/       sminv   h19, v20.8H\n+    __ sminv(v15, __ T4S, v16);                        \/\/       sminv   s15, v16.4S\n+    __ fminv(v0, __ T4S, v1);                          \/\/       fminv   s0, v1.4S\n+\n+\/\/ TwoRegNEONOp\n+    __ absr(v4, __ T8B, v5);                           \/\/       abs     v4.8B, v5.8B\n+    __ absr(v20, __ T16B, v21);                        \/\/       abs     v20.16B, v21.16B\n+    __ absr(v11, __ T4H, v12);                         \/\/       abs     v11.4H, v12.4H\n+    __ absr(v29, __ T8H, v30);                         \/\/       abs     v29.8H, v30.8H\n+    __ absr(v15, __ T2S, v16);                         \/\/       abs     v15.2S, v16.2S\n+    __ absr(v21, __ T4S, v22);                         \/\/       abs     v21.4S, v22.4S\n+    __ absr(v4, __ T2D, v5);                           \/\/       abs     v4.2D, v5.2D\n+    __ fabs(v14, __ T2S, v15);                         \/\/       fabs    v14.2S, v15.2S\n+    __ fabs(v22, __ T4S, v23);                         \/\/       fabs    v22.4S, v23.4S\n+    __ fabs(v25, __ T2D, v26);                         \/\/       fabs    v25.2D, v26.2D\n+    __ fneg(v6, __ T2S, v7);                           \/\/       fneg    v6.2S, v7.2S\n+    __ fneg(v12, __ T4S, v13);                         \/\/       fneg    v12.4S, v13.4S\n+    __ fneg(v14, __ T2D, v15);                         \/\/       fneg    v14.2D, v15.2D\n+    __ fsqrt(v13, __ T2S, v14);                        \/\/       fsqrt   v13.2S, v14.2S\n+    __ fsqrt(v14, __ T4S, v15);                        \/\/       fsqrt   v14.4S, v15.4S\n+    __ fsqrt(v9, __ T2D, v10);                         \/\/       fsqrt   v9.2D, v10.2D\n+    __ notr(v25, __ T8B, v26);                         \/\/       not     v25.8B, v26.8B\n+    __ notr(v28, __ T16B, v29);                        \/\/       not     v28.16B, v29.16B\n+\n+\/\/ ThreeRegNEONOp\n+    __ andr(v10, __ T8B, v11, v12);                    \/\/       and     v10.8B, v11.8B, v12.8B\n+    __ andr(v19, __ T16B, v20, v21);                   \/\/       and     v19.16B, v20.16B, v21.16B\n+    __ orr(v11, __ T8B, v12, v13);                     \/\/       orr     v11.8B, v12.8B, v13.8B\n+    __ orr(v17, __ T16B, v18, v19);                    \/\/       orr     v17.16B, v18.16B, v19.16B\n+    __ eor(v21, __ T8B, v22, v23);                     \/\/       eor     v21.8B, v22.8B, v23.8B\n+    __ eor(v15, __ T16B, v16, v17);                    \/\/       eor     v15.16B, v16.16B, v17.16B\n+    __ addv(v20, __ T8B, v21, v22);                    \/\/       add     v20.8B, v21.8B, v22.8B\n+    __ addv(v23, __ T16B, v24, v25);                   \/\/       add     v23.16B, v24.16B, v25.16B\n+    __ addv(v26, __ T4H, v27, v28);                    \/\/       add     v26.4H, v27.4H, v28.4H\n+    __ addv(v5, __ T8H, v6, v7);                       \/\/       add     v5.8H, v6.8H, v7.8H\n+    __ addv(v6, __ T2S, v7, v8);                       \/\/       add     v6.2S, v7.2S, v8.2S\n+    __ addv(v15, __ T4S, v16, v17);                    \/\/       add     v15.4S, v16.4S, v17.4S\n+    __ addv(v15, __ T2D, v16, v17);                    \/\/       add     v15.2D, v16.2D, v17.2D\n+    __ fadd(v25, __ T2S, v26, v27);                    \/\/       fadd    v25.2S, v26.2S, v27.2S\n+    __ fadd(v16, __ T4S, v17, v18);                    \/\/       fadd    v16.4S, v17.4S, v18.4S\n+    __ fadd(v27, __ T2D, v28, v29);                    \/\/       fadd    v27.2D, v28.2D, v29.2D\n+    __ subv(v24, __ T8B, v25, v26);                    \/\/       sub     v24.8B, v25.8B, v26.8B\n+    __ subv(v15, __ T16B, v16, v17);                   \/\/       sub     v15.16B, v16.16B, v17.16B\n+    __ subv(v25, __ T4H, v26, v27);                    \/\/       sub     v25.4H, v26.4H, v27.4H\n+    __ subv(v14, __ T8H, v15, v16);                    \/\/       sub     v14.8H, v15.8H, v16.8H\n+    __ subv(v10, __ T2S, v11, v12);                    \/\/       sub     v10.2S, v11.2S, v12.2S\n+    __ subv(v13, __ T4S, v14, v15);                    \/\/       sub     v13.4S, v14.4S, v15.4S\n+    __ subv(v14, __ T2D, v15, v16);                    \/\/       sub     v14.2D, v15.2D, v16.2D\n+    __ fsub(v20, __ T2S, v21, v22);                    \/\/       fsub    v20.2S, v21.2S, v22.2S\n+    __ fsub(v1, __ T4S, v2, v3);                       \/\/       fsub    v1.4S, v2.4S, v3.4S\n+    __ fsub(v22, __ T2D, v23, v24);                    \/\/       fsub    v22.2D, v23.2D, v24.2D\n+    __ mulv(v30, __ T8B, v31, v0);                     \/\/       mul     v30.8B, v31.8B, v0.8B\n+    __ mulv(v14, __ T16B, v15, v16);                   \/\/       mul     v14.16B, v15.16B, v16.16B\n+    __ mulv(v2, __ T4H, v3, v4);                       \/\/       mul     v2.4H, v3.4H, v4.4H\n+    __ mulv(v6, __ T8H, v7, v8);                       \/\/       mul     v6.8H, v7.8H, v8.8H\n+    __ mulv(v3, __ T2S, v4, v5);                       \/\/       mul     v3.2S, v4.2S, v5.2S\n+    __ mulv(v7, __ T4S, v8, v9);                       \/\/       mul     v7.4S, v8.4S, v9.4S\n+    __ fmul(v24, __ T2S, v25, v26);                    \/\/       fmul    v24.2S, v25.2S, v26.2S\n+    __ fmul(v0, __ T4S, v1, v2);                       \/\/       fmul    v0.4S, v1.4S, v2.4S\n+    __ fmul(v27, __ T2D, v28, v29);                    \/\/       fmul    v27.2D, v28.2D, v29.2D\n+    __ mlav(v29, __ T4H, v30, v31);                    \/\/       mla     v29.4H, v30.4H, v31.4H\n+    __ mlav(v5, __ T8H, v6, v7);                       \/\/       mla     v5.8H, v6.8H, v7.8H\n+    __ mlav(v5, __ T2S, v6, v7);                       \/\/       mla     v5.2S, v6.2S, v7.2S\n+    __ mlav(v29, __ T4S, v30, v31);                    \/\/       mla     v29.4S, v30.4S, v31.4S\n+    __ fmla(v11, __ T2S, v12, v13);                    \/\/       fmla    v11.2S, v12.2S, v13.2S\n+    __ fmla(v25, __ T4S, v26, v27);                    \/\/       fmla    v25.4S, v26.4S, v27.4S\n+    __ fmla(v0, __ T2D, v1, v2);                       \/\/       fmla    v0.2D, v1.2D, v2.2D\n+    __ mlsv(v30, __ T4H, v31, v0);                     \/\/       mls     v30.4H, v31.4H, v0.4H\n+    __ mlsv(v0, __ T8H, v1, v2);                       \/\/       mls     v0.8H, v1.8H, v2.8H\n+    __ mlsv(v17, __ T2S, v18, v19);                    \/\/       mls     v17.2S, v18.2S, v19.2S\n+    __ mlsv(v28, __ T4S, v29, v30);                    \/\/       mls     v28.4S, v29.4S, v30.4S\n+    __ fmls(v25, __ T2S, v26, v27);                    \/\/       fmls    v25.2S, v26.2S, v27.2S\n+    __ fmls(v9, __ T4S, v10, v11);                     \/\/       fmls    v9.4S, v10.4S, v11.4S\n+    __ fmls(v25, __ T2D, v26, v27);                    \/\/       fmls    v25.2D, v26.2D, v27.2D\n+    __ fdiv(v12, __ T2S, v13, v14);                    \/\/       fdiv    v12.2S, v13.2S, v14.2S\n+    __ fdiv(v15, __ T4S, v16, v17);                    \/\/       fdiv    v15.4S, v16.4S, v17.4S\n+    __ fdiv(v11, __ T2D, v12, v13);                    \/\/       fdiv    v11.2D, v12.2D, v13.2D\n+    __ maxv(v10, __ T8B, v11, v12);                    \/\/       smax    v10.8B, v11.8B, v12.8B\n+    __ maxv(v17, __ T16B, v18, v19);                   \/\/       smax    v17.16B, v18.16B, v19.16B\n+    __ maxv(v24, __ T4H, v25, v26);                    \/\/       smax    v24.4H, v25.4H, v26.4H\n+    __ maxv(v21, __ T8H, v22, v23);                    \/\/       smax    v21.8H, v22.8H, v23.8H\n+    __ maxv(v23, __ T2S, v24, v25);                    \/\/       smax    v23.2S, v24.2S, v25.2S\n+    __ maxv(v0, __ T4S, v1, v2);                       \/\/       smax    v0.4S, v1.4S, v2.4S\n+    __ fmax(v16, __ T2S, v17, v18);                    \/\/       fmax    v16.2S, v17.2S, v18.2S\n+    __ fmax(v10, __ T4S, v11, v12);                    \/\/       fmax    v10.4S, v11.4S, v12.4S\n+    __ fmax(v6, __ T2D, v7, v8);                       \/\/       fmax    v6.2D, v7.2D, v8.2D\n+    __ minv(v28, __ T8B, v29, v30);                    \/\/       smin    v28.8B, v29.8B, v30.8B\n+    __ minv(v6, __ T16B, v7, v8);                      \/\/       smin    v6.16B, v7.16B, v8.16B\n+    __ minv(v5, __ T4H, v6, v7);                       \/\/       smin    v5.4H, v6.4H, v7.4H\n+    __ minv(v5, __ T8H, v6, v7);                       \/\/       smin    v5.8H, v6.8H, v7.8H\n+    __ minv(v20, __ T2S, v21, v22);                    \/\/       smin    v20.2S, v21.2S, v22.2S\n+    __ minv(v17, __ T4S, v18, v19);                    \/\/       smin    v17.4S, v18.4S, v19.4S\n+    __ fmin(v15, __ T2S, v16, v17);                    \/\/       fmin    v15.2S, v16.2S, v17.2S\n+    __ fmin(v17, __ T4S, v18, v19);                    \/\/       fmin    v17.4S, v18.4S, v19.4S\n+    __ fmin(v29, __ T2D, v30, v31);                    \/\/       fmin    v29.2D, v30.2D, v31.2D\n+    __ cmeq(v26, __ T8B, v27, v28);                    \/\/       cmeq    v26.8B, v27.8B, v28.8B\n+    __ cmeq(v28, __ T16B, v29, v30);                   \/\/       cmeq    v28.16B, v29.16B, v30.16B\n+    __ cmeq(v1, __ T4H, v2, v3);                       \/\/       cmeq    v1.4H, v2.4H, v3.4H\n+    __ cmeq(v27, __ T8H, v28, v29);                    \/\/       cmeq    v27.8H, v28.8H, v29.8H\n+    __ cmeq(v0, __ T2S, v1, v2);                       \/\/       cmeq    v0.2S, v1.2S, v2.2S\n+    __ cmeq(v20, __ T4S, v21, v22);                    \/\/       cmeq    v20.4S, v21.4S, v22.4S\n+    __ cmeq(v28, __ T2D, v29, v30);                    \/\/       cmeq    v28.2D, v29.2D, v30.2D\n+    __ fcmeq(v15, __ T2S, v16, v17);                   \/\/       fcmeq   v15.2S, v16.2S, v17.2S\n+    __ fcmeq(v12, __ T4S, v13, v14);                   \/\/       fcmeq   v12.4S, v13.4S, v14.4S\n+    __ fcmeq(v10, __ T2D, v11, v12);                   \/\/       fcmeq   v10.2D, v11.2D, v12.2D\n+    __ cmgt(v28, __ T8B, v29, v30);                    \/\/       cmgt    v28.8B, v29.8B, v30.8B\n+    __ cmgt(v28, __ T16B, v29, v30);                   \/\/       cmgt    v28.16B, v29.16B, v30.16B\n+    __ cmgt(v19, __ T4H, v20, v21);                    \/\/       cmgt    v19.4H, v20.4H, v21.4H\n+    __ cmgt(v22, __ T8H, v23, v24);                    \/\/       cmgt    v22.8H, v23.8H, v24.8H\n+    __ cmgt(v10, __ T2S, v11, v12);                    \/\/       cmgt    v10.2S, v11.2S, v12.2S\n+    __ cmgt(v4, __ T4S, v5, v6);                       \/\/       cmgt    v4.4S, v5.4S, v6.4S\n+    __ cmgt(v30, __ T2D, v31, v0);                     \/\/       cmgt    v30.2D, v31.2D, v0.2D\n+    __ fcmgt(v20, __ T2S, v21, v22);                   \/\/       fcmgt   v20.2S, v21.2S, v22.2S\n+    __ fcmgt(v8, __ T4S, v9, v10);                     \/\/       fcmgt   v8.4S, v9.4S, v10.4S\n+    __ fcmgt(v30, __ T2D, v31, v0);                    \/\/       fcmgt   v30.2D, v31.2D, v0.2D\n+    __ cmge(v17, __ T8B, v18, v19);                    \/\/       cmge    v17.8B, v18.8B, v19.8B\n+    __ cmge(v10, __ T16B, v11, v12);                   \/\/       cmge    v10.16B, v11.16B, v12.16B\n+    __ cmge(v27, __ T4H, v28, v29);                    \/\/       cmge    v27.4H, v28.4H, v29.4H\n+    __ cmge(v2, __ T8H, v3, v4);                       \/\/       cmge    v2.8H, v3.8H, v4.8H\n+    __ cmge(v24, __ T2S, v25, v26);                    \/\/       cmge    v24.2S, v25.2S, v26.2S\n+    __ cmge(v4, __ T4S, v5, v6);                       \/\/       cmge    v4.4S, v5.4S, v6.4S\n+    __ cmge(v3, __ T2D, v4, v5);                       \/\/       cmge    v3.2D, v4.2D, v5.2D\n+    __ fcmge(v8, __ T2S, v9, v10);                     \/\/       fcmge   v8.2S, v9.2S, v10.2S\n+    __ fcmge(v22, __ T4S, v23, v24);                   \/\/       fcmge   v22.4S, v23.4S, v24.4S\n+    __ fcmge(v17, __ T2D, v18, v19);                   \/\/       fcmge   v17.2D, v18.2D, v19.2D\n+\n@@ -618,4 +759,4 @@\n-    __ sha512h(v22, __ T2D, v27, v4);                  \/\/       sha512h         q22, q27, v4.2D\n-    __ sha512h2(v7, __ T2D, v6, v1);                   \/\/       sha512h2                q7, q6, v1.2D\n-    __ sha512su0(v26, __ T2D, v15);                    \/\/       sha512su0               v26.2D, v15.2D\n-    __ sha512su1(v2, __ T2D, v13, v13);                \/\/       sha512su1               v2.2D, v13.2D, v13.2D\n+    __ sha512h(v13, __ T2D, v4, v28);                  \/\/       sha512h         q13, q4, v28.2D\n+    __ sha512h2(v23, __ T2D, v21, v25);                \/\/       sha512h2                q23, q21, v25.2D\n+    __ sha512su0(v24, __ T2D, v3);                     \/\/       sha512su0               v24.2D, v3.2D\n+    __ sha512su1(v23, __ T2D, v26, v23);               \/\/       sha512su1               v23.2D, v26.2D, v23.2D\n@@ -708,9 +849,9 @@\n-    __ swp(Assembler::xword, r24, r24, r4);            \/\/       swp     x24, x24, [x4]\n-    __ ldadd(Assembler::xword, r20, r16, r0);          \/\/       ldadd   x20, x16, [x0]\n-    __ ldbic(Assembler::xword, r4, r21, r11);          \/\/       ldclr   x4, x21, [x11]\n-    __ ldeor(Assembler::xword, r30, r16, r22);         \/\/       ldeor   x30, x16, [x22]\n-    __ ldorr(Assembler::xword, r4, r15, r23);          \/\/       ldset   x4, x15, [x23]\n-    __ ldsmin(Assembler::xword, r26, r6, r12);         \/\/       ldsmin  x26, x6, [x12]\n-    __ ldsmax(Assembler::xword, r15, r14, r15);        \/\/       ldsmax  x15, x14, [x15]\n-    __ ldumin(Assembler::xword, r9, r25, r29);         \/\/       ldumin  x9, x25, [x29]\n-    __ ldumax(Assembler::xword, r11, r20, r12);        \/\/       ldumax  x11, x20, [x12]\n+    __ swp(Assembler::xword, r15, r21, r3);            \/\/       swp     x15, x21, [x3]\n+    __ ldadd(Assembler::xword, r24, r8, r25);          \/\/       ldadd   x24, x8, [x25]\n+    __ ldbic(Assembler::xword, r20, r16, r17);         \/\/       ldclr   x20, x16, [x17]\n+    __ ldeor(Assembler::xword, r2, r1, r0);            \/\/       ldeor   x2, x1, [x0]\n+    __ ldorr(Assembler::xword, r24, r4, r3);           \/\/       ldset   x24, x4, [x3]\n+    __ ldsmin(Assembler::xword, r12, zr, r28);         \/\/       ldsmin  x12, xzr, [x28]\n+    __ ldsmax(Assembler::xword, r10, r26, r2);         \/\/       ldsmax  x10, x26, [x2]\n+    __ ldumin(Assembler::xword, r12, r16, sp);         \/\/       ldumin  x12, x16, [sp]\n+    __ ldumax(Assembler::xword, r1, r13, r29);         \/\/       ldumax  x1, x13, [x29]\n@@ -719,9 +860,9 @@\n-    __ swpa(Assembler::xword, r16, r22, r16);          \/\/       swpa    x16, x22, [x16]\n-    __ ldadda(Assembler::xword, r21, r24, r26);        \/\/       ldadda  x21, x24, [x26]\n-    __ ldbica(Assembler::xword, r6, r6, r16);          \/\/       ldclra  x6, x6, [x16]\n-    __ ldeora(Assembler::xword, r16, r25, r16);        \/\/       ldeora  x16, x25, [x16]\n-    __ ldorra(Assembler::xword, r28, r24, r16);        \/\/       ldseta  x28, x24, [x16]\n-    __ ldsmina(Assembler::xword, r26, r15, r10);       \/\/       ldsmina x26, x15, [x10]\n-    __ ldsmaxa(Assembler::xword, r13, r14, r20);       \/\/       ldsmaxa x13, x14, [x20]\n-    __ ldumina(Assembler::xword, r1, r23, r30);        \/\/       ldumina x1, x23, [x30]\n-    __ ldumaxa(Assembler::xword, r14, r2, r6);         \/\/       ldumaxa x14, x2, [x6]\n+    __ swpa(Assembler::xword, r0, r19, r12);           \/\/       swpa    x0, x19, [x12]\n+    __ ldadda(Assembler::xword, r17, r22, r13);        \/\/       ldadda  x17, x22, [x13]\n+    __ ldbica(Assembler::xword, r28, r30, sp);         \/\/       ldclra  x28, x30, [sp]\n+    __ ldeora(Assembler::xword, r1, r26, r28);         \/\/       ldeora  x1, x26, [x28]\n+    __ ldorra(Assembler::xword, r4, r30, r4);          \/\/       ldseta  x4, x30, [x4]\n+    __ ldsmina(Assembler::xword, r6, r30, r26);        \/\/       ldsmina x6, x30, [x26]\n+    __ ldsmaxa(Assembler::xword, r16, r9, r8);         \/\/       ldsmaxa x16, x9, [x8]\n+    __ ldumina(Assembler::xword, r12, r0, r20);        \/\/       ldumina x12, x0, [x20]\n+    __ ldumaxa(Assembler::xword, r1, r24, r2);         \/\/       ldumaxa x1, x24, [x2]\n@@ -730,9 +871,9 @@\n-    __ swpal(Assembler::xword, r3, r8, r25);           \/\/       swpal   x3, x8, [x25]\n-    __ ldaddal(Assembler::xword, r0, r27, r30);        \/\/       ldaddal x0, x27, [x30]\n-    __ ldbical(Assembler::xword, r5, r5, r30);         \/\/       ldclral x5, x5, [x30]\n-    __ ldeoral(Assembler::xword, r11, r25, r0);        \/\/       ldeoral x11, x25, [x0]\n-    __ ldorral(Assembler::xword, zr, r0, r19);         \/\/       ldsetal xzr, x0, [x19]\n-    __ ldsminal(Assembler::xword, r29, r26, r9);       \/\/       ldsminal        x29, x26, [x9]\n-    __ ldsmaxal(Assembler::xword, r26, r12, r15);      \/\/       ldsmaxal        x26, x12, [x15]\n-    __ lduminal(Assembler::xword, r11, r11, r15);      \/\/       lduminal        x11, x11, [x15]\n-    __ ldumaxal(Assembler::xword, r25, r22, r24);      \/\/       ldumaxal        x25, x22, [x24]\n+    __ swpal(Assembler::xword, r0, r9, r24);           \/\/       swpal   x0, x9, [x24]\n+    __ ldaddal(Assembler::xword, r26, r16, r30);       \/\/       ldaddal x26, x16, [x30]\n+    __ ldbical(Assembler::xword, r3, r10, r23);        \/\/       ldclral x3, x10, [x23]\n+    __ ldeoral(Assembler::xword, r10, r4, r15);        \/\/       ldeoral x10, x4, [x15]\n+    __ ldorral(Assembler::xword, r2, r11, r8);         \/\/       ldsetal x2, x11, [x8]\n+    __ ldsminal(Assembler::xword, r10, r15, r17);      \/\/       ldsminal        x10, x15, [x17]\n+    __ ldsmaxal(Assembler::xword, r2, r10, r12);       \/\/       ldsmaxal        x2, x10, [x12]\n+    __ lduminal(Assembler::xword, r12, r15, r13);      \/\/       lduminal        x12, x15, [x13]\n+    __ ldumaxal(Assembler::xword, r2, r7, r20);        \/\/       ldumaxal        x2, x7, [x20]\n@@ -741,9 +882,9 @@\n-    __ swpl(Assembler::xword, r0, r17, r11);           \/\/       swpl    x0, x17, [x11]\n-    __ ldaddl(Assembler::xword, r6, r29, r6);          \/\/       ldaddl  x6, x29, [x6]\n-    __ ldbicl(Assembler::xword, r5, r5, r21);          \/\/       ldclrl  x5, x5, [x21]\n-    __ ldeorl(Assembler::xword, r19, r16, r15);        \/\/       ldeorl  x19, x16, [x15]\n-    __ ldorrl(Assembler::xword, r30, r27, r28);        \/\/       ldsetl  x30, x27, [x28]\n-    __ ldsminl(Assembler::xword, r1, r28, r1);         \/\/       ldsminl x1, x28, [x1]\n-    __ ldsmaxl(Assembler::xword, r20, r29, r16);       \/\/       ldsmaxl x20, x29, [x16]\n-    __ lduminl(Assembler::xword, r13, r10, r29);       \/\/       lduminl x13, x10, [x29]\n-    __ ldumaxl(Assembler::xword, r29, r19, r22);       \/\/       ldumaxl x29, x19, [x22]\n+    __ swpl(Assembler::xword, r26, r16, r4);           \/\/       swpl    x26, x16, [x4]\n+    __ ldaddl(Assembler::xword, r2, r4, r12);          \/\/       ldaddl  x2, x4, [x12]\n+    __ ldbicl(Assembler::xword, r16, r21, r16);        \/\/       ldclrl  x16, x21, [x16]\n+    __ ldeorl(Assembler::xword, r16, r11, r21);        \/\/       ldeorl  x16, x11, [x21]\n+    __ ldorrl(Assembler::xword, r23, r12, r26);        \/\/       ldsetl  x23, x12, [x26]\n+    __ ldsminl(Assembler::xword, r23, r28, r14);       \/\/       ldsminl x23, x28, [x14]\n+    __ ldsmaxl(Assembler::xword, r11, r24, r1);        \/\/       ldsmaxl x11, x24, [x1]\n+    __ lduminl(Assembler::xword, r12, zr, r10);        \/\/       lduminl x12, xzr, [x10]\n+    __ ldumaxl(Assembler::xword, r16, r7, r2);         \/\/       ldumaxl x16, x7, [x2]\n@@ -752,9 +893,9 @@\n-    __ swp(Assembler::word, r10, r4, sp);              \/\/       swp     w10, w4, [sp]\n-    __ ldadd(Assembler::word, r21, r8, sp);            \/\/       ldadd   w21, w8, [sp]\n-    __ ldbic(Assembler::word, r19, r10, r28);          \/\/       ldclr   w19, w10, [x28]\n-    __ ldeor(Assembler::word, r2, r25, r5);            \/\/       ldeor   w2, w25, [x5]\n-    __ ldorr(Assembler::word, r3, r8, r22);            \/\/       ldset   w3, w8, [x22]\n-    __ ldsmin(Assembler::word, r19, r13, r5);          \/\/       ldsmin  w19, w13, [x5]\n-    __ ldsmax(Assembler::word, r29, r24, r21);         \/\/       ldsmax  w29, w24, [x21]\n-    __ ldumin(Assembler::word, r26, r24, r3);          \/\/       ldumin  w26, w24, [x3]\n-    __ ldumax(Assembler::word, r24, r26, r23);         \/\/       ldumax  w24, w26, [x23]\n+    __ swp(Assembler::word, r3, r13, r19);             \/\/       swp     w3, w13, [x19]\n+    __ ldadd(Assembler::word, r17, r16, r3);           \/\/       ldadd   w17, w16, [x3]\n+    __ ldbic(Assembler::word, r1, r11, r30);           \/\/       ldclr   w1, w11, [x30]\n+    __ ldeor(Assembler::word, r5, r8, r15);            \/\/       ldeor   w5, w8, [x15]\n+    __ ldorr(Assembler::word, r29, r30, r0);           \/\/       ldset   w29, w30, [x0]\n+    __ ldsmin(Assembler::word, r20, r7, r20);          \/\/       ldsmin  w20, w7, [x20]\n+    __ ldsmax(Assembler::word, r23, r28, r21);         \/\/       ldsmax  w23, w28, [x21]\n+    __ ldumin(Assembler::word, r27, r25, r5);          \/\/       ldumin  w27, w25, [x5]\n+    __ ldumax(Assembler::word, r1, r23, r16);          \/\/       ldumax  w1, w23, [x16]\n@@ -763,9 +904,9 @@\n-    __ swpa(Assembler::word, r15, r21, r3);            \/\/       swpa    w15, w21, [x3]\n-    __ ldadda(Assembler::word, r24, r8, r25);          \/\/       ldadda  w24, w8, [x25]\n-    __ ldbica(Assembler::word, r20, r16, r17);         \/\/       ldclra  w20, w16, [x17]\n-    __ ldeora(Assembler::word, r2, r1, r0);            \/\/       ldeora  w2, w1, [x0]\n-    __ ldorra(Assembler::word, r24, r4, r3);           \/\/       ldseta  w24, w4, [x3]\n-    __ ldsmina(Assembler::word, r12, zr, r28);         \/\/       ldsmina w12, wzr, [x28]\n-    __ ldsmaxa(Assembler::word, r10, r26, r2);         \/\/       ldsmaxa w10, w26, [x2]\n-    __ ldumina(Assembler::word, r12, r16, sp);         \/\/       ldumina w12, w16, [sp]\n-    __ ldumaxa(Assembler::word, r1, r13, r29);         \/\/       ldumaxa w1, w13, [x29]\n+    __ swpa(Assembler::word, zr, r5, r12);             \/\/       swpa    wzr, w5, [x12]\n+    __ ldadda(Assembler::word, r9, r28, r15);          \/\/       ldadda  w9, w28, [x15]\n+    __ ldbica(Assembler::word, r29, r22, sp);          \/\/       ldclra  w29, w22, [sp]\n+    __ ldeora(Assembler::word, r19, zr, r5);           \/\/       ldeora  w19, wzr, [x5]\n+    __ ldorra(Assembler::word, r14, r16, sp);          \/\/       ldseta  w14, w16, [sp]\n+    __ ldsmina(Assembler::word, r16, r27, r20);        \/\/       ldsmina w16, w27, [x20]\n+    __ ldsmaxa(Assembler::word, r16, r12, r11);        \/\/       ldsmaxa w16, w12, [x11]\n+    __ ldumina(Assembler::word, r9, r6, r30);          \/\/       ldumina w9, w6, [x30]\n+    __ ldumaxa(Assembler::word, r17, r27, r28);        \/\/       ldumaxa w17, w27, [x28]\n@@ -774,9 +915,9 @@\n-    __ swpal(Assembler::word, r0, r19, r12);           \/\/       swpal   w0, w19, [x12]\n-    __ ldaddal(Assembler::word, r17, r22, r13);        \/\/       ldaddal w17, w22, [x13]\n-    __ ldbical(Assembler::word, r28, r30, sp);         \/\/       ldclral w28, w30, [sp]\n-    __ ldeoral(Assembler::word, r1, r26, r28);         \/\/       ldeoral w1, w26, [x28]\n-    __ ldorral(Assembler::word, r4, r30, r4);          \/\/       ldsetal w4, w30, [x4]\n-    __ ldsminal(Assembler::word, r6, r30, r26);        \/\/       ldsminal        w6, w30, [x26]\n-    __ ldsmaxal(Assembler::word, r16, r9, r8);         \/\/       ldsmaxal        w16, w9, [x8]\n-    __ lduminal(Assembler::word, r12, r0, r20);        \/\/       lduminal        w12, w0, [x20]\n-    __ ldumaxal(Assembler::word, r1, r24, r2);         \/\/       ldumaxal        w1, w24, [x2]\n+    __ swpal(Assembler::word, r30, r7, r10);           \/\/       swpal   w30, w7, [x10]\n+    __ ldaddal(Assembler::word, r20, r10, r4);         \/\/       ldaddal w20, w10, [x4]\n+    __ ldbical(Assembler::word, r24, r17, r17);        \/\/       ldclral w24, w17, [x17]\n+    __ ldeoral(Assembler::word, r22, r3, r29);         \/\/       ldeoral w22, w3, [x29]\n+    __ ldorral(Assembler::word, r15, r22, r19);        \/\/       ldsetal w15, w22, [x19]\n+    __ ldsminal(Assembler::word, r19, r22, r2);        \/\/       ldsminal        w19, w22, [x2]\n+    __ ldsmaxal(Assembler::word, r15, r6, r12);        \/\/       ldsmaxal        w15, w6, [x12]\n+    __ lduminal(Assembler::word, r16, r11, r13);       \/\/       lduminal        w16, w11, [x13]\n+    __ ldumaxal(Assembler::word, r23, r1, r30);        \/\/       ldumaxal        w23, w1, [x30]\n@@ -785,9 +926,9 @@\n-    __ swpl(Assembler::word, r0, r9, r24);             \/\/       swpl    w0, w9, [x24]\n-    __ ldaddl(Assembler::word, r26, r16, r30);         \/\/       ldaddl  w26, w16, [x30]\n-    __ ldbicl(Assembler::word, r3, r10, r23);          \/\/       ldclrl  w3, w10, [x23]\n-    __ ldeorl(Assembler::word, r10, r4, r15);          \/\/       ldeorl  w10, w4, [x15]\n-    __ ldorrl(Assembler::word, r2, r11, r8);           \/\/       ldsetl  w2, w11, [x8]\n-    __ ldsminl(Assembler::word, r10, r15, r17);        \/\/       ldsminl w10, w15, [x17]\n-    __ ldsmaxl(Assembler::word, r2, r10, r12);         \/\/       ldsmaxl w2, w10, [x12]\n-    __ lduminl(Assembler::word, r12, r15, r13);        \/\/       lduminl w12, w15, [x13]\n-    __ ldumaxl(Assembler::word, r2, r7, r20);          \/\/       ldumaxl w2, w7, [x20]\n+    __ swpl(Assembler::word, r19, r5, r17);            \/\/       swpl    w19, w5, [x17]\n+    __ ldaddl(Assembler::word, r2, r16, r22);          \/\/       ldaddl  w2, w16, [x22]\n+    __ ldbicl(Assembler::word, r13, r10, r21);         \/\/       ldclrl  w13, w10, [x21]\n+    __ ldeorl(Assembler::word, r29, r27, r12);         \/\/       ldeorl  w29, w27, [x12]\n+    __ ldorrl(Assembler::word, r27, r3, r1);           \/\/       ldsetl  w27, w3, [x1]\n+    __ ldsminl(Assembler::word, zr, r24, r19);         \/\/       ldsminl wzr, w24, [x19]\n+    __ ldsmaxl(Assembler::word, r17, r9, r28);         \/\/       ldsmaxl w17, w9, [x28]\n+    __ lduminl(Assembler::word, r27, r15, r7);         \/\/       lduminl w27, w15, [x7]\n+    __ ldumaxl(Assembler::word, r21, r23, sp);         \/\/       ldumaxl w21, w23, [sp]\n@@ -796,38 +937,38 @@\n-    __ sve_add(z25, __ B, z15, z4);                    \/\/       add     z25.b, z15.b, z4.b\n-    __ sve_sub(z4, __ S, z11, z17);                    \/\/       sub     z4.s, z11.s, z17.s\n-    __ sve_fadd(z16, __ D, z17, z10);                  \/\/       fadd    z16.d, z17.d, z10.d\n-    __ sve_fmul(z22, __ D, z12, z25);                  \/\/       fmul    z22.d, z12.d, z25.d\n-    __ sve_fsub(z28, __ D, z14, z10);                  \/\/       fsub    z28.d, z14.d, z10.d\n-    __ sve_abs(z1, __ H, p3, z30);                     \/\/       abs     z1.h, p3\/m, z30.h\n-    __ sve_add(z15, __ B, p1, z2);                     \/\/       add     z15.b, p1\/m, z15.b, z2.b\n-    __ sve_asr(z13, __ S, p4, z16);                    \/\/       asr     z13.s, p4\/m, z13.s, z16.s\n-    __ sve_cnt(z3, __ D, p0, z11);                     \/\/       cnt     z3.d, p0\/m, z11.d\n-    __ sve_lsl(z5, __ D, p2, z14);                     \/\/       lsl     z5.d, p2\/m, z5.d, z14.d\n-    __ sve_lsr(z29, __ B, p0, z20);                    \/\/       lsr     z29.b, p0\/m, z29.b, z20.b\n-    __ sve_mul(z20, __ S, p5, z27);                    \/\/       mul     z20.s, p5\/m, z20.s, z27.s\n-    __ sve_neg(z26, __ B, p6, z4);                     \/\/       neg     z26.b, p6\/m, z4.b\n-    __ sve_not(z22, __ B, p4, z30);                    \/\/       not     z22.b, p4\/m, z30.b\n-    __ sve_smax(z11, __ H, p2, z27);                   \/\/       smax    z11.h, p2\/m, z11.h, z27.h\n-    __ sve_smin(z28, __ S, p5, z30);                   \/\/       smin    z28.s, p5\/m, z28.s, z30.s\n-    __ sve_sub(z30, __ S, p1, z13);                    \/\/       sub     z30.s, p1\/m, z30.s, z13.s\n-    __ sve_fabs(z30, __ D, p4, z26);                   \/\/       fabs    z30.d, p4\/m, z26.d\n-    __ sve_fadd(z15, __ S, p3, z11);                   \/\/       fadd    z15.s, p3\/m, z15.s, z11.s\n-    __ sve_fdiv(z6, __ D, p7, z16);                    \/\/       fdiv    z6.d, p7\/m, z6.d, z16.d\n-    __ sve_fmax(z27, __ S, p7, z7);                    \/\/       fmax    z27.s, p7\/m, z27.s, z7.s\n-    __ sve_fmin(z19, __ D, p2, z4);                    \/\/       fmin    z19.d, p2\/m, z19.d, z4.d\n-    __ sve_fmul(z17, __ S, p4, z22);                   \/\/       fmul    z17.s, p4\/m, z17.s, z22.s\n-    __ sve_fneg(z28, __ D, p3, z21);                   \/\/       fneg    z28.d, p3\/m, z21.d\n-    __ sve_frintm(z17, __ S, p5, z2);                  \/\/       frintm  z17.s, p5\/m, z2.s\n-    __ sve_frintn(z6, __ S, p3, z15);                  \/\/       frintn  z6.s, p3\/m, z15.s\n-    __ sve_frintp(z12, __ D, p5, z1);                  \/\/       frintp  z12.d, p5\/m, z1.d\n-    __ sve_fsqrt(z17, __ S, p1, z17);                  \/\/       fsqrt   z17.s, p1\/m, z17.s\n-    __ sve_fsub(z15, __ S, p5, z13);                   \/\/       fsub    z15.s, p5\/m, z15.s, z13.s\n-    __ sve_fmla(z20, __ D, p7, z27, z11);              \/\/       fmla    z20.d, p7\/m, z27.d, z11.d\n-    __ sve_fmls(z3, __ D, p0, z30, z23);               \/\/       fmls    z3.d, p0\/m, z30.d, z23.d\n-    __ sve_fnmla(z17, __ S, p2, z27, z26);             \/\/       fnmla   z17.s, p2\/m, z27.s, z26.s\n-    __ sve_fnmls(z6, __ D, p5, z22, z30);              \/\/       fnmls   z6.d, p5\/m, z22.d, z30.d\n-    __ sve_mla(z2, __ H, p7, z26, z17);                \/\/       mla     z2.h, p7\/m, z26.h, z17.h\n-    __ sve_mls(z22, __ B, p4, z2, z17);                \/\/       mls     z22.b, p4\/m, z2.b, z17.b\n-    __ sve_and(z24, z25, z22);                         \/\/       and     z24.d, z25.d, z22.d\n-    __ sve_eor(z17, z12, z3);                          \/\/       eor     z17.d, z12.d, z3.d\n-    __ sve_orr(z29, z28, z16);                         \/\/       orr     z29.d, z28.d, z16.d\n+    __ sve_add(z24, __ D, z2, z30);                    \/\/       add     z24.d, z2.d, z30.d\n+    __ sve_sub(z17, __ S, z10, z22);                   \/\/       sub     z17.s, z10.s, z22.s\n+    __ sve_fadd(z2, __ D, z17, z0);                    \/\/       fadd    z2.d, z17.d, z0.d\n+    __ sve_fmul(z25, __ D, z22, z2);                   \/\/       fmul    z25.d, z22.d, z2.d\n+    __ sve_fsub(z12, __ D, z3, z27);                   \/\/       fsub    z12.d, z3.d, z27.d\n+    __ sve_abs(z28, __ B, p4, z26);                    \/\/       abs     z28.b, p4\/m, z26.b\n+    __ sve_add(z9, __ B, p7, z17);                     \/\/       add     z9.b, p7\/m, z9.b, z17.b\n+    __ sve_asr(z4, __ H, p1, z15);                     \/\/       asr     z4.h, p1\/m, z4.h, z15.h\n+    __ sve_cnt(z22, __ D, p2, z2);                     \/\/       cnt     z22.d, p2\/m, z2.d\n+    __ sve_lsl(z20, __ D, p7, z5);                     \/\/       lsl     z20.d, p7\/m, z20.d, z5.d\n+    __ sve_lsr(z0, __ B, p4, z14);                     \/\/       lsr     z0.b, p4\/m, z0.b, z14.b\n+    __ sve_mul(z25, __ S, p2, z27);                    \/\/       mul     z25.s, p2\/m, z25.s, z27.s\n+    __ sve_neg(z26, __ S, p6, z24);                    \/\/       neg     z26.s, p6\/m, z24.s\n+    __ sve_not(z0, __ S, p1, z6);                      \/\/       not     z0.s, p1\/m, z6.s\n+    __ sve_smax(z0, __ B, p1, z15);                    \/\/       smax    z0.b, p1\/m, z0.b, z15.b\n+    __ sve_smin(z9, __ H, p1, z5);                     \/\/       smin    z9.h, p1\/m, z9.h, z5.h\n+    __ sve_sub(z27, __ S, p1, z20);                    \/\/       sub     z27.s, p1\/m, z27.s, z20.s\n+    __ sve_fabs(z20, __ S, p1, z10);                   \/\/       fabs    z20.s, p1\/m, z10.s\n+    __ sve_fadd(z16, __ D, p7, z6);                    \/\/       fadd    z16.d, p7\/m, z16.d, z6.d\n+    __ sve_fdiv(z2, __ D, p3, z29);                    \/\/       fdiv    z2.d, p3\/m, z2.d, z29.d\n+    __ sve_fmax(z2, __ D, p6, z22);                    \/\/       fmax    z2.d, p6\/m, z2.d, z22.d\n+    __ sve_fmin(z14, __ D, p3, z27);                   \/\/       fmin    z14.d, p3\/m, z14.d, z27.d\n+    __ sve_fmul(z23, __ S, p1, z2);                    \/\/       fmul    z23.s, p1\/m, z23.s, z2.s\n+    __ sve_fneg(z10, __ D, p4, z10);                   \/\/       fneg    z10.d, p4\/m, z10.d\n+    __ sve_frintm(z22, __ D, p3, z3);                  \/\/       frintm  z22.d, p3\/m, z3.d\n+    __ sve_frintn(z16, __ D, p1, z1);                  \/\/       frintn  z16.d, p1\/m, z1.d\n+    __ sve_frintp(z16, __ S, p4, z12);                 \/\/       frintp  z16.s, p4\/m, z12.s\n+    __ sve_fsqrt(z12, __ S, p0, z16);                  \/\/       fsqrt   z12.s, p0\/m, z16.s\n+    __ sve_fsub(z20, __ S, p5, z5);                    \/\/       fsub    z20.s, p5\/m, z20.s, z5.s\n+    __ sve_fmla(z7, __ D, p4, z12, z27);               \/\/       fmla    z7.d, p4\/m, z12.d, z27.d\n+    __ sve_fmls(z16, __ S, p1, z2, z28);               \/\/       fmls    z16.s, p1\/m, z2.s, z28.s\n+    __ sve_fnmla(z4, __ S, p1, z17, z19);              \/\/       fnmla   z4.s, p1\/m, z17.s, z19.s\n+    __ sve_fnmls(z12, __ D, p5, z8, z24);              \/\/       fnmls   z12.d, p5\/m, z8.d, z24.d\n+    __ sve_mla(z17, __ B, p0, z10, z23);               \/\/       mla     z17.b, p0\/m, z10.b, z23.b\n+    __ sve_mls(z19, __ B, p7, z13, z16);               \/\/       mls     z19.b, p7\/m, z13.b, z16.b\n+    __ sve_and(z0, z7, z14);                           \/\/       and     z0.d, z7.d, z14.d\n+    __ sve_eor(z25, z8, z10);                          \/\/       eor     z25.d, z8.d, z10.d\n+    __ sve_orr(z20, z22, z27);                         \/\/       orr     z20.d, z22.d, z27.d\n@@ -836,9 +977,9 @@\n-    __ sve_andv(v6, __ S, p2, z28);                    \/\/       andv s6, p2, z28.s\n-    __ sve_orv(v7, __ H, p1, z7);                      \/\/       orv h7, p1, z7.h\n-    __ sve_eorv(v9, __ B, p5, z8);                     \/\/       eorv b9, p5, z8.b\n-    __ sve_smaxv(v27, __ B, p5, z30);                  \/\/       smaxv b27, p5, z30.b\n-    __ sve_sminv(v26, __ H, p0, z16);                  \/\/       sminv h26, p0, z16.h\n-    __ sve_fminv(v3, __ D, p6, z8);                    \/\/       fminv d3, p6, z8.d\n-    __ sve_fmaxv(v21, __ D, p6, z26);                  \/\/       fmaxv d21, p6, z26.d\n-    __ sve_fadda(v22, __ S, p0, z4);                   \/\/       fadda s22, p0, s22, z4.s\n-    __ sve_uaddv(v17, __ H, p0, z3);                   \/\/       uaddv d17, p0, z3.h\n+    __ sve_andv(v3, __ S, p3, z17);                    \/\/       andv s3, p3, z17.s\n+    __ sve_orv(v7, __ B, p1, z28);                     \/\/       orv b7, p1, z28.b\n+    __ sve_eorv(v0, __ S, p2, z16);                    \/\/       eorv s0, p2, z16.s\n+    __ sve_smaxv(v22, __ H, p1, z15);                  \/\/       smaxv h22, p1, z15.h\n+    __ sve_sminv(v22, __ B, p2, z25);                  \/\/       sminv b22, p2, z25.b\n+    __ sve_fminv(v30, __ D, p4, z13);                  \/\/       fminv d30, p4, z13.d\n+    __ sve_fmaxv(v11, __ S, p0, z13);                  \/\/       fmaxv s11, p0, z13.s\n+    __ sve_fadda(v20, __ S, p4, z25);                  \/\/       fadda s20, p4, s20, z25.s\n+    __ sve_uaddv(v4, __ H, p1, z17);                   \/\/       uaddv d4, p1, z17.h\n@@ -849,627 +990,1 @@\n-aarch64ops.o:     file format elf64-littleaarch64\n-\n-\n-Disassembly of section .text:\n-\n-0000000000000000 <back>:\n-   0:   8b0d82fa        add     x26, x23, x13, lsl #32\n-   4:   cb49970c        sub     x12, x24, x9, lsr #37\n-   8:   ab889dfc        adds    x28, x15, x8, asr #39\n-   c:   eb9ee787        subs    x7, x28, x30, asr #57\n-  10:   0b9b3ec9        add     w9, w22, w27, asr #15\n-  14:   4b9179a3        sub     w3, w13, w17, asr #30\n-  18:   2b88474e        adds    w14, w26, w8, asr #17\n-  1c:   6b8c56c0        subs    w0, w22, w12, asr #21\n-  20:   8a1a51e0        and     x0, x15, x26, lsl #20\n-  24:   aa11f4ba        orr     x26, x5, x17, lsl #61\n-  28:   ca0281b8        eor     x24, x13, x2, lsl #32\n-  2c:   ea918c7c        ands    x28, x3, x17, asr #35\n-  30:   0a5d4a19        and     w25, w16, w29, lsr #18\n-  34:   2a4b262d        orr     w13, w17, w11, lsr #9\n-  38:   4a513ca5        eor     w5, w5, w17, lsr #15\n-  3c:   6a9b6ae2        ands    w2, w23, w27, asr #26\n-  40:   8a70b79b        bic     x27, x28, x16, lsr #45\n-  44:   aaba9728        orn     x8, x25, x26, asr #37\n-  48:   ca6dfe3d        eon     x29, x17, x13, lsr #63\n-  4c:   ea627f1c        bics    x28, x24, x2, lsr #31\n-  50:   0aa70f53        bic     w19, w26, w7, asr #3\n-  54:   2aaa0f06        orn     w6, w24, w10, asr #3\n-  58:   4a6176a4        eon     w4, w21, w1, lsr #29\n-  5c:   6a604eb0        bics    w16, w21, w0, lsr #19\n-  60:   1105ed91        add     w17, w12, #0x17b\n-  64:   3100583e        adds    w30, w1, #0x16\n-  68:   5101f8bd        sub     w29, w5, #0x7e\n-  6c:   710f0306        subs    w6, w24, #0x3c0\n-  70:   9101a1a0        add     x0, x13, #0x68\n-  74:   b10a5cc8        adds    x8, x6, #0x297\n-  78:   d10810aa        sub     x10, x5, #0x204\n-  7c:   f10fd061        subs    x1, x3, #0x3f4\n-  80:   120cb166        and     w6, w11, #0xfff1fff1\n-  84:   321764bc        orr     w28, w5, #0xfffffe07\n-  88:   52174681        eor     w1, w20, #0x7fffe00\n-  8c:   720c0227        ands    w7, w17, #0x100000\n-  90:   9241018e        and     x14, x12, #0x8000000000000000\n-  94:   b25a2969        orr     x9, x11, #0x1ffc000000000\n-  98:   d278b411        eor     x17, x0, #0x3fffffffffff00\n-  9c:   f26aad01        ands    x1, x8, #0xffffffffffc00003\n-  a0:   14000000        b       a0 <back+0xa0>\n-  a4:   17ffffd7        b       0 <back>\n-  a8:   14000242        b       9b0 <forth>\n-  ac:   94000000        bl      ac <back+0xac>\n-  b0:   97ffffd4        bl      0 <back>\n-  b4:   9400023f        bl      9b0 <forth>\n-  b8:   3400000a        cbz     w10, b8 <back+0xb8>\n-  bc:   34fffa2a        cbz     w10, 0 <back>\n-  c0:   3400478a        cbz     w10, 9b0 <forth>\n-  c4:   35000008        cbnz    w8, c4 <back+0xc4>\n-  c8:   35fff9c8        cbnz    w8, 0 <back>\n-  cc:   35004728        cbnz    w8, 9b0 <forth>\n-  d0:   b400000b        cbz     x11, d0 <back+0xd0>\n-  d4:   b4fff96b        cbz     x11, 0 <back>\n-  d8:   b40046cb        cbz     x11, 9b0 <forth>\n-  dc:   b500001d        cbnz    x29, dc <back+0xdc>\n-  e0:   b5fff91d        cbnz    x29, 0 <back>\n-  e4:   b500467d        cbnz    x29, 9b0 <forth>\n-  e8:   10000013        adr     x19, e8 <back+0xe8>\n-  ec:   10fff8b3        adr     x19, 0 <back>\n-  f0:   10004613        adr     x19, 9b0 <forth>\n-  f4:   90000013        adrp    x19, 0 <back>\n-  f8:   36300016        tbz     w22, #6, f8 <back+0xf8>\n-  fc:   3637f836        tbz     w22, #6, 0 <back>\n- 100:   36304596        tbz     w22, #6, 9b0 <forth>\n- 104:   3758000c        tbnz    w12, #11, 104 <back+0x104>\n- 108:   375ff7cc        tbnz    w12, #11, 0 <back>\n- 10c:   3758452c        tbnz    w12, #11, 9b0 <forth>\n- 110:   128313a0        mov     w0, #0xffffe762                 \/\/ #-6302\n- 114:   528a32c7        mov     w7, #0x5196                     \/\/ #20886\n- 118:   7289173b        movk    w27, #0x48b9\n- 11c:   92ab3acc        mov     x12, #0xffffffffa629ffff        \/\/ #-1507196929\n- 120:   d2a0bf94        mov     x20, #0x5fc0000                 \/\/ #100401152\n- 124:   f2c285e8        movk    x8, #0x142f, lsl #32\n- 128:   9358722f        sbfx    x15, x17, #24, #5\n- 12c:   330e652f        bfxil   w15, w9, #14, #12\n- 130:   53067f3b        lsr     w27, w25, #6\n- 134:   93577c53        sbfx    x19, x2, #23, #9\n- 138:   b34a1aac        bfi     x12, x21, #54, #7\n- 13c:   d35a4016        ubfiz   x22, x0, #38, #17\n- 140:   13946c63        extr    w3, w3, w20, #27\n- 144:   93c3dbc8        extr    x8, x30, x3, #54\n- 148:   54000000        b.eq    148 <back+0x148>  \/\/ b.none\n- 14c:   54fff5a0        b.eq    0 <back>  \/\/ b.none\n- 150:   54004300        b.eq    9b0 <forth>  \/\/ b.none\n- 154:   54000001        b.ne    154 <back+0x154>  \/\/ b.any\n- 158:   54fff541        b.ne    0 <back>  \/\/ b.any\n- 15c:   540042a1        b.ne    9b0 <forth>  \/\/ b.any\n- 160:   54000002        b.cs    160 <back+0x160>  \/\/ b.hs, b.nlast\n- 164:   54fff4e2        b.cs    0 <back>  \/\/ b.hs, b.nlast\n- 168:   54004242        b.cs    9b0 <forth>  \/\/ b.hs, b.nlast\n- 16c:   54000002        b.cs    16c <back+0x16c>  \/\/ b.hs, b.nlast\n- 170:   54fff482        b.cs    0 <back>  \/\/ b.hs, b.nlast\n- 174:   540041e2        b.cs    9b0 <forth>  \/\/ b.hs, b.nlast\n- 178:   54000003        b.cc    178 <back+0x178>  \/\/ b.lo, b.ul, b.last\n- 17c:   54fff423        b.cc    0 <back>  \/\/ b.lo, b.ul, b.last\n- 180:   54004183        b.cc    9b0 <forth>  \/\/ b.lo, b.ul, b.last\n- 184:   54000003        b.cc    184 <back+0x184>  \/\/ b.lo, b.ul, b.last\n- 188:   54fff3c3        b.cc    0 <back>  \/\/ b.lo, b.ul, b.last\n- 18c:   54004123        b.cc    9b0 <forth>  \/\/ b.lo, b.ul, b.last\n- 190:   54000004        b.mi    190 <back+0x190>  \/\/ b.first\n- 194:   54fff364        b.mi    0 <back>  \/\/ b.first\n- 198:   540040c4        b.mi    9b0 <forth>  \/\/ b.first\n- 19c:   54000005        b.pl    19c <back+0x19c>  \/\/ b.nfrst\n- 1a0:   54fff305        b.pl    0 <back>  \/\/ b.nfrst\n- 1a4:   54004065        b.pl    9b0 <forth>  \/\/ b.nfrst\n- 1a8:   54000006        b.vs    1a8 <back+0x1a8>\n- 1ac:   54fff2a6        b.vs    0 <back>\n- 1b0:   54004006        b.vs    9b0 <forth>\n- 1b4:   54000007        b.vc    1b4 <back+0x1b4>\n- 1b8:   54fff247        b.vc    0 <back>\n- 1bc:   54003fa7        b.vc    9b0 <forth>\n- 1c0:   54000008        b.hi    1c0 <back+0x1c0>  \/\/ b.pmore\n- 1c4:   54fff1e8        b.hi    0 <back>  \/\/ b.pmore\n- 1c8:   54003f48        b.hi    9b0 <forth>  \/\/ b.pmore\n- 1cc:   54000009        b.ls    1cc <back+0x1cc>  \/\/ b.plast\n- 1d0:   54fff189        b.ls    0 <back>  \/\/ b.plast\n- 1d4:   54003ee9        b.ls    9b0 <forth>  \/\/ b.plast\n- 1d8:   5400000a        b.ge    1d8 <back+0x1d8>  \/\/ b.tcont\n- 1dc:   54fff12a        b.ge    0 <back>  \/\/ b.tcont\n- 1e0:   54003e8a        b.ge    9b0 <forth>  \/\/ b.tcont\n- 1e4:   5400000b        b.lt    1e4 <back+0x1e4>  \/\/ b.tstop\n- 1e8:   54fff0cb        b.lt    0 <back>  \/\/ b.tstop\n- 1ec:   54003e2b        b.lt    9b0 <forth>  \/\/ b.tstop\n- 1f0:   5400000c        b.gt    1f0 <back+0x1f0>\n- 1f4:   54fff06c        b.gt    0 <back>\n- 1f8:   54003dcc        b.gt    9b0 <forth>\n- 1fc:   5400000d        b.le    1fc <back+0x1fc>\n- 200:   54fff00d        b.le    0 <back>\n- 204:   54003d6d        b.le    9b0 <forth>\n- 208:   5400000e        b.al    208 <back+0x208>\n- 20c:   54ffefae        b.al    0 <back>\n- 210:   54003d0e        b.al    9b0 <forth>\n- 214:   5400000f        b.nv    214 <back+0x214>\n- 218:   54ffef4f        b.nv    0 <back>\n- 21c:   54003caf        b.nv    9b0 <forth>\n- 220:   d40658e1        svc     #0x32c7\n- 224:   d4014d22        hvc     #0xa69\n- 228:   d4046543        smc     #0x232a\n- 22c:   d4273f60        brk     #0x39fb\n- 230:   d44cad80        hlt     #0x656c\n- 234:   d503201f        nop\n- 238:   d69f03e0        eret\n- 23c:   d6bf03e0        drps\n- 240:   d5033fdf        isb\n- 244:   d5033e9f        dsb     st\n- 248:   d50332bf        dmb     oshst\n- 24c:   d61f0200        br      x16\n- 250:   d63f0280        blr     x20\n- 254:   c80a7d1b        stxr    w10, x27, [x8]\n- 258:   c800fea1        stlxr   w0, x1, [x21]\n- 25c:   c85f7fb1        ldxr    x17, [x29]\n- 260:   c85fff9d        ldaxr   x29, [x28]\n- 264:   c89ffee1        stlr    x1, [x23]\n- 268:   c8dffe95        ldar    x21, [x20]\n- 26c:   88167e7b        stxr    w22, w27, [x19]\n- 270:   880bfcd0        stlxr   w11, w16, [x6]\n- 274:   885f7c11        ldxr    w17, [x0]\n- 278:   885ffd44        ldaxr   w4, [x10]\n- 27c:   889ffed8        stlr    w24, [x22]\n- 280:   88dffe6a        ldar    w10, [x19]\n- 284:   48017fc5        stxrh   w1, w5, [x30]\n- 288:   4808fe2c        stlxrh  w8, w12, [x17]\n- 28c:   485f7dc9        ldxrh   w9, [x14]\n- 290:   485ffc27        ldaxrh  w7, [x1]\n- 294:   489ffe05        stlrh   w5, [x16]\n- 298:   48dffd82        ldarh   w2, [x12]\n- 29c:   080a7c6c        stxrb   w10, w12, [x3]\n- 2a0:   081cff4e        stlxrb  w28, w14, [x26]\n- 2a4:   085f7d5e        ldxrb   w30, [x10]\n- 2a8:   085ffeae        ldaxrb  w14, [x21]\n- 2ac:   089ffd2d        stlrb   w13, [x9]\n- 2b0:   08dfff76        ldarb   w22, [x27]\n- 2b4:   c87f4d7c        ldxp    x28, x19, [x11]\n- 2b8:   c87fcc5e        ldaxp   x30, x19, [x2]\n- 2bc:   c8220417        stxp    w2, x23, x1, [x0]\n- 2c0:   c82cb5f0        stlxp   w12, x16, x13, [x15]\n- 2c4:   887f55b1        ldxp    w17, w21, [x13]\n- 2c8:   887ff90b        ldaxp   w11, w30, [x8]\n- 2cc:   88382c2d        stxp    w24, w13, w11, [x1]\n- 2d0:   883aedb5        stlxp   w26, w21, w27, [x13]\n- 2d4:   f819928b        stur    x11, [x20, #-103]\n- 2d8:   b803e21c        stur    w28, [x16, #62]\n- 2dc:   381f713b        sturb   w27, [x9, #-9]\n- 2e0:   781ce322        sturh   w2, [x25, #-50]\n- 2e4:   f850f044        ldur    x4, [x2, #-241]\n- 2e8:   b85e129e        ldur    w30, [x20, #-31]\n- 2ec:   385e92f1        ldurb   w17, [x23, #-23]\n- 2f0:   785ff35d        ldurh   w29, [x26, #-1]\n- 2f4:   39801921        ldrsb   x1, [x9, #6]\n- 2f8:   7881318b        ldursh  x11, [x12, #19]\n- 2fc:   78dce02b        ldursh  w11, [x1, #-50]\n- 300:   b8829313        ldursw  x19, [x24, #41]\n- 304:   fc45f318        ldur    d24, [x24, #95]\n- 308:   bc5d50af        ldur    s15, [x5, #-43]\n- 30c:   fc001375        stur    d21, [x27, #1]\n- 310:   bc1951b7        stur    s23, [x13, #-107]\n- 314:   f8008c0b        str     x11, [x0, #8]!\n- 318:   b801dc03        str     w3, [x0, #29]!\n- 31c:   38009dcb        strb    w11, [x14, #9]!\n- 320:   781fdf1d        strh    w29, [x24, #-3]!\n- 324:   f8570e2d        ldr     x13, [x17, #-144]!\n- 328:   b85faecc        ldr     w12, [x22, #-6]!\n- 32c:   385f6d8d        ldrb    w13, [x12, #-10]!\n- 330:   785ebea0        ldrh    w0, [x21, #-21]!\n- 334:   38804cf7        ldrsb   x23, [x7, #4]!\n- 338:   789cbce3        ldrsh   x3, [x7, #-53]!\n- 33c:   78df9cbc        ldrsh   w28, [x5, #-7]!\n- 340:   b89eed38        ldrsw   x24, [x9, #-18]!\n- 344:   fc40cd6e        ldr     d14, [x11, #12]!\n- 348:   bc5bdd93        ldr     s19, [x12, #-67]!\n- 34c:   fc103c14        str     d20, [x0, #-253]!\n- 350:   bc040c08        str     s8, [x0, #64]!\n- 354:   f81a2784        str     x4, [x28], #-94\n- 358:   b81ca4ec        str     w12, [x7], #-54\n- 35c:   381e855b        strb    w27, [x10], #-24\n- 360:   7801b506        strh    w6, [x8], #27\n- 364:   f853654e        ldr     x14, [x10], #-202\n- 368:   b85d74b0        ldr     w16, [x5], #-41\n- 36c:   384095c2        ldrb    w2, [x14], #9\n- 370:   785ec5bc        ldrh    w28, [x13], #-20\n- 374:   389e15a9        ldrsb   x9, [x13], #-31\n- 378:   789dc703        ldrsh   x3, [x24], #-36\n- 37c:   78c06474        ldrsh   w20, [x3], #6\n- 380:   b89ff667        ldrsw   x7, [x19], #-1\n- 384:   fc57e51e        ldr     d30, [x8], #-130\n- 388:   bc4155f9        ldr     s25, [x15], #21\n- 38c:   fc05a6ee        str     d14, [x23], #90\n- 390:   bc1df408        str     s8, [x0], #-33\n- 394:   f835da2a        str     x10, [x17, w21, sxtw #3]\n- 398:   b836d9a4        str     w4, [x13, w22, sxtw #2]\n- 39c:   3833580d        strb    w13, [x0, w19, uxtw #0]\n- 3a0:   7826cb6c        strh    w12, [x27, w6, sxtw]\n- 3a4:   f8706900        ldr     x0, [x8, x16]\n- 3a8:   b87ae880        ldr     w0, [x4, x26, sxtx]\n- 3ac:   3865db2e        ldrb    w14, [x25, w5, sxtw #0]\n- 3b0:   78714889        ldrh    w9, [x4, w17, uxtw]\n- 3b4:   38a7789b        ldrsb   x27, [x4, x7, lsl #0]\n- 3b8:   78beca2f        ldrsh   x15, [x17, w30, sxtw]\n- 3bc:   78f6c810        ldrsh   w16, [x0, w22, sxtw]\n- 3c0:   b8bef956        ldrsw   x22, [x10, x30, sxtx #2]\n- 3c4:   fc6afabd        ldr     d29, [x21, x10, sxtx #3]\n- 3c8:   bc734963        ldr     s3, [x11, w19, uxtw]\n- 3cc:   fc3d5b8d        str     d13, [x28, w29, uxtw #3]\n- 3d0:   bc25fbb7        str     s23, [x29, x5, sxtx #2]\n- 3d4:   f9189d05        str     x5, [x8, #12600]\n- 3d8:   b91ecb1d        str     w29, [x24, #7880]\n- 3dc:   39187a33        strb    w19, [x17, #1566]\n- 3e0:   791f226d        strh    w13, [x19, #3984]\n- 3e4:   f95aa2f3        ldr     x19, [x23, #13632]\n- 3e8:   b9587bb7        ldr     w23, [x29, #6264]\n- 3ec:   395f7176        ldrb    w22, [x11, #2012]\n- 3f0:   795d9143        ldrh    w3, [x10, #3784]\n- 3f4:   399e7e08        ldrsb   x8, [x16, #1951]\n- 3f8:   799a2697        ldrsh   x23, [x20, #3346]\n- 3fc:   79df3422        ldrsh   w2, [x1, #3994]\n- 400:   b99c2624        ldrsw   x4, [x17, #7204]\n- 404:   fd5c2374        ldr     d20, [x27, #14400]\n- 408:   bd5fa1d9        ldr     s25, [x14, #8096]\n- 40c:   fd1d595a        str     d26, [x10, #15024]\n- 410:   bd1b1869        str     s9, [x3, #6936]\n- 414:   58002cfb        ldr     x27, 9b0 <forth>\n- 418:   1800000b        ldr     w11, 418 <back+0x418>\n- 41c:   f8945060        prfum   pldl1keep, [x3, #-187]\n- 420:   d8000000        prfm    pldl1keep, 420 <back+0x420>\n- 424:   f8ae6ba0        prfm    pldl1keep, [x29, x14]\n- 428:   f99a0080        prfm    pldl1keep, [x4, #13312]\n- 42c:   1a070035        adc     w21, w1, w7\n- 430:   3a0700a8        adcs    w8, w5, w7\n- 434:   5a0e0367        sbc     w7, w27, w14\n- 438:   7a11009b        sbcs    w27, w4, w17\n- 43c:   9a000380        adc     x0, x28, x0\n- 440:   ba1e030c        adcs    x12, x24, x30\n- 444:   da0f0320        sbc     x0, x25, x15\n- 448:   fa030301        sbcs    x1, x24, x3\n- 44c:   0b340b11        add     w17, w24, w20, uxtb #2\n- 450:   2b2a278d        adds    w13, w28, w10, uxth #1\n- 454:   cb22aa0f        sub     x15, x16, w2, sxth #2\n- 458:   6b2d29bd        subs    w29, w13, w13, uxth #2\n- 45c:   8b2cce8c        add     x12, x20, w12, sxtw #3\n- 460:   ab2b877e        adds    x30, x27, w11, sxtb #1\n- 464:   cb21c8ee        sub     x14, x7, w1, sxtw #2\n- 468:   eb3ba47d        subs    x29, x3, w27, sxth #1\n- 46c:   3a4d400e        ccmn    w0, w13, #0xe, mi  \/\/ mi = first\n- 470:   7a5132c6        ccmp    w22, w17, #0x6, cc  \/\/ cc = lo, ul, last\n- 474:   ba5e622e        ccmn    x17, x30, #0xe, vs\n- 478:   fa53814c        ccmp    x10, x19, #0xc, hi  \/\/ hi = pmore\n- 47c:   3a52d8c2        ccmn    w6, #0x12, #0x2, le\n- 480:   7a4d8924        ccmp    w9, #0xd, #0x4, hi  \/\/ hi = pmore\n- 484:   ba4b3aab        ccmn    x21, #0xb, #0xb, cc  \/\/ cc = lo, ul, last\n- 488:   fa4d7882        ccmp    x4, #0xd, #0x2, vc\n- 48c:   1a96804c        csel    w12, w2, w22, hi  \/\/ hi = pmore\n- 490:   1a912618        csinc   w24, w16, w17, cs  \/\/ cs = hs, nlast\n- 494:   5a90b0e6        csinv   w6, w7, w16, lt  \/\/ lt = tstop\n- 498:   5a96976b        csneg   w11, w27, w22, ls  \/\/ ls = plast\n- 49c:   9a9db06a        csel    x10, x3, x29, lt  \/\/ lt = tstop\n- 4a0:   9a9b374c        csinc   x12, x26, x27, cc  \/\/ cc = lo, ul, last\n- 4a4:   da95c14f        csinv   x15, x10, x21, gt\n- 4a8:   da89c6fe        csneg   x30, x23, x9, gt\n- 4ac:   5ac0015e        rbit    w30, w10\n- 4b0:   5ac005fd        rev16   w29, w15\n- 4b4:   5ac00bdd        rev     w29, w30\n- 4b8:   5ac012b9        clz     w25, w21\n- 4bc:   5ac01404        cls     w4, w0\n- 4c0:   dac002b1        rbit    x17, x21\n- 4c4:   dac0061d        rev16   x29, x16\n- 4c8:   dac00a95        rev32   x21, x20\n- 4cc:   dac00e66        rev     x6, x19\n- 4d0:   dac0107e        clz     x30, x3\n- 4d4:   dac01675        cls     x21, x19\n- 4d8:   1ac00b0b        udiv    w11, w24, w0\n- 4dc:   1ace0f3b        sdiv    w27, w25, w14\n- 4e0:   1ad121c3        lsl     w3, w14, w17\n- 4e4:   1ad825e7        lsr     w7, w15, w24\n- 4e8:   1ad92a3c        asr     w28, w17, w25\n- 4ec:   1adc2f42        ror     w2, w26, w28\n- 4f0:   9ada0b25        udiv    x5, x25, x26\n- 4f4:   9ad10e1b        sdiv    x27, x16, x17\n- 4f8:   9acc22a6        lsl     x6, x21, x12\n- 4fc:   9acc2480        lsr     x0, x4, x12\n- 500:   9adc2a3b        asr     x27, x17, x28\n- 504:   9ad12c5c        ror     x28, x2, x17\n- 508:   9bce7dea        umulh   x10, x15, x14\n- 50c:   9b597c6e        smulh   x14, x3, x25\n- 510:   1b0e166f        madd    w15, w19, w14, w5\n- 514:   1b1ae490        msub    w16, w4, w26, w25\n- 518:   9b023044        madd    x4, x2, x2, x12\n- 51c:   9b089e3d        msub    x29, x17, x8, x7\n- 520:   9b391083        smaddl  x3, w4, w25, x4\n- 524:   9b24c73a        smsubl  x26, w25, w4, x17\n- 528:   9bb15f40        umaddl  x0, w26, w17, x23\n- 52c:   9bbcc6af        umsubl  x15, w21, w28, x17\n- 530:   1e23095b        fmul    s27, s10, s3\n- 534:   1e3918e0        fdiv    s0, s7, s25\n- 538:   1e2f28c9        fadd    s9, s6, s15\n- 53c:   1e2a39fd        fsub    s29, s15, s10\n- 540:   1e270a22        fmul    s2, s17, s7\n- 544:   1e77096b        fmul    d11, d11, d23\n- 548:   1e771ba7        fdiv    d7, d29, d23\n- 54c:   1e6b2b6e        fadd    d14, d27, d11\n- 550:   1e78388b        fsub    d11, d4, d24\n- 554:   1e6e09ec        fmul    d12, d15, d14\n- 558:   1f1c3574        fmadd   s20, s11, s28, s13\n- 55c:   1f17f98b        fmsub   s11, s12, s23, s30\n- 560:   1f2935da        fnmadd  s26, s14, s9, s13\n- 564:   1f2574ea        fnmadd  s10, s7, s5, s29\n- 568:   1f4b306f        fmadd   d15, d3, d11, d12\n- 56c:   1f5ec7cf        fmsub   d15, d30, d30, d17\n- 570:   1f6f3e93        fnmadd  d19, d20, d15, d15\n- 574:   1f6226a9        fnmadd  d9, d21, d2, d9\n- 578:   1e2040fb        fmov    s27, s7\n- 57c:   1e20c3dd        fabs    s29, s30\n- 580:   1e214031        fneg    s17, s1\n- 584:   1e21c0c2        fsqrt   s2, s6\n- 588:   1e22c06a        fcvt    d10, s3\n- 58c:   1e604178        fmov    d24, d11\n- 590:   1e60c027        fabs    d7, d1\n- 594:   1e61400b        fneg    d11, d0\n- 598:   1e61c223        fsqrt   d3, d17\n- 59c:   1e6240dc        fcvt    s28, d6\n- 5a0:   1e3800d6        fcvtzs  w22, s6\n- 5a4:   9e380360        fcvtzs  x0, s27\n- 5a8:   1e78005a        fcvtzs  w26, d2\n- 5ac:   9e7800e5        fcvtzs  x5, d7\n- 5b0:   1e22017c        scvtf   s28, w11\n- 5b4:   9e2201b9        scvtf   s25, x13\n- 5b8:   1e6202eb        scvtf   d11, w23\n- 5bc:   9e620113        scvtf   d19, x8\n- 5c0:   1e2602b1        fmov    w17, s21\n- 5c4:   9e660299        fmov    x25, d20\n- 5c8:   1e270233        fmov    s19, w17\n- 5cc:   9e6703a2        fmov    d2, x29\n- 5d0:   1e2822c0        fcmp    s22, s8\n- 5d4:   1e7322a0        fcmp    d21, d19\n- 5d8:   1e202288        fcmp    s20, #0.0\n- 5dc:   1e602168        fcmp    d11, #0.0\n- 5e0:   293c19f4        stp     w20, w6, [x15, #-32]\n- 5e4:   2966387b        ldp     w27, w14, [x3, #-208]\n- 5e8:   69762971        ldpsw   x17, x10, [x11, #-80]\n- 5ec:   a9041dc7        stp     x7, x7, [x14, #64]\n- 5f0:   a9475c0c        ldp     x12, x23, [x0, #112]\n- 5f4:   29b61ccd        stp     w13, w7, [x6, #-80]!\n- 5f8:   29ee405e        ldp     w30, w16, [x2, #-144]!\n- 5fc:   69ee0744        ldpsw   x4, x1, [x26, #-144]!\n- 600:   a9843977        stp     x23, x14, [x11, #64]!\n- 604:   a9f46ebd        ldp     x29, x27, [x21, #-192]!\n- 608:   28ba16b6        stp     w22, w5, [x21], #-48\n- 60c:   28fc44db        ldp     w27, w17, [x6], #-32\n- 610:   68f61831        ldpsw   x17, x6, [x1], #-80\n- 614:   a8b352ad        stp     x13, x20, [x21], #-208\n- 618:   a8c56d5e        ldp     x30, x27, [x10], #80\n- 61c:   28024565        stnp    w5, w17, [x11, #16]\n- 620:   2874134e        ldnp    w14, w4, [x26, #-96]\n- 624:   a8027597        stnp    x23, x29, [x12, #32]\n- 628:   a87b1aa0        ldnp    x0, x6, [x21, #-80]\n- 62c:   0c40734f        ld1     {v15.8b}, [x26]\n- 630:   4cdfa177        ld1     {v23.16b, v24.16b}, [x11], #32\n- 634:   0cc76ee8        ld1     {v8.1d-v10.1d}, [x23], x7\n- 638:   4cdf2733        ld1     {v19.8h-v22.8h}, [x25], #64\n- 63c:   0d40c23d        ld1r    {v29.8b}, [x17]\n- 640:   4ddfcaf8        ld1r    {v24.4s}, [x23], #4\n- 644:   0dd9ccaa        ld1r    {v10.1d}, [x5], x25\n- 648:   4c408d51        ld2     {v17.2d, v18.2d}, [x10]\n- 64c:   0cdf85ec        ld2     {v12.4h, v13.4h}, [x15], #16\n- 650:   4d60c239        ld2r    {v25.16b, v26.16b}, [x17]\n- 654:   0dffcbc1        ld2r    {v1.2s, v2.2s}, [x30], #8\n- 658:   4de9ce30        ld2r    {v16.2d, v17.2d}, [x17], x9\n- 65c:   4cc24999        ld3     {v25.4s-v27.4s}, [x12], x2\n- 660:   0c404a7a        ld3     {v26.2s-v28.2s}, [x19]\n- 664:   4d40e6af        ld3r    {v15.8h-v17.8h}, [x21]\n- 668:   4ddfe9b9        ld3r    {v25.4s-v27.4s}, [x13], #12\n- 66c:   0dddef8e        ld3r    {v14.1d-v16.1d}, [x28], x29\n- 670:   4cdf07b1        ld4     {v17.8h-v20.8h}, [x29], #64\n- 674:   0cc000fb        ld4     {v27.8b-v30.8b}, [x7], x0\n- 678:   0d60e238        ld4r    {v24.8b-v27.8b}, [x17]\n- 67c:   0dffe740        ld4r    {v0.4h-v3.4h}, [x26], #8\n- 680:   0de2eb2c        ld4r    {v12.2s-v15.2s}, [x25], x2\n- 684:   ce648376        sha512h q22, q27, v4.2d\n- 688:   ce6184c7        sha512h2        q7, q6, v1.2d\n- 68c:   cec081fa        sha512su0       v26.2d, v15.2d\n- 690:   ce6d89a2        sha512su1       v2.2d, v13.2d, v13.2d\n- 694:   ba5fd3e3        ccmn    xzr, xzr, #0x3, le\n- 698:   3a5f03e5        ccmn    wzr, wzr, #0x5, eq  \/\/ eq = none\n- 69c:   fa411be4        ccmp    xzr, #0x1, #0x4, ne  \/\/ ne = any\n- 6a0:   7a42cbe2        ccmp    wzr, #0x2, #0x2, gt\n- 6a4:   93df03ff        ror     xzr, xzr, #0\n- 6a8:   c820ffff        stlxp   w0, xzr, xzr, [sp]\n- 6ac:   8822fc7f        stlxp   w2, wzr, wzr, [x3]\n- 6b0:   c8247cbf        stxp    w4, xzr, xzr, [x5]\n- 6b4:   88267fff        stxp    w6, wzr, wzr, [sp]\n- 6b8:   4e010fe0        dup     v0.16b, wzr\n- 6bc:   4e081fe1        mov     v1.d[0], xzr\n- 6c0:   4e0c1fe1        mov     v1.s[1], wzr\n- 6c4:   4e0a1fe1        mov     v1.h[2], wzr\n- 6c8:   4e071fe1        mov     v1.b[3], wzr\n- 6cc:   4cc0ac3f        ld1     {v31.2d, v0.2d}, [x1], x0\n- 6d0:   05a08020        mov     z0.s, p0\/m, s1\n- 6d4:   04b0e3e0        incw    x0\n- 6d8:   0470e7e1        dech    x1\n- 6dc:   042f9c20        lsl     z0.b, z1.b, #7\n- 6e0:   043f9c35        lsl     z21.h, z1.h, #15\n- 6e4:   047f9c20        lsl     z0.s, z1.s, #31\n- 6e8:   04ff9c20        lsl     z0.d, z1.d, #63\n- 6ec:   04299420        lsr     z0.b, z1.b, #7\n- 6f0:   04319160        asr     z0.h, z11.h, #15\n- 6f4:   0461943e        lsr     z30.s, z1.s, #31\n- 6f8:   04a19020        asr     z0.d, z1.d, #63\n- 6fc:   042053ff        addvl   sp, x0, #31\n- 700:   047f5401        addpl   x1, sp, #-32\n- 704:   25208028        cntp    x8, p0, p1.b\n- 708:   2538cfe0        mov     z0.b, #127\n- 70c:   2578d001        mov     z1.h, #-128\n- 710:   25b8efe2        mov     z2.s, #32512\n- 714:   25f8f007        mov     z7.d, #-32768\n- 718:   a400a3e0        ld1b    {z0.b}, p0\/z, [sp]\n- 71c:   a4a8a7ea        ld1h    {z10.h}, p1\/z, [sp, #-8, mul vl]\n- 720:   a547a814        ld1w    {z20.s}, p2\/z, [x0, #7, mul vl]\n- 724:   a4084ffe        ld1b    {z30.b}, p3\/z, [sp, x8]\n- 728:   a55c53e0        ld1w    {z0.s}, p4\/z, [sp, x28, lsl #2]\n- 72c:   a5e1540b        ld1d    {z11.d}, p5\/z, [x0, x1, lsl #3]\n- 730:   e400fbf6        st1b    {z22.b}, p6, [sp]\n- 734:   e408ffff        st1b    {z31.b}, p7, [sp, #-8, mul vl]\n- 738:   e547e400        st1w    {z0.s}, p1, [x0, #7, mul vl]\n- 73c:   e4014be0        st1b    {z0.b}, p2, [sp, x1]\n- 740:   e4a84fe0        st1h    {z0.h}, p3, [sp, x8, lsl #1]\n- 744:   e5f15000        st1d    {z0.d}, p4, [x0, x17, lsl #3]\n- 748:   858043e0        ldr     z0, [sp]\n- 74c:   85a043ff        ldr     z31, [sp, #-256, mul vl]\n- 750:   e59f5d08        str     z8, [x8, #255, mul vl]\n- 754:   1e601000        fmov    d0, #2.000000000000000000e+00\n- 758:   1e603000        fmov    d0, #2.125000000000000000e+00\n- 75c:   1e621000        fmov    d0, #4.000000000000000000e+00\n- 760:   1e623000        fmov    d0, #4.250000000000000000e+00\n- 764:   1e641000        fmov    d0, #8.000000000000000000e+00\n- 768:   1e643000        fmov    d0, #8.500000000000000000e+00\n- 76c:   1e661000        fmov    d0, #1.600000000000000000e+01\n- 770:   1e663000        fmov    d0, #1.700000000000000000e+01\n- 774:   1e681000        fmov    d0, #1.250000000000000000e-01\n- 778:   1e683000        fmov    d0, #1.328125000000000000e-01\n- 77c:   1e6a1000        fmov    d0, #2.500000000000000000e-01\n- 780:   1e6a3000        fmov    d0, #2.656250000000000000e-01\n- 784:   1e6c1000        fmov    d0, #5.000000000000000000e-01\n- 788:   1e6c3000        fmov    d0, #5.312500000000000000e-01\n- 78c:   1e6e1000        fmov    d0, #1.000000000000000000e+00\n- 790:   1e6e3000        fmov    d0, #1.062500000000000000e+00\n- 794:   1e701000        fmov    d0, #-2.000000000000000000e+00\n- 798:   1e703000        fmov    d0, #-2.125000000000000000e+00\n- 79c:   1e721000        fmov    d0, #-4.000000000000000000e+00\n- 7a0:   1e723000        fmov    d0, #-4.250000000000000000e+00\n- 7a4:   1e741000        fmov    d0, #-8.000000000000000000e+00\n- 7a8:   1e743000        fmov    d0, #-8.500000000000000000e+00\n- 7ac:   1e761000        fmov    d0, #-1.600000000000000000e+01\n- 7b0:   1e763000        fmov    d0, #-1.700000000000000000e+01\n- 7b4:   1e781000        fmov    d0, #-1.250000000000000000e-01\n- 7b8:   1e783000        fmov    d0, #-1.328125000000000000e-01\n- 7bc:   1e7a1000        fmov    d0, #-2.500000000000000000e-01\n- 7c0:   1e7a3000        fmov    d0, #-2.656250000000000000e-01\n- 7c4:   1e7c1000        fmov    d0, #-5.000000000000000000e-01\n- 7c8:   1e7c3000        fmov    d0, #-5.312500000000000000e-01\n- 7cc:   1e7e1000        fmov    d0, #-1.000000000000000000e+00\n- 7d0:   1e7e3000        fmov    d0, #-1.062500000000000000e+00\n- 7d4:   f8388098        swp     x24, x24, [x4]\n- 7d8:   f8340010        ldadd   x20, x16, [x0]\n- 7dc:   f8241175        ldclr   x4, x21, [x11]\n- 7e0:   f83e22d0        ldeor   x30, x16, [x22]\n- 7e4:   f82432ef        ldset   x4, x15, [x23]\n- 7e8:   f83a5186        ldsmin  x26, x6, [x12]\n- 7ec:   f82f41ee        ldsmax  x15, x14, [x15]\n- 7f0:   f82973b9        ldumin  x9, x25, [x29]\n- 7f4:   f82b6194        ldumax  x11, x20, [x12]\n- 7f8:   f8b08216        swpa    x16, x22, [x16]\n- 7fc:   f8b50358        ldadda  x21, x24, [x26]\n- 800:   f8a61206        ldclra  x6, x6, [x16]\n- 804:   f8b02219        ldeora  x16, x25, [x16]\n- 808:   f8bc3218        ldseta  x28, x24, [x16]\n- 80c:   f8ba514f        ldsmina x26, x15, [x10]\n- 810:   f8ad428e        ldsmaxa x13, x14, [x20]\n- 814:   f8a173d7        ldumina x1, x23, [x30]\n- 818:   f8ae60c2        ldumaxa x14, x2, [x6]\n- 81c:   f8e38328        swpal   x3, x8, [x25]\n- 820:   f8e003db        ldaddal x0, x27, [x30]\n- 824:   f8e513c5        ldclral x5, x5, [x30]\n- 828:   f8eb2019        ldeoral x11, x25, [x0]\n- 82c:   f8ff3260        ldsetal xzr, x0, [x19]\n- 830:   f8fd513a        ldsminal        x29, x26, [x9]\n- 834:   f8fa41ec        ldsmaxal        x26, x12, [x15]\n- 838:   f8eb71eb        lduminal        x11, x11, [x15]\n- 83c:   f8f96316        ldumaxal        x25, x22, [x24]\n- 840:   f8608171        swpl    x0, x17, [x11]\n- 844:   f86600dd        ldaddl  x6, x29, [x6]\n- 848:   f86512a5        ldclrl  x5, x5, [x21]\n- 84c:   f87321f0        ldeorl  x19, x16, [x15]\n- 850:   f87e339b        ldsetl  x30, x27, [x28]\n- 854:   f861503c        ldsminl x1, x28, [x1]\n- 858:   f874421d        ldsmaxl x20, x29, [x16]\n- 85c:   f86d73aa        lduminl x13, x10, [x29]\n- 860:   f87d62d3        ldumaxl x29, x19, [x22]\n- 864:   b82a83e4        swp     w10, w4, [sp]\n- 868:   b83503e8        ldadd   w21, w8, [sp]\n- 86c:   b833138a        ldclr   w19, w10, [x28]\n- 870:   b82220b9        ldeor   w2, w25, [x5]\n- 874:   b82332c8        ldset   w3, w8, [x22]\n- 878:   b83350ad        ldsmin  w19, w13, [x5]\n- 87c:   b83d42b8        ldsmax  w29, w24, [x21]\n- 880:   b83a7078        ldumin  w26, w24, [x3]\n- 884:   b83862fa        ldumax  w24, w26, [x23]\n- 888:   b8af8075        swpa    w15, w21, [x3]\n- 88c:   b8b80328        ldadda  w24, w8, [x25]\n- 890:   b8b41230        ldclra  w20, w16, [x17]\n- 894:   b8a22001        ldeora  w2, w1, [x0]\n- 898:   b8b83064        ldseta  w24, w4, [x3]\n- 89c:   b8ac539f        ldsmina w12, wzr, [x28]\n- 8a0:   b8aa405a        ldsmaxa w10, w26, [x2]\n- 8a4:   b8ac73f0        ldumina w12, w16, [sp]\n- 8a8:   b8a163ad        ldumaxa w1, w13, [x29]\n- 8ac:   b8e08193        swpal   w0, w19, [x12]\n- 8b0:   b8f101b6        ldaddal w17, w22, [x13]\n- 8b4:   b8fc13fe        ldclral w28, w30, [sp]\n- 8b8:   b8e1239a        ldeoral w1, w26, [x28]\n- 8bc:   b8e4309e        ldsetal w4, w30, [x4]\n- 8c0:   b8e6535e        ldsminal        w6, w30, [x26]\n- 8c4:   b8f04109        ldsmaxal        w16, w9, [x8]\n- 8c8:   b8ec7280        lduminal        w12, w0, [x20]\n- 8cc:   b8e16058        ldumaxal        w1, w24, [x2]\n- 8d0:   b8608309        swpl    w0, w9, [x24]\n- 8d4:   b87a03d0        ldaddl  w26, w16, [x30]\n- 8d8:   b86312ea        ldclrl  w3, w10, [x23]\n- 8dc:   b86a21e4        ldeorl  w10, w4, [x15]\n- 8e0:   b862310b        ldsetl  w2, w11, [x8]\n- 8e4:   b86a522f        ldsminl w10, w15, [x17]\n- 8e8:   b862418a        ldsmaxl w2, w10, [x12]\n- 8ec:   b86c71af        lduminl w12, w15, [x13]\n- 8f0:   b8626287        ldumaxl w2, w7, [x20]\n- 8f4:   042401f9        add     z25.b, z15.b, z4.b\n- 8f8:   04b10564        sub     z4.s, z11.s, z17.s\n- 8fc:   65ca0230        fadd    z16.d, z17.d, z10.d\n- 900:   65d90996        fmul    z22.d, z12.d, z25.d\n- 904:   65ca05dc        fsub    z28.d, z14.d, z10.d\n- 908:   0456afc1        abs     z1.h, p3\/m, z30.h\n- 90c:   0400044f        add     z15.b, p1\/m, z15.b, z2.b\n- 910:   0490920d        asr     z13.s, p4\/m, z13.s, z16.s\n- 914:   04daa163        cnt     z3.d, p0\/m, z11.d\n- 918:   04d389c5        lsl     z5.d, p2\/m, z5.d, z14.d\n- 91c:   0411829d        lsr     z29.b, p0\/m, z29.b, z20.b\n- 920:   04901774        mul     z20.s, p5\/m, z20.s, z27.s\n- 924:   0417b89a        neg     z26.b, p6\/m, z4.b\n- 928:   041eb3d6        not     z22.b, p4\/m, z30.b\n- 92c:   04480b6b        smax    z11.h, p2\/m, z11.h, z27.h\n- 930:   048a17dc        smin    z28.s, p5\/m, z28.s, z30.s\n- 934:   048105be        sub     z30.s, p1\/m, z30.s, z13.s\n- 938:   04dcb35e        fabs    z30.d, p4\/m, z26.d\n- 93c:   65808d6f        fadd    z15.s, p3\/m, z15.s, z11.s\n- 940:   65cd9e06        fdiv    z6.d, p7\/m, z6.d, z16.d\n- 944:   65869cfb        fmax    z27.s, p7\/m, z27.s, z7.s\n- 948:   65c78893        fmin    z19.d, p2\/m, z19.d, z4.d\n- 94c:   658292d1        fmul    z17.s, p4\/m, z17.s, z22.s\n- 950:   04ddaebc        fneg    z28.d, p3\/m, z21.d\n- 954:   6582b451        frintm  z17.s, p5\/m, z2.s\n- 958:   6580ade6        frintn  z6.s, p3\/m, z15.s\n- 95c:   65c1b42c        frintp  z12.d, p5\/m, z1.d\n- 960:   658da631        fsqrt   z17.s, p1\/m, z17.s\n- 964:   658195af        fsub    z15.s, p5\/m, z15.s, z13.s\n- 968:   65eb1f74        fmla    z20.d, p7\/m, z27.d, z11.d\n- 96c:   65f723c3        fmls    z3.d, p0\/m, z30.d, z23.d\n- 970:   65ba4b71        fnmla   z17.s, p2\/m, z27.s, z26.s\n- 974:   65fe76c6        fnmls   z6.d, p5\/m, z22.d, z30.d\n- 978:   04515f42        mla     z2.h, p7\/m, z26.h, z17.h\n- 97c:   04117056        mls     z22.b, p4\/m, z2.b, z17.b\n- 980:   04363338        and     z24.d, z25.d, z22.d\n- 984:   04a33191        eor     z17.d, z12.d, z3.d\n- 988:   0470339d        orr     z29.d, z28.d, z16.d\n- 98c:   049a2b86        andv    s6, p2, z28.s\n- 990:   045824e7        orv     h7, p1, z7.h\n- 994:   04193509        eorv    b9, p5, z8.b\n- 998:   040837db        smaxv   b27, p5, z30.b\n- 99c:   044a221a        sminv   h26, p0, z16.h\n- 9a0:   65c73903        fminv   d3, p6, z8.d\n- 9a4:   65c63b55        fmaxv   d21, p6, z26.d\n- 9a8:   65982096        fadda   s22, p0, s22, z4.s\n- 9ac:   04412071        uaddv   d17, p0, z3.h\n- *\/\n+*\/\n@@ -1489,7 +1004,7 @@\n-    0x14000000,     0x17ffffd7,     0x14000242,     0x94000000,\n-    0x97ffffd4,     0x9400023f,     0x3400000a,     0x34fffa2a,\n-    0x3400478a,     0x35000008,     0x35fff9c8,     0x35004728,\n-    0xb400000b,     0xb4fff96b,     0xb40046cb,     0xb500001d,\n-    0xb5fff91d,     0xb500467d,     0x10000013,     0x10fff8b3,\n-    0x10004613,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36304596,     0x3758000c,     0x375ff7cc,     0x3758452c,\n+    0x14000000,     0x17ffffd7,     0x140002c9,     0x94000000,\n+    0x97ffffd4,     0x940002c6,     0x3400000a,     0x34fffa2a,\n+    0x3400586a,     0x35000008,     0x35fff9c8,     0x35005808,\n+    0xb400000b,     0xb4fff96b,     0xb40057ab,     0xb500001d,\n+    0xb5fff91d,     0xb500575d,     0x10000013,     0x10fff8b3,\n+    0x100056f3,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36305676,     0x3758000c,     0x375ff7cc,     0x3758560c,\n@@ -1500,13 +1015,13 @@\n-    0x54004300,     0x54000001,     0x54fff541,     0x540042a1,\n-    0x54000002,     0x54fff4e2,     0x54004242,     0x54000002,\n-    0x54fff482,     0x540041e2,     0x54000003,     0x54fff423,\n-    0x54004183,     0x54000003,     0x54fff3c3,     0x54004123,\n-    0x54000004,     0x54fff364,     0x540040c4,     0x54000005,\n-    0x54fff305,     0x54004065,     0x54000006,     0x54fff2a6,\n-    0x54004006,     0x54000007,     0x54fff247,     0x54003fa7,\n-    0x54000008,     0x54fff1e8,     0x54003f48,     0x54000009,\n-    0x54fff189,     0x54003ee9,     0x5400000a,     0x54fff12a,\n-    0x54003e8a,     0x5400000b,     0x54fff0cb,     0x54003e2b,\n-    0x5400000c,     0x54fff06c,     0x54003dcc,     0x5400000d,\n-    0x54fff00d,     0x54003d6d,     0x5400000e,     0x54ffefae,\n-    0x54003d0e,     0x5400000f,     0x54ffef4f,     0x54003caf,\n+    0x540053e0,     0x54000001,     0x54fff541,     0x54005381,\n+    0x54000002,     0x54fff4e2,     0x54005322,     0x54000002,\n+    0x54fff482,     0x540052c2,     0x54000003,     0x54fff423,\n+    0x54005263,     0x54000003,     0x54fff3c3,     0x54005203,\n+    0x54000004,     0x54fff364,     0x540051a4,     0x54000005,\n+    0x54fff305,     0x54005145,     0x54000006,     0x54fff2a6,\n+    0x540050e6,     0x54000007,     0x54fff247,     0x54005087,\n+    0x54000008,     0x54fff1e8,     0x54005028,     0x54000009,\n+    0x54fff189,     0x54004fc9,     0x5400000a,     0x54fff12a,\n+    0x54004f6a,     0x5400000b,     0x54fff0cb,     0x54004f0b,\n+    0x5400000c,     0x54fff06c,     0x54004eac,     0x5400000d,\n+    0x54fff00d,     0x54004e4d,     0x5400000e,     0x54ffefae,\n+    0x54004dee,     0x5400000f,     0x54ffef4f,     0x54004d8f,\n@@ -1544,1 +1059,1 @@\n-    0xbd1b1869,     0x58002cfb,     0x1800000b,     0xf8945060,\n+    0xbd1b1869,     0x58003ddb,     0x1800000b,     0xf8945060,\n@@ -1583,52 +1098,85 @@\n-    0x0de2eb2c,     0xce648376,     0xce6184c7,     0xcec081fa,\n-    0xce6d89a2,     0xba5fd3e3,     0x3a5f03e5,     0xfa411be4,\n-    0x7a42cbe2,     0x93df03ff,     0xc820ffff,     0x8822fc7f,\n-    0xc8247cbf,     0x88267fff,     0x4e010fe0,     0x4e081fe1,\n-    0x4e0c1fe1,     0x4e0a1fe1,     0x4e071fe1,     0x4cc0ac3f,\n-    0x05a08020,     0x04b0e3e0,     0x0470e7e1,     0x042f9c20,\n-    0x043f9c35,     0x047f9c20,     0x04ff9c20,     0x04299420,\n-    0x04319160,     0x0461943e,     0x04a19020,     0x042053ff,\n-    0x047f5401,     0x25208028,     0x2538cfe0,     0x2578d001,\n-    0x25b8efe2,     0x25f8f007,     0xa400a3e0,     0xa4a8a7ea,\n-    0xa547a814,     0xa4084ffe,     0xa55c53e0,     0xa5e1540b,\n-    0xe400fbf6,     0xe408ffff,     0xe547e400,     0xe4014be0,\n-    0xe4a84fe0,     0xe5f15000,     0x858043e0,     0x85a043ff,\n-    0xe59f5d08,     0x1e601000,     0x1e603000,     0x1e621000,\n-    0x1e623000,     0x1e641000,     0x1e643000,     0x1e661000,\n-    0x1e663000,     0x1e681000,     0x1e683000,     0x1e6a1000,\n-    0x1e6a3000,     0x1e6c1000,     0x1e6c3000,     0x1e6e1000,\n-    0x1e6e3000,     0x1e701000,     0x1e703000,     0x1e721000,\n-    0x1e723000,     0x1e741000,     0x1e743000,     0x1e761000,\n-    0x1e763000,     0x1e781000,     0x1e783000,     0x1e7a1000,\n-    0x1e7a3000,     0x1e7c1000,     0x1e7c3000,     0x1e7e1000,\n-    0x1e7e3000,     0xf8388098,     0xf8340010,     0xf8241175,\n-    0xf83e22d0,     0xf82432ef,     0xf83a5186,     0xf82f41ee,\n-    0xf82973b9,     0xf82b6194,     0xf8b08216,     0xf8b50358,\n-    0xf8a61206,     0xf8b02219,     0xf8bc3218,     0xf8ba514f,\n-    0xf8ad428e,     0xf8a173d7,     0xf8ae60c2,     0xf8e38328,\n-    0xf8e003db,     0xf8e513c5,     0xf8eb2019,     0xf8ff3260,\n-    0xf8fd513a,     0xf8fa41ec,     0xf8eb71eb,     0xf8f96316,\n-    0xf8608171,     0xf86600dd,     0xf86512a5,     0xf87321f0,\n-    0xf87e339b,     0xf861503c,     0xf874421d,     0xf86d73aa,\n-    0xf87d62d3,     0xb82a83e4,     0xb83503e8,     0xb833138a,\n-    0xb82220b9,     0xb82332c8,     0xb83350ad,     0xb83d42b8,\n-    0xb83a7078,     0xb83862fa,     0xb8af8075,     0xb8b80328,\n-    0xb8b41230,     0xb8a22001,     0xb8b83064,     0xb8ac539f,\n-    0xb8aa405a,     0xb8ac73f0,     0xb8a163ad,     0xb8e08193,\n-    0xb8f101b6,     0xb8fc13fe,     0xb8e1239a,     0xb8e4309e,\n-    0xb8e6535e,     0xb8f04109,     0xb8ec7280,     0xb8e16058,\n-    0xb8608309,     0xb87a03d0,     0xb86312ea,     0xb86a21e4,\n-    0xb862310b,     0xb86a522f,     0xb862418a,     0xb86c71af,\n-    0xb8626287,     0x042401f9,     0x04b10564,     0x65ca0230,\n-    0x65d90996,     0x65ca05dc,     0x0456afc1,     0x0400044f,\n-    0x0490920d,     0x04daa163,     0x04d389c5,     0x0411829d,\n-    0x04901774,     0x0417b89a,     0x041eb3d6,     0x04480b6b,\n-    0x048a17dc,     0x048105be,     0x04dcb35e,     0x65808d6f,\n-    0x65cd9e06,     0x65869cfb,     0x65c78893,     0x658292d1,\n-    0x04ddaebc,     0x6582b451,     0x6580ade6,     0x65c1b42c,\n-    0x658da631,     0x658195af,     0x65eb1f74,     0x65f723c3,\n-    0x65ba4b71,     0x65fe76c6,     0x04515f42,     0x04117056,\n-    0x04363338,     0x04a33191,     0x0470339d,     0x049a2b86,\n-    0x045824e7,     0x04193509,     0x040837db,     0x044a221a,\n-    0x65c73903,     0x65c63b55,     0x65982096,     0x04412071,\n-\n+    0x0de2eb2c,     0x0e31baf6,     0x4e31bb9b,     0x0e71b8a4,\n+    0x4e71b907,     0x4eb1b8e6,     0x0e30a841,     0x4e30ab7a,\n+    0x0e70aa0f,     0x4e70a862,     0x4eb0a9cd,     0x6e30f9cd,\n+    0x0e31ab38,     0x4e31ab17,     0x0e71a8a4,     0x4e71aa93,\n+    0x4eb1aa0f,     0x6eb0f820,     0x0e20b8a4,     0x4e20bab4,\n+    0x0e60b98b,     0x4e60bbdd,     0x0ea0ba0f,     0x4ea0bad5,\n+    0x4ee0b8a4,     0x0ea0f9ee,     0x4ea0faf6,     0x4ee0fb59,\n+    0x2ea0f8e6,     0x6ea0f9ac,     0x6ee0f9ee,     0x2ea1f9cd,\n+    0x6ea1f9ee,     0x6ee1f949,     0x2e205b59,     0x6e205bbc,\n+    0x0e2c1d6a,     0x4e351e93,     0x0ead1d8b,     0x4eb31e51,\n+    0x2e371ed5,     0x6e311e0f,     0x0e3686b4,     0x4e398717,\n+    0x0e7c877a,     0x4e6784c5,     0x0ea884e6,     0x4eb1860f,\n+    0x4ef1860f,     0x0e3bd759,     0x4e32d630,     0x4e7dd79b,\n+    0x2e3a8738,     0x6e31860f,     0x2e7b8759,     0x6e7085ee,\n+    0x2eac856a,     0x6eaf85cd,     0x6ef085ee,     0x0eb6d6b4,\n+    0x4ea3d441,     0x4ef8d6f6,     0x0e209ffe,     0x4e309dee,\n+    0x0e649c62,     0x4e689ce6,     0x0ea59c83,     0x4ea99d07,\n+    0x2e3adf38,     0x6e22dc20,     0x6e7ddf9b,     0x0e7f97dd,\n+    0x4e6794c5,     0x0ea794c5,     0x4ebf97dd,     0x0e2dcd8b,\n+    0x4e3bcf59,     0x4e62cc20,     0x2e6097fe,     0x6e629420,\n+    0x2eb39651,     0x6ebe97bc,     0x0ebbcf59,     0x4eabcd49,\n+    0x4efbcf59,     0x2e2efdac,     0x6e31fe0f,     0x6e6dfd8b,\n+    0x0e2c656a,     0x4e336651,     0x0e7a6738,     0x4e7766d5,\n+    0x0eb96717,     0x4ea26420,     0x0e32f630,     0x4e2cf56a,\n+    0x4e68f4e6,     0x0e3e6fbc,     0x4e286ce6,     0x0e676cc5,\n+    0x4e676cc5,     0x0eb66eb4,     0x4eb36e51,     0x0eb1f60f,\n+    0x4eb3f651,     0x4efff7dd,     0x2e3c8f7a,     0x6e3e8fbc,\n+    0x2e638c41,     0x6e7d8f9b,     0x2ea28c20,     0x6eb68eb4,\n+    0x6efe8fbc,     0x0e31e60f,     0x4e2ee5ac,     0x4e6ce56a,\n+    0x0e3e37bc,     0x4e3e37bc,     0x0e753693,     0x4e7836f6,\n+    0x0eac356a,     0x4ea634a4,     0x4ee037fe,     0x2eb6e6b4,\n+    0x6eaae528,     0x6ee0e7fe,     0x0e333e51,     0x4e2c3d6a,\n+    0x0e7d3f9b,     0x4e643c62,     0x0eba3f38,     0x4ea63ca4,\n+    0x4ee53c83,     0x2e2ae528,     0x6e38e6f6,     0x6e73e651,\n+    0xce7c808d,     0xce7986b7,     0xcec08078,     0xce778b57,\n+    0xba5fd3e3,     0x3a5f03e5,     0xfa411be4,     0x7a42cbe2,\n+    0x93df03ff,     0xc820ffff,     0x8822fc7f,     0xc8247cbf,\n+    0x88267fff,     0x4e010fe0,     0x4e081fe1,     0x4e0c1fe1,\n+    0x4e0a1fe1,     0x4e071fe1,     0x4cc0ac3f,     0x05a08020,\n+    0x04b0e3e0,     0x0470e7e1,     0x042f9c20,     0x043f9c35,\n+    0x047f9c20,     0x04ff9c20,     0x04299420,     0x04319160,\n+    0x0461943e,     0x04a19020,     0x042053ff,     0x047f5401,\n+    0x25208028,     0x2538cfe0,     0x2578d001,     0x25b8efe2,\n+    0x25f8f007,     0xa400a3e0,     0xa4a8a7ea,     0xa547a814,\n+    0xa4084ffe,     0xa55c53e0,     0xa5e1540b,     0xe400fbf6,\n+    0xe408ffff,     0xe547e400,     0xe4014be0,     0xe4a84fe0,\n+    0xe5f15000,     0x858043e0,     0x85a043ff,     0xe59f5d08,\n+    0x1e601000,     0x1e603000,     0x1e621000,     0x1e623000,\n+    0x1e641000,     0x1e643000,     0x1e661000,     0x1e663000,\n+    0x1e681000,     0x1e683000,     0x1e6a1000,     0x1e6a3000,\n+    0x1e6c1000,     0x1e6c3000,     0x1e6e1000,     0x1e6e3000,\n+    0x1e701000,     0x1e703000,     0x1e721000,     0x1e723000,\n+    0x1e741000,     0x1e743000,     0x1e761000,     0x1e763000,\n+    0x1e781000,     0x1e783000,     0x1e7a1000,     0x1e7a3000,\n+    0x1e7c1000,     0x1e7c3000,     0x1e7e1000,     0x1e7e3000,\n+    0xf82f8075,     0xf8380328,     0xf8341230,     0xf8222001,\n+    0xf8383064,     0xf82c539f,     0xf82a405a,     0xf82c73f0,\n+    0xf82163ad,     0xf8a08193,     0xf8b101b6,     0xf8bc13fe,\n+    0xf8a1239a,     0xf8a4309e,     0xf8a6535e,     0xf8b04109,\n+    0xf8ac7280,     0xf8a16058,     0xf8e08309,     0xf8fa03d0,\n+    0xf8e312ea,     0xf8ea21e4,     0xf8e2310b,     0xf8ea522f,\n+    0xf8e2418a,     0xf8ec71af,     0xf8e26287,     0xf87a8090,\n+    0xf8620184,     0xf8701215,     0xf87022ab,     0xf877334c,\n+    0xf87751dc,     0xf86b4038,     0xf86c715f,     0xf8706047,\n+    0xb823826d,     0xb8310070,     0xb82113cb,     0xb82521e8,\n+    0xb83d301e,     0xb8345287,     0xb83742bc,     0xb83b70b9,\n+    0xb8216217,     0xb8bf8185,     0xb8a901fc,     0xb8bd13f6,\n+    0xb8b320bf,     0xb8ae33f0,     0xb8b0529b,     0xb8b0416c,\n+    0xb8a973c6,     0xb8b1639b,     0xb8fe8147,     0xb8f4008a,\n+    0xb8f81231,     0xb8f623a3,     0xb8ef3276,     0xb8f35056,\n+    0xb8ef4186,     0xb8f071ab,     0xb8f763c1,     0xb8738225,\n+    0xb86202d0,     0xb86d12aa,     0xb87d219b,     0xb87b3023,\n+    0xb87f5278,     0xb8714389,     0xb87b70ef,     0xb87563f7,\n+    0x04fe0058,     0x04b60551,     0x65c00222,     0x65c20ad9,\n+    0x65db046c,     0x0416b35c,     0x04001e29,     0x045085e4,\n+    0x04daa856,     0x04d39cb4,     0x041191c0,     0x04900b79,\n+    0x0497bb1a,     0x049ea4c0,     0x040805e0,     0x044a04a9,\n+    0x0481069b,     0x049ca554,     0x65c09cd0,     0x65cd8fa2,\n+    0x65c69ac2,     0x65c78f6e,     0x65828457,     0x04ddb14a,\n+    0x65c2ac76,     0x65c0a430,     0x6581b190,     0x658da20c,\n+    0x658194b4,     0x65fb1187,     0x65bc2450,     0x65b34624,\n+    0x65f8750c,     0x04174151,     0x04107db3,     0x042e30e0,\n+    0x04aa3119,     0x047b32d4,     0x049a2e23,     0x04182787,\n+    0x04992a00,     0x044825f6,     0x040a2b36,     0x65c731be,\n+    0x658621ab,     0x65983334,     0x04412624,\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.cpp","additions":372,"deletions":824,"binary":false,"changes":1196,"status":"modified"},{"patch":"@@ -1374,0 +1374,15 @@\n+#define INSN(NAME, size, opc)                                           \\\n+  void NAME(FloatRegister Rt, Register Rn) {                            \\\n+    starti;                                                             \\\n+    f(size, 31, 30), f(0b111100, 29, 24), f(opc, 23, 22), f(0, 21);     \\\n+    f(0, 20, 12), f(0b01, 11, 10);                                      \\\n+    rf(Rn, 5), rf((Register)Rt, 0);                                     \\\n+  }\n+\n+  INSN(ldrs, 0b10, 0b01);\n+  INSN(ldrd, 0b11, 0b01);\n+  INSN(ldrq, 0b00, 0b11);\n+\n+#undef INSN\n+\n+\n@@ -1511,0 +1526,15 @@\n+\/* SIMD extensions\n+ *\n+ * We just use FloatRegister in the following. They are exactly the same\n+ * as SIMD registers.\n+ *\/\n+public:\n+\n+  enum SIMD_Arrangement {\n+    T8B, T16B, T4H, T8H, T2S, T4S, T1D, T2D, T1Q\n+  };\n+\n+  enum SIMD_RegVariant {\n+    B, H, S, D, Q\n+  };\n+\n@@ -1890,0 +1920,24 @@\n+private:\n+  void _fcvt_narrow_extend(FloatRegister Vd, SIMD_Arrangement Ta,\n+                           FloatRegister Vn, SIMD_Arrangement Tb, bool do_extend) {\n+    assert((do_extend && (Tb >> 1) + 1 == (Ta >> 1))\n+           || (!do_extend && (Ta >> 1) + 1 == (Tb >> 1)), \"Incompatible arrangement\");\n+    starti;\n+    int op30 = (do_extend ? Tb : Ta) & 1;\n+    int op22 = ((do_extend ? Ta : Tb) >> 1) & 1;\n+    f(0, 31), f(op30, 30), f(0b0011100, 29, 23), f(op22, 22);\n+    f(0b100001011, 21, 13), f(do_extend ? 1 : 0, 12), f(0b10, 11, 10);\n+    rf(Vn, 5), rf(Vd, 0);\n+  }\n+\n+public:\n+  void fcvtl(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,  SIMD_Arrangement Tb) {\n+    assert(Tb == T4H || Tb == T8H|| Tb == T2S || Tb == T4S, \"invalid arrangement\");\n+    _fcvt_narrow_extend(Vd, Ta, Vn, Tb, true);\n+  }\n+\n+  void fcvtn(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,  SIMD_Arrangement Tb) {\n+    assert(Ta == T4H || Ta == T8H|| Ta == T2S || Ta == T4S, \"invalid arrangement\");\n+    _fcvt_narrow_extend(Vd, Ta, Vn, Tb, false);\n+  }\n+\n@@ -2026,0 +2080,37 @@\n+  enum sign_kind { SIGNED, UNSIGNED };\n+\n+private:\n+  void _xcvtf_scalar_integer(sign_kind sign, unsigned sz,\n+                             FloatRegister Rd, FloatRegister Rn) {\n+    starti;\n+    f(0b01, 31, 30), f(sign == SIGNED ? 0 : 1, 29);\n+    f(0b111100, 27, 23), f((sz >> 1) & 1, 22), f(0b100001110110, 21, 10);\n+    rf(Rn, 5), rf(Rd, 0);\n+  }\n+\n+public:\n+#define INSN(NAME, sign, sz)                        \\\n+  void NAME(FloatRegister Rd, FloatRegister Rn) {   \\\n+    _xcvtf_scalar_integer(sign, sz, Rd, Rn);        \\\n+  }\n+\n+  INSN(scvtfs, SIGNED, 0);\n+  INSN(scvtfd, SIGNED, 1);\n+\n+#undef INSN\n+\n+private:\n+  void _xcvtf_vector_integer(sign_kind sign, SIMD_Arrangement T,\n+                             FloatRegister Rd, FloatRegister Rn) {\n+    assert(T == T2S || T == T4S || T == T2D, \"invalid arrangement\");\n+    starti;\n+    f(0, 31), f(T & 1, 30), f(sign == SIGNED ? 0 : 1, 29);\n+    f(0b011100, 28, 23), f((T >> 1) & 1, 22), f(0b100001110110, 21, 10);\n+    rf(Rn, 5), rf(Rd, 0);\n+  }\n+\n+public:\n+  void scvtfv(SIMD_Arrangement T, FloatRegister Rd, FloatRegister Rn) {\n+    _xcvtf_vector_integer(SIGNED, T, Rd, Rn);\n+  }\n+\n@@ -2155,15 +2246,0 @@\n-\/* SIMD extensions\n- *\n- * We just use FloatRegister in the following. They are exactly the same\n- * as SIMD registers.\n- *\/\n- public:\n-\n-  enum SIMD_Arrangement {\n-       T8B, T16B, T4H, T8H, T2S, T4S, T1D, T2D, T1Q\n-  };\n-\n-  enum SIMD_RegVariant {\n-       B, H, S, D, Q\n-  };\n-\n@@ -2327,0 +2403,5 @@\n+  INSN(maxv,   0, 0b011001, false); \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S\n+  INSN(minv,   0, 0b011011, false); \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S\n+  INSN(cmeq,   1, 0b100011, true);  \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S, T2D\n+  INSN(cmgt,   0, 0b001101, true);  \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S, T2D\n+  INSN(cmge,   0, 0b001111, true);  \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S, T2D\n@@ -2346,0 +2427,2 @@\n+  INSN(smaxv,  0, 0b110000101010, 1); \/\/ accepted arrangements: T8B, T16B, T4H, T8H,      T4S\n+  INSN(sminv,  0, 0b110001101010, 1); \/\/ accepted arrangements: T8B, T16B, T4H, T8H,      T4S\n@@ -2410,0 +2493,3 @@\n+  INSN(fcmeq, 0, 0, 0b111001);\n+  INSN(fcmgt, 1, 1, 0b111001);\n+  INSN(fcmge, 1, 0, 0b111001);\n@@ -2509,2 +2595,2 @@\n-  \/\/ (double) {a, b} -> (a + b)\n-  void faddpd(FloatRegister Vd, FloatRegister Vn) {\n+  \/\/ (long) {a, b} -> (a + b)\n+  void addpd(FloatRegister Vd, FloatRegister Vn) {\n@@ -2512,1 +2598,11 @@\n-    f(0b0111111001110000110110, 31, 10);\n+    f(0b0101111011110001101110, 31, 10);\n+    rf(Vn, 5), rf(Vd, 0);\n+  }\n+\n+  \/\/ (Floating-point) {a, b} -> (a + b)\n+  void faddp(FloatRegister Vd, FloatRegister Vn, SIMD_RegVariant type) {\n+    assert(type == D || type == S, \"Wrong type for faddp\");\n+    starti;\n+    f(0b011111100, 31, 23);\n+    f(type == D ? 1 : 0, 22);\n+    f(0b110000110110, 21, 10);\n@@ -2579,1 +2675,1 @@\n-  void _ushll(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn, SIMD_Arrangement Tb, int shift) {\n+  void _xshll(sign_kind sign, FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn, SIMD_Arrangement Tb, int shift) {\n@@ -2582,1 +2678,1 @@\n-     *   0001 xxx       8H, 8B\/16b shift = xxx\n+     *   0001 xxx       8H, 8B\/16B shift = xxx\n@@ -2589,1 +2685,2 @@\n-    f(0, 31), f(Tb & 1, 30), f(0b1011110, 29, 23), f((1 << ((Tb>>1)+3))|shift, 22, 16);\n+    f(0, 31), f(Tb & 1, 30), f(sign == SIGNED ? 0 : 1, 29), f(0b011110, 28, 23);\n+    f((1 << ((Tb>>1)+3))|shift, 22, 16);\n@@ -2596,1 +2693,1 @@\n-    _ushll(Vd, Ta, Vn, Tb, shift);\n+    _xshll(UNSIGNED, Vd, Ta, Vn, Tb, shift);\n@@ -2601,1 +2698,19 @@\n-    _ushll(Vd, Ta, Vn, Tb, shift);\n+    _xshll(UNSIGNED, Vd, Ta, Vn, Tb, shift);\n+  }\n+\n+  void uxtl(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,  SIMD_Arrangement Tb) {\n+    ushll(Vd, Ta, Vn, Tb, 0);\n+  }\n+\n+  void sshll(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,  SIMD_Arrangement Tb, int shift) {\n+    assert(Tb == T8B || Tb == T4H || Tb == T2S, \"invalid arrangement\");\n+    _xshll(SIGNED, Vd, Ta, Vn, Tb, shift);\n+  }\n+\n+  void sshll2(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,  SIMD_Arrangement Tb, int shift) {\n+    assert(Tb == T16B || Tb == T8H || Tb == T4S, \"invalid arrangement\");\n+    _xshll(SIGNED, Vd, Ta, Vn, Tb, shift);\n+  }\n+\n+  void sxtl(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,  SIMD_Arrangement Tb) {\n+    sshll(Vd, Ta, Vn, Tb, 0);\n@@ -2652,0 +2767,9 @@\n+  void xtn(FloatRegister Vd, SIMD_Arrangement Tb, FloatRegister Vn, SIMD_Arrangement Ta) {\n+    starti;\n+    int size_b = (int)Tb >> 1;\n+    int size_a = (int)Ta >> 1;\n+    assert(size_b < 3 && size_b == size_a - 1, \"Invalid size specifier\");\n+    f(0, 31), f(Tb & 1, 30), f(0b001110, 29, 24), f(size_b, 23, 22);\n+    f(0b100001001010, 21, 10), rf(Vn, 5), rf(Vd, 0);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":147,"deletions":23,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -614,0 +614,10 @@\n+  \/\/ Generate indices for iota vector.\n+  address generate_iota_indices(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0706050403020100, relocInfo::none);\n+    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -5961,0 +5971,2 @@\n+    StubRoutines::aarch64::_vector_iota_indices    = generate_iota_indices(\"iota_indices\");\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2164,0 +2164,4 @@\n+bool Matcher::supports_vector_variable_shifts(void) {\n+  return false; \/\/ not supported\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -987,0 +987,2 @@\n+    case 0x1f: \/\/ evpcmpd\/evpcmpq\n+    case 0x3f: \/\/ evpcmpb\/evpcmpw\n@@ -1212,0 +1214,5 @@\n+void Assembler::addw(Register dst, Register src) {\n+  (void)prefix_and_encode(dst->encoding(), src->encoding());\n+  emit_arith(0x03, 0xC0, dst, src);\n+}\n+\n@@ -1418,0 +1425,5 @@\n+void Assembler::andw(Register dst, Register src) {\n+  (void)prefix_and_encode(dst->encoding(), src->encoding());\n+  emit_arith(0x23, 0xC0, dst, src);\n+}\n+\n@@ -1786,0 +1798,7 @@\n+void Assembler::vcvtdq2pd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE6, (0xC0 | encode));\n+}\n+\n@@ -1793,0 +1812,7 @@\n+void Assembler::vcvtdq2ps(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5B, (0xC0 | encode));\n+}\n+\n@@ -1915,3 +1941,3 @@\n-  assert(vector_len == AVX_128bit? VM_Version::supports_avx() :\n-  vector_len == AVX_256bit? VM_Version::supports_avx2() :\n-  vector_len == AVX_512bit? VM_Version::supports_avx512bw() : 0, \"\");\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx()      :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2()     :\n+         vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : false, \"not supported\");\n@@ -1924,3 +1950,3 @@\n-  assert(vector_len == AVX_128bit? VM_Version::supports_avx() :\n-  vector_len == AVX_256bit? VM_Version::supports_avx2() :\n-  vector_len == AVX_512bit? VM_Version::supports_avx512bw() : 0, \"\");\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx()      :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2()     :\n+         vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : false, \"\");\n@@ -1949,0 +1975,79 @@\n+void Assembler::vcvtps2pd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5A, (0xC0 | encode));\n+}\n+\n+void Assembler::vcvtpd2ps(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  attributes.set_rex_vex_w_reverted();\n+  emit_int16(0x5A, (0xC0 | encode));\n+}\n+\n+void Assembler::evcvtqq2ps(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5B, (0xC0 | encode));\n+}\n+\n+void Assembler::evcvtqq2pd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE6, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovwb(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2  && VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovdw(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x33, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovdb(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovqd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x35, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovqb(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovqw(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(src->encoding(), 0, dst->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x34, (0xC0 | encode));\n+}\n+\n@@ -2546,1 +2651,1 @@\n-void Assembler::evmovdqub(XMMRegister dst, XMMRegister src, int vector_len) {\n+void Assembler::evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len) {\n@@ -2550,0 +2655,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2555,1 +2663,1 @@\n-void Assembler::evmovdqub(XMMRegister dst, Address src, int vector_len) {\n+void Assembler::evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len) {\n@@ -2562,0 +2670,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2567,1 +2678,1 @@\n-void Assembler::evmovdqub(Address dst, XMMRegister src, int vector_len) {\n+void Assembler::evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len) {\n@@ -2575,0 +2686,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2580,1 +2694,1 @@\n-void Assembler::evmovdqub(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+void Assembler::evmovdqub(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n@@ -2587,0 +2701,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2624,1 +2741,1 @@\n-void Assembler::evmovdquw(XMMRegister dst, Address src, int vector_len) {\n+void Assembler::evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len) {\n@@ -2630,0 +2747,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2636,1 +2756,1 @@\n-void Assembler::evmovdquw(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+void Assembler::evmovdquw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n@@ -2643,0 +2763,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2648,1 +2771,1 @@\n-void Assembler::evmovdquw(Address dst, XMMRegister src, int vector_len) {\n+void Assembler::evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len) {\n@@ -2655,0 +2778,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2661,1 +2787,1 @@\n-void Assembler::evmovdquw(Address dst, KRegister mask, XMMRegister src, int vector_len) {\n+void Assembler::evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -2667,1 +2793,0 @@\n-  attributes.reset_is_clear_context();\n@@ -2670,0 +2795,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2676,0 +2804,5 @@\n+  \/\/ Unmasked instruction\n+  evmovdqul(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -2677,1 +2810,2 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -2679,0 +2813,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2684,0 +2821,5 @@\n+  \/\/ Unmasked instruction\n+  evmovdqul(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n@@ -2686,1 +2828,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true , \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false , \/* uses_vl *\/ true);\n@@ -2688,0 +2830,1 @@\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -2689,0 +2832,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2695,0 +2841,5 @@\n+  \/\/ Unmasked isntruction\n+  evmovdqul(dst, k0, src, \/*merge*\/ true, vector_len);\n+}\n+\n+void Assembler::evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -2698,1 +2849,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -2700,1 +2851,1 @@\n-  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -2702,0 +2853,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2708,0 +2862,6 @@\n+  \/\/ Unmasked instruction\n+  if (dst->encoding() == src->encoding()) return;\n+  evmovdquq(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdquq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -2709,1 +2869,2 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -2711,0 +2872,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2716,0 +2880,5 @@\n+  \/\/ Unmasked instruction\n+  evmovdquq(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n@@ -2718,1 +2887,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -2720,0 +2889,1 @@\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -2721,0 +2891,3 @@\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2727,0 +2900,5 @@\n+  \/\/ Unmasked instruction\n+  evmovdquq(dst, k0, src, \/*merge*\/ true, vector_len);\n+}\n+\n+void Assembler::evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n@@ -2730,1 +2908,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -2732,1 +2910,4 @@\n-  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n@@ -2810,0 +2991,23 @@\n+void Assembler::movq(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = simd_prefix_and_encode(src, xnoreg, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD6, (0xC0 | encode));\n+}\n+\n+void Assembler::movq(Register dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  \/\/ swap src\/dst to get correct prefix\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x7E, (0xC0 | encode));\n+}\n+\n+void Assembler::movq(XMMRegister dst, Register src) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6E, (0xC0 | encode));\n+}\n+\n@@ -3309,0 +3513,5 @@\n+void Assembler::orw(Register dst, Register src) {\n+  (void)prefix_and_encode(dst->encoding(), src->encoding());\n+  emit_arith(0x0B, 0xC0, dst, src);\n+}\n+\n@@ -3347,0 +3556,28 @@\n+void Assembler::packsswb(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x63, (0xC0 | encode));\n+}\n+\n+void Assembler::vpacksswb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"some form of AVX must be enabled\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x63, (0xC0 | encode));\n+}\n+\n+void Assembler::packssdw(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6B, (0xC0 | encode));\n+}\n+\n+void Assembler::vpackssdw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"some form of AVX must be enabled\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6B, (0xC0 | encode));\n+}\n+\n@@ -3372,0 +3609,14 @@\n+void Assembler::packusdw(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x2B, (0xC0 | encode));\n+}\n+\n+void Assembler::vpackusdw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"some form of AVX must be enabled\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x2B, (0xC0 | encode));\n+}\n+\n@@ -3374,0 +3625,2 @@\n+  assert(vector_len != AVX_128bit, \"\");\n+  \/\/ VEX.256.66.0F3A.W1 00 \/r ib\n@@ -3380,1 +3633,2 @@\n-  assert(UseAVX > 2, \"requires AVX512F\");\n+  assert(vector_len == AVX_256bit ? VM_Version::supports_avx512vl() :\n+         vector_len == AVX_512bit ? VM_Version::supports_evex()     : false, \"not supported\");\n@@ -3387,0 +3641,36 @@\n+void Assembler::vpermb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::vpermw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx512vlbw() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx512vlbw() :\n+         vector_len == AVX_512bit ? VM_Version::supports_avx512bw()   : false, \"not supported\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::vpermd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex(), \"\");\n+  \/\/ VEX.NDS.256.66.0F38.W0 36 \/r\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::vpermd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex(), \"\");\n+  \/\/ VEX.NDS.256.66.0F38.W0 36 \/r\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n@@ -3401,0 +3691,22 @@\n+void Assembler::vpermilps(XMMRegister dst, XMMRegister src, int imm8, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x04, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::vpermilpd(XMMRegister dst, XMMRegister src, int imm8, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ VM_Version::supports_evex(),\/* legacy_mode *\/ false,\/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x05, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::vpermpd(XMMRegister dst, XMMRegister src, int imm8, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x01, (0xC0 | encode), imm8);\n+}\n+\n@@ -3409,1 +3721,0 @@\n-\n@@ -3443,0 +3754,8 @@\n+void Assembler::vpcmpCCbwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cond_encoding, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(cond_encoding, (0xC0 | encode));\n+}\n+\n@@ -3445,1 +3764,2 @@\n-  assert(VM_Version::supports_avx(), \"\");\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n@@ -3532,1 +3852,1 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_reg_mask *\/ false, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3552,1 +3872,2 @@\n-  assert(VM_Version::supports_avx(), \"\");\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n@@ -3589,2 +3910,3 @@\n-  assert(VM_Version::supports_avx(), \"\");\n-  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3596,1 +3918,1 @@\n-void Assembler::evpcmpeqd(KRegister kdst, XMMRegister nds, XMMRegister src, int vector_len) {\n+void Assembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src, int vector_len) {\n@@ -3598,1 +3920,1 @@\n-  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3601,0 +3923,1 @@\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -3605,1 +3928,1 @@\n-void Assembler::evpcmpeqd(KRegister kdst, XMMRegister nds, Address src, int vector_len) {\n+void Assembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, Address src, int vector_len) {\n@@ -3608,1 +3931,1 @@\n-  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3610,1 +3933,2 @@\n-  attributes.reset_is_clear_context();\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n@@ -3626,0 +3950,7 @@\n+void Assembler::vpcmpCCq(XMMRegister dst, XMMRegister nds, XMMRegister src, int cond_encoding, int vector_len) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(cond_encoding, (0xC0 | encode));\n+}\n+\n@@ -3658,0 +3989,25 @@\n+void Assembler::evpmovd2m(KRegister kdst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovq2m(KRegister kdst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::pcmpgtq(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x37, (0xC0 | encode));\n+}\n+\n@@ -3674,1 +4030,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3681,1 +4037,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3691,1 +4047,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3698,1 +4054,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3708,1 +4064,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3715,1 +4071,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3723,0 +4079,7 @@\n+void Assembler::pextrb(Register dst, XMMRegister src, int imm8) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x14, (0xC0 | encode), imm8);\n+}\n+\n@@ -3725,1 +4088,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3735,1 +4098,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3742,1 +4105,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3750,0 +4113,7 @@\n+void Assembler::vpinsrd(XMMRegister dst, XMMRegister nds, Register src, int imm8) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x22, (0xC0 | encode), imm8);\n+}\n+\n@@ -3752,1 +4122,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3759,1 +4129,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3767,0 +4137,7 @@\n+void Assembler::vpinsrq(XMMRegister dst, XMMRegister nds, Register src, int imm8) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_dq, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x22, (0xC0 | encode), imm8);\n+}\n+\n@@ -3769,1 +4146,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3776,1 +4153,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3784,0 +4161,7 @@\n+void Assembler::vpinsrw(XMMRegister dst, XMMRegister nds, Register src, int imm8) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC4, (0xC0 | encode), imm8);\n+}\n+\n@@ -3786,1 +4170,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3794,0 +4178,28 @@\n+void Assembler::pinsrb(XMMRegister dst, Register src, int imm8) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x20, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::vpinsrb(XMMRegister dst, XMMRegister nds, Register src, int imm8) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x20, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::insertps(XMMRegister dst, XMMRegister src, int imm8) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x21, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::vinsertps(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x21, (0xC0 | encode), imm8);\n+}\n+\n@@ -3818,0 +4230,35 @@\n+void Assembler::pmovzxdq(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x35, (0xC0 | encode));\n+}\n+\n+void Assembler::pmovsxbd(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x21, (0xC0 | encode));\n+}\n+\n+void Assembler::pmovzxbd(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+}\n+\n+void Assembler::pmovsxbq(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x22, (0xC0 | encode));\n+}\n+\n+void Assembler::pmovsxwd(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x23, (0xC0 | encode));\n+}\n+\n@@ -3851,1 +4298,1 @@\n-  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n@@ -3859,0 +4306,80 @@\n+\n+void Assembler::evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W0 DB \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xDB, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovzxdq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len > AVX_128bit ? VM_Version::supports_avx2() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x35, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovzxbd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len > AVX_128bit ? VM_Version::supports_avx2() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovzxbq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len > AVX_128bit ? VM_Version::supports_avx2() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovsxbd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+             VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x21, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovsxbq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+             VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x22, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovsxwd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+             VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x23, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovsxwq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+             VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x24, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmovsxdq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+             VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x25, (0xC0 | encode));\n+}\n+\n@@ -4085,0 +4612,8 @@\n+void Assembler::pshufhw(XMMRegister dst, XMMRegister src, int mode) {\n+  assert(isByte(mode), \"invalid value\");\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x70, (0xC0 | encode), mode & 0xFF);\n+}\n+\n@@ -4115,0 +4650,29 @@\n+void Assembler::pshufpd(XMMRegister dst, XMMRegister src, int imm8) {\n+  assert(isByte(imm8), \"invalid value\");\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC6, (0xC0 | encode), imm8 & 0xFF);\n+}\n+\n+void Assembler::vpshufpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC6, (0xC0 | encode), imm8 & 0xFF);\n+}\n+\n+void Assembler::pshufps(XMMRegister dst, XMMRegister src, int imm8) {\n+  assert(isByte(imm8), \"invalid value\");\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC6, (0xC0 | encode), imm8 & 0xFF);\n+}\n+\n+void Assembler::vpshufps(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC6, (0xC0 | encode), imm8 & 0xFF);\n+}\n+\n@@ -4186,0 +4750,7 @@\n+void Assembler::vptest(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x17, (0xC0 | encode));\n+}\n+\n@@ -4916,0 +5487,5 @@\n+void Assembler::xorw(Register dst, Register src) {\n+  (void)prefix_and_encode(dst->encoding(), src->encoding());\n+  emit_arith(0x33, 0xC0, dst, src);\n+}\n+\n@@ -5829,0 +6405,7 @@\n+void Assembler::pmuludq(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF4, (0xC0 | encode));\n+}\n+\n@@ -5851,0 +6434,7 @@\n+void Assembler::vpmuludq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF4, (0xC0 | encode));\n+}\n+\n@@ -5882,3 +6472,3 @@\n-\/\/ Shift packed integers left by specified number of bits.\n-void Assembler::psllw(XMMRegister dst, int shift) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+\/\/ Min, max\n+void Assembler::pminsb(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n@@ -5886,3 +6476,2 @@\n-  \/\/ XMM6 is for \/6 encoding: 66 0F 71 \/6 ib\n-  int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n@@ -5891,6 +6480,6 @@\n-void Assembler::pslld(XMMRegister dst, int shift) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  \/\/ XMM6 is for \/6 encoding: 66 0F 72 \/6 ib\n-  int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+void Assembler::vpminsb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n@@ -5899,6 +6488,5 @@\n-void Assembler::psllq(XMMRegister dst, int shift) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  \/\/ XMM6 is for \/6 encoding: 66 0F 73 \/6 ib\n-  int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+void Assembler::pminsw(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEA, (0xC0 | encode));\n@@ -5907,5 +6495,6 @@\n-void Assembler::psllw(XMMRegister dst, XMMRegister shift) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+void Assembler::vpminsw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEA, (0xC0 | encode));\n@@ -5914,2 +6503,2 @@\n-void Assembler::pslld(XMMRegister dst, XMMRegister shift) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+void Assembler::pminsd(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n@@ -5917,2 +6506,2 @@\n-  int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n@@ -5921,6 +6510,6 @@\n-void Assembler::psllq(XMMRegister dst, XMMRegister shift) {\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_rex_vex_w_reverted();\n-  int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xF3, (0xC0 | encode));\n+void Assembler::vpminsd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n@@ -5929,6 +6518,6 @@\n-void Assembler::vpsllw(XMMRegister dst, XMMRegister src, int shift, int vector_len) {\n-  assert(UseAVX > 0, \"requires some form of AVX\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  \/\/ XMM6 is for \/6 encoding: 66 0F 71 \/6 ib\n-  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+void Assembler::vpminsq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"requires AVX512F\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n@@ -5937,3 +6526,8 @@\n-void Assembler::vpslld(XMMRegister dst, XMMRegister src, int shift, int vector_len) {\n-  assert(UseAVX > 0, \"requires some form of AVX\");\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+void Assembler::minps(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5D, (0xC0 | encode));\n+}\n+void Assembler::vminps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len >= AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n@@ -5941,3 +6535,2 @@\n-  \/\/ XMM6 is for \/6 encoding: 66 0F 72 \/6 ib\n-  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5D, (0xC0 | encode));\n@@ -5946,7 +6539,5 @@\n-void Assembler::vpsllq(XMMRegister dst, XMMRegister src, int shift, int vector_len) {\n-  assert(UseAVX > 0, \"requires some form of AVX\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_rex_vex_w_reverted();\n-  \/\/ XMM6 is for \/6 encoding: 66 0F 73 \/6 ib\n-  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+void Assembler::minpd(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5D, (0xC0 | encode));\n@@ -5954,6 +6545,5 @@\n-\n-void Assembler::vpsllw(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n-  assert(UseAVX > 0, \"requires some form of AVX\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+void Assembler::vminpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len >= AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5D, (0xC0 | encode));\n@@ -5962,5 +6552,5 @@\n-void Assembler::vpslld(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n-  assert(UseAVX > 0, \"requires some form of AVX\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n-  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+void Assembler::pmaxsb(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3C, (0xC0 | encode));\n@@ -5969,2 +6559,163 @@\n-void Assembler::vpsllq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n-  assert(UseAVX > 0, \"requires some form of AVX\");\n+void Assembler::vpmaxsb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3C, (0xC0 | encode));\n+}\n+\n+void Assembler::pmaxsw(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEE, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmaxsw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEE, (0xC0 | encode));\n+}\n+\n+void Assembler::pmaxsd(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmaxsd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::vpmaxsq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"requires AVX512F\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::maxps(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5F, (0xC0 | encode));\n+}\n+\n+void Assembler::vmaxps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len >= AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5F, (0xC0 | encode));\n+}\n+\n+void Assembler::maxpd(XMMRegister dst, XMMRegister src) {\n+  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5F, (0xC0 | encode));\n+}\n+\n+void Assembler::vmaxpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len >= AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5F, (0xC0 | encode));\n+}\n+\n+\/\/ Shift packed integers left by specified number of bits.\n+void Assembler::psllw(XMMRegister dst, int shift) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  \/\/ XMM6 is for \/6 encoding: 66 0F 71 \/6 ib\n+  int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::pslld(XMMRegister dst, int shift) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  \/\/ XMM6 is for \/6 encoding: 66 0F 72 \/6 ib\n+  int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::psllq(XMMRegister dst, int shift) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  \/\/ XMM6 is for \/6 encoding: 66 0F 73 \/6 ib\n+  int encode = simd_prefix_and_encode(xmm6, dst, dst, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::psllw(XMMRegister dst, XMMRegister shift) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+}\n+\n+void Assembler::pslld(XMMRegister dst, XMMRegister shift) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+}\n+\n+void Assembler::psllq(XMMRegister dst, XMMRegister shift) {\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = simd_prefix_and_encode(dst, dst, shift, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF3, (0xC0 | encode));\n+}\n+\n+void Assembler::vpsllw(XMMRegister dst, XMMRegister src, int shift, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  \/\/ XMM6 is for \/6 encoding: 66 0F 71 \/6 ib\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::vpslld(XMMRegister dst, XMMRegister src, int shift, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  \/\/ XMM6 is for \/6 encoding: 66 0F 72 \/6 ib\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::vpsllq(XMMRegister dst, XMMRegister src, int shift, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_rex_vex_w_reverted();\n+  \/\/ XMM6 is for \/6 encoding: 66 0F 73 \/6 ib\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::vpsllw(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+}\n+\n+void Assembler::vpslld(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+}\n+\n+void Assembler::vpsllq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n@@ -6203,0 +6954,55 @@\n+\/\/Variable Shift packed integers logically left.\n+void Assembler::vpsllvd(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 1, \"requires AVX2\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::vpsllvq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 1, \"requires AVX2\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+\/\/Variable Shift packed integers logically right.\n+void Assembler::vpsrlvd(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 1, \"requires AVX2\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::vpsrlvq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 1, \"requires AVX2\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+\/\/Variable right Shift arithmetic packed integers .\n+void Assembler::vpsravd(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 1, \"requires AVX2\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len) {\n+  assert(UseAVX > 2, \"requires AVX512\");\n+  assert(vector_len == Assembler::AVX_512bit || VM_Version::supports_avx512vl(), \"requires AVX512VL\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), shift->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n@@ -6208,2 +7014,1 @@\n-  emit_int8(0x71);\n-  emit_int8((0xC0 | encode));\n+  emit_int16(0x71, (0xC0 | encode));\n@@ -6235,1 +7040,0 @@\n-\n@@ -6268,0 +7072,29 @@\n+void Assembler::evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W0 EB \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W0 EB \/r\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEB);\n+  emit_operand(dst, src);\n+}\n+\n@@ -6292,0 +7125,21 @@\n+void Assembler::vpxorq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2, \"requires some form of EVEX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ VM_Version::supports_evex(), \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_rex_vex_w_reverted();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n+}\n+\n+void Assembler::evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W0 EF \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n+}\n+\n@@ -6297,2 +7151,1 @@\n-  emit_int8((unsigned char)0xEF);\n-  emit_int8((0xC0 | encode));\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n@@ -6995,0 +7848,52 @@\n+\n+void Assembler::vpgatherdd(XMMRegister dst, Address src, XMMRegister mask, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(vector_len == Assembler::AVX_128bit || vector_len == Assembler::AVX_256bit, \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x90);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::vpgatherdq(XMMRegister dst, Address src, XMMRegister mask, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(vector_len == Assembler::AVX_128bit || vector_len == Assembler::AVX_256bit, \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x90);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::vgatherdpd(XMMRegister dst, Address src, XMMRegister mask, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(vector_len == Assembler::AVX_128bit || vector_len == Assembler::AVX_256bit, \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x92);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::vgatherdps(XMMRegister dst, Address src, XMMRegister mask, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(vector_len == Assembler::AVX_128bit || vector_len == Assembler::AVX_256bit, \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  vex_prefix(src, mask->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x92);\n+  emit_operand(dst, src);\n+}\n@@ -6998,0 +7903,3 @@\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n@@ -7000,1 +7908,1 @@\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_64bit);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -7009,13 +7917,123 @@\n-\/\/ Carry-Less Multiplication Quadword\n-void Assembler::pclmulqdq(XMMRegister dst, XMMRegister src, int mask) {\n-  assert(VM_Version::supports_clmul(), \"\");\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n-  emit_int24(0x44, (0xC0 | encode), (unsigned char)mask);\n-}\n-\/\/ Carry-Less Multiplication Quadword\n-void Assembler::vpclmulqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int mask) {\n-  assert(VM_Version::supports_avx() && VM_Version::supports_clmul(), \"\");\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n-  emit_int24(0x44, (0xC0 | encode), (unsigned char)mask);\n+void Assembler::evpgatherdq(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x90);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evgatherdpd(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x92);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evgatherdps(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  assert(src.isxmmindex(),\"expected to be xmm index\");\n+  assert(dst != src.xmmindex(), \"instruction will #UD if dst and index are the same\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x92);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpscatterdd(Address dst, KRegister mask, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA0);\n+  emit_operand(src, dst);\n+}\n+\n+void Assembler::evpscatterdq(Address dst, KRegister mask, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA0);\n+  emit_operand(src, dst);\n+}\n+\n+void Assembler::evscatterdps(Address dst, KRegister mask, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA2);\n+  emit_operand(src, dst);\n+}\n+\n+void Assembler::evscatterdpd(Address dst, KRegister mask, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(mask != k0, \"instruction will #UD if mask is in k0\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA2);\n+  emit_operand(src, dst);\n+}\n+\/\/ Carry-Less Multiplication Quadword\n+void Assembler::pclmulqdq(XMMRegister dst, XMMRegister src, int mask) {\n+  assert(VM_Version::supports_clmul(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x44, (0xC0 | encode), (unsigned char)mask);\n+}\n+\n+\/\/ Carry-Less Multiplication Quadword\n+void Assembler::vpclmulqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int mask) {\n+  assert(VM_Version::supports_avx() && VM_Version::supports_clmul(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x44, (0xC0 | encode), (unsigned char)mask);\n@@ -7606,1 +8624,2 @@\n-  if (_attributes->is_no_reg_mask() == false) {\n+  if (_attributes->is_no_reg_mask() == false &&\n+      _attributes->get_embedded_opmask_register_specifier() != 0) {\n@@ -7774,1 +8793,1 @@\n-void Assembler::cmppd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len) {\n+void Assembler::vcmppd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len) {\n@@ -7791,2 +8810,2 @@\n-void Assembler::blendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len) {\n-  assert(VM_Version::supports_avx(), \"\");\n+void Assembler::vblendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  assert(UseAVX > 0 && (vector_len == AVX_128bit || vector_len == AVX_256bit), \"\");\n@@ -7800,2 +8819,2 @@\n-void Assembler::cmpps(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len) {\n-  assert(VM_Version::supports_avx(), \"\");\n+void Assembler::vpblendd(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n@@ -7804,2 +8823,2 @@\n-  int encode = simd_prefix_and_encode(dst, nds, src, VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n-  emit_int24((unsigned char)0xC2, (0xC0 | encode), (0xF & cop));\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x02, (0xC0 | encode), (unsigned char)imm8);\n@@ -7808,1 +8827,1 @@\n-void Assembler::blendvps(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len) {\n+void Assembler::vcmpps(XMMRegister dst, XMMRegister nds, XMMRegister src, int comparison, int vector_len) {\n@@ -7812,0 +8831,55 @@\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC2, (0xC0 | encode), (unsigned char)comparison);\n+}\n+\n+void Assembler::evcmpps(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+                        ComparisonPredicateFP comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.0F.W0 C2 \/r ib\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC2, (0xC0 | encode), comparison);\n+}\n+\n+void Assembler::evcmppd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+                        ComparisonPredicateFP comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W1 C2 \/r ib\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24((unsigned char)0xC2, (0xC0 | encode), comparison);\n+}\n+\n+void Assembler::blendvps(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  assert(UseAVX <= 0, \"sse encoding is inconsistent with avx encoding\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::blendvpd(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  assert(UseAVX <= 0, \"sse encoding is inconsistent with avx encoding\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n+void Assembler::pblendvb(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  assert(UseAVX <= 0, \"sse encoding is inconsistent with avx encoding\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x10, (0xC0 | encode));\n+}\n+\n+void Assembler::vblendvps(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  assert(UseAVX > 0 && (vector_len == AVX_128bit || vector_len == AVX_256bit), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -7817,3 +8891,2 @@\n-void Assembler::vpblendd(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+void Assembler::vblendps(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8, int vector_len) {\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -7821,1 +8894,249 @@\n-  emit_int24(0x02, (0xC0 | encode), (unsigned char)imm8);\n+  emit_int24(0x0C, (0xC0 | encode), imm8);\n+}\n+\n+void Assembler::vpcmpgtb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x64, (0xC0 | encode));\n+}\n+\n+void Assembler::vpcmpgtw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x65, (0xC0 | encode));\n+}\n+\n+void Assembler::vpcmpgtd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x66, (0xC0 | encode));\n+}\n+\n+void Assembler::vpcmpgtq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x37, (0xC0 | encode));\n+}\n+\n+void Assembler::evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W0 1F \/r ib\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x1F, (0xC0 | encode), comparison);\n+}\n+\n+void Assembler::evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W0 1F \/r ib\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int dst_enc = kdst->encoding();\n+  vex_prefix(src, nds->encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8((unsigned char)0x1F);\n+  emit_operand(as_Register(dst_enc), src);\n+  emit_int8((unsigned char)comparison);\n+}\n+\n+void Assembler::evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W1 1F \/r ib\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x1F, (0xC0 | encode), comparison);\n+}\n+\n+void Assembler::evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W1 1F \/r ib\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int dst_enc = kdst->encoding();\n+  vex_prefix(src, nds->encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8((unsigned char)0x1F);\n+  emit_operand(as_Register(dst_enc), src);\n+  emit_int8((unsigned char)comparison);\n+}\n+\n+void Assembler::evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W0 3F \/r ib\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x3F, (0xC0 | encode), comparison);\n+}\n+\n+void Assembler::evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W0 3F \/r ib\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int dst_enc = kdst->encoding();\n+  vex_prefix(src, nds->encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8((unsigned char)0x3F);\n+  emit_operand(as_Register(dst_enc), src);\n+  emit_int8((unsigned char)comparison);\n+}\n+\n+void Assembler::evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W1 3F \/r ib\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x3F, (0xC0 | encode), comparison);\n+}\n+\n+void Assembler::evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+                        int comparison, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(comparison >= Assembler::eq && comparison <= Assembler::_true, \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F3A.W1 3F \/r ib\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.reset_is_clear_context();\n+  int dst_enc = kdst->encoding();\n+  vex_prefix(src, nds->encoding(), dst_enc, VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int8((unsigned char)0x3F);\n+  emit_operand(as_Register(dst_enc), src);\n+  emit_int8((unsigned char)comparison);\n+}\n+\n+void Assembler::vpblendvb(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int mask_enc = mask->encoding();\n+  emit_int24(0x4C, (0xC0 | encode), 0xF0 & mask_enc << 4);\n+}\n+\n+void Assembler::evblendmpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F38.W1 65 \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x65, (0xC0 | encode));\n+}\n+\n+void Assembler::evblendmps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F38.W0 65 \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x65, (0xC0 | encode));\n+}\n+\n+void Assembler::evpblendmb (XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  \/\/ Encoding: EVEX.NDS.512.66.0F38.W0 66 \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x66, (0xC0 | encode));\n+}\n+\n+void Assembler::evpblendmw (XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  \/\/ Encoding: EVEX.NDS.512.66.0F38.W1 66 \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x66, (0xC0 | encode));\n+}\n+\n+void Assembler::evpblendmd (XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/Encoding: EVEX.NDS.512.66.0F38.W0 64 \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x64, (0xC0 | encode));\n+}\n+\n+void Assembler::evpblendmq (XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  \/\/Encoding: EVEX.NDS.512.66.0F38.W1 64 \/r\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x64, (0xC0 | encode));\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":1468,"deletions":147,"binary":false,"changes":1615,"status":"modified"},{"patch":"@@ -591,0 +591,1 @@\n+  \/\/ Comparison predicates for integral types & FP types when using SSE\n@@ -602,0 +603,45 @@\n+  \/\/ Comparison predicates for FP types when using AVX\n+  \/\/ O means ordered. U is unordered. When using ordered, any NaN comparison is false. Otherwise, it is true.\n+  \/\/ S means signaling. Q means non-signaling. When signaling is true, instruction signals #IA on NaN.\n+  enum ComparisonPredicateFP {\n+    EQ_OQ = 0,\n+    LT_OS = 1,\n+    LE_OS = 2,\n+    UNORD_Q = 3,\n+    NEQ_UQ = 4,\n+    NLT_US = 5,\n+    NLE_US = 6,\n+    ORD_Q = 7,\n+    EQ_UQ = 8,\n+    NGE_US = 9,\n+    NGT_US = 0xA,\n+    FALSE_OQ = 0XB,\n+    NEQ_OQ = 0xC,\n+    GE_OS = 0xD,\n+    GT_OS = 0xE,\n+    TRUE_UQ = 0xF,\n+    EQ_OS = 0x10,\n+    LT_OQ = 0x11,\n+    LE_OQ = 0x12,\n+    UNORD_S = 0x13,\n+    NEQ_US = 0x14,\n+    NLT_UQ = 0x15,\n+    NLE_UQ = 0x16,\n+    ORD_S = 0x17,\n+    EQ_US = 0x18,\n+    NGE_UQ = 0x19,\n+    NGT_UQ = 0x1A,\n+    FALSE_OS = 0x1B,\n+    NEQ_OS = 0x1C,\n+    GE_OQ = 0x1D,\n+    GT_OQ = 0x1E,\n+    TRUE_US =0x1F\n+  };\n+\n+  enum Width {\n+    B = 0,\n+    W = 1,\n+    D = 2,\n+    Q = 3\n+  };\n+\n@@ -921,0 +967,1 @@\n+  void addw(Register dst, Register src);\n@@ -971,0 +1018,2 @@\n+  void andw(Register dst, Register src);\n+\n@@ -1096,0 +1145,1 @@\n+  void vcvtdq2pd(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1099,0 +1149,1 @@\n+  void vcvtdq2ps(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1114,0 +1165,1 @@\n+  \/\/ Convert vector double to int\n@@ -1116,0 +1168,16 @@\n+  \/\/ Convert vector float and double\n+  void vcvtps2pd(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vcvtpd2ps(XMMRegister dst, XMMRegister src, int vector_len);\n+\n+  \/\/ Convert vector long to vector FP\n+  void evcvtqq2ps(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evcvtqq2pd(XMMRegister dst, XMMRegister src, int vector_len);\n+\n+  \/\/ Evex casts with truncation\n+  void evpmovwb(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evpmovdw(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evpmovdb(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evpmovqd(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evpmovqb(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evpmovqw(XMMRegister dst, XMMRegister src, int vector_len);\n+\n@@ -1475,8 +1543,8 @@\n-  void evmovdqub(Address dst, XMMRegister src, int vector_len);\n-  void evmovdqub(XMMRegister dst, Address src, int vector_len);\n-  void evmovdqub(XMMRegister dst, XMMRegister src, int vector_len);\n-  void evmovdqub(XMMRegister dst, KRegister mask, Address src, int vector_len);\n-  void evmovdquw(Address dst, XMMRegister src, int vector_len);\n-  void evmovdquw(Address dst, KRegister mask, XMMRegister src, int vector_len);\n-  void evmovdquw(XMMRegister dst, Address src, int vector_len);\n-  void evmovdquw(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqub(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdquw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n@@ -1486,0 +1554,3 @@\n+  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -1489,0 +1560,3 @@\n+  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evmovdquq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n@@ -1524,0 +1598,3 @@\n+  void movq(XMMRegister dst, XMMRegister src);\n+  void movq(Register dst, XMMRegister src);\n+  void movq(XMMRegister dst, Register src);\n@@ -1604,0 +1681,2 @@\n+  void orw(Register dst, Register src);\n+\n@@ -1617,0 +1696,6 @@\n+  \/\/ Pack with signed saturation\n+  void packsswb(XMMRegister dst, XMMRegister src);\n+  void vpacksswb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void packssdw(XMMRegister dst, XMMRegister src);\n+  void vpackssdw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n@@ -1620,0 +1705,1 @@\n+  void packusdw(XMMRegister dst, XMMRegister src);\n@@ -1621,0 +1707,1 @@\n+  void vpackusdw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1622,1 +1709,1 @@\n-  \/\/ Pemutation of 64bit words\n+  \/\/ Permutations\n@@ -1626,0 +1713,4 @@\n+  void vpermb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void vpermw(XMMRegister dst,  XMMRegister nds, XMMRegister src, int vector_len);\n+  void vpermd(XMMRegister dst,  XMMRegister nds, Address src, int vector_len);\n+  void vpermd(XMMRegister dst,  XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1628,0 +1719,3 @@\n+  void vpermilps(XMMRegister dst, XMMRegister src, int imm8, int vector_len);\n+  void vpermilpd(XMMRegister dst, XMMRegister src, int imm8, int vector_len);\n+  void vpermpd(XMMRegister dst, XMMRegister src, int imm8, int vector_len);\n@@ -1640,0 +1734,2 @@\n+  void vpcmpCCbwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cond_encoding, int vector_len);\n+\n@@ -1645,0 +1741,1 @@\n+  void vpcmpgtb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1657,0 +1754,2 @@\n+  void vpcmpgtw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n@@ -1659,2 +1758,2 @@\n-  void evpcmpeqd(KRegister kdst, XMMRegister nds, XMMRegister src, int vector_len);\n-  void evpcmpeqd(KRegister kdst, XMMRegister nds, Address src, int vector_len);\n+  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, Address src, int vector_len);\n@@ -1663,0 +1762,1 @@\n+  void vpcmpCCq(XMMRegister dst, XMMRegister nds, XMMRegister src, int cond_encoding, int vector_len);\n@@ -1667,0 +1767,3 @@\n+  void pcmpgtq(XMMRegister dst, XMMRegister src);\n+  void vpcmpgtq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n@@ -1675,0 +1778,1 @@\n+  void pextrb(Register dst, XMMRegister src, int imm8);\n@@ -1683,0 +1787,1 @@\n+  void pinsrb(XMMRegister dst, Register src, int imm8);\n@@ -1686,0 +1791,1 @@\n+  void insertps(XMMRegister dst, XMMRegister src, int imm8);\n@@ -1690,1 +1796,8 @@\n-  \/\/ SSE4.1 packed move\n+  \/\/ AVX insert\n+  void vpinsrd(XMMRegister dst, XMMRegister nds, Register src, int imm8);\n+  void vpinsrb(XMMRegister dst, XMMRegister nds, Register src, int imm8);\n+  void vpinsrq(XMMRegister dst, XMMRegister nds, Register src, int imm8);\n+  void vpinsrw(XMMRegister dst, XMMRegister nds, Register src, int imm8);\n+  void vinsertps(XMMRegister dst, XMMRegister nds, XMMRegister src, int imm8);\n+\n+  \/\/ Zero extend moves\n@@ -1693,1 +1806,1 @@\n-\n+  void pmovzxbd(XMMRegister dst, XMMRegister src);\n@@ -1695,0 +1808,1 @@\n+  void pmovzxdq(XMMRegister dst, XMMRegister src);\n@@ -1696,0 +1810,3 @@\n+  void vpmovzxdq(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovzxbd(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovzxbq(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1698,0 +1815,12 @@\n+  \/\/ Sign extend moves\n+  void pmovsxbd(XMMRegister dst, XMMRegister src);\n+  void pmovsxbq(XMMRegister dst, XMMRegister src);\n+  void pmovsxbw(XMMRegister dst, XMMRegister src);\n+  void pmovsxwd(XMMRegister dst, XMMRegister src);\n+  void vpmovsxbd(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovsxbq(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovsxbw(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovsxwd(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovsxwq(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vpmovsxdq(XMMRegister dst, XMMRegister src, int vector_len);\n+\n@@ -1705,4 +1834,0 @@\n-  \/\/ Sign extend moves\n-  void pmovsxbw(XMMRegister dst, XMMRegister src);\n-  void vpmovsxbw(XMMRegister dst, XMMRegister src, int vector_len);\n-\n@@ -1752,1 +1877,2 @@\n-  \/\/ Shuffle Packed Low Words\n+  \/\/ Shuffle Packed High\/Low Words\n+  void pshufhw(XMMRegister dst, XMMRegister src, int mode);\n@@ -1756,0 +1882,6 @@\n+  \/\/shuffle floats and doubles\n+  void pshufps(XMMRegister, XMMRegister, int);\n+  void pshufpd(XMMRegister, XMMRegister, int);\n+  void vpshufps(XMMRegister, XMMRegister, XMMRegister, int, int);\n+  void vpshufpd(XMMRegister, XMMRegister, XMMRegister, int, int);\n+\n@@ -1771,0 +1903,3 @@\n+  \/\/ Vector compare\n+  void vptest(XMMRegister dst, XMMRegister src, int vector_len);\n+\n@@ -1844,0 +1979,1 @@\n+  void vblendps(XMMRegister dst, XMMRegister src1, XMMRegister src2, int imm8, int vector_len);\n@@ -1962,0 +2098,1 @@\n+  void xorw(Register dst, Register src);\n@@ -2000,0 +2137,2 @@\n+  void evpmovd2m(KRegister kdst, XMMRegister src, int vector_len);\n+  void evpmovq2m(KRegister kdst, XMMRegister src, int vector_len);\n@@ -2109,0 +2248,1 @@\n+  void pmuludq(XMMRegister dst, XMMRegister src);\n@@ -2112,0 +2252,1 @@\n+  void vpmuludq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2116,0 +2257,26 @@\n+  \/\/ Minimum of packed integers\n+  void pminsb(XMMRegister dst, XMMRegister src);\n+  void vpminsb(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void pminsw(XMMRegister dst, XMMRegister src);\n+  void vpminsw(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void pminsd(XMMRegister dst, XMMRegister src);\n+  void vpminsd(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void vpminsq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void minps(XMMRegister dst, XMMRegister src);\n+  void vminps(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void minpd(XMMRegister dst, XMMRegister src);\n+  void vminpd(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+\n+  \/\/ Maximum of packed integers\n+  void pmaxsb(XMMRegister dst, XMMRegister src);\n+  void vpmaxsb(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void pmaxsw(XMMRegister dst, XMMRegister src);\n+  void vpmaxsw(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void pmaxsd(XMMRegister dst, XMMRegister src);\n+  void vpmaxsd(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void vpmaxsq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void maxps(XMMRegister dst, XMMRegister src);\n+  void vmaxps(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void maxpd(XMMRegister dst, XMMRegister src);\n+  void vmaxpd(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+\n@@ -2157,0 +2324,1 @@\n+  void evpsravw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2160,0 +2328,12 @@\n+  \/\/ Variable shift left packed integers\n+  void vpsllvd(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len);\n+  void vpsllvq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len);\n+\n+  \/\/ Variable shift right packed integers\n+  void vpsrlvd(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len);\n+  void vpsrlvq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len);\n+\n+  \/\/ Variable shift right arithmetic packed integers\n+  void vpsravd(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len);\n+  void evpsravq(XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len);\n+\n@@ -2167,0 +2347,1 @@\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2179,0 +2360,3 @@\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n@@ -2183,0 +2367,2 @@\n+  void vpxorq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2260,1 +2446,15 @@\n-  void evpgatherdd(XMMRegister dst, KRegister k1, Address src, int vector_len);\n+  \/\/ Gather AVX2 and AVX3\n+  void vpgatherdd(XMMRegister dst, Address src, XMMRegister mask, int vector_len);\n+  void vpgatherdq(XMMRegister dst, Address src, XMMRegister mask, int vector_len);\n+  void vgatherdpd(XMMRegister dst, Address src, XMMRegister mask, int vector_len);\n+  void vgatherdps(XMMRegister dst, Address src, XMMRegister mask, int vector_len);\n+  void evpgatherdd(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evpgatherdq(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evgatherdpd(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evgatherdps(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+\n+  \/\/Scatter AVX3 only\n+  void evpscatterdd(Address dst, KRegister mask, XMMRegister src, int vector_len);\n+  void evpscatterdq(Address dst, KRegister mask, XMMRegister src, int vector_len);\n+  void evscatterdps(Address dst, KRegister mask, XMMRegister src, int vector_len);\n+  void evscatterdpd(Address dst, KRegister mask, XMMRegister src, int vector_len);\n@@ -2273,1 +2473,39 @@\n-  \/\/ AVX support for vectorized conditional move (float\/double). The following two instructions used only coupled.\n+  \/\/ Vector double compares\n+  void vcmppd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len);\n+  void evcmppd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               ComparisonPredicateFP comparison, int vector_len);\n+\n+  \/\/ Vector float compares\n+  void vcmpps(XMMRegister dst, XMMRegister nds, XMMRegister src, int comparison, int vector_len);\n+  void evcmpps(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               ComparisonPredicateFP comparison, int vector_len);\n+\n+  \/\/ Vector integer compares\n+  void vpcmpgtd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len);\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+               int comparison, int vector_len);\n+\n+  \/\/ Vector long compares\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len);\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+               int comparison, int vector_len);\n+\n+  \/\/ Vector byte compares\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len);\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+               int comparison, int vector_len);\n+\n+  \/\/ Vector short compares\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len);\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, Address src,\n+               int comparison, int vector_len);\n+\n+  \/\/ Vector blends\n+  void blendvps(XMMRegister dst, XMMRegister src);\n+  void blendvpd(XMMRegister dst, XMMRegister src);\n+  void pblendvb(XMMRegister dst, XMMRegister src);\n@@ -2275,4 +2513,3 @@\n-  void cmppd(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len);\n-  void blendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len);\n-  void cmpps(XMMRegister dst, XMMRegister nds, XMMRegister src, int cop, int vector_len);\n-  void blendvps(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void vblendvps(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len);\n+  void vblendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void vpblendvb(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len);\n@@ -2280,1 +2517,6 @@\n-\n+  void evblendmpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evblendmps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpblendmb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpblendmw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpblendmd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpblendmq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2376,1 +2618,2 @@\n-  \/\/ Set the Evex.Z field to be used to clear all non directed XMM\/YMM\/ZMM components\n+  \/\/ When the Evex.Z field is set (true), it is used to clear all non directed XMM\/YMM\/ZMM components.\n+  \/\/ This method unsets it so that merge semantics are used instead.\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":269,"deletions":26,"binary":false,"changes":295,"status":"modified"},{"patch":"@@ -36,0 +36,15 @@\n+inline Assembler::AvxVectorLen C2_MacroAssembler::vector_length_encoding(int vlen_in_bytes) {\n+  switch (vlen_in_bytes) {\n+    case  4: \/\/ fall-through\n+    case  8: \/\/ fall-through\n+    case 16: return Assembler::AVX_128bit;\n+    case 32: return Assembler::AVX_256bit;\n+    case 64: return Assembler::AVX_512bit;\n+\n+    default: {\n+      ShouldNotReachHere();\n+      return Assembler::AVX_NoVec;\n+    }\n+  }\n+}\n+\n@@ -864,0 +879,161 @@\n+void C2_MacroAssembler::pminmax(int opcode, BasicType elem_bt, XMMRegister dst, XMMRegister src, XMMRegister tmp) {\n+  assert(opcode == Op_MinV || opcode == Op_MaxV, \"sanity\");\n+\n+  if (opcode == Op_MinV) {\n+    if (elem_bt == T_BYTE) {\n+      pminsb(dst, src);\n+    } else if (elem_bt == T_SHORT) {\n+      pminsw(dst, src);\n+    } else if (elem_bt == T_INT) {\n+      pminsd(dst, src);\n+    } else {\n+      assert(elem_bt == T_LONG, \"required\");\n+      assert(tmp == xmm0, \"required\");\n+      movdqu(xmm0, dst);\n+      pcmpgtq(xmm0, src);\n+      blendvpd(dst, src);  \/\/ xmm0 as mask\n+    }\n+  } else { \/\/ opcode == Op_MaxV\n+    if (elem_bt == T_BYTE) {\n+      pmaxsb(dst, src);\n+    } else if (elem_bt == T_SHORT) {\n+      pmaxsw(dst, src);\n+    } else if (elem_bt == T_INT) {\n+      pmaxsd(dst, src);\n+    } else {\n+      assert(elem_bt == T_LONG, \"required\");\n+      assert(tmp == xmm0, \"required\");\n+      movdqu(xmm0, src);\n+      pcmpgtq(xmm0, dst);\n+      blendvpd(dst, src);  \/\/ xmm0 as mask\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vpminmax(int opcode, BasicType elem_bt,\n+                                 XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                                 int vlen_enc) {\n+  assert(opcode == Op_MinV || opcode == Op_MaxV, \"sanity\");\n+\n+  if (opcode == Op_MinV) {\n+    if (elem_bt == T_BYTE) {\n+      vpminsb(dst, src1, src2, vlen_enc);\n+    } else if (elem_bt == T_SHORT) {\n+      vpminsw(dst, src1, src2, vlen_enc);\n+    } else if (elem_bt == T_INT) {\n+      vpminsd(dst, src1, src2, vlen_enc);\n+    } else {\n+      assert(elem_bt == T_LONG, \"required\");\n+      if (UseAVX > 2 && (vlen_enc == Assembler::AVX_512bit || VM_Version::supports_avx512vl())) {\n+        vpminsq(dst, src1, src2, vlen_enc);\n+      } else {\n+        vpcmpgtq(dst, src1, src2, vlen_enc);\n+        vblendvpd(dst, src1, src2, dst, vlen_enc);\n+      }\n+    }\n+  } else { \/\/ opcode == Op_MaxV\n+    if (elem_bt == T_BYTE) {\n+      vpmaxsb(dst, src1, src2, vlen_enc);\n+    } else if (elem_bt == T_SHORT) {\n+      vpmaxsw(dst, src1, src2, vlen_enc);\n+    } else if (elem_bt == T_INT) {\n+      vpmaxsd(dst, src1, src2, vlen_enc);\n+    } else {\n+      assert(elem_bt == T_LONG, \"required\");\n+      if (UseAVX > 2 && (vlen_enc == Assembler::AVX_512bit || VM_Version::supports_avx512vl())) {\n+        vpmaxsq(dst, src1, src2, vlen_enc);\n+      } else {\n+        vpcmpgtq(dst, src1, src2, vlen_enc);\n+        vblendvpd(dst, src2, src1, dst, vlen_enc);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Float\/Double min max\n+\n+void C2_MacroAssembler::vminmax_fp(int opcode, BasicType elem_bt,\n+                                   XMMRegister dst, XMMRegister a, XMMRegister b,\n+                                   XMMRegister tmp, XMMRegister atmp, XMMRegister btmp,\n+                                   int vlen_enc) {\n+  assert(UseAVX > 0, \"required\");\n+  assert(opcode == Op_MinV || opcode == Op_MinReductionV ||\n+         opcode == Op_MaxV || opcode == Op_MaxReductionV, \"sanity\");\n+  assert(elem_bt == T_FLOAT || elem_bt == T_DOUBLE, \"sanity\");\n+\n+  bool is_min = (opcode == Op_MinV || opcode == Op_MinReductionV);\n+  bool is_double_word = is_double_word_type(elem_bt);\n+\n+  if (!is_double_word && is_min) {\n+    vblendvps(atmp, a, b, a, vlen_enc);\n+    vblendvps(btmp, b, a, a, vlen_enc);\n+    vminps(tmp, atmp, btmp, vlen_enc);\n+    vcmpps(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    vblendvps(dst, tmp, atmp, btmp, vlen_enc);\n+  } else if (!is_double_word && !is_min) {\n+    vblendvps(btmp, b, a, b, vlen_enc);\n+    vblendvps(atmp, a, b, b, vlen_enc);\n+    vmaxps(tmp, atmp, btmp, vlen_enc);\n+    vcmpps(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    vblendvps(dst, tmp, atmp, btmp, vlen_enc);\n+  } else if (is_double_word && is_min) {\n+    vblendvpd(atmp, a, b, a, vlen_enc);\n+    vblendvpd(btmp, b, a, a, vlen_enc);\n+    vminpd(tmp, atmp, btmp, vlen_enc);\n+    vcmppd(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    vblendvpd(dst, tmp, atmp, btmp, vlen_enc);\n+  } else {\n+    assert(is_double_word && !is_min, \"sanity\");\n+    vblendvpd(btmp, b, a, b, vlen_enc);\n+    vblendvpd(atmp, a, b, b, vlen_enc);\n+    vmaxpd(tmp, atmp, btmp, vlen_enc);\n+    vcmppd(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    vblendvpd(dst, tmp, atmp, btmp, vlen_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::evminmax_fp(int opcode, BasicType elem_bt,\n+                                    XMMRegister dst, XMMRegister a, XMMRegister b,\n+                                    KRegister ktmp, XMMRegister atmp, XMMRegister btmp,\n+                                    int vlen_enc) {\n+  assert(UseAVX > 2, \"required\");\n+  assert(opcode == Op_MinV || opcode == Op_MinReductionV ||\n+         opcode == Op_MaxV || opcode == Op_MaxReductionV, \"sanity\");\n+  assert(elem_bt == T_FLOAT || elem_bt == T_DOUBLE, \"sanity\");\n+\n+  bool is_min = (opcode == Op_MinV || opcode == Op_MinReductionV);\n+  bool is_double_word = is_double_word_type(elem_bt);\n+  bool merge = true;\n+\n+  if (!is_double_word && is_min) {\n+    evpmovd2m(ktmp, a, vlen_enc);\n+    evblendmps(atmp, ktmp, a, b, merge, vlen_enc);\n+    evblendmps(btmp, ktmp, b, a, merge, vlen_enc);\n+    vminps(dst, atmp, btmp, vlen_enc);\n+    evcmpps(ktmp, k0, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    evmovdqul(dst, ktmp, atmp, merge, vlen_enc);\n+  } else if (!is_double_word && !is_min) {\n+    evpmovd2m(ktmp, b, vlen_enc);\n+    evblendmps(atmp, ktmp, a, b, merge, vlen_enc);\n+    evblendmps(btmp, ktmp, b, a, merge, vlen_enc);\n+    vmaxps(dst, atmp, btmp, vlen_enc);\n+    evcmpps(ktmp, k0, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    evmovdqul(dst, ktmp, atmp, merge, vlen_enc);\n+  } else if (is_double_word && is_min) {\n+    evpmovq2m(ktmp, a, vlen_enc);\n+    evblendmpd(atmp, ktmp, a, b, merge, vlen_enc);\n+    evblendmpd(btmp, ktmp, b, a, merge, vlen_enc);\n+    vminpd(dst, atmp, btmp, vlen_enc);\n+    evcmppd(ktmp, k0, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    evmovdquq(dst, ktmp, atmp, merge, vlen_enc);\n+  } else {\n+    assert(is_double_word && !is_min, \"sanity\");\n+    evpmovq2m(ktmp, b, vlen_enc);\n+    evblendmpd(atmp, ktmp, a, b, merge, vlen_enc);\n+    evblendmpd(btmp, ktmp, b, a, merge, vlen_enc);\n+    vmaxpd(dst, atmp, btmp, vlen_enc);\n+    evcmppd(ktmp, k0, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+    evmovdquq(dst, ktmp, atmp, merge, vlen_enc);\n+  }\n+}\n+\n@@ -880,0 +1056,16 @@\n+void C2_MacroAssembler::vextendbd(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {\n+  if (sign) {\n+    vpmovsxbd(dst, src, vector_len);\n+  } else {\n+    vpmovzxbd(dst, src, vector_len);\n+  }\n+}\n+\n+void C2_MacroAssembler::vextendwd(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {\n+  if (sign) {\n+    vpmovsxwd(dst, src, vector_len);\n+  } else {\n+    vpmovzxwd(dst, src, vector_len);\n+  }\n+}\n+\n@@ -931,8 +1123,7 @@\n-void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {\n-  if (opcode == Op_RShiftVI) {\n-    psrad(dst, src);\n-  } else if (opcode == Op_LShiftVI) {\n-    pslld(dst, src);\n-  } else {\n-    assert((opcode == Op_URShiftVI),\"opcode should be Op_URShiftVI\");\n-    psrld(dst, src);\n+void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister shift) {\n+  switch (opcode) {\n+    case Op_RShiftVI:  psrad(dst, shift); break;\n+    case Op_LShiftVI:  pslld(dst, shift); break;\n+    case Op_URShiftVI: psrld(dst, shift); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n@@ -953,8 +1144,7 @@\n-void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-  if (opcode == Op_RShiftVI) {\n-    vpsrad(dst, nds, src, vector_len);\n-  } else if (opcode == Op_LShiftVI) {\n-    vpslld(dst, nds, src, vector_len);\n-  } else {\n-    assert((opcode == Op_URShiftVI),\"opcode should be Op_URShiftVI\");\n-    vpsrld(dst, nds, src, vector_len);\n+void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vlen_enc) {\n+  switch (opcode) {\n+    case Op_RShiftVI:  vpsrad(dst, src, shift, vlen_enc); break;\n+    case Op_LShiftVI:  vpslld(dst, src, shift, vlen_enc); break;\n+    case Op_URShiftVI: vpsrld(dst, src, shift, vlen_enc); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n@@ -964,8 +1154,12 @@\n-void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {\n-  if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {\n-    psraw(dst, src);\n-  } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {\n-    psllw(dst, src);\n-  } else {\n-    assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),\"opcode should be one of Op_URShiftVS or Op_URShiftVB\");\n-    psrlw(dst, src);\n+void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister shift) {\n+  switch (opcode) {\n+    case Op_RShiftVB:  \/\/ fall-through\n+    case Op_RShiftVS:  psraw(dst, shift); break;\n+\n+    case Op_LShiftVB:  \/\/ fall-through\n+    case Op_LShiftVS:  psllw(dst, shift);   break;\n+\n+    case Op_URShiftVS: \/\/ fall-through\n+    case Op_URShiftVB: psrlw(dst, shift);  break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n@@ -975,8 +1169,12 @@\n-void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-  if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {\n-    vpsraw(dst, nds, src, vector_len);\n-  } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {\n-    vpsllw(dst, nds, src, vector_len);\n-  } else {\n-    assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),\"opcode should be one of Op_URShiftVS or Op_URShiftVB\");\n-    vpsrlw(dst, nds, src, vector_len);\n+void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vlen_enc) {\n+  switch (opcode) {\n+    case Op_RShiftVB:  \/\/ fall-through\n+    case Op_RShiftVS:  vpsraw(dst, src, shift, vlen_enc); break;\n+\n+    case Op_LShiftVB:  \/\/ fall-through\n+    case Op_LShiftVS:  vpsllw(dst, src, shift, vlen_enc); break;\n+\n+    case Op_URShiftVS: \/\/ fall-through\n+    case Op_URShiftVB: vpsrlw(dst, src, shift, vlen_enc); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n@@ -986,8 +1184,7 @@\n-void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {\n-  if (opcode == Op_RShiftVL) {\n-    psrlq(dst, src);  \/\/ using srl to implement sra on pre-avs512 systems\n-  } else if (opcode == Op_LShiftVL) {\n-    psllq(dst, src);\n-  } else {\n-    assert((opcode == Op_URShiftVL),\"opcode should be Op_URShiftVL\");\n-    psrlq(dst, src);\n+void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister shift) {\n+  switch (opcode) {\n+    case Op_RShiftVL:  psrlq(dst, shift); break; \/\/ using srl to implement sra on pre-avs512 systems\n+    case Op_LShiftVL:  psllq(dst, shift); break;\n+    case Op_URShiftVL: psrlq(dst, shift); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n@@ -1008,8 +1205,7 @@\n-void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-  if (opcode == Op_RShiftVL) {\n-    evpsraq(dst, nds, src, vector_len);\n-  } else if (opcode == Op_LShiftVL) {\n-    vpsllq(dst, nds, src, vector_len);\n-  } else {\n-    assert((opcode == Op_URShiftVL),\"opcode should be Op_URShiftVL\");\n-    vpsrlq(dst, nds, src, vector_len);\n+void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vlen_enc) {\n+  switch (opcode) {\n+    case Op_RShiftVL: evpsraq(dst, src, shift, vlen_enc); break;\n+    case Op_LShiftVL:  vpsllq(dst, src, shift, vlen_enc); break;\n+    case Op_URShiftVL: vpsrlq(dst, src, shift, vlen_enc); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n@@ -1030,1 +1226,9 @@\n-\/\/ Reductions for vectors of ints, longs, floats, and doubles.\n+void C2_MacroAssembler::varshiftd(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vlen_enc) {\n+  switch (opcode) {\n+    case Op_RShiftVB:  \/\/ fall-through\n+    case Op_RShiftVS:  \/\/ fall-through\n+    case Op_RShiftVI:  vpsravd(dst, src, shift, vlen_enc); break;\n+\n+    case Op_LShiftVB:  \/\/ fall-through\n+    case Op_LShiftVS:  \/\/ fall-through\n+    case Op_LShiftVI:  vpsllvd(dst, src, shift, vlen_enc); break;\n@@ -1032,1 +1236,238 @@\n-void C2_MacroAssembler::reduce_operation_128(int opcode, XMMRegister dst, XMMRegister src) {\n+    case Op_URShiftVB: \/\/ fall-through\n+    case Op_URShiftVS: \/\/ fall-through\n+    case Op_URShiftVI: vpsrlvd(dst, src, shift, vlen_enc); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n+  }\n+}\n+\n+void C2_MacroAssembler::varshiftw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vlen_enc) {\n+  switch (opcode) {\n+    case Op_RShiftVB:  \/\/ fall-through\n+    case Op_RShiftVS:  evpsravw(dst, src, shift, vlen_enc); break;\n+\n+    case Op_LShiftVB:  \/\/ fall-through\n+    case Op_LShiftVS:  evpsllvw(dst, src, shift, vlen_enc); break;\n+\n+    case Op_URShiftVB: \/\/ fall-through\n+    case Op_URShiftVS: evpsrlvw(dst, src, shift, vlen_enc); break;\n+\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n+  }\n+}\n+\n+void C2_MacroAssembler::varshiftq(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vlen_enc, XMMRegister tmp) {\n+  assert(UseAVX >= 2, \"required\");\n+  switch (opcode) {\n+    case Op_RShiftVL: {\n+      if (UseAVX > 2) {\n+        assert(tmp == xnoreg, \"not used\");\n+        if (!VM_Version::supports_avx512vl()) {\n+          vlen_enc = Assembler::AVX_512bit;\n+        }\n+        evpsravq(dst, src, shift, vlen_enc);\n+      } else {\n+        vmovdqu(tmp, ExternalAddress(StubRoutines::x86::vector_long_sign_mask()));\n+        vpsrlvq(dst, src, shift, vlen_enc);\n+        vpsrlvq(tmp, tmp, shift, vlen_enc);\n+        vpxor(dst, dst, tmp, vlen_enc);\n+        vpsubq(dst, dst, tmp, vlen_enc);\n+      }\n+      break;\n+    }\n+    case Op_LShiftVL: {\n+      assert(tmp == xnoreg, \"not used\");\n+      vpsllvq(dst, src, shift, vlen_enc);\n+      break;\n+    }\n+    case Op_URShiftVL: {\n+      assert(tmp == xnoreg, \"not used\");\n+      vpsrlvq(dst, src, shift, vlen_enc);\n+      break;\n+    }\n+    default: assert(false, \"%s\", NodeClassNames[opcode]);\n+  }\n+}\n+\n+\/\/ Variable shift src by shift using vtmp and scratch as TEMPs giving word result in dst\n+void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+  assert(opcode == Op_LShiftVB ||\n+         opcode == Op_RShiftVB ||\n+         opcode == Op_URShiftVB, \"%s\", NodeClassNames[opcode]);\n+  bool sign = (opcode != Op_URShiftVB);\n+  assert(vector_len == 0, \"required\");\n+  vextendbd(sign, dst, src, 1);\n+  vpmovzxbd(vtmp, shift, 1);\n+  varshiftd(opcode, dst, dst, vtmp, 1);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, scratch);\n+  vextracti128_high(vtmp, dst);\n+  vpackusdw(dst, dst, vtmp, 0);\n+}\n+\n+\/\/ Variable shift src by shift using vtmp and scratch as TEMPs giving byte result in dst\n+void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+  assert(opcode == Op_LShiftVB ||\n+         opcode == Op_RShiftVB ||\n+         opcode == Op_URShiftVB, \"%s\", NodeClassNames[opcode]);\n+  bool sign = (opcode != Op_URShiftVB);\n+  int ext_vector_len = vector_len + 1;\n+  vextendbw(sign, dst, src, ext_vector_len);\n+  vpmovzxbw(vtmp, shift, ext_vector_len);\n+  varshiftw(opcode, dst, dst, vtmp, ext_vector_len);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, scratch);\n+  if (vector_len == 0) {\n+    vextracti128_high(vtmp, dst);\n+    vpackuswb(dst, dst, vtmp, vector_len);\n+  } else {\n+    vextracti64x4_high(vtmp, dst);\n+    vpackuswb(dst, dst, vtmp, vector_len);\n+    vpermq(dst, dst, 0xD8, vector_len);\n+  }\n+}\n+\n+void C2_MacroAssembler::insert(BasicType typ, XMMRegister dst, Register val, int idx) {\n+  switch(typ) {\n+    case T_BYTE:\n+      pinsrb(dst, val, idx);\n+      break;\n+    case T_SHORT:\n+      pinsrw(dst, val, idx);\n+      break;\n+    case T_INT:\n+      pinsrd(dst, val, idx);\n+      break;\n+    case T_LONG:\n+      pinsrq(dst, val, idx);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vinsert(BasicType typ, XMMRegister dst, XMMRegister src, Register val, int idx) {\n+  switch(typ) {\n+    case T_BYTE:\n+      vpinsrb(dst, src, val, idx);\n+      break;\n+    case T_SHORT:\n+      vpinsrw(dst, src, val, idx);\n+      break;\n+    case T_INT:\n+      vpinsrd(dst, src, val, idx);\n+      break;\n+    case T_LONG:\n+      vpinsrq(dst, src, val, idx);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather(BasicType typ, XMMRegister dst, Register base, XMMRegister idx, XMMRegister mask, int vector_len) {\n+  switch(typ) {\n+    case T_INT:\n+      vpgatherdd(dst, Address(base, idx, Address::times_4), mask, vector_len);\n+      break;\n+    case T_FLOAT:\n+      vgatherdps(dst, Address(base, idx, Address::times_4), mask, vector_len);\n+      break;\n+    case T_LONG:\n+      vpgatherdq(dst, Address(base, idx, Address::times_8), mask, vector_len);\n+      break;\n+    case T_DOUBLE:\n+      vgatherdpd(dst, Address(base, idx, Address::times_8), mask, vector_len);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evgather(BasicType typ, XMMRegister dst, KRegister mask, Register base, XMMRegister idx, int vector_len) {\n+  switch(typ) {\n+    case T_INT:\n+      evpgatherdd(dst, mask, Address(base, idx, Address::times_4), vector_len);\n+      break;\n+    case T_FLOAT:\n+      evgatherdps(dst, mask, Address(base, idx, Address::times_4), vector_len);\n+      break;\n+    case T_LONG:\n+      evpgatherdq(dst, mask, Address(base, idx, Address::times_8), vector_len);\n+      break;\n+    case T_DOUBLE:\n+      evgatherdpd(dst, mask, Address(base, idx, Address::times_8), vector_len);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evscatter(BasicType typ, Register base, XMMRegister idx, KRegister mask, XMMRegister src, int vector_len) {\n+  switch(typ) {\n+    case T_INT:\n+      evpscatterdd(Address(base, idx, Address::times_4), mask, src, vector_len);\n+      break;\n+    case T_FLOAT:\n+      evscatterdps(Address(base, idx, Address::times_4), mask, src, vector_len);\n+      break;\n+    case T_LONG:\n+      evpscatterdq(Address(base, idx, Address::times_8), mask, src, vector_len);\n+      break;\n+    case T_DOUBLE:\n+      evscatterdpd(Address(base, idx, Address::times_8), mask, src, vector_len);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::load_vector_mask(XMMRegister dst, XMMRegister src, int vlen_in_bytes, BasicType elem_bt) {\n+  if (vlen_in_bytes <= 16) {\n+    pxor (dst, dst);\n+    psubb(dst, src);\n+    switch (elem_bt) {\n+      case T_BYTE:   \/* nothing to do *\/ break;\n+      case T_SHORT:  pmovsxbw(dst, dst); break;\n+      case T_INT:    pmovsxbd(dst, dst); break;\n+      case T_FLOAT:  pmovsxbd(dst, dst); break;\n+      case T_LONG:   pmovsxbq(dst, dst); break;\n+      case T_DOUBLE: pmovsxbq(dst, dst); break;\n+\n+      default: assert(false, \"%s\", type2name(elem_bt));\n+    }\n+  } else {\n+    int vlen_enc = vector_length_encoding(vlen_in_bytes);\n+\n+    vpxor (dst, dst, dst, vlen_enc);\n+    vpsubb(dst, dst, src, vlen_enc);\n+    switch (elem_bt) {\n+      case T_BYTE:   \/* nothing to do *\/            break;\n+      case T_SHORT:  vpmovsxbw(dst, dst, vlen_enc); break;\n+      case T_INT:    vpmovsxbd(dst, dst, vlen_enc); break;\n+      case T_FLOAT:  vpmovsxbd(dst, dst, vlen_enc); break;\n+      case T_LONG:   vpmovsxbq(dst, dst, vlen_enc); break;\n+      case T_DOUBLE: vpmovsxbq(dst, dst, vlen_enc); break;\n+\n+      default: assert(false, \"%s\", type2name(elem_bt));\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes) {\n+  ExternalAddress addr(StubRoutines::x86::vector_iota_indices());\n+  if (vlen_in_bytes <= 16) {\n+    movdqu(dst, addr, scratch);\n+  } else if (vlen_in_bytes == 32) {\n+    vmovdqu(dst, addr, scratch);\n+  } else {\n+    assert(vlen_in_bytes == 64, \"%d\", vlen_in_bytes);\n+    evmovdqub(dst, k0, addr, false \/*merge*\/, Assembler::AVX_512bit, scratch);\n+  }\n+}\n+\/\/ Reductions for vectors of bytes, shorts, ints, longs, floats, and doubles.\n+\n+void C2_MacroAssembler::reduce_operation_128(BasicType typ, int opcode, XMMRegister dst, XMMRegister src) {\n@@ -1039,1 +1480,20 @@\n-\n+    case Op_MinReductionV:\n+      switch (typ) {\n+        case T_BYTE:        pminsb(dst, src); break;\n+        case T_SHORT:       pminsw(dst, src); break;\n+        case T_INT:         pminsd(dst, src); break;\n+        case T_LONG:        assert(UseAVX > 2, \"required\");\n+                            vpminsq(dst, dst, src, Assembler::AVX_128bit); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n+    case Op_MaxReductionV:\n+      switch (typ) {\n+        case T_BYTE:        pmaxsb(dst, src); break;\n+        case T_SHORT:       pmaxsw(dst, src); break;\n+        case T_INT:         pmaxsd(dst, src); break;\n+        case T_LONG:        assert(UseAVX > 2, \"required\");\n+                            vpmaxsq(dst, dst, src, Assembler::AVX_128bit); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n@@ -1042,1 +1502,8 @@\n-    case Op_AddReductionVI: paddd(dst, src); break;\n+    case Op_AddReductionVI:\n+      switch (typ) {\n+        case T_BYTE:        paddb(dst, src); break;\n+        case T_SHORT:       paddw(dst, src); break;\n+        case T_INT:         paddd(dst, src); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n@@ -1044,1 +1511,0 @@\n-\n@@ -1047,4 +1513,10 @@\n-    case Op_MulReductionVI: pmulld(dst, src); break;\n-    case Op_MulReductionVL: vpmullq(dst, dst, src, vector_len); break;\n-\n-    default: assert(false, \"wrong opcode\");\n+    case Op_MulReductionVI:\n+      switch (typ) {\n+        case T_SHORT:       pmullw(dst, src); break;\n+        case T_INT:         pmulld(dst, src); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n+    case Op_MulReductionVL: assert(UseAVX > 2, \"required\");\n+                            vpmullq(dst, dst, src, vector_len); break;\n+    default:                assert(false, \"wrong opcode\");\n@@ -1054,1 +1526,1 @@\n-void C2_MacroAssembler::reduce_operation_256(int opcode, XMMRegister dst,  XMMRegister src1, XMMRegister src2) {\n+void C2_MacroAssembler::reduce_operation_256(BasicType typ, int opcode, XMMRegister dst,  XMMRegister src1, XMMRegister src2) {\n@@ -1061,2 +1533,28 @@\n-\n-    case Op_AddReductionVI: vpaddd(dst, src1, src2, vector_len); break;\n+    case Op_MinReductionV:\n+      switch (typ) {\n+        case T_BYTE:        vpminsb(dst, src1, src2, vector_len); break;\n+        case T_SHORT:       vpminsw(dst, src1, src2, vector_len); break;\n+        case T_INT:         vpminsd(dst, src1, src2, vector_len); break;\n+        case T_LONG:        assert(UseAVX > 2, \"required\");\n+                            vpminsq(dst, src1, src2, vector_len); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n+    case Op_MaxReductionV:\n+      switch (typ) {\n+        case T_BYTE:        vpmaxsb(dst, src1, src2, vector_len); break;\n+        case T_SHORT:       vpmaxsw(dst, src1, src2, vector_len); break;\n+        case T_INT:         vpmaxsd(dst, src1, src2, vector_len); break;\n+        case T_LONG:        assert(UseAVX > 2, \"required\");\n+                            vpmaxsq(dst, src1, src2, vector_len); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n+    case Op_AddReductionVI:\n+      switch (typ) {\n+        case T_BYTE:        vpaddb(dst, src1, src2, vector_len); break;\n+        case T_SHORT:       vpaddw(dst, src1, src2, vector_len); break;\n+        case T_INT:         vpaddd(dst, src1, src2, vector_len); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n@@ -1064,2 +1562,7 @@\n-\n-    case Op_MulReductionVI: vpmulld(dst, src1, src2, vector_len); break;\n+    case Op_MulReductionVI:\n+      switch (typ) {\n+        case T_SHORT:       vpmullw(dst, src1, src2, vector_len); break;\n+        case T_INT:         vpmulld(dst, src1, src2, vector_len); break;\n+        default:            assert(false, \"wrong type\");\n+      }\n+      break;\n@@ -1067,2 +1570,1 @@\n-\n-    default: assert(false, \"wrong opcode\");\n+    default:                assert(false, \"wrong opcode\");\n@@ -1090,0 +1592,39 @@\n+void C2_MacroAssembler::reduceB(int opcode, int vlen,\n+                             Register dst, Register src1, XMMRegister src2,\n+                             XMMRegister vtmp1, XMMRegister vtmp2) {\n+  switch (vlen) {\n+    case  8: reduce8B (opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 16: reduce16B(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 32: reduce32B(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 64: reduce64B(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+\n+    default: assert(false, \"wrong vector length\");\n+  }\n+}\n+\n+void C2_MacroAssembler::mulreduceB(int opcode, int vlen,\n+                             Register dst, Register src1, XMMRegister src2,\n+                             XMMRegister vtmp1, XMMRegister vtmp2) {\n+  switch (vlen) {\n+    case  8: mulreduce8B (opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 16: mulreduce16B(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 32: mulreduce32B(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 64: mulreduce64B(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+\n+    default: assert(false, \"wrong vector length\");\n+  }\n+}\n+\n+void C2_MacroAssembler::reduceS(int opcode, int vlen,\n+                             Register dst, Register src1, XMMRegister src2,\n+                             XMMRegister vtmp1, XMMRegister vtmp2) {\n+  switch (vlen) {\n+    case  4: reduce4S (opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case  8: reduce8S (opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 16: reduce16S(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+    case 32: reduce32S(opcode, dst, src1, src2, vtmp1, vtmp2); break;\n+\n+    default: assert(false, \"wrong vector length\");\n+  }\n+}\n+\n@@ -1091,2 +1632,2 @@\n-                                Register dst, Register src1, XMMRegister src2,\n-                                XMMRegister vtmp1, XMMRegister vtmp2) {\n+                             Register dst, Register src1, XMMRegister src2,\n+                             XMMRegister vtmp1, XMMRegister vtmp2) {\n@@ -1105,2 +1646,2 @@\n-                                Register dst, Register src1, XMMRegister src2,\n-                                XMMRegister vtmp1, XMMRegister vtmp2) {\n+                             Register dst, Register src1, XMMRegister src2,\n+                             XMMRegister vtmp1, XMMRegister vtmp2) {\n@@ -1161,1 +1702,1 @@\n-    reduce_operation_128(opcode, vtmp1, src2);\n+    reduce_operation_128(T_INT, opcode, vtmp1, src2);\n@@ -1164,1 +1705,1 @@\n-  reduce_operation_128(opcode, vtmp1, vtmp2);\n+  reduce_operation_128(T_INT, opcode, vtmp1, vtmp2);\n@@ -1177,1 +1718,1 @@\n-    reduce_operation_128(opcode, vtmp2, src2);\n+    reduce_operation_128(T_INT, opcode, vtmp2, src2);\n@@ -1190,1 +1731,1 @@\n-    reduce_operation_128(opcode, vtmp1, src2);\n+    reduce_operation_128(T_INT, opcode, vtmp1, src2);\n@@ -1197,1 +1738,1 @@\n-  reduce_operation_256(opcode, vtmp2, vtmp2, src2);\n+  reduce_operation_256(T_INT, opcode, vtmp2, vtmp2, src2);\n@@ -1201,0 +1742,125 @@\n+void C2_MacroAssembler::reduce8B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  pshufd(vtmp2, src2, 0x1);\n+  reduce_operation_128(T_BYTE, opcode, vtmp2, src2);\n+  movdqu(vtmp1, vtmp2);\n+  psrldq(vtmp1, 2);\n+  reduce_operation_128(T_BYTE, opcode, vtmp1, vtmp2);\n+  movdqu(vtmp2, vtmp1);\n+  psrldq(vtmp2, 1);\n+  reduce_operation_128(T_BYTE, opcode, vtmp1, vtmp2);\n+  movdl(vtmp2, src1);\n+  pmovsxbd(vtmp1, vtmp1);\n+  reduce_operation_128(T_INT, opcode, vtmp1, vtmp2);\n+  pextrb(dst, vtmp1, 0x0);\n+  movsbl(dst, dst);\n+}\n+\n+void C2_MacroAssembler::reduce16B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  pshufd(vtmp1, src2, 0xE);\n+  reduce_operation_128(T_BYTE, opcode, vtmp1, src2);\n+  reduce8B(opcode, dst, src1, vtmp1, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::reduce32B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  vextracti128_high(vtmp2, src2);\n+  reduce_operation_128(T_BYTE, opcode, vtmp2, src2);\n+  reduce16B(opcode, dst, src1, vtmp2, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::reduce64B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  vextracti64x4_high(vtmp1, src2);\n+  reduce_operation_256(T_BYTE, opcode, vtmp1, vtmp1, src2);\n+  reduce32B(opcode, dst, src1, vtmp1, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::mulreduce8B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  pmovsxbw(vtmp2, src2);\n+  reduce8S(opcode, dst, src1, vtmp2, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::mulreduce16B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  if (UseAVX > 1) {\n+    int vector_len = Assembler::AVX_256bit;\n+    vpmovsxbw(vtmp1, src2, vector_len);\n+    reduce16S(opcode, dst, src1, vtmp1, vtmp1, vtmp2);\n+  } else {\n+    pmovsxbw(vtmp2, src2);\n+    reduce8S(opcode, dst, src1, vtmp2, vtmp1, vtmp2);\n+    pshufd(vtmp2, src2, 0x1);\n+    pmovsxbw(vtmp2, src2);\n+    reduce8S(opcode, dst, dst, vtmp2, vtmp1, vtmp2);\n+  }\n+}\n+\n+void C2_MacroAssembler::mulreduce32B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  if (UseAVX > 2 && VM_Version::supports_avx512bw()) {\n+    int vector_len = Assembler::AVX_512bit;\n+    vpmovsxbw(vtmp1, src2, vector_len);\n+    reduce32S(opcode, dst, src1, vtmp1, vtmp1, vtmp2);\n+  } else {\n+    assert(UseAVX >= 2,\"Should not reach here.\");\n+    mulreduce16B(opcode, dst, src1, src2, vtmp1, vtmp2);\n+    vextracti128_high(vtmp2, src2);\n+    mulreduce16B(opcode, dst, dst, vtmp2, vtmp1, vtmp2);\n+  }\n+}\n+\n+void C2_MacroAssembler::mulreduce64B(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  mulreduce32B(opcode, dst, src1, src2, vtmp1, vtmp2);\n+  vextracti64x4_high(vtmp2, src2);\n+  mulreduce32B(opcode, dst, dst, vtmp2, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::reduce4S(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  if (opcode == Op_AddReductionVI) {\n+    if (vtmp1 != src2) {\n+      movdqu(vtmp1, src2);\n+    }\n+    phaddw(vtmp1, vtmp1);\n+    phaddw(vtmp1, vtmp1);\n+  } else {\n+    pshufd(vtmp2, src2, 0x1);\n+    reduce_operation_128(T_SHORT, opcode, vtmp2, src2);\n+    movdqu(vtmp1, vtmp2);\n+    psrldq(vtmp1, 2);\n+    reduce_operation_128(T_SHORT, opcode, vtmp1, vtmp2);\n+  }\n+  movdl(vtmp2, src1);\n+  pmovsxwd(vtmp1, vtmp1);\n+  reduce_operation_128(T_INT, opcode, vtmp1, vtmp2);\n+  pextrw(dst, vtmp1, 0x0);\n+  movswl(dst, dst);\n+}\n+\n+void C2_MacroAssembler::reduce8S(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  if (opcode == Op_AddReductionVI) {\n+    if (vtmp1 != src2) {\n+      movdqu(vtmp1, src2);\n+    }\n+    phaddw(vtmp1, src2);\n+  } else {\n+    pshufd(vtmp1, src2, 0xE);\n+    reduce_operation_128(T_SHORT, opcode, vtmp1, src2);\n+  }\n+  reduce4S(opcode, dst, src1, vtmp1, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::reduce16S(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  if (opcode == Op_AddReductionVI) {\n+    int vector_len = Assembler::AVX_256bit;\n+    vphaddw(vtmp2, src2, src2, vector_len);\n+    vpermq(vtmp2, vtmp2, 0xD8, vector_len);\n+  } else {\n+    vextracti128_high(vtmp2, src2);\n+    reduce_operation_128(T_SHORT, opcode, vtmp2, src2);\n+  }\n+  reduce8S(opcode, dst, src1, vtmp2, vtmp1, vtmp2);\n+}\n+\n+void C2_MacroAssembler::reduce32S(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+  int vector_len = Assembler::AVX_256bit;\n+  vextracti64x4_high(vtmp1, src2);\n+  reduce_operation_256(T_SHORT, opcode, vtmp1, vtmp1, src2);\n+  reduce16S(opcode, dst, src1, vtmp1, vtmp1, vtmp2);\n+}\n+\n@@ -1204,1 +1870,1 @@\n-  reduce_operation_128(opcode, vtmp2, src2);\n+  reduce_operation_128(T_LONG, opcode, vtmp2, src2);\n@@ -1206,1 +1872,1 @@\n-  reduce_operation_128(opcode, vtmp1, vtmp2);\n+  reduce_operation_128(T_LONG, opcode, vtmp1, vtmp2);\n@@ -1212,1 +1878,1 @@\n-  reduce_operation_128(opcode, vtmp1, src2);\n+  reduce_operation_128(T_LONG, opcode, vtmp1, src2);\n@@ -1218,1 +1884,1 @@\n-  reduce_operation_256(opcode, vtmp2, vtmp2, src2);\n+  reduce_operation_256(T_LONG, opcode, vtmp2, vtmp2, src2);\n@@ -1224,1 +1890,1 @@\n-  reduce_operation_128(opcode, dst, src);\n+  reduce_operation_128(T_FLOAT, opcode, dst, src);\n@@ -1226,1 +1892,1 @@\n-  reduce_operation_128(opcode, dst, vtmp);\n+  reduce_operation_128(T_FLOAT, opcode, dst, vtmp);\n@@ -1232,1 +1898,1 @@\n-  reduce_operation_128(opcode, dst, vtmp);\n+  reduce_operation_128(T_FLOAT, opcode, dst, vtmp);\n@@ -1234,1 +1900,1 @@\n-  reduce_operation_128(opcode, dst, vtmp);\n+  reduce_operation_128(T_FLOAT, opcode, dst, vtmp);\n@@ -1250,1 +1916,1 @@\n-  reduce_operation_128(opcode, dst, src);\n+  reduce_operation_128(T_DOUBLE, opcode, dst, src);\n@@ -1252,1 +1918,1 @@\n-  reduce_operation_128(opcode, dst, vtmp);\n+  reduce_operation_128(T_DOUBLE, opcode, dst, vtmp);\n@@ -1267,0 +1933,201 @@\n+void C2_MacroAssembler::reduceFloatMinMax(int opcode, int vlen, bool is_dst_valid,\n+                                          XMMRegister dst, XMMRegister src,\n+                                          XMMRegister tmp, XMMRegister atmp, XMMRegister btmp,\n+                                          XMMRegister xmm_0, XMMRegister xmm_1) {\n+  int permconst[] = {1, 14};\n+  XMMRegister wsrc = src;\n+  XMMRegister wdst = xmm_0;\n+  XMMRegister wtmp = (xmm_1 == xnoreg) ? xmm_0: xmm_1;\n+\n+  int vlen_enc = Assembler::AVX_128bit;\n+  if (vlen == 16) {\n+    vlen_enc = Assembler::AVX_256bit;\n+  }\n+\n+  for (int i = log2(vlen) - 1; i >=0; i--) {\n+    if (i == 0 && !is_dst_valid) {\n+      wdst = dst;\n+    }\n+    if (i == 3) {\n+      vextracti64x4_high(wtmp, wsrc);\n+    } else if (i == 2) {\n+      vextracti128_high(wtmp, wsrc);\n+    } else { \/\/ i = [0,1]\n+      vpermilps(wtmp, wsrc, permconst[i], vlen_enc);\n+    }\n+    vminmax_fp(opcode, T_FLOAT, wdst, wtmp, wsrc, tmp, atmp, btmp, vlen_enc);\n+    wsrc = wdst;\n+    vlen_enc = Assembler::AVX_128bit;\n+  }\n+  if (is_dst_valid) {\n+    vminmax_fp(opcode, T_FLOAT, dst, wdst, dst, tmp, atmp, btmp, Assembler::AVX_128bit);\n+  }\n+}\n+\n+void C2_MacroAssembler::reduceDoubleMinMax(int opcode, int vlen, bool is_dst_valid, XMMRegister dst, XMMRegister src,\n+                                        XMMRegister tmp, XMMRegister atmp, XMMRegister btmp,\n+                                        XMMRegister xmm_0, XMMRegister xmm_1) {\n+  XMMRegister wsrc = src;\n+  XMMRegister wdst = xmm_0;\n+  XMMRegister wtmp = (xmm_1 == xnoreg) ? xmm_0: xmm_1;\n+  int vlen_enc = Assembler::AVX_128bit;\n+  if (vlen == 8) {\n+    vlen_enc = Assembler::AVX_256bit;\n+  }\n+  for (int i = log2(vlen) - 1; i >=0; i--) {\n+    if (i == 0 && !is_dst_valid) {\n+      wdst = dst;\n+    }\n+    if (i == 1) {\n+      vextracti128_high(wtmp, wsrc);\n+    } else if (i == 2) {\n+      vextracti64x4_high(wtmp, wsrc);\n+    } else {\n+      assert(i == 0, \"%d\", i);\n+      vpermilpd(wtmp, wsrc, 1, vlen_enc);\n+    }\n+    vminmax_fp(opcode, T_DOUBLE, wdst, wtmp, wsrc, tmp, atmp, btmp, vlen_enc);\n+    wsrc = wdst;\n+    vlen_enc = Assembler::AVX_128bit;\n+  }\n+  if (is_dst_valid) {\n+    vminmax_fp(opcode, T_DOUBLE, dst, wdst, dst, tmp, atmp, btmp, Assembler::AVX_128bit);\n+  }\n+}\n+\n+void C2_MacroAssembler::extract(BasicType bt, Register dst, XMMRegister src, int idx) {\n+  switch (bt) {\n+    case T_BYTE:  pextrb(dst, src, idx); break;\n+    case T_SHORT: pextrw(dst, src, idx); break;\n+    case T_INT:   pextrd(dst, src, idx); break;\n+    case T_LONG:  pextrq(dst, src, idx); break;\n+\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+XMMRegister C2_MacroAssembler::get_lane(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex) {\n+  int esize =  type2aelembytes(typ);\n+  int elem_per_lane = 16\/esize;\n+  int lane = elemindex \/ elem_per_lane;\n+  int eindex = elemindex % elem_per_lane;\n+\n+  if (lane >= 2) {\n+    assert(UseAVX > 2, \"required\");\n+    vextractf32x4(dst, src, lane & 3);\n+    return dst;\n+  } else if (lane > 0) {\n+    assert(UseAVX > 0, \"required\");\n+    vextractf128(dst, src, lane);\n+    return dst;\n+  } else {\n+    return src;\n+  }\n+}\n+\n+void C2_MacroAssembler::get_elem(BasicType typ, Register dst, XMMRegister src, int elemindex) {\n+  int esize =  type2aelembytes(typ);\n+  int elem_per_lane = 16\/esize;\n+  int eindex = elemindex % elem_per_lane;\n+  assert(is_integral_type(typ),\"required\");\n+\n+  if (eindex == 0) {\n+    if (typ == T_LONG) {\n+      movq(dst, src);\n+    } else {\n+      movdl(dst, src);\n+      if (typ == T_BYTE)\n+        movsbl(dst, dst);\n+      else if (typ == T_SHORT)\n+        movswl(dst, dst);\n+    }\n+  } else {\n+    extract(typ, dst, src, eindex);\n+  }\n+}\n+\n+void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp, XMMRegister vtmp) {\n+  int esize =  type2aelembytes(typ);\n+  int elem_per_lane = 16\/esize;\n+  int eindex = elemindex % elem_per_lane;\n+  assert((typ == T_FLOAT || typ == T_DOUBLE),\"required\");\n+\n+  if (eindex == 0) {\n+    movq(dst, src);\n+  } else {\n+    if (typ == T_FLOAT) {\n+      if (UseAVX == 0) {\n+        movdqu(dst, src);\n+        pshufps(dst, dst, eindex);\n+      } else {\n+        vpshufps(dst, src, src, eindex, Assembler::AVX_128bit);\n+      }\n+    } else {\n+      if (UseAVX == 0) {\n+        movdqu(dst, src);\n+        psrldq(dst, eindex*esize);\n+      } else {\n+        vpsrldq(dst, src, eindex*esize, Assembler::AVX_128bit);\n+      }\n+      movq(dst, dst);\n+    }\n+  }\n+  \/\/ Zero upper bits\n+  if (typ == T_FLOAT) {\n+    if (UseAVX == 0) {\n+      assert((vtmp != xnoreg) && (tmp != noreg), \"required.\");\n+      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), tmp);\n+      pand(dst, vtmp);\n+    } else {\n+      assert((tmp != noreg), \"required.\");\n+      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, tmp);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch) {\n+  switch(typ) {\n+    case T_BYTE:\n+      evpcmpb(kdmask, ksmask, src1, adr, comparison, vector_len, scratch);\n+      break;\n+    case T_SHORT:\n+      evpcmpw(kdmask, ksmask, src1, adr, comparison, vector_len, scratch);\n+      break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpcmpd(kdmask, ksmask, src1, adr, comparison, vector_len, scratch);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpcmpq(kdmask, ksmask, src1, adr, comparison, vector_len, scratch);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evpblend(BasicType typ, XMMRegister dst, KRegister kmask, XMMRegister src1, XMMRegister src2, bool merge, int vector_len) {\n+  switch(typ) {\n+    case T_BYTE:\n+      evpblendmb(dst, kmask, src1, src2, merge, vector_len);\n+      break;\n+    case T_SHORT:\n+      evpblendmw(dst, kmask, src1, src2, merge, vector_len);\n+      break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpblendmd(dst, kmask, src1, src2, merge, vector_len);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpblendmq(dst, kmask, src1, src2, merge, vector_len);\n+      break;\n+    default:\n+      assert(false,\"Should not reach here.\");\n+      break;\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":950,"deletions":83,"binary":false,"changes":1033,"status":"modified"},{"patch":"@@ -115,0 +115,1 @@\n+\n@@ -2498,0 +2499,1 @@\n+    if (dst->encoding() == src->encoding()) return;\n@@ -2522,0 +2524,1 @@\n+    if (dst->encoding() == src->encoding()) return;\n@@ -2535,0 +2538,58 @@\n+\n+void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+  if (reachable(src)) {\n+    kmovwl(dst, as_Address(src));\n+  } else {\n+    lea(scratch_reg, src);\n+    kmovwl(dst, Address(scratch_reg, 0));\n+  }\n+}\n+\n+void MacroAssembler::evmovdqub(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n+                               int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    if (mask == k0) {\n+      Assembler::evmovdqub(dst, as_Address(src), merge, vector_len);\n+    } else {\n+      Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n+    }\n+  } else {\n+    lea(scratch_reg, src);\n+    if (mask == k0) {\n+      Assembler::evmovdqub(dst, Address(scratch_reg, 0), merge, vector_len);\n+    } else {\n+      Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    }\n+  }\n+}\n+\n+void MacroAssembler::evmovdquw(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n+                               int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evmovdquw(dst, mask, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evmovdquw(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n+                               int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evmovdqul(dst, mask, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evmovdqul(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n+                               int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evmovdquq(dst, mask, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evmovdquq(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+  }\n+}\n+\n@@ -3022,0 +3083,92 @@\n+void MacroAssembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds,\n+                               AddressLiteral src, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evpcmpeqd(kdst, mask, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evpcmpeqd(kdst, mask, nds, Address(scratch_reg, 0), vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+                             int comparison, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evpcmpd(kdst, mask, nds, as_Address(src), comparison, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evpcmpd(kdst, mask, nds, Address(scratch_reg, 0), comparison, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+                             int comparison, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evpcmpq(kdst, mask, nds, as_Address(src), comparison, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evpcmpq(kdst, mask, nds, Address(scratch_reg, 0), comparison, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+                             int comparison, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evpcmpb(kdst, mask, nds, as_Address(src), comparison, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evpcmpb(kdst, mask, nds, Address(scratch_reg, 0), comparison, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+                             int comparison, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evpcmpw(kdst, mask, nds, as_Address(src), comparison, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evpcmpw(kdst, mask, nds, Address(scratch_reg, 0), comparison, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::vpcmpCC(XMMRegister dst, XMMRegister nds, XMMRegister src, int cond_encoding, Width width, int vector_len) {\n+  if (width == Assembler::Q) {\n+    Assembler::vpcmpCCq(dst, nds, src, cond_encoding, vector_len);\n+  } else {\n+    Assembler::vpcmpCCbwd(dst, nds, src, cond_encoding, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::vpcmpCCW(XMMRegister dst, XMMRegister nds, XMMRegister src, ComparisonPredicate cond, Width width, int vector_len, Register scratch_reg) {\n+  int eq_cond_enc = 0x29;\n+  int gt_cond_enc = 0x37;\n+  if (width != Assembler::Q) {\n+    eq_cond_enc = 0x74 + width;\n+    gt_cond_enc = 0x64 + width;\n+  }\n+  switch (cond) {\n+  case eq:\n+    vpcmpCC(dst, nds, src, eq_cond_enc, width, vector_len);\n+    break;\n+  case neq:\n+    vpcmpCC(dst, nds, src, eq_cond_enc, width, vector_len);\n+    vpxor(dst, dst, ExternalAddress(StubRoutines::x86::vector_all_bits_set()), vector_len, scratch_reg);\n+    break;\n+  case le:\n+    vpcmpCC(dst, nds, src, gt_cond_enc, width, vector_len);\n+    vpxor(dst, dst, ExternalAddress(StubRoutines::x86::vector_all_bits_set()), vector_len, scratch_reg);\n+    break;\n+  case nlt:\n+    vpcmpCC(dst, src, nds, gt_cond_enc, width, vector_len);\n+    vpxor(dst, dst, ExternalAddress(StubRoutines::x86::vector_all_bits_set()), vector_len, scratch_reg);\n+    break;\n+  case lt:\n+    vpcmpCC(dst, src, nds, gt_cond_enc, width, vector_len);\n+    break;\n+  case nle:\n+    vpcmpCC(dst, nds, src, gt_cond_enc, width, vector_len);\n+    break;\n+  default:\n+    assert(false, \"Should not reach here\");\n+  }\n+}\n+\n@@ -3146,0 +3299,10 @@\n+void MacroAssembler::evpord(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+                            bool merge, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::evpord(dst, mask, nds, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::evpord(dst, mask, nds, Address(scratch_reg, 0), merge, vector_len);\n+  }\n+}\n+\n@@ -3242,1 +3405,8 @@\n-\/\/-------------------------------------------------------------------------------------------\n+void MacroAssembler::vpermd(XMMRegister dst,  XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+  if (reachable(src)) {\n+    Assembler::vpermd(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(scratch_reg, src);\n+    Assembler::vpermd(dst, nds, Address(scratch_reg, 0), vector_len);\n+  }\n+}\n@@ -5768,1 +5938,1 @@\n-    evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);\n+    evmovdqub(rymm0, Address(obja, result), false, Assembler::AVX_512bit);\n@@ -5787,1 +5957,1 @@\n-    evmovdqub(rymm0, k3, Address(obja, result), Assembler::AVX_512bit);\n+    evmovdqub(rymm0, k3, Address(obja, result), false, Assembler::AVX_512bit);\n@@ -7582,1 +7752,1 @@\n-    evmovdquw(tmp1Reg, k3, Address(src, 0), Assembler::AVX_512bit);\n+    evmovdquw(tmp1Reg, k3, Address(src, 0), \/*merge*\/ false, Assembler::AVX_512bit);\n@@ -7607,1 +7777,1 @@\n-    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), Assembler::AVX_512bit);\n+    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), \/*merge*\/ false, Assembler::AVX_512bit);\n@@ -7632,1 +7802,1 @@\n-    evmovdquw(tmp1Reg, k3, Address(src, 0), Assembler::AVX_512bit);\n+    evmovdquw(tmp1Reg, k3, Address(src, 0), \/*merge*\/ false, Assembler::AVX_512bit);\n@@ -7777,1 +7947,1 @@\n-    evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);\n+    evmovdquw(Address(dst, len, Address::times_2), tmp1, \/*merge*\/ false, Assembler::AVX_512bit);\n@@ -7792,1 +7962,1 @@\n-    evmovdquw(Address(dst, 0), k2, tmp1, Assembler::AVX_512bit);\n+    evmovdquw(Address(dst, 0), k2, tmp1, \/*merge*\/ true, Assembler::AVX_512bit);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":178,"deletions":8,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -1079,0 +1079,6 @@\n+\n+  void kmovwl(KRegister dst, Register src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(Register dst, KRegister src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Address src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+\n@@ -1084,0 +1090,28 @@\n+\n+  \/\/ AVX512 Unaligned\n+  void evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+\n+  void evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n+  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+\n+  void evmovdqul(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+  void evmovdqul(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+  void evmovdqul(XMMRegister dst, XMMRegister src, int vector_len) {\n+     if (dst->encoding() == src->encoding()) return;\n+     Assembler::evmovdqul(dst, src, vector_len);\n+  }\n+  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() == src->encoding() && mask == k0) return;\n+    Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n+   }\n+  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+\n@@ -1085,1 +1119,0 @@\n-  void evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n@@ -1088,0 +1121,11 @@\n+  void evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) {\n+    if (dst->encoding() == src->encoding()) return;\n+    Assembler::evmovdquq(dst, src, vector_len);\n+  }\n+  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() == src->encoding() && mask == k0) return;\n+    Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+  }\n+  void evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n@@ -1209,0 +1253,24 @@\n+  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+\n+  \/\/ Vector compares\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len) { Assembler::evpcmpd(kdst, mask, nds, src, comparison, vector_len); }\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+               int comparison, int vector_len, Register scratch_reg);\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len) { Assembler::evpcmpq(kdst, mask, nds, src, comparison, vector_len); }\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+               int comparison, int vector_len, Register scratch_reg);\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len) { Assembler::evpcmpb(kdst, mask, nds, src, comparison, vector_len); }\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+               int comparison, int vector_len, Register scratch_reg);\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n+               int comparison, int vector_len) { Assembler::evpcmpw(kdst, mask, nds, src, comparison, vector_len); }\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n+               int comparison, int vector_len, Register scratch_reg);\n+\n+\n+  \/\/ Emit comparison instruction for the specified comparison predicate.\n+  void vpcmpCCW(XMMRegister dst, XMMRegister nds, XMMRegister src, ComparisonPredicate cond, Width width, int vector_len, Register scratch_reg);\n+  void vpcmpCC(XMMRegister dst, XMMRegister nds, XMMRegister src, int cond_encoding, Width width, int vector_len);\n@@ -1237,0 +1305,1 @@\n+  void vptest(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::vptest(dst, src, vector_len); }\n@@ -1255,0 +1324,2 @@\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+\n@@ -1310,0 +1381,3 @@\n+  void vpermd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpermd(dst, nds, src, vector_len); }\n+  void vpermd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":75,"deletions":1,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -812,0 +812,15 @@\n+  address generate_iota_indices(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0706050403020100, relocInfo::none);\n+    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+    __ emit_data64(0x1716151413121110, relocInfo::none);\n+    __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n+    __ emit_data64(0x2726252423222120, relocInfo::none);\n+    __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n+    __ emit_data64(0x3736353433323130, relocInfo::none);\n+    __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -857,0 +872,51 @@\n+  address generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+    __ emit_data64(mask, relocInfo::none);\n+\n+    return start;\n+  }\n+\n+  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n+                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n+                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+\n+    assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n+    __ emit_data(val0, relocInfo::none, 0);\n+    __ emit_data(val1, relocInfo::none, 0);\n+    __ emit_data(val2, relocInfo::none, 0);\n+    __ emit_data(val3, relocInfo::none, 0);\n+    if (len >= Assembler::AVX_256bit) {\n+      __ emit_data(val4, relocInfo::none, 0);\n+      __ emit_data(val5, relocInfo::none, 0);\n+      __ emit_data(val6, relocInfo::none, 0);\n+      __ emit_data(val7, relocInfo::none, 0);\n+      if (len >= Assembler::AVX_512bit) {\n+        __ emit_data(val8, relocInfo::none, 0);\n+        __ emit_data(val9, relocInfo::none, 0);\n+        __ emit_data(val10, relocInfo::none, 0);\n+        __ emit_data(val11, relocInfo::none, 0);\n+        __ emit_data(val12, relocInfo::none, 0);\n+        __ emit_data(val13, relocInfo::none, 0);\n+        __ emit_data(val14, relocInfo::none, 0);\n+        __ emit_data(val15, relocInfo::none, 0);\n+      }\n+    }\n+\n+    return start;\n+  }\n+\n@@ -6755,0 +6821,1 @@\n+    StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n@@ -6757,0 +6824,9 @@\n+    StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n+    StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n+    StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n+                                                                        0xFFFFFFFF, 0, 0, 0);\n+    StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n+                                                                        0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n+    StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n+    StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n+    StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n@@ -6758,0 +6834,1 @@\n+    StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -149,1 +149,10 @@\n-  static address _vector_byte_perm_mask;\n+  static address _vector_all_bits_set;\n+  static address _vector_byte_perm_mask;\n+  static address _vector_int_to_byte_mask;\n+  static address _vector_int_to_short_mask;\n+  static address _vector_32_bit_mask;\n+  static address _vector_64_bit_mask;\n+  static address _vector_int_shuffle_mask;\n+  static address _vector_short_shuffle_mask;\n+  static address _vector_long_shuffle_mask;\n+  static address _vector_iota_indices;\n@@ -251,0 +260,4 @@\n+  static address vector_all_bits_set() {\n+    return _vector_all_bits_set;\n+  }\n+\n@@ -255,0 +268,28 @@\n+  static address vector_int_to_byte_mask() {\n+    return _vector_int_to_byte_mask;\n+  }\n+\n+  static address vector_int_to_short_mask() {\n+    return _vector_int_to_short_mask;\n+  }\n+\n+  static address vector_32_bit_mask() {\n+    return _vector_32_bit_mask;\n+  }\n+\n+  static address vector_64_bit_mask() {\n+    return _vector_64_bit_mask;\n+  }\n+\n+  static address vector_int_shuffle_mask() {\n+    return _vector_int_shuffle_mask;\n+  }\n+\n+  static address vector_short_shuffle_mask() {\n+    return _vector_short_shuffle_mask;\n+  }\n+\n+  static address vector_long_shuffle_mask() {\n+    return _vector_long_shuffle_mask;\n+  }\n+\n@@ -258,0 +299,5 @@\n+\n+  static address vector_iota_indices() {\n+    return _vector_iota_indices;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":47,"deletions":1,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2874,1 +2874,1 @@\n-operand immI0()\n+operand immI_0()\n@@ -2885,1 +2885,1 @@\n-operand immI1()\n+operand immI_1()\n@@ -2906,0 +2906,30 @@\n+operand immI_2()\n+%{\n+  predicate(n->get_int() == 2);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_4()\n+%{\n+  predicate(n->get_int() == 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_8()\n+%{\n+  predicate(n->get_int() == 8);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -5220,2 +5250,2 @@\n-     \"blendvps         $btmp,$b,$a,$b           \\n\\t\"\n-     \"blendvps         $atmp,$a,$b,$b           \\n\\t\"\n+     \"vblendvps        $btmp,$b,$a,$b           \\n\\t\"\n+     \"vblendvps        $atmp,$a,$b,$b           \\n\\t\"\n@@ -5223,2 +5253,2 @@\n-     \"cmpps.unordered  $btmp,$atmp,$atmp        \\n\\t\"\n-     \"blendvps         $dst,$tmp,$atmp,$btmp    \\n\\t\"\n+     \"vcmpps.unordered $btmp,$atmp,$atmp        \\n\\t\"\n+     \"vblendvps        $dst,$tmp,$atmp,$btmp    \\n\\t\"\n@@ -5228,2 +5258,2 @@\n-    __ blendvps($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vector_len);\n-    __ blendvps($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $b$$XMMRegister, vector_len);\n+    __ vblendvps($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vector_len);\n+    __ vblendvps($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $b$$XMMRegister, vector_len);\n@@ -5231,2 +5261,2 @@\n-    __ cmpps($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ blendvps($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vcmpps($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n+    __ vblendvps($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n@@ -5256,2 +5286,2 @@\n-     \"blendvpd         $btmp,$b,$a,$b            \\n\\t\"\n-     \"blendvpd         $atmp,$a,$b,$b            \\n\\t\"\n+     \"vblendvpd        $btmp,$b,$a,$b            \\n\\t\"\n+     \"vblendvpd        $atmp,$a,$b,$b            \\n\\t\"\n@@ -5259,2 +5289,2 @@\n-     \"cmppd.unordered  $btmp,$atmp,$atmp         \\n\\t\"\n-     \"blendvpd         $dst,$tmp,$atmp,$btmp     \\n\\t\"\n+     \"vcmppd.unordered $btmp,$atmp,$atmp         \\n\\t\"\n+     \"vblendvpd        $dst,$tmp,$atmp,$btmp     \\n\\t\"\n@@ -5264,2 +5294,2 @@\n-    __ blendvpd($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vector_len);\n-    __ blendvpd($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $b$$XMMRegister, vector_len);\n+    __ vblendvpd($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vector_len);\n+    __ vblendvpd($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $b$$XMMRegister, vector_len);\n@@ -5267,2 +5297,2 @@\n-    __ cmppd($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ blendvpd($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vcmppd($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n+    __ vblendvpd($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n@@ -5292,2 +5322,2 @@\n-     \"blendvps         $atmp,$a,$b,$a             \\n\\t\"\n-     \"blendvps         $btmp,$b,$a,$a             \\n\\t\"\n+     \"vblendvps        $atmp,$a,$b,$a             \\n\\t\"\n+     \"vblendvps        $btmp,$b,$a,$a             \\n\\t\"\n@@ -5295,2 +5325,2 @@\n-     \"cmpps.unordered  $btmp,$atmp,$atmp          \\n\\t\"\n-     \"blendvps         $dst,$tmp,$atmp,$btmp      \\n\\t\"\n+     \"vcmpps.unordered $btmp,$atmp,$atmp          \\n\\t\"\n+     \"vblendvps        $dst,$tmp,$atmp,$btmp      \\n\\t\"\n@@ -5300,2 +5330,2 @@\n-    __ blendvps($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, vector_len);\n-    __ blendvps($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $a$$XMMRegister, vector_len);\n+    __ vblendvps($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, vector_len);\n+    __ vblendvps($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $a$$XMMRegister, vector_len);\n@@ -5303,2 +5333,2 @@\n-    __ cmpps($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ blendvps($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vcmpps($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n+    __ vblendvps($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n@@ -5328,2 +5358,2 @@\n-     \"blendvpd         $atmp,$a,$b,$a           \\n\\t\"\n-     \"blendvpd         $btmp,$b,$a,$a           \\n\\t\"\n+     \"vblendvpd        $atmp,$a,$b,$a           \\n\\t\"\n+     \"vblendvpd        $btmp,$b,$a,$a           \\n\\t\"\n@@ -5331,2 +5361,2 @@\n-     \"cmppd.unordered  $btmp,$atmp,$atmp        \\n\\t\"\n-     \"blendvpd         $dst,$tmp,$atmp,$btmp    \\n\\t\"\n+     \"vcmppd.unordered $btmp,$atmp,$atmp        \\n\\t\"\n+     \"vblendvpd        $dst,$tmp,$atmp,$btmp    \\n\\t\"\n@@ -5336,2 +5366,2 @@\n-    __ blendvpd($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, vector_len);\n-    __ blendvpd($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $a$$XMMRegister, vector_len);\n+    __ vblendvpd($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, vector_len);\n+    __ vblendvpd($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $a$$XMMRegister, vector_len);\n@@ -5339,2 +5369,2 @@\n-    __ cmppd($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ blendvpd($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vcmppd($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n+    __ vblendvpd($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n@@ -5564,1 +5594,1 @@\n-instruct loadConI0(rRegI dst, immI0 src, rFlagsReg cr)\n+instruct loadConI0(rRegI dst, immI_0 src, rFlagsReg cr)\n@@ -6000,1 +6030,1 @@\n-instruct storeImmI0(memory mem, immI0 zero)\n+instruct storeImmI0(memory mem, immI_0 zero)\n@@ -6050,1 +6080,1 @@\n-instruct storeImmC0(memory mem, immI0 zero)\n+instruct storeImmC0(memory mem, immI_0 zero)\n@@ -6076,1 +6106,1 @@\n-instruct storeImmB0(memory mem, immI0 zero)\n+instruct storeImmB0(memory mem, immI_0 zero)\n@@ -6101,1 +6131,1 @@\n-instruct storeImmCM0_reg(memory mem, immI0 zero)\n+instruct storeImmCM0_reg(memory mem, immI_0 zero)\n@@ -6114,1 +6144,1 @@\n-instruct storeImmCM0(memory mem, immI0 src)\n+instruct storeImmCM0(memory mem, immI_0 src)\n@@ -7199,1 +7229,1 @@\n-instruct incI_rReg(rRegI dst, immI1 src, rFlagsReg cr)\n+instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n@@ -7211,1 +7241,1 @@\n-instruct incI_mem(memory dst, immI1 src, rFlagsReg cr)\n+instruct incI_mem(memory dst, immI_1 src, rFlagsReg cr)\n@@ -8094,1 +8124,1 @@\n-instruct subP_rReg(rRegP dst, rRegI src, immI0 zero, rFlagsReg cr)\n+instruct subP_rReg(rRegP dst, rRegI src, immI_0 zero, rFlagsReg cr)\n@@ -8105,1 +8135,1 @@\n-instruct negI_rReg(rRegI dst, immI0 zero, rFlagsReg cr)\n+instruct negI_rReg(rRegI dst, immI_0 zero, rFlagsReg cr)\n@@ -8116,1 +8146,13 @@\n-instruct negI_mem(memory dst, immI0 zero, rFlagsReg cr)\n+instruct negI_rReg_2(rRegI dst, rFlagsReg cr)\n+%{\n+  match(Set dst (NegI dst));\n+  effect(KILL cr);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_mem(memory dst, immI_0 zero, rFlagsReg cr)\n@@ -8138,0 +8180,12 @@\n+instruct negL_rReg_2(rRegL dst, rFlagsReg cr)\n+%{\n+  match(Set dst (NegL dst));\n+  effect(KILL cr);\n+\n+  format %{ \"negq    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -8463,1 +8517,1 @@\n-instruct salI_rReg_1(rRegI dst, immI1 shift, rFlagsReg cr)\n+instruct salI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n@@ -8475,1 +8529,1 @@\n-instruct salI_mem_1(memory dst, immI1 shift, rFlagsReg cr)\n+instruct salI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n@@ -8535,1 +8589,1 @@\n-instruct sarI_rReg_1(rRegI dst, immI1 shift, rFlagsReg cr)\n+instruct sarI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n@@ -8547,1 +8601,1 @@\n-instruct sarI_mem_1(memory dst, immI1 shift, rFlagsReg cr)\n+instruct sarI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n@@ -8607,1 +8661,1 @@\n-instruct shrI_rReg_1(rRegI dst, immI1 shift, rFlagsReg cr)\n+instruct shrI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n@@ -8619,1 +8673,1 @@\n-instruct shrI_mem_1(memory dst, immI1 shift, rFlagsReg cr)\n+instruct shrI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n@@ -8680,1 +8734,1 @@\n-instruct salL_rReg_1(rRegL dst, immI1 shift, rFlagsReg cr)\n+instruct salL_rReg_1(rRegL dst, immI_1 shift, rFlagsReg cr)\n@@ -8692,1 +8746,1 @@\n-instruct salL_mem_1(memory dst, immI1 shift, rFlagsReg cr)\n+instruct salL_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n@@ -8753,1 +8807,1 @@\n-instruct sarL_rReg_1(rRegL dst, immI1 shift, rFlagsReg cr)\n+instruct sarL_rReg_1(rRegL dst, immI_1 shift, rFlagsReg cr)\n@@ -8765,1 +8819,1 @@\n-instruct sarL_mem_1(memory dst, immI1 shift, rFlagsReg cr)\n+instruct sarL_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n@@ -8826,1 +8880,1 @@\n-instruct shrL_rReg_1(rRegL dst, immI1 shift, rFlagsReg cr)\n+instruct shrL_rReg_1(rRegL dst, immI_1 shift, rFlagsReg cr)\n@@ -8838,1 +8892,1 @@\n-instruct shrL_mem_1(memory dst, immI1 shift, rFlagsReg cr)\n+instruct shrL_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n@@ -9210,1 +9264,1 @@\n-instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI0 imm_zero, rFlagsReg cr) %{\n+instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, rFlagsReg cr) %{\n@@ -9223,1 +9277,1 @@\n-instruct blsiI_rReg_mem(rRegI dst, memory src, immI0 imm_zero, rFlagsReg cr) %{\n+instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, rFlagsReg cr) %{\n@@ -9906,1 +9960,1 @@\n-instruct cmpLTMask0(rRegI dst, immI0 zero, rFlagsReg cr)\n+instruct cmpLTMask0(rRegI dst, immI_0 zero, rFlagsReg cr)\n@@ -11239,1 +11293,1 @@\n-instruct overflowNegI_rReg(rFlagsReg cr, immI0 zero, rax_RegI op2)\n+instruct overflowNegI_rReg(rFlagsReg cr, immI_0 zero, rax_RegI op2)\n@@ -11348,1 +11402,1 @@\n-instruct testI_reg(rFlagsReg cr, rRegI src, immI0 zero)\n+instruct testI_reg(rFlagsReg cr, rRegI src, immI_0 zero)\n@@ -11358,1 +11412,1 @@\n-instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI0 zero)\n+instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI_0 zero)\n@@ -11368,1 +11422,1 @@\n-instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI0 zero)\n+instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI_0 zero)\n@@ -11422,1 +11476,1 @@\n-instruct testU_reg(rFlagsRegU cr, rRegI src, immI0 zero)\n+instruct testU_reg(rFlagsRegU cr, rRegI src, immI_0 zero)\n@@ -11760,1 +11814,1 @@\n-instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI0 zero)\n+instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)\n@@ -11770,1 +11824,1 @@\n-instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI0 zero)\n+instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI_0 zero)\n@@ -12493,1 +12547,1 @@\n-\/\/ instruct incI_rReg(rRegI dst, immI1 src, rFlagsReg cr)\n+\/\/ instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":122,"deletions":68,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -4541,0 +4541,24 @@\n+int vector_VectorPayload::_payload_offset;\n+\n+#define VECTORPAYLOAD_FIELDS_DO(macro) \\\n+  macro(_payload_offset, k, \"payload\", object_signature, false)\n+\n+void vector_VectorPayload::compute_offsets() {\n+  InstanceKlass* k = SystemDictionary::vector_VectorPayload_klass();\n+  VECTORPAYLOAD_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void vector_VectorPayload::serialize_offsets(SerializeClosure* f) {\n+  VECTORPAYLOAD_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+void vector_VectorPayload::set_payload(oop o, oop val) {\n+  o->obj_field_put(_payload_offset, val);\n+}\n+\n+bool vector_VectorPayload::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+  f(vector_VectorPayload) \\\n@@ -1567,0 +1568,18 @@\n+\/\/ Interface to jdk.internal.vm.vector.VectorSupport.VectorPayload objects\n+\n+class vector_VectorPayload : AllStatic {\n+ private:\n+  static int _payload_offset;\n+ public:\n+  static void set_payload(oop o, oop val);\n+\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return klass->is_subclass_of(SystemDictionary::vector_VectorPayload_klass());\n+  }\n+  static bool is_instance(oop obj);\n+};\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -229,0 +229,7 @@\n+  \/* support for vectors*\/                                                                                      \\\n+  do_klass(vector_VectorSupport_klass,                  jdk_internal_vm_vector_VectorSupport                  ) \\\n+  do_klass(vector_VectorPayload_klass,                  jdk_internal_vm_vector_VectorPayload                  ) \\\n+  do_klass(vector_Vector_klass,                         jdk_internal_vm_vector_Vector                         ) \\\n+  do_klass(vector_VectorMask_klass,                     jdk_internal_vm_vector_VectorMask                     ) \\\n+  do_klass(vector_VectorShuffle_klass,                  jdk_internal_vm_vector_VectorShuffle                  ) \\\n+                                                                                                                \\\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -780,0 +780,116 @@\n+                                                                                                                                               \\\n+  \/* Vector API intrinsification support *\/                                                                                                    \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorUnaryOp, jdk_internal_vm_vector_VectorSupport, vector_unary_op_name, vector_unary_op_sig, F_S)                           \\\n+   do_signature(vector_unary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/util\/function\/Function;)Ljava\/lang\/Object;\") \\\n+   do_name(vector_unary_op_name,     \"unaryOp\")                                                                                                \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorBinaryOp, jdk_internal_vm_vector_VectorSupport, vector_binary_op_name, vector_binary_op_sig, F_S)                        \\\n+   do_signature(vector_binary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                              \\\n+                                       \"Ljava\/util\/function\/BiFunction;)Ljava\/lang\/Object;\")                                                   \\\n+   do_name(vector_binary_op_name,     \"binaryOp\")                                                                                              \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorTernaryOp, jdk_internal_vm_vector_VectorSupport, vector_ternary_op_name, vector_ternary_op_sig, F_S)                     \\\n+   do_signature(vector_ternary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                             \\\n+                                        \"Ljava\/lang\/Object;Ljdk\/internal\/vm\/vector\/VectorSupport$TernaryOperation;)Ljava\/lang\/Object;\")        \\\n+   do_name(vector_ternary_op_name,     \"ternaryOp\")                                                                                            \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorBroadcastCoerced, jdk_internal_vm_vector_VectorSupport, vector_broadcast_coerced_name, vector_broadcast_coerced_sig, F_S)\\\n+   do_signature(vector_broadcast_coerced_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;IJLjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"      \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$BroadcastOperation;)Ljava\/lang\/Object;\")                 \\\n+   do_name(vector_broadcast_coerced_name, \"broadcastCoerced\")                                                                                  \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorShuffleIota, jdk_internal_vm_vector_VectorSupport, vector_shuffle_step_iota_name, vector_shuffle_step_iota_sig, F_S)     \\\n+   do_signature(vector_shuffle_step_iota_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"        \\\n+                                               \"IIIILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleIotaOperation;)Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\") \\\n+   do_name(vector_shuffle_step_iota_name, \"shuffleIota\")                                                                                       \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorShuffleToVector, jdk_internal_vm_vector_VectorSupport, vector_shuffle_to_vector_name, vector_shuffle_to_vector_sig, F_S) \\\n+   do_signature(vector_shuffle_to_vector_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\" \\\n+                                               \"ILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleToVectorOperation;)Ljava\/lang\/Object;\")          \\\n+   do_name(vector_shuffle_to_vector_name, \"shuffleToVector\")                                                                                   \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorLoadOp, jdk_internal_vm_vector_VectorSupport, vector_load_op_name, vector_load_op_sig, F_S)                              \\\n+   do_signature(vector_load_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjava\/lang\/Object;\"                                \\\n+                                     \"ILjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;Ljdk\/internal\/vm\/vector\/VectorSupport$LoadOperation;)Ljava\/lang\/Object;\") \\\n+   do_name(vector_load_op_name,     \"load\")                                                                                                    \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorStoreOp, jdk_internal_vm_vector_VectorSupport, vector_store_op_name, vector_store_op_sig, F_S)                           \\\n+   do_signature(vector_store_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$Vector;\"    \\\n+                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)V\")                      \\\n+   do_name(vector_store_op_name,     \"store\")                                                                                                  \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S) \\\n+   do_signature(vector_reduction_coerced_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljava\/util\/function\/Function;)J\") \\\n+   do_name(vector_reduction_coerced_name, \"reductionCoerced\")                                                                                  \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorTest, jdk_internal_vm_vector_VectorSupport, vector_test_name, vector_test_sig, F_S)                                      \\\n+   do_signature(vector_test_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;Ljava\/util\/function\/BiFunction;)Z\") \\\n+   do_name(vector_test_name, \"test\")                                                                                                           \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorBlend, jdk_internal_vm_vector_VectorSupport, vector_blend_name, vector_blend_sig, F_S)                                   \\\n+   do_signature(vector_blend_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                      \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\" \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBlendOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")       \\\n+   do_name(vector_blend_name, \"blend\")                                                                                                         \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorCompare, jdk_internal_vm_vector_VectorSupport, vector_compare_name, vector_compare_sig, F_S)                             \\\n+   do_signature(vector_compare_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                   \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\" \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"           \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorCompareOp;\" \")\" \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\") \\\n+   do_name(vector_compare_name, \"compare\")                                                                                                     \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorRearrange, jdk_internal_vm_vector_VectorSupport, vector_rearrange_name, vector_rearrange_sig, F_S)                       \\\n+   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorRearrangeOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\") \\\n+   do_name(vector_rearrange_name, \"rearrangeOp\")                                                                                               \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorExtract, jdk_internal_vm_vector_VectorSupport, vector_extract_name, vector_extract_sig, F_S)                             \\\n+   do_signature(vector_extract_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                     \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                          \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecExtractOp;)J\")                                                  \\\n+   do_name(vector_extract_name, \"extract\")                                                                                                     \\\n+                                                                                                                                               \\\n+ do_intrinsic(_VectorInsert, jdk_internal_vm_vector_VectorSupport, vector_insert_name, vector_insert_sig, F_S)                                 \\\n+   do_signature(vector_insert_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                      \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;IJ\"                                                          \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecInsertOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")        \\\n+   do_name(vector_insert_name, \"insert\")                                                                                                       \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorBroadcastInt, jdk_internal_vm_vector_VectorSupport, vector_broadcast_int_name, vector_broadcast_int_sig, F_S)            \\\n+   do_signature(vector_broadcast_int_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                              \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                    \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBroadcastIntOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\") \\\n+   do_name(vector_broadcast_int_name, \"broadcastInt\")                                                                                          \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorConvert, jdk_internal_vm_vector_VectorSupport, vector_convert_name, vector_convert_sig, F_S)                             \\\n+   do_signature(vector_convert_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                    \\\n+                                     \"Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                     \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                    \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                                    \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorConvertOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\") \\\n+   do_name(vector_convert_name, \"convert\")                                                                                                     \\\n+                                                                                                                                               \\\n+   do_intrinsic(_VectorGatherOp, jdk_internal_vm_vector_VectorSupport, vector_gather_name, vector_gather_sig, F_S)                             \\\n+    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                    \\\n+                                     \"Ljava\/lang\/Object;J\"                                                                                     \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"Ljava\/lang\/Object;I[II\"                                                                                  \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                                    \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$LoadVectorOperationWithMap;)\"                                      \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                          \\\n+    do_name(vector_gather_name, \"loadWithMap\")                                                                                                 \\\n+                                                                                                                                               \\\n+   do_intrinsic(_VectorScatterOp, jdk_internal_vm_vector_VectorSupport, vector_scatter_name, vector_scatter_sig, F_S)                          \\\n+    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                   \\\n+                                      \"Ljava\/lang\/Object;J\"                                                                                    \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"             \\\n+                                      \"Ljava\/lang\/Object;I[II\"                                                                                 \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperationWithMap;)V\")                                  \\\n+    do_name(vector_scatter_name, \"storeWithMap\")                                                                                               \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorRebox, jdk_internal_vm_vector_VectorSupport, vector_rebox_name, vector_rebox_sig, F_S)                                   \\\n+   do_alias(vector_rebox_sig, object_object_signature)                                                                                         \\\n+   do_name(vector_rebox_name, \"maybeRebox\")                                                                                                    \\\n+                                                                                                                                               \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":116,"deletions":0,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -84,0 +84,10 @@\n+                                                                                                  \\\n+  template(jdk_internal_vm_vector_VectorSupport,      \"jdk\/internal\/vm\/vector\/VectorSupport\")               \\\n+  template(jdk_internal_vm_vector_VectorPayload,      \"jdk\/internal\/vm\/vector\/VectorSupport$VectorPayload\") \\\n+  template(jdk_internal_vm_vector_Vector,             \"jdk\/internal\/vm\/vector\/VectorSupport$Vector\")        \\\n+  template(jdk_internal_vm_vector_VectorMask,         \"jdk\/internal\/vm\/vector\/VectorSupport$VectorMask\")    \\\n+  template(jdk_internal_vm_vector_VectorShuffle,      \"jdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle\") \\\n+  template(payload_name,                              \"payload\")                                            \\\n+  template(ETYPE_name,                                \"ETYPE\")                                              \\\n+  template(VLENGTH_name,                              \"VLENGTH\")                                            \\\n+                                                                                                  \\\n@@ -771,1 +781,1 @@\n-    LAST_COMPILER_INLINE = _getAndSetReference,\n+    LAST_COMPILER_INLINE = _VectorScatterOp,\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -746,0 +746,9 @@\n+  product(bool, EnableVectorSupport, false, EXPERIMENTAL,                   \\\n+          \"Enables VectorSupport intrinsics\")                               \\\n+                                                                            \\\n+  product(bool, EnableVectorReboxing, false, EXPERIMENTAL,                  \\\n+          \"Enables reboxing of vectors\")                                    \\\n+                                                                            \\\n+  product(bool, EnableVectorAggressiveReboxing, false, EXPERIMENTAL,        \\\n+          \"Enables aggressive reboxing of vectors\")                         \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -539,1 +539,1 @@\n-    JVMState* new_jvms =  DirectCallGenerator::generate(jvms);\n+    JVMState* new_jvms = DirectCallGenerator::generate(jvms);\n@@ -563,1 +563,1 @@\n-    JVMState* new_jvms =  DirectCallGenerator::generate(jvms);\n+    JVMState* new_jvms = DirectCallGenerator::generate(jvms);\n@@ -572,0 +572,22 @@\n+class LateInlineVectorReboxingCallGenerator : public LateInlineCallGenerator {\n+\n+ public:\n+  LateInlineVectorReboxingCallGenerator(ciMethod* method, CallGenerator* inline_cg) :\n+    LateInlineCallGenerator(method, inline_cg, \/*is_pure=*\/true) {}\n+\n+  virtual JVMState* generate(JVMState* jvms) {\n+    Compile *C = Compile::current();\n+\n+    C->log_inline_id(this);\n+\n+    C->add_vector_reboxing_late_inline(this);\n+\n+    JVMState* new_jvms = DirectCallGenerator::generate(jvms);\n+    return new_jvms;\n+  }\n+};\n+\n+\/\/   static CallGenerator* for_vector_reboxing_late_inline(ciMethod* m, CallGenerator* inline_cg);\n+CallGenerator* CallGenerator::for_vector_reboxing_late_inline(ciMethod* method, CallGenerator* inline_cg) {\n+  return new LateInlineVectorReboxingCallGenerator(method, inline_cg);\n+}\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"opto\/vectornode.hpp\"\n@@ -2390,0 +2391,41 @@\n+  \/\/ Phi (VB ... VB) => VB (Phi ...) (Phi ...)\n+  if (EnableVectorReboxing && can_reshape && progress == NULL) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+\n+    bool all_inputs_are_equiv_vboxes = true;\n+    for (uint i = 1; i < req(); ++i) {\n+      Node* n = in(i);\n+      if (in(i)->Opcode() != Op_VectorBox) {\n+        all_inputs_are_equiv_vboxes = false;\n+        break;\n+      }\n+      \/\/ Check that vector type of vboxes is equivalent\n+      if (i != 1) {\n+        if (Type::cmp(in(i-0)->in(VectorBoxNode::Value)->bottom_type(),\n+                      in(i-1)->in(VectorBoxNode::Value)->bottom_type()) != 0) {\n+          all_inputs_are_equiv_vboxes = false;\n+          break;\n+        }\n+        if (Type::cmp(in(i-0)->in(VectorBoxNode::Box)->bottom_type(),\n+                      in(i-1)->in(VectorBoxNode::Box)->bottom_type()) != 0) {\n+          all_inputs_are_equiv_vboxes = false;\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (all_inputs_are_equiv_vboxes) {\n+      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(in(1));\n+      PhiNode* new_vbox_phi = new PhiNode(r, vbox->box_type());\n+      PhiNode* new_vect_phi = new PhiNode(r, vbox->vec_type());\n+      for (uint i = 1; i < req(); ++i) {\n+        VectorBoxNode* old_vbox = static_cast<VectorBoxNode*>(in(i));\n+        new_vbox_phi->set_req(i, old_vbox->in(VectorBoxNode::Box));\n+        new_vect_phi->set_req(i, old_vbox->in(VectorBoxNode::Value));\n+      }\n+      igvn->register_new_node_with_optimizer(new_vbox_phi, this);\n+      igvn->register_new_node_with_optimizer(new_vect_phi, this);\n+      progress = new VectorBoxNode(igvn->C, new_vbox_phi, new_vect_phi, vbox->box_type(), vbox->vec_type());\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"opto\/vector.hpp\"\n@@ -415,0 +416,1 @@\n+  remove_useless_late_inlines(&_vector_reboxing_late_inlines, useful);\n@@ -548,0 +550,1 @@\n+                  _vector_reboxing_late_inlines(comp_arena(), 2, 0, NULL),\n@@ -1965,0 +1968,2 @@\n+    print_method(PHASE_INCREMENTAL_INLINE_STEP, 3);\n+\n@@ -2099,0 +2104,10 @@\n+  assert(EnableVectorSupport || !has_vbox_nodes(), \"sanity\");\n+  if (EnableVectorSupport && has_vbox_nodes()) {\n+    TracePhase tp(\"\", &timers[_t_vector]);\n+    PhaseVector pv(igvn);\n+    pv.optimize_vector_boxes();\n+\n+    print_method(PHASE_ITER_GVN_AFTER_VECTOR, 2);\n+  }\n+  assert(!has_vbox_nodes(), \"sanity\");\n+\n@@ -2275,0 +2290,29 @@\n+void Compile::inline_vector_reboxing_calls() {\n+  if (C->_vector_reboxing_late_inlines.length() > 0) {\n+    PhaseGVN* gvn = C->initial_gvn();\n+\n+    _late_inlines_pos = C->_late_inlines.length();\n+    while (_vector_reboxing_late_inlines.length() > 0) {\n+      CallGenerator* cg = _vector_reboxing_late_inlines.pop();\n+      cg->do_late_inline();\n+      if (failing())  return;\n+      print_method(PHASE_INLINE_VECTOR_REBOX, cg->call_node());\n+    }\n+    _vector_reboxing_late_inlines.trunc_to(0);\n+  }\n+}\n+\n+bool Compile::has_vbox_nodes() {\n+  if (C->_vector_reboxing_late_inlines.length() > 0) {\n+    return true;\n+  }\n+  for (int macro_idx = C->macro_count() - 1; macro_idx >= 0; macro_idx--) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorUnbox || n->Opcode() == Op_VectorBox || n->Opcode() == Op_VectorBoxAllocate) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -2621,0 +2665,1 @@\n+    print_method(PHASE_AFTER_MATCHING, 3);\n@@ -2622,1 +2667,0 @@\n-\n@@ -2801,1 +2845,2 @@\n-    case Op_MaxI:  case Op_MinI:\n+    case Op_MaxI:  case Op_MaxL:  case Op_MaxF:  case Op_MaxD:\n+    case Op_MinI:  case Op_MinL:  case Op_MinF:  case Op_MinD:\n@@ -3351,0 +3396,2 @@\n+  case Op_LoadVectorGather:\n+  case Op_StoreVectorScatter:\n@@ -4571,1 +4618,1 @@\n-void Compile::print_method(CompilerPhaseType cpt, int level, int idx) {\n+void Compile::print_method(CompilerPhaseType cpt, const char *name, int level, int idx) {\n@@ -4576,1 +4623,0 @@\n-\n@@ -4579,7 +4625,1 @@\n-    char output[1024];\n-    if (idx != 0) {\n-      jio_snprintf(output, sizeof(output), \"%s:%d\", CompilerPhaseTypeHelper::to_string(cpt), idx);\n-    } else {\n-      jio_snprintf(output, sizeof(output), \"%s\", CompilerPhaseTypeHelper::to_string(cpt));\n-    }\n-    _printer->print_method(output, level);\n+    _printer->print_method(name, level);\n@@ -4591,0 +4631,24 @@\n+void Compile::print_method(CompilerPhaseType cpt, int level, int idx) {\n+  char output[1024];\n+#ifndef PRODUCT\n+  if (idx != 0) {\n+    jio_snprintf(output, sizeof(output), \"%s:%d\", CompilerPhaseTypeHelper::to_string(cpt), idx);\n+  } else {\n+    jio_snprintf(output, sizeof(output), \"%s\", CompilerPhaseTypeHelper::to_string(cpt));\n+  }\n+#endif\n+  print_method(cpt, output, level, idx);\n+}\n+\n+void Compile::print_method(CompilerPhaseType cpt, Node* n, int level) {\n+  ResourceMark rm;\n+  stringStream ss;\n+  ss.print_raw(CompilerPhaseTypeHelper::to_string(cpt));\n+  if (n != NULL) {\n+    ss.print(\": %d %s \", n->_idx, NodeClassNames[n->Opcode()]);\n+  } else {\n+    ss.print_raw(\": NULL\");\n+  }\n+  C->print_method(cpt, ss.as_string(), level);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":75,"deletions":11,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -385,0 +385,2 @@\n+  GrowableArray<CallGenerator*> _vector_reboxing_late_inlines; \/\/ same but for vector reboxing operations\n+\n@@ -647,0 +649,1 @@\n+  void print_method(CompilerPhaseType cpt, const char *name, int level = 1, int idx = 0);\n@@ -648,0 +651,1 @@\n+  void print_method(CompilerPhaseType cpt, Node* n, int level = 3);\n@@ -868,1 +872,2 @@\n-           should_delay_boxing_inlining(call_method, jvms);\n+           should_delay_boxing_inlining(call_method, jvms) ||\n+           should_delay_vector_inlining(call_method, jvms);\n@@ -872,0 +877,2 @@\n+  bool should_delay_vector_inlining(ciMethod* call_method, JVMState* jvms);\n+  bool should_delay_vector_reboxing_inlining(ciMethod* call_method, JVMState* jvms);\n@@ -943,0 +950,4 @@\n+  void              add_vector_reboxing_late_inline(CallGenerator* cg) {\n+    _vector_reboxing_late_inlines.push(cg);\n+  }\n+\n@@ -972,0 +983,3 @@\n+  void inline_vector_reboxing_calls();\n+  bool has_vbox_nodes();\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -137,0 +137,2 @@\n+      } else if (should_delay_vector_inlining(callee, jvms)) {\n+        return CallGenerator::for_late_inline(callee, cg);\n@@ -187,0 +189,2 @@\n+          } else if (should_delay_vector_reboxing_inlining(callee, jvms)) {\n+            return CallGenerator::for_vector_reboxing_late_inline(callee, cg);\n@@ -424,0 +428,8 @@\n+bool Compile::should_delay_vector_inlining(ciMethod* call_method, JVMState* jvms) {\n+  return EnableVectorSupport && call_method->is_vector_method();\n+}\n+\n+bool Compile::should_delay_vector_reboxing_inlining(ciMethod* call_method, JVMState* jvms) {\n+  return EnableVectorSupport && (call_method->intrinsic_id() == vmIntrinsics::_VectorRebox);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -689,0 +689,1 @@\n+        case Op_StoreVectorScatter:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -644,1 +644,2 @@\n-        const int MAX_STORE = BytesPerLong;\n+        const int MAX_STORE = MAX2(BytesPerLong, (int)MaxVectorSize);\n+        assert(mem->as_Store()->memory_size() <= MAX_STORE, \"\");\n@@ -1114,5 +1115,10 @@\n-      if (ReduceBulkZeroing || find_array_copy_clone(phase, ld_alloc, in(MemNode::Memory)) == NULL) {\n-        \/\/ If ReduceBulkZeroing is disabled, we need to check if the allocation does not belong to an\n-        \/\/ ArrayCopyNode clone. If it does, then we cannot assume zero since the initialization is done\n-        \/\/ by the ArrayCopyNode.\n-        return phase->zerocon(memory_type());\n+      if (memory_type() != T_VOID) {\n+        if (ReduceBulkZeroing || find_array_copy_clone(phase, ld_alloc, in(MemNode::Memory)) == NULL) {\n+          \/\/ If ReduceBulkZeroing is disabled, we need to check if the allocation does not belong to an\n+          \/\/ ArrayCopyNode clone. If it does, then we cannot assume zero since the initialization is done\n+          \/\/ by the ArrayCopyNode.\n+          return phase->zerocon(memory_type());\n+        }\n+      } else {\n+        \/\/ TODO: materialize all-zero vector constant\n+        assert(!isa_Load() || as_Load()->type()->isa_vect(), \"\");\n@@ -2564,0 +2570,2 @@\n+             st->Opcode() == Op_StoreVectorScatter ||\n+             Opcode() == Op_StoreVectorScatter ||\n@@ -3747,1 +3755,1 @@\n-  const int FAIL = 0, MAX_STORE = BytesPerLong;\n+  const int FAIL = 0, MAX_STORE = MAX2(BytesPerLong, (int)MaxVectorSize);\n@@ -3777,0 +3785,1 @@\n+      assert(st->as_Store()->memory_size() <= MAX_STORE, \"\");\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":16,"deletions":7,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -155,0 +155,1 @@\n+class LoadVectorGatherNode;\n@@ -156,0 +157,2 @@\n+class StoreVectorScatterNode;\n+class VectorMaskCmpNode;\n@@ -691,0 +694,1 @@\n+          DEFINE_CLASS_ID(LoadVectorGather, LoadVector, 0)\n@@ -693,0 +697,1 @@\n+          DEFINE_CLASS_ID(StoreVectorScatter, StoreVector, 0)\n@@ -717,0 +722,1 @@\n+      DEFINE_CLASS_ID(VectorMaskCmp, Vector, 0)\n@@ -887,0 +893,1 @@\n+  DEFINE_CLASS_QUERY(LoadVectorGather)\n@@ -888,0 +895,2 @@\n+  DEFINE_CLASS_QUERY(StoreVectorScatter)\n+  DEFINE_CLASS_QUERY(VectorMaskCmp)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -895,0 +895,4 @@\n+    } else if ( t->base() == Type::VectorS || t->base() == Type::VectorD ||\n+                t->base() == Type::VectorX || t->base() == Type::VectorY ||\n+                t->base() == Type::VectorZ) {\n+      array->append(new_loc_value( C->regalloc(), regnum, Location::vector ));\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,466 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/phaseX.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/vector.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+void PhaseVector::optimize_vector_boxes() {\n+  Compile::TracePhase tp(\"vector_elimination\", &timers[_t_vector_elimination]);\n+\n+  \/\/ Signal GraphKit it's post-parse phase.\n+  assert(C->inlining_incrementally() == false, \"sanity\");\n+  C->set_inlining_incrementally(true);\n+\n+  C->for_igvn()->clear();\n+  C->initial_gvn()->replace_with(&_igvn);\n+\n+  expand_vunbox_nodes();\n+  scalarize_vbox_nodes();\n+\n+  C->inline_vector_reboxing_calls();\n+\n+  expand_vbox_nodes();\n+  eliminate_vbox_alloc_nodes();\n+\n+  C->set_inlining_incrementally(false);\n+\n+  do_cleanup();\n+}\n+\n+void PhaseVector::do_cleanup() {\n+  if (C->failing())  return;\n+  {\n+    Compile::TracePhase tp(\"vector_pru\", &timers[_t_vector_pru]);\n+    ResourceMark rm;\n+    PhaseRemoveUseless pru(C->initial_gvn(), C->for_igvn());\n+    if (C->failing())  return;\n+  }\n+  {\n+    Compile::TracePhase tp(\"incrementalInline_igvn\", &timers[_t_vector_igvn]);\n+    _igvn = PhaseIterGVN(C->initial_gvn());\n+    _igvn.optimize();\n+    if (C->failing())  return;\n+  }\n+  C->print_method(PHASE_ITER_GVN_BEFORE_EA, 3);\n+}\n+\n+void PhaseVector::scalarize_vbox_nodes() {\n+  if (C->failing())  return;\n+\n+  if (!EnableVectorReboxing) {\n+    return; \/\/ don't scalarize vector boxes\n+  }\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorBox) {\n+      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(n);\n+      scalarize_vbox_node(vbox);\n+      if (C->failing())  return;\n+      C->print_method(PHASE_SCALARIZE_VBOX, vbox, 3);\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+void PhaseVector::expand_vbox_nodes() {\n+  if (C->failing())  return;\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorBox) {\n+      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(n);\n+      expand_vbox_node(vbox);\n+      if (C->failing())  return;\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+void PhaseVector::expand_vunbox_nodes() {\n+  if (C->failing())  return;\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorUnbox) {\n+      VectorUnboxNode* vec_unbox = static_cast<VectorUnboxNode*>(n);\n+      expand_vunbox_node(vec_unbox);\n+      if (C->failing())  return;\n+      C->print_method(PHASE_EXPAND_VUNBOX, vec_unbox, 3);\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+void PhaseVector::eliminate_vbox_alloc_nodes() {\n+  if (C->failing())  return;\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorBoxAllocate) {\n+      VectorBoxAllocateNode* vbox_alloc = static_cast<VectorBoxAllocateNode*>(n);\n+      eliminate_vbox_alloc_node(vbox_alloc);\n+      if (C->failing())  return;\n+      C->print_method(PHASE_ELIMINATE_VBOX_ALLOC, vbox_alloc, 3);\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+static JVMState* clone_jvms(Compile* C, SafePointNode* sfpt) {\n+  JVMState* new_jvms = sfpt->jvms()->clone_shallow(C);\n+  uint size = sfpt->req();\n+  SafePointNode* map = new SafePointNode(size, new_jvms);\n+  for (uint i = 0; i < size; i++) {\n+    map->init_req(i, sfpt->in(i));\n+  }\n+  new_jvms->set_map(map);\n+  return new_jvms;\n+}\n+\n+void PhaseVector::scalarize_vbox_node(VectorBoxNode* vec_box) {\n+  Node* vec_value = vec_box->in(VectorBoxNode::Value);\n+  PhaseGVN& gvn = *C->initial_gvn();\n+\n+  \/\/ Process merged VBAs\n+\n+  if (EnableVectorAggressiveReboxing) {\n+    Unique_Node_List calls(C->comp_arena());\n+    for (DUIterator_Fast imax, i = vec_box->fast_outs(imax); i < imax; i++) {\n+      Node* use = vec_box->fast_out(i);\n+      if (use->is_CallJava()) {\n+        CallJavaNode* call = use->as_CallJava();\n+        if (call->has_non_debug_use(vec_box) && vec_box->in(VectorBoxNode::Box)->is_Phi()) {\n+          calls.push(call);\n+        }\n+      }\n+    }\n+\n+    while (calls.size() > 0) {\n+      CallJavaNode* call = calls.pop()->as_CallJava();\n+      \/\/ Attach new VBA to the call and use it instead of Phi (VBA ... VBA).\n+\n+      JVMState* jvms = clone_jvms(C, call);\n+      GraphKit kit(jvms);\n+      PhaseGVN& gvn = kit.gvn();\n+\n+      \/\/ Adjust JVMS from post-call to pre-call state: put args on stack\n+      uint nargs = call->method()->arg_size();\n+      kit.ensure_stack(kit.sp() + nargs);\n+      for (uint i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+        kit.push(call->in(i));\n+      }\n+      jvms = kit.sync_jvms();\n+\n+      Node* new_vbox = NULL;\n+      {\n+        PreserveReexecuteState prs(&kit);\n+\n+        kit.jvms()->set_should_reexecute(true);\n+\n+        const TypeInstPtr* vbox_type = vec_box->box_type();\n+        const TypeVect* vect_type = vec_box->vec_type();\n+        Node* vect = vec_box->in(VectorBoxNode::Value);\n+\n+        VectorBoxAllocateNode* alloc = new VectorBoxAllocateNode(C, vbox_type);\n+        kit.set_edges_for_java_call(alloc, \/*must_throw=*\/false, \/*separate_io_proj=*\/true);\n+        kit.make_slow_call_ex(alloc, C->env()->Throwable_klass(), \/*separate_io_proj=*\/true, \/*deoptimize=*\/true);\n+        kit.set_i_o(gvn.transform( new ProjNode(alloc, TypeFunc::I_O) ));\n+        kit.set_all_memory(gvn.transform( new ProjNode(alloc, TypeFunc::Memory) ));\n+        Node* ret = gvn.transform(new ProjNode(alloc, TypeFunc::Parms));\n+\n+        new_vbox = gvn.transform(new VectorBoxNode(C, ret, vect, vbox_type, vect_type));\n+\n+        kit.replace_in_map(vec_box, new_vbox);\n+      }\n+\n+      kit.dec_sp(nargs);\n+      jvms = kit.sync_jvms();\n+\n+      call->set_req(TypeFunc::Control , kit.control());\n+      call->set_req(TypeFunc::I_O     , kit.i_o());\n+      call->set_req(TypeFunc::Memory  , kit.reset_memory());\n+      call->set_req(TypeFunc::FramePtr, kit.frameptr());\n+      call->replace_edge(vec_box, new_vbox);\n+\n+      C->record_for_igvn(call);\n+    }\n+  }\n+\n+  \/\/ Process debug uses at safepoints\n+  Unique_Node_List safepoints(C->comp_arena());\n+\n+  for (DUIterator_Fast imax, i = vec_box->fast_outs(imax); i < imax; i++) {\n+    Node* use = vec_box->fast_out(i);\n+    if (use->is_SafePoint()) {\n+      SafePointNode* sfpt = use->as_SafePoint();\n+      if (!sfpt->is_Call() || !sfpt->as_Call()->has_non_debug_use(vec_box)) {\n+        safepoints.push(sfpt);\n+      }\n+    }\n+  }\n+\n+  while (safepoints.size() > 0) {\n+    SafePointNode* sfpt = safepoints.pop()->as_SafePoint();\n+\n+    uint first_ind = (sfpt->req() - sfpt->jvms()->scloff());\n+    Node* sobj = new SafePointScalarObjectNode(vec_box->box_type(),\n+#ifdef ASSERT\n+                                               NULL,\n+#endif \/\/ ASSERT\n+                                               first_ind, \/*n_fields=*\/1);\n+    sobj->init_req(0, C->root());\n+    sfpt->add_req(vec_value);\n+\n+    sobj = gvn.transform(sobj);\n+\n+    JVMState *jvms = sfpt->jvms();\n+\n+    jvms->set_endoff(sfpt->req());\n+    \/\/ Now make a pass over the debug information replacing any references\n+    \/\/ to the allocated object with \"sobj\"\n+    int start = jvms->debug_start();\n+    int end   = jvms->debug_end();\n+    sfpt->replace_edges_in_range(vec_box, sobj, start, end);\n+\n+    C->record_for_igvn(sfpt);\n+  }\n+}\n+\n+void PhaseVector::expand_vbox_node(VectorBoxNode* vec_box) {\n+  if (vec_box->outcnt() > 0) {\n+    Node* vbox = vec_box->in(VectorBoxNode::Box);\n+    Node* vect = vec_box->in(VectorBoxNode::Value);\n+    Node* result = expand_vbox_node_helper(vbox, vect, vec_box->box_type(), vec_box->vec_type());\n+    C->gvn_replace_by(vec_box, result);\n+    C->print_method(PHASE_EXPAND_VBOX, vec_box, 3);\n+  }\n+  C->remove_macro_node(vec_box);\n+}\n+\n+Node* PhaseVector::expand_vbox_node_helper(Node* vbox,\n+                                           Node* vect,\n+                                           const TypeInstPtr* box_type,\n+                                           const TypeVect* vect_type) {\n+  if (vbox->is_Phi() && vect->is_Phi()) {\n+    assert(vbox->as_Phi()->region() == vect->as_Phi()->region(), \"\");\n+    Node* new_phi = new PhiNode(vbox->as_Phi()->region(), box_type);\n+    for (uint i = 1; i < vbox->req(); i++) {\n+      Node* new_box = expand_vbox_node_helper(vbox->in(i), vect->in(i), box_type, vect_type);\n+      new_phi->set_req(i, new_box);\n+    }\n+    new_phi = C->initial_gvn()->transform(new_phi);\n+    return new_phi;\n+  } else if (vbox->is_Proj() && vbox->in(0)->Opcode() == Op_VectorBoxAllocate) {\n+    VectorBoxAllocateNode* vbox_alloc = static_cast<VectorBoxAllocateNode*>(vbox->in(0));\n+    return expand_vbox_alloc_node(vbox_alloc, vect, box_type, vect_type);\n+  } else {\n+    assert(!vbox->is_Phi(), \"\");\n+    \/\/ TODO: assert that expanded vbox is initialized with the same value (vect).\n+    return vbox; \/\/ already expanded\n+  }\n+}\n+\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+Node* PhaseVector::expand_vbox_alloc_node(VectorBoxAllocateNode* vbox_alloc,\n+                                          Node* value,\n+                                          const TypeInstPtr* box_type,\n+                                          const TypeVect* vect_type) {\n+  JVMState* jvms = clone_jvms(C, vbox_alloc);\n+  GraphKit kit(jvms);\n+  PhaseGVN& gvn = kit.gvn();\n+\n+  ciInstanceKlass* box_klass = box_type->klass()->as_instance_klass();\n+  BasicType bt = vect_type->element_basic_type();\n+  int num_elem = vect_type->length();\n+\n+  bool is_mask = is_vector_mask(box_klass);\n+  if (is_mask && bt != T_BOOLEAN) {\n+    value = gvn.transform(VectorStoreMaskNode::make(gvn, value, bt, num_elem));\n+    \/\/ Although type of mask depends on its definition, in terms of storage everything is stored in boolean array.\n+    bt = T_BOOLEAN;\n+    assert(value->as_Vector()->bottom_type()->is_vect()->element_basic_type() == bt,\n+           \"must be consistent with mask representation\");\n+  }\n+\n+  \/\/ Generate array allocation for the field which holds the values.\n+  const TypeKlassPtr* array_klass = TypeKlassPtr::make(ciTypeArrayKlass::make(bt));\n+  Node* arr = kit.new_array(kit.makecon(array_klass), kit.intcon(num_elem), 1);\n+\n+  \/\/ Store the vector value into the array.\n+  \/\/ (The store should be captured by InitializeNode and turned into initialized store later.)\n+  Node* arr_adr = kit.array_element_address(arr, kit.intcon(0), bt);\n+  const TypePtr* arr_adr_type = arr_adr->bottom_type()->is_ptr();\n+  Node* arr_mem = kit.memory(arr_adr);\n+  Node* vstore = gvn.transform(StoreVectorNode::make(0,\n+                                                     kit.control(),\n+                                                     arr_mem,\n+                                                     arr_adr,\n+                                                     arr_adr_type,\n+                                                     value,\n+                                                     num_elem));\n+  kit.set_memory(vstore, arr_adr_type);\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), vect_type->length_in_bytes()));\n+\n+  \/\/ Generate the allocate for the Vector object.\n+  const TypeKlassPtr* klass_type = box_type->as_klass_type();\n+  Node* klass_node = kit.makecon(klass_type);\n+  Node* vec_obj = kit.new_instance(klass_node);\n+\n+  \/\/ Store the allocated array into object.\n+  ciField* field = ciEnv::current()->vector_VectorPayload_klass()->get_field_by_name(ciSymbol::payload_name(),\n+                                                                                     ciSymbol::object_signature(),\n+                                                                                     false);\n+  assert(field != NULL, \"\");\n+  Node* vec_field = kit.basic_plus_adr(vec_obj, field->offset_in_bytes());\n+  const TypePtr* vec_adr_type = vec_field->bottom_type()->is_ptr();\n+\n+  \/\/ The store should be captured by InitializeNode and turned into initialized store later.\n+  Node* field_store = gvn.transform(kit.access_store_at(vec_obj,\n+                                                            vec_field,\n+                                                            vec_adr_type,\n+                                                            arr,\n+                                                            TypeOopPtr::make_from_klass(field->type()->as_klass()),\n+                                                            T_OBJECT,\n+                                                            IN_HEAP));\n+  kit.set_memory(field_store, vec_adr_type);\n+\n+  kit.replace_call(vbox_alloc, vec_obj, true);\n+  C->remove_macro_node(vbox_alloc);\n+\n+  return vec_obj;\n+}\n+\n+void PhaseVector::expand_vunbox_node(VectorUnboxNode* vec_unbox) {\n+  if (vec_unbox->outcnt() > 0) {\n+    GraphKit kit;\n+    PhaseGVN& gvn = kit.gvn();\n+\n+    Node* obj = vec_unbox->obj();\n+    const TypeInstPtr* tinst = gvn.type(obj)->isa_instptr();\n+    ciInstanceKlass* from_kls = tinst->klass()->as_instance_klass();\n+    BasicType bt = vec_unbox->vect_type()->element_basic_type();\n+    BasicType masktype = bt;\n+    BasicType elem_bt;\n+\n+    if (is_vector_mask(from_kls)) {\n+      bt = T_BOOLEAN;\n+    } else if (is_vector_shuffle(from_kls)) {\n+      if (vec_unbox->is_shuffle_to_vector() == true) {\n+        elem_bt = bt;\n+      }\n+      bt = T_BYTE;\n+    }\n+\n+    ciField* field = ciEnv::current()->vector_VectorPayload_klass()->get_field_by_name(ciSymbol::payload_name(),\n+                                                                                       ciSymbol::object_signature(),\n+                                                                                       false);\n+    assert(field != NULL, \"\");\n+    int offset = field->offset_in_bytes();\n+    Node* vec_adr = kit.basic_plus_adr(obj, offset);\n+\n+    Node* mem = vec_unbox->mem();\n+    Node* ctrl = vec_unbox->in(0);\n+    Node* vec_field_ld = LoadNode::make(gvn,\n+                                        ctrl,\n+                                        mem,\n+                                        vec_adr,\n+                                        vec_adr->bottom_type()->is_ptr(),\n+                                        TypeOopPtr::make_from_klass(field->type()->as_klass()),\n+                                        T_OBJECT,\n+                                        MemNode::unordered);\n+    vec_field_ld = gvn.transform(vec_field_ld);\n+\n+    \/\/ For proper aliasing, attach concrete payload type.\n+    ciKlass* payload_klass = ciTypeArrayKlass::make(bt);\n+    const Type* payload_type = TypeAryPtr::make_from_klass(payload_klass)->cast_to_ptr_type(TypePtr::NotNull);\n+    vec_field_ld = gvn.transform(new CastPPNode(vec_field_ld, payload_type));\n+\n+    Node* adr = kit.array_element_address(vec_field_ld, gvn.intcon(0), bt);\n+    const TypePtr* adr_type = adr->bottom_type()->is_ptr();\n+    const TypeVect* vt = vec_unbox->bottom_type()->is_vect();\n+    int num_elem = vt->length();\n+    Node* vec_val_load = LoadVectorNode::make(0,\n+                                              ctrl,\n+                                              mem,\n+                                              adr,\n+                                              adr_type,\n+                                              num_elem,\n+                                              bt);\n+    vec_val_load = gvn.transform(vec_val_load);\n+\n+    C->set_max_vector_size(MAX2(C->max_vector_size(), vt->length_in_bytes()));\n+\n+    if (is_vector_mask(from_kls) && masktype != T_BOOLEAN) {\n+      assert(vec_unbox->bottom_type()->is_vect()->element_basic_type() == masktype, \"expect mask type consistency\");\n+      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+    } else if (is_vector_shuffle(from_kls)) {\n+      if (vec_unbox->is_shuffle_to_vector() == false) {\n+        assert(vec_unbox->bottom_type()->is_vect()->element_basic_type() == masktype, \"expect shuffle type consistency\");\n+        vec_val_load = gvn.transform(new VectorLoadShuffleNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+      } else if (elem_bt != T_BYTE) {\n+        vec_val_load = gvn.transform(VectorCastNode::make(Op_VectorCastB2X, vec_val_load, elem_bt, num_elem));\n+      }\n+    }\n+\n+    gvn.hash_delete(vec_unbox);\n+    vec_unbox->disconnect_inputs(C);\n+    C->gvn_replace_by(vec_unbox, vec_val_load);\n+  }\n+  C->remove_macro_node(vec_unbox);\n+}\n+\n+void PhaseVector::eliminate_vbox_alloc_node(VectorBoxAllocateNode* vbox_alloc) {\n+  JVMState* jvms = clone_jvms(C, vbox_alloc);\n+  GraphKit kit(jvms);\n+  \/\/ Remove VBA, but leave a safepoint behind.\n+  \/\/ Otherwise, it may end up with a loop without any safepoint polls.\n+  kit.replace_call(vbox_alloc, kit.map(), true);\n+  C->remove_macro_node(vbox_alloc);\n+}\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":466,"deletions":0,"binary":false,"changes":466,"status":"added"},{"patch":"@@ -123,0 +123,36 @@\n+  case Op_MinI:\n+    switch (bt) {\n+    case T_BOOLEAN:\n+    case T_CHAR:   return 0;\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:    return Op_MinV;\n+    default:       ShouldNotReachHere(); return 0;\n+    }\n+  case Op_MinL:\n+    assert(bt == T_LONG, \"must be\");\n+    return Op_MinV;\n+  case Op_MinF:\n+    assert(bt == T_FLOAT, \"must be\");\n+    return Op_MinV;\n+  case Op_MinD:\n+    assert(bt == T_DOUBLE, \"must be\");\n+    return Op_MinV;\n+  case Op_MaxI:\n+    switch (bt) {\n+    case T_BOOLEAN:\n+    case T_CHAR:   return 0;\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:    return Op_MaxV;\n+    default:       ShouldNotReachHere(); return 0;\n+    }\n+  case Op_MaxL:\n+    assert(bt == T_LONG, \"must be\");\n+    return Op_MaxV;\n+  case Op_MaxF:\n+    assert(bt == T_FLOAT, \"must be\");\n+    return Op_MaxV;\n+  case Op_MaxD:\n+    assert(bt == T_DOUBLE, \"must be\");\n+    return Op_MaxV;\n@@ -129,0 +165,3 @@\n+  case Op_NegI:\n+    assert(bt == T_INT, \"must be\");\n+    return Op_NegVI;\n@@ -181,0 +220,6 @@\n+  case Op_URShiftB:\n+    assert(bt == T_BYTE, \"must be\");\n+    return Op_URShiftVB;\n+  case Op_URShiftS:\n+    assert(bt == T_SHORT, \"must be\");\n+    return Op_URShiftVS;\n@@ -206,12 +251,0 @@\n-  case Op_MinF:\n-    assert(bt == T_FLOAT, \"must be\");\n-    return Op_MinV;\n-  case Op_MinD:\n-    assert(bt == T_DOUBLE, \"must be\");\n-    return Op_MinV;\n-  case Op_MaxF:\n-    assert(bt == T_FLOAT, \"must be\");\n-    return Op_MaxV;\n-  case Op_MaxD:\n-    assert(bt == T_DOUBLE, \"must be\");\n-    return Op_MaxV;\n@@ -244,0 +277,22 @@\n+int VectorNode::replicate_opcode(BasicType bt) {\n+  switch(bt) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      return Op_ReplicateB;\n+    case T_SHORT:\n+    case T_CHAR:\n+      return Op_ReplicateS;\n+    case T_INT:\n+      return Op_ReplicateI;\n+    case T_LONG:\n+      return Op_ReplicateL;\n+    case T_FLOAT:\n+      return Op_ReplicateF;\n+    case T_DOUBLE:\n+      return Op_ReplicateD;\n+    default:\n+      assert(false, \"wrong type: %s\", type2name(bt));\n+      return 0;\n+  }\n+}\n+\n@@ -334,0 +389,10 @@\n+bool VectorNode::is_vshift_cnt(Node* n) {\n+  switch (n->Opcode()) {\n+  case Op_LShiftCntV:\n+  case Op_RShiftCntV:\n+    return true;\n+  default:\n+    return false;\n+  }\n+}\n+\n@@ -400,4 +465,2 @@\n-\/\/ Return the vector version of a scalar operation node.\n-VectorNode* VectorNode::make(int opc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n-  const TypeVect* vt = TypeVect::make(bt, vlen);\n-  int vopc = VectorNode::opcode(opc, bt);\n+\/\/ Make a vector node for binary operation\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt) {\n@@ -405,1 +468,1 @@\n-  guarantee(vopc > 0, \"Vector for '%s' is not implemented\", NodeClassNames[opc]);\n+  guarantee(vopc > 0, \"vopc must be > 0\");\n@@ -431,0 +494,5 @@\n+  case Op_MinV: return new MinVNode(n1, n2, vt);\n+  case Op_MaxV: return new MaxVNode(n1, n2, vt);\n+\n+  case Op_AbsVF: return new AbsVFNode(n1, vt);\n+  case Op_AbsVD: return new AbsVDNode(n1, vt);\n@@ -435,2 +503,1 @@\n-  case Op_AbsVF: return new AbsVFNode(n1, vt);\n-  case Op_AbsVD: return new AbsVDNode(n1, vt);\n+  case Op_NegVI: return new NegVINode(n1, vt);\n@@ -467,3 +534,0 @@\n-  case Op_MinV: return new MinVNode(n1, n2, vt);\n-  case Op_MaxV: return new MaxVNode(n1, n2, vt);\n-\n@@ -479,1 +543,2 @@\n-VectorNode* VectorNode::make(int opc, Node* n1, Node* n2, Node* n3, uint vlen, BasicType bt) {\n+\/\/ Return the vector version of a scalar binary operation node.\n+VectorNode* VectorNode::make(int opc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n@@ -484,0 +549,7 @@\n+  return make(vopc, n1, n2, vt);\n+}\n+\n+\/\/ Make a vector node for ternary operation\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, Node* n3, const TypeVect* vt) {\n+  \/\/ This method should not be called for unimplemented vectors.\n+  guarantee(vopc > 0, \"vopc must be > 0\");\n@@ -493,0 +565,9 @@\n+\/\/ Return the vector version of a scalar ternary operation node.\n+VectorNode* VectorNode::make(int opc, Node* n1, Node* n2, Node* n3, uint vlen, BasicType bt) {\n+  const TypeVect* vt = TypeVect::make(bt, vlen);\n+  int vopc = VectorNode::opcode(opc, bt);\n+  \/\/ This method should not be called for unimplemented vectors.\n+  guarantee(vopc > 0, \"Vector for '%s' is not implemented\", NodeClassNames[opc]);\n+  return make(vopc, n1, n2, n3, vt);\n+}\n+\n@@ -519,2 +600,1 @@\n-VectorNode* VectorNode::shift_count(Node* shift, Node* cnt, uint vlen, BasicType bt) {\n-  assert(VectorNode::is_shift(shift), \"sanity\");\n+VectorNode* VectorNode::shift_count(int opc, Node* cnt, uint vlen, BasicType bt) {\n@@ -523,1 +603,1 @@\n-  switch (shift->Opcode()) {\n+  switch (opc) {\n@@ -529,0 +609,2 @@\n+  case Op_URShiftB:\n+  case Op_URShiftS:\n@@ -533,1 +615,1 @@\n-    fatal(\"Missed vector creation for '%s'\", NodeClassNames[shift->Opcode()]);\n+    fatal(\"Missed vector creation for '%s'\", NodeClassNames[opc]);\n@@ -680,0 +762,16 @@\n+int ExtractNode::opcode(BasicType bt) {\n+  switch (bt) {\n+    case T_BOOLEAN: return Op_ExtractUB;\n+    case T_BYTE:    return Op_ExtractB;\n+    case T_CHAR:    return Op_ExtractC;\n+    case T_SHORT:   return Op_ExtractS;\n+    case T_INT:     return Op_ExtractI;\n+    case T_LONG:    return Op_ExtractL;\n+    case T_FLOAT:   return Op_ExtractF;\n+    case T_DOUBLE:  return Op_ExtractD;\n+    default:\n+      assert(false, \"wrong type: %s\", type2name(bt));\n+      return 0;\n+  }\n+}\n+\n@@ -685,16 +783,8 @@\n-  case T_BOOLEAN:\n-    return new ExtractUBNode(v, pos);\n-  case T_BYTE:\n-    return new ExtractBNode(v, pos);\n-  case T_CHAR:\n-    return new ExtractCNode(v, pos);\n-  case T_SHORT:\n-    return new ExtractSNode(v, pos);\n-  case T_INT:\n-    return new ExtractINode(v, pos);\n-  case T_LONG:\n-    return new ExtractLNode(v, pos);\n-  case T_FLOAT:\n-    return new ExtractFNode(v, pos);\n-  case T_DOUBLE:\n-    return new ExtractDNode(v, pos);\n+  case T_BOOLEAN: return new ExtractUBNode(v, pos);\n+  case T_BYTE:    return new ExtractBNode(v, pos);\n+  case T_CHAR:    return new ExtractCNode(v, pos);\n+  case T_SHORT:   return new ExtractSNode(v, pos);\n+  case T_INT:     return new ExtractINode(v, pos);\n+  case T_LONG:    return new ExtractLNode(v, pos);\n+  case T_FLOAT:   return new ExtractFNode(v, pos);\n+  case T_DOUBLE:  return new ExtractDNode(v, pos);\n@@ -702,1 +792,1 @@\n-    fatal(\"Type '%s' is not supported for vectors\", type2name(bt));\n+    assert(false, \"wrong type: %s\", type2name(bt));\n@@ -711,2 +801,10 @@\n-      assert(bt == T_INT, \"must be\");\n-      vopc = Op_AddReductionVI;\n+      switch (bt) {\n+        case T_BOOLEAN:\n+        case T_CHAR: return 0;\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          vopc = Op_AddReductionVI;\n+          break;\n+        default: ShouldNotReachHere(); return 0;\n+      }\n@@ -727,2 +825,10 @@\n-      assert(bt == T_INT, \"must be\");\n-      vopc = Op_MulReductionVI;\n+      switch (bt) {\n+        case T_BOOLEAN:\n+        case T_CHAR: return 0;\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          vopc = Op_MulReductionVI;\n+          break;\n+        default: ShouldNotReachHere(); return 0;\n+      }\n@@ -742,0 +848,16 @@\n+    case Op_MinI:\n+      switch (bt) {\n+        case T_BOOLEAN:\n+        case T_CHAR: return 0;\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          vopc = Op_MinReductionV;\n+          break;\n+        default: ShouldNotReachHere(); return 0;\n+      }\n+      break;\n+    case Op_MinL:\n+      assert(bt == T_LONG, \"must be\");\n+      vopc = Op_MinReductionV;\n+      break;\n@@ -750,0 +872,16 @@\n+    case Op_MaxI:\n+      switch (bt) {\n+        case T_BOOLEAN:\n+        case T_CHAR: return 0;\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          vopc = Op_MaxReductionV;\n+          break;\n+        default: ShouldNotReachHere(); return 0;\n+      }\n+      break;\n+    case Op_MaxL:\n+      assert(bt == T_LONG, \"must be\");\n+      vopc = Op_MaxReductionV;\n+      break;\n@@ -759,2 +897,10 @@\n-      assert(bt == T_INT, \"must be\");\n-      vopc = Op_AndReductionV;\n+      switch (bt) {\n+      case T_BOOLEAN:\n+      case T_CHAR: return 0;\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT:\n+        vopc = Op_AndReductionV;\n+        break;\n+      default: ShouldNotReachHere(); return 0;\n+      }\n@@ -767,2 +913,10 @@\n-      assert(bt == T_INT, \"must be\");\n-      vopc = Op_OrReductionV;\n+      switch(bt) {\n+      case T_BOOLEAN:\n+      case T_CHAR: return 0;\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT:\n+        vopc = Op_OrReductionV;\n+        break;\n+      default: ShouldNotReachHere(); return 0;\n+      }\n@@ -775,2 +929,10 @@\n-      assert(bt == T_INT, \"must be\");\n-      vopc = Op_XorReductionV;\n+      switch(bt) {\n+      case T_BOOLEAN:\n+      case T_CHAR: return 0;\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT:\n+        vopc = Op_XorReductionV;\n+        break;\n+      default: ShouldNotReachHere(); return 0;\n+      }\n@@ -811,1 +973,1 @@\n-    fatal(\"Missed vector creation for '%s'\", NodeClassNames[vopc]);\n+    assert(false, \"unknown node: %s\", NodeClassNames[vopc]);\n@@ -816,0 +978,105 @@\n+VectorStoreMaskNode* VectorStoreMaskNode::make(PhaseGVN& gvn, Node* in, BasicType in_type, uint num_elem) {\n+  assert(in->bottom_type()->isa_vect(), \"sanity\");\n+  const TypeVect* vt = TypeVect::make(T_BOOLEAN, num_elem);\n+  int elem_size = type2aelembytes(in_type);\n+  return new VectorStoreMaskNode(in, gvn.intcon(elem_size), vt);\n+}\n+\n+VectorCastNode* VectorCastNode::make(int vopc, Node* n1, BasicType bt, uint vlen) {\n+  const TypeVect* vt = TypeVect::make(bt, vlen);\n+  switch (vopc) {\n+    case Op_VectorCastB2X: return new VectorCastB2XNode(n1, vt);\n+    case Op_VectorCastS2X: return new VectorCastS2XNode(n1, vt);\n+    case Op_VectorCastI2X: return new VectorCastI2XNode(n1, vt);\n+    case Op_VectorCastL2X: return new VectorCastL2XNode(n1, vt);\n+    case Op_VectorCastF2X: return new VectorCastF2XNode(n1, vt);\n+    case Op_VectorCastD2X: return new VectorCastD2XNode(n1, vt);\n+    default:\n+      assert(false, \"unknown node: %s\", NodeClassNames[vopc]);\n+      return NULL;\n+  }\n+}\n+\n+int VectorCastNode::opcode(BasicType bt) {\n+  switch (bt) {\n+    case T_BYTE:   return Op_VectorCastB2X;\n+    case T_SHORT:  return Op_VectorCastS2X;\n+    case T_INT:    return Op_VectorCastI2X;\n+    case T_LONG:   return Op_VectorCastL2X;\n+    case T_FLOAT:  return Op_VectorCastF2X;\n+    case T_DOUBLE: return Op_VectorCastD2X;\n+    default:\n+      assert(false, \"unknown type: %s\", type2name(bt));\n+      return 0;\n+  }\n+}\n+\n+Node* ReductionNode::make_reduction_input(PhaseGVN& gvn, int opc, BasicType bt) {\n+  int vopc = opcode(opc, bt);\n+  guarantee(vopc != opc, \"Vector reduction for '%s' is not implemented\", NodeClassNames[opc]);\n+\n+  switch (vopc) {\n+    case Op_AndReductionV:\n+      switch (bt) {\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          return gvn.makecon(TypeInt::MINUS_1);\n+        case T_LONG:\n+          return gvn.makecon(TypeLong::MINUS_1);\n+        default:\n+          fatal(\"Missed vector creation for '%s' as the basic type is not correct.\", NodeClassNames[vopc]);\n+          return NULL;\n+      }\n+      break;\n+    case Op_AddReductionVI: \/\/ fallthrough\n+    case Op_AddReductionVL: \/\/ fallthrough\n+    case Op_AddReductionVF: \/\/ fallthrough\n+    case Op_AddReductionVD:\n+    case Op_OrReductionV:\n+    case Op_XorReductionV:\n+      return gvn.zerocon(bt);\n+    case Op_MulReductionVI:\n+      return gvn.makecon(TypeInt::ONE);\n+    case Op_MulReductionVL:\n+      return gvn.makecon(TypeLong::ONE);\n+    case Op_MulReductionVF:\n+      return gvn.makecon(TypeF::ONE);\n+    case Op_MulReductionVD:\n+      return gvn.makecon(TypeD::ONE);\n+    case Op_MinReductionV:\n+      switch (bt) {\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          return gvn.makecon(TypeInt::MAX);\n+        case T_LONG:\n+          return gvn.makecon(TypeLong::MAX);\n+        case T_FLOAT:\n+          return gvn.makecon(TypeF::POS_INF);\n+        case T_DOUBLE:\n+          return gvn.makecon(TypeD::POS_INF);\n+          default: Unimplemented(); return NULL;\n+      }\n+      break;\n+    case Op_MaxReductionV:\n+      switch (bt) {\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:\n+          return gvn.makecon(TypeInt::MIN);\n+        case T_LONG:\n+          return gvn.makecon(TypeLong::MIN);\n+        case T_FLOAT:\n+          return gvn.makecon(TypeF::NEG_INF);\n+        case T_DOUBLE:\n+          return gvn.makecon(TypeD::NEG_INF);\n+          default: Unimplemented(); return NULL;\n+      }\n+      break;\n+    default:\n+      fatal(\"Missed vector creation for '%s'\", NodeClassNames[vopc]);\n+      return NULL;\n+  }\n+}\n+\n@@ -827,1 +1094,1 @@\n-                                      uint truth_table, const TypeVect* vt) {\n+                                       uint truth_table, const TypeVect* vt) {\n@@ -898,0 +1165,48 @@\n+#ifndef PRODUCT\n+void VectorMaskCmpNode::dump_spec(outputStream *st) const {\n+  st->print(\" %d #\", _predicate); _type->dump_on(st);\n+}\n+#endif \/\/ PRODUCT\n+\n+Node* VectorReinterpretNode::Identity(PhaseGVN *phase) {\n+  Node* n = in(1);\n+  if (n->Opcode() == Op_VectorReinterpret) {\n+    if (Type::cmp(bottom_type(), n->in(1)->bottom_type()) == 0) {\n+      return n->in(1);\n+    }\n+  }\n+  return this;\n+}\n+\n+Node* VectorInsertNode::make(Node* vec, Node* new_val, int position) {\n+  assert(position < (int)vec->bottom_type()->is_vect()->length(), \"pos in range\");\n+  ConINode* pos = ConINode::make(position);\n+  return new VectorInsertNode(vec, new_val, pos, vec->bottom_type()->is_vect());\n+}\n+\n+Node* VectorUnboxNode::Identity(PhaseGVN *phase) {\n+  Node* n = obj()->uncast();\n+  if (EnableVectorReboxing && n->Opcode() == Op_VectorBox) {\n+    if (Type::cmp(bottom_type(), n->in(VectorBoxNode::Value)->bottom_type()) == 0) {\n+      return n->in(VectorBoxNode::Value);\n+    }\n+  }\n+  return this;\n+}\n+\n+const TypeFunc* VectorBoxNode::vec_box_type(const TypeInstPtr* box_type) {\n+  const Type** fields = TypeTuple::fields(0);\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = box_type;\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+#ifndef PRODUCT\n+void VectorBoxAllocateNode::dump_spec(outputStream *st) const {\n+  CallStaticJavaNode::dump_spec(st);\n+}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":371,"deletions":56,"binary":false,"changes":427,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"opto\/callnode.hpp\"\n@@ -71,1 +72,1 @@\n-  static VectorNode* shift_count(Node* shift, Node* cnt, uint vlen, BasicType bt);\n+  static VectorNode* shift_count(int opc, Node* cnt, uint vlen, BasicType bt);\n@@ -73,0 +74,1 @@\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt);\n@@ -74,0 +76,1 @@\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, Node* n3, const TypeVect* vt);\n@@ -76,0 +79,1 @@\n+  static int replicate_opcode(BasicType bt);\n@@ -78,0 +82,1 @@\n+  static bool is_vshift_cnt(Node* n);\n@@ -163,0 +168,1 @@\n+  static Node* make_reduction_input(PhaseGVN& gvn, int opc, BasicType bt);\n@@ -165,1 +171,1 @@\n-    BasicType vbt = in(2)->bottom_type()->is_vect()->element_basic_type();\n+    BasicType vbt = in(1)->bottom_type()->basic_type();\n@@ -175,1 +181,1 @@\n-\/\/ Vector add int as a reduction\n+\/\/ Vector add byte, short and int as a reduction\n@@ -180,2 +186,0 @@\n-  virtual const Type* bottom_type() const { return TypeInt::INT; }\n-  virtual uint ideal_reg() const { return Op_RegI; }\n@@ -190,2 +194,0 @@\n-  virtual const Type* bottom_type() const { return TypeLong::LONG; }\n-  virtual uint ideal_reg() const { return Op_RegL; }\n@@ -200,2 +202,0 @@\n-  virtual const Type* bottom_type() const { return Type::FLOAT; }\n-  virtual uint ideal_reg() const { return Op_RegF; }\n@@ -210,2 +210,0 @@\n-  virtual const Type* bottom_type() const { return Type::DOUBLE; }\n-  virtual uint ideal_reg() const { return Op_RegD; }\n@@ -351,1 +349,1 @@\n-\/\/ Vector multiply int as a reduction\n+\/\/ Vector multiply byte, short and int as a reduction\n@@ -356,2 +354,0 @@\n-  virtual const Type* bottom_type() const { return TypeInt::INT; }\n-  virtual uint ideal_reg() const { return Op_RegI; }\n@@ -366,2 +362,0 @@\n-  virtual const Type* bottom_type() const { return TypeLong::LONG; }\n-  virtual uint ideal_reg() const { return Op_RegI; }\n@@ -376,2 +370,0 @@\n-  virtual const Type* bottom_type() const { return Type::FLOAT; }\n-  virtual uint ideal_reg() const { return Op_RegF; }\n@@ -386,2 +378,0 @@\n-  virtual const Type* bottom_type() const { return Type::DOUBLE; }\n-  virtual uint ideal_reg() const { return Op_RegD; }\n@@ -422,0 +412,16 @@\n+\/\/------------------------------MinVNode--------------------------------------\n+\/\/ Vector Min\n+class MinVNode : public VectorNode {\n+public:\n+  MinVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/------------------------------MaxVNode--------------------------------------\n+\/\/ Vector Max\n+class MaxVNode : public VectorNode {\n+ public:\n+  MaxVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -425,1 +431,1 @@\n-public:\n+ public:\n@@ -454,0 +460,8 @@\n+\/\/------------------------------NegVINode--------------------------------------\n+\/\/ Vector Neg int\n+class NegVINode : public VectorNode {\n+ public:\n+  NegVINode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -621,3 +635,3 @@\n-\/\/------------------------------OrVNode---------------------------------------\n-\/\/ Vector or integer\n-class OrVNode : public VectorNode {\n+\/\/------------------------------AndReductionVNode--------------------------------------\n+\/\/ Vector and byte, short, int, long as a reduction\n+class AndReductionVNode : public ReductionNode {\n@@ -625,1 +639,1 @@\n-  OrVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1,in2,vt) {}\n+  AndReductionVNode(Node *ctrl, Node* in1, Node* in2) : ReductionNode(ctrl, in1, in2) {}\n@@ -629,3 +643,3 @@\n-\/\/------------------------------XorVNode---------------------------------------\n-\/\/ Vector xor integer\n-class XorVNode : public VectorNode {\n+\/\/------------------------------OrVNode---------------------------------------\n+\/\/ Vector or byte, short, int, long as a reduction\n+class OrVNode : public VectorNode {\n@@ -633,9 +647,1 @@\n-  XorVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1,in2,vt) {}\n-  virtual int Opcode() const;\n-};\n-\n-\/\/------------------------------AndReductionVNode--------------------------------------\n-\/\/ Vector and int, long as a reduction\n-class AndReductionVNode : public ReductionNode {\n-public:\n-  AndReductionVNode(Node *ctrl, Node* in1, Node* in2) : ReductionNode(ctrl, in1, in2) {}\n+  OrVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1,in2,vt) {}\n@@ -646,1 +652,1 @@\n-\/\/ Vector or int, long as a reduction\n+\/\/ Vector xor byte, short, int, long as a reduction\n@@ -648,1 +654,1 @@\n-public:\n+ public:\n@@ -654,1 +660,1 @@\n-\/\/ Vector xor int, long as a reduction\n+\/\/ Vector and int, long as a reduction\n@@ -656,1 +662,1 @@\n-public:\n+ public:\n@@ -661,13 +667,5 @@\n-\/\/------------------------------MinVNode--------------------------------------\n-\/\/ Vector min\n-class MinVNode : public VectorNode {\n-public:\n-  MinVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n-  virtual int Opcode() const;\n-};\n-\n-\/\/------------------------------MaxVNode--------------------------------------\n-\/\/ Vector max\n-class MaxVNode : public VectorNode {\n-public:\n-  MaxVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+\/\/------------------------------XorVNode---------------------------------------\n+\/\/ Vector xor integer\n+class XorVNode : public VectorNode {\n+ public:\n+  XorVNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1,in2,vt) {}\n@@ -678,1 +676,1 @@\n-\/\/ Vector min as a reduction\n+\/\/ Vector min byte, short, int, long, float, double as a reduction\n@@ -686,1 +684,1 @@\n-\/\/ Vector max as a reduction\n+\/\/ Vector min byte, short, int, long, float, double as a reduction\n@@ -723,0 +721,16 @@\n+\/\/------------------------------LoadVectorGatherNode------------------------------\n+\/\/ Load Vector from memory via index map\n+class LoadVectorGatherNode : public LoadVectorNode {\n+ public:\n+  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices)\n+    : LoadVectorNode(c, mem, adr, at, vt) {\n+    init_class_id(Class_LoadVectorGather);\n+    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    add_req(indices);\n+    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that last input is in MemNode::ValueIn\");\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address || idx == MemNode::ValueIn; }\n+};\n+\n@@ -729,1 +743,0 @@\n-    assert(val->is_Vector() || val->is_LoadVector(), \"sanity\");\n@@ -750,0 +763,17 @@\n+\/\/------------------------------StoreVectorScatterNode------------------------------\n+\/\/ Store Vector into memory via index map\n+\n+ class StoreVectorScatterNode : public StoreVectorNode {\n+  public:\n+   StoreVectorScatterNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* indices)\n+     : StoreVectorNode(c, mem, adr, at, val) {\n+     init_class_id(Class_StoreVectorScatter);\n+     assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+     add_req(indices);\n+     assert(req() == MemNode::ValueIn + 2, \"match_edge expects that last input is in MemNode::ValueIn+1\");\n+   }\n+   virtual int Opcode() const;\n+   virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                     idx == MemNode::ValueIn ||\n+                                                     idx == MemNode::ValueIn + 1; }\n+};\n@@ -891,0 +921,6 @@\n+class VectorLoadConstNode : public VectorNode {\n+ public:\n+  VectorLoadConstNode(Node* in1, const TypeVect* vt) : VectorNode(in1, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -904,0 +940,1 @@\n+  static int opcode(BasicType bt);\n@@ -932,1 +969,1 @@\n-  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual const Type *bottom_type() const { return TypeInt::CHAR; }\n@@ -942,1 +979,1 @@\n-  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual const Type *bottom_type() const { return TypeInt::SHORT; }\n@@ -1010,0 +1047,280 @@\n+class VectorMaskCmpNode : public VectorNode {\n+ private:\n+  BoolTest::mask _predicate;\n+\n+ protected:\n+  uint size_of() const { return sizeof(*this); }\n+\n+ public:\n+  VectorMaskCmpNode(BoolTest::mask predicate, Node* in1, Node* in2, ConINode* predicate_node, const TypeVect* vt) :\n+      VectorNode(in1, in2, predicate_node, vt),\n+      _predicate(predicate) {\n+    assert(in1->bottom_type()->is_vect()->element_basic_type() == in2->bottom_type()->is_vect()->element_basic_type(),\n+           \"VectorMaskCmp inputs must have same type for elements\");\n+    assert(in1->bottom_type()->is_vect()->length() == in2->bottom_type()->is_vect()->length(),\n+           \"VectorMaskCmp inputs must have same number of elements\");\n+    init_class_id(Class_VectorMaskCmp);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual uint hash() const { return VectorNode::hash() + _predicate; }\n+  virtual bool cmp( const Node &n ) const {\n+    return VectorNode::cmp(n) && _predicate == ((VectorMaskCmpNode&)n)._predicate;\n+  }\n+  BoolTest::mask get_predicate() { return _predicate; }\n+#ifndef PRODUCT\n+  virtual void dump_spec(outputStream *st) const;\n+#endif \/\/ !PRODUCT\n+};\n+\n+\/\/ Used to wrap other vector nodes in order to add masking functionality.\n+class VectorMaskWrapperNode : public VectorNode {\n+ public:\n+  VectorMaskWrapperNode(Node* vector, Node* mask)\n+    : VectorNode(vector, mask, vector->bottom_type()->is_vect()) {\n+    assert(mask->is_VectorMaskCmp(), \"VectorMaskWrapper requires that second argument be a mask\");\n+  }\n+\n+  virtual int Opcode() const;\n+  Node* vector_val() const { return in(1); }\n+  Node* vector_mask() const { return in(2); }\n+};\n+\n+class VectorTestNode : public Node {\n+ private:\n+  BoolTest::mask _predicate;\n+\n+ protected:\n+  uint size_of() const { return sizeof(*this); }\n+\n+ public:\n+  VectorTestNode( Node *in1, Node *in2, BoolTest::mask predicate) : Node(NULL, in1, in2), _predicate(predicate) {\n+    assert(in1->is_Vector() || in1->is_LoadVector(), \"must be vector\");\n+    assert(in2->is_Vector() || in2->is_LoadVector(), \"must be vector\");\n+    assert(in1->bottom_type()->is_vect()->element_basic_type() == in2->bottom_type()->is_vect()->element_basic_type(),\n+           \"same type elements are needed\");\n+    assert(in1->bottom_type()->is_vect()->length() == in2->bottom_type()->is_vect()->length(),\n+           \"same number of elements is needed\");\n+  }\n+  virtual int Opcode() const;\n+  virtual uint hash() const { return Node::hash() + _predicate; }\n+  virtual bool cmp( const Node &n ) const {\n+    return Node::cmp(n) && _predicate == ((VectorTestNode&)n)._predicate;\n+  }\n+  virtual const Type *bottom_type() const { return TypeInt::BOOL; }\n+  virtual uint ideal_reg() const { return Op_RegI; }  \/\/ TODO Should be RegFlags but due to missing comparison flags for BoolTest\n+                                                      \/\/ in middle-end, we make it boolean result directly.\n+  BoolTest::mask get_predicate() const { return _predicate; }\n+};\n+\n+class VectorBlendNode : public VectorNode {\n+ public:\n+  VectorBlendNode(Node* vec1, Node* vec2, Node* mask)\n+    : VectorNode(vec1, vec2, mask, vec1->bottom_type()->is_vect()) {\n+    \/\/ assert(mask->is_VectorMask(), \"VectorBlendNode requires that third argument be a mask\");\n+  }\n+\n+  virtual int Opcode() const;\n+  Node* vec1() const { return in(1); }\n+  Node* vec2() const { return in(2); }\n+  Node* vec_mask() const { return in(3); }\n+};\n+\n+class VectorRearrangeNode : public VectorNode {\n+ public:\n+  VectorRearrangeNode(Node* vec1, Node* shuffle)\n+    : VectorNode(vec1, shuffle, vec1->bottom_type()->is_vect()) {\n+    \/\/ assert(mask->is_VectorMask(), \"VectorBlendNode requires that third argument be a mask\");\n+  }\n+\n+  virtual int Opcode() const;\n+  Node* vec1() const { return in(1); }\n+  Node* vec_shuffle() const { return in(2); }\n+};\n+\n+\n+class VectorLoadMaskNode : public VectorNode {\n+ public:\n+  VectorLoadMaskNode(Node* in, const TypeVect* vt)\n+    : VectorNode(in, vt) {\n+    assert(in->is_LoadVector(), \"expected load vector\");\n+    assert(in->as_LoadVector()->vect_type()->element_basic_type() == T_BOOLEAN, \"must be boolean\");\n+  }\n+\n+  virtual int Opcode() const;\n+};\n+\n+class VectorLoadShuffleNode : public VectorNode {\n+ public:\n+  VectorLoadShuffleNode(Node* in, const TypeVect* vt)\n+    : VectorNode(in, vt) {\n+    assert(in->is_LoadVector(), \"expected load vector\");\n+    assert(in->as_LoadVector()->vect_type()->element_basic_type() == T_BYTE, \"must be BYTE\");\n+  }\n+\n+  int GetOutShuffleSize() const { return type2aelembytes(vect_type()->element_basic_type()); }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorStoreMaskNode : public VectorNode {\n+ protected:\n+  VectorStoreMaskNode(Node* in1, ConINode* in2, const TypeVect* vt)\n+    : VectorNode(in1, in2, vt) { }\n+\n+ public:\n+  virtual int Opcode() const;\n+\n+  static VectorStoreMaskNode* make(PhaseGVN& gvn, Node* in, BasicType in_type, uint num_elem);\n+};\n+\n+\/\/ This is intended for use as a simple reinterpret node that has no cast.\n+class VectorReinterpretNode : public VectorNode {\n+ private:\n+  const TypeVect* _src_vt;\n+ protected:\n+  uint size_of() const { return sizeof(*this); }\n+ public:\n+  VectorReinterpretNode(Node* in, const TypeVect* src_vt, const TypeVect* dst_vt)\n+      : VectorNode(in, dst_vt), _src_vt(src_vt) { }\n+\n+  virtual uint hash() const { return VectorNode::hash() + _src_vt->hash(); }\n+  virtual bool cmp( const Node &n ) const {\n+    return VectorNode::cmp(n) && !Type::cmp(_src_vt,((VectorReinterpretNode&)n)._src_vt);\n+  }\n+  virtual Node *Identity(PhaseGVN *phase);\n+\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastNode : public VectorNode {\n+ public:\n+  VectorCastNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const;\n+\n+  static VectorCastNode* make(int vopc, Node* n1, BasicType bt, uint vlen);\n+  static int  opcode(BasicType bt);\n+  static bool implemented(BasicType bt, uint vlen);\n+};\n+\n+class VectorCastB2XNode : public VectorCastNode {\n+ public:\n+  VectorCastB2XNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_BYTE, \"must be byte\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastS2XNode : public VectorCastNode {\n+ public:\n+  VectorCastS2XNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_SHORT, \"must be short\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastI2XNode : public VectorCastNode {\n+ public:\n+  VectorCastI2XNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_INT, \"must be int\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastL2XNode : public VectorCastNode {\n+ public:\n+  VectorCastL2XNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_LONG, \"must be long\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastF2XNode : public VectorCastNode {\n+ public:\n+  VectorCastF2XNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_FLOAT, \"must be float\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorCastD2XNode : public VectorCastNode {\n+ public:\n+  VectorCastD2XNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE, \"must be double\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n+class VectorInsertNode : public VectorNode {\n+ public:\n+  VectorInsertNode(Node* vsrc, Node* new_val, ConINode* pos, const TypeVect* vt) : VectorNode(vsrc, new_val, (Node*)pos, vt) {\n+   assert(pos->get_int() >= 0, \"positive constants\");\n+   assert(pos->get_int() < (int)vt->length(), \"index must be less than vector length\");\n+   assert(Type::cmp(vt, vsrc->bottom_type()) == 0, \"input and output must be same type\");\n+  }\n+  virtual int Opcode() const;\n+  uint pos() const { return in(3)->get_int(); }\n+\n+  static Node* make(Node* vec, Node* new_val, int position);\n+};\n+\n+class VectorBoxNode : public Node {\n+ private:\n+  const TypeInstPtr* const _box_type;\n+  const TypeVect*    const _vec_type;\n+ public:\n+  enum {\n+     Box   = 1,\n+     Value = 2\n+  };\n+  VectorBoxNode(Compile* C, Node* box, Node* val,\n+                const TypeInstPtr* box_type, const TypeVect* vt)\n+    : Node(NULL, box, val), _box_type(box_type), _vec_type(vt) {\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+\n+  const  TypeInstPtr* box_type() const { assert(_box_type != NULL, \"\"); return _box_type; };\n+  const  TypeVect*    vec_type() const { assert(_vec_type != NULL, \"\"); return _vec_type; };\n+\n+  virtual int Opcode() const;\n+  virtual const Type* bottom_type() const { return _box_type; }\n+  virtual       uint  ideal_reg() const { return box_type()->ideal_reg(); }\n+  virtual       uint  size_of() const { return sizeof(*this); }\n+\n+  static const TypeFunc* vec_box_type(const TypeInstPtr* box_type);\n+};\n+\n+class VectorBoxAllocateNode : public CallStaticJavaNode {\n+ public:\n+  VectorBoxAllocateNode(Compile* C, const TypeInstPtr* vbox_type)\n+    : CallStaticJavaNode(C, VectorBoxNode::vec_box_type(vbox_type), NULL, NULL, -1) {\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+\n+  virtual int Opcode() const;\n+#ifndef PRODUCT\n+  virtual void dump_spec(outputStream *st) const;\n+#endif \/\/ !PRODUCT\n+};\n+\n+class VectorUnboxNode : public VectorNode {\n+ private:\n+  bool _shuffle_to_vector;\n+ protected:\n+  uint size_of() const { return sizeof(*this); }\n+ public:\n+  VectorUnboxNode(Compile* C, const TypeVect* vec_type, Node* obj, Node* mem, bool shuffle_to_vector)\n+    : VectorNode(mem, obj, vec_type) {\n+    _shuffle_to_vector = shuffle_to_vector;\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+\n+  virtual int Opcode() const;\n+  Node* obj() const { return in(2); }\n+  Node* mem() const { return in(1); }\n+  virtual Node *Identity(PhaseGVN *phase);\n+  bool is_shuffle_to_vector() { return _shuffle_to_vector; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":376,"deletions":59,"binary":false,"changes":435,"status":"modified"},{"patch":"@@ -4198,1 +4198,17 @@\n-#endif\n+\n+  if (!EnableVectorSupport) {\n+    if (!FLAG_IS_DEFAULT(EnableVectorReboxing) && EnableVectorReboxing) {\n+      warning(\"Disabling EnableVectorReboxing since EnableVectorSupport is turned off.\");\n+    }\n+    FLAG_SET_DEFAULT(EnableVectorReboxing, false);\n+\n+    if (!FLAG_IS_DEFAULT(EnableVectorAggressiveReboxing) && EnableVectorAggressiveReboxing) {\n+      if (!EnableVectorReboxing) {\n+        warning(\"Disabling EnableVectorAggressiveReboxing since EnableVectorReboxing is turned off.\");\n+      } else {\n+        warning(\"Disabling EnableVectorAggressiveReboxing since EnableVectorSupport is turned off.\");\n+      }\n+    }\n+    FLAG_SET_DEFAULT(EnableVectorAggressiveReboxing, false);\n+  }\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"prims\/vectorSupport.hpp\"\n@@ -1017,0 +1018,7 @@\n+#ifdef COMPILER2\n+        if (EnableVectorSupport && VectorSupport::is_vector(ik)) {\n+          obj = VectorSupport::allocate_vector(ik, fr, reg_map, sv, THREAD);\n+        } else {\n+          obj = ik->allocate_instance(THREAD);\n+        }\n+#else\n@@ -1018,0 +1026,1 @@\n+#endif \/\/ COMPILER2\n@@ -1354,0 +1363,5 @@\n+#ifdef COMPILER2\n+    if (EnableVectorSupport && VectorSupport::is_vector(k)) {\n+      continue; \/\/ skip field reassignment for vectors\n+    }\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -153,1 +153,1 @@\n-    case Location::invalid:\n+    case Location::invalid: {\n@@ -155,0 +155,4 @@\n+    }\n+    case Location::vector: {\n+      ShouldNotReachHere(); \/\/ should be handled by Deoptimization::realloc_objects()\n+    }\n@@ -225,1 +229,1 @@\n-     break;\n+      break;\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1505,0 +1505,2 @@\n+  declare_c2_type(MaxLNode, MaxNode)                                      \\\n+  declare_c2_type(MinLNode, MaxNode)                                      \\\n@@ -1739,0 +1741,2 @@\n+  declare_c2_type(NegINode, NegNode)                                      \\\n+  declare_c2_type(NegLNode, NegNode)                                      \\\n@@ -1748,4 +1752,6 @@\n-  declare_c2_type(AbsVBNode, VectorNode)                                   \\\n-  declare_c2_type(AbsVSNode, VectorNode)                                   \\\n-  declare_c2_type(AbsVINode, VectorNode)                                   \\\n-  declare_c2_type(AbsVLNode, VectorNode)                                   \\\n+  declare_c2_type(AbsVFNode, VectorNode)                                  \\\n+  declare_c2_type(AbsVDNode, VectorNode)                                  \\\n+  declare_c2_type(AbsVBNode, VectorNode)                                  \\\n+  declare_c2_type(AbsVSNode, VectorNode)                                  \\\n+  declare_c2_type(AbsVINode, VectorNode)                                  \\\n+  declare_c2_type(AbsVLNode, VectorNode)                                  \\\n@@ -1777,0 +1783,1 @@\n+  declare_c2_type(NegVINode, VectorNode)                                  \\\n@@ -1799,0 +1806,2 @@\n+  declare_c2_type(MinReductionVNode, ReductionNode)                       \\\n+  declare_c2_type(MaxReductionVNode, ReductionNode)                       \\\n@@ -1807,2 +1816,0 @@\n-  declare_c2_type(MaxReductionVNode, ReductionNode)                       \\\n-  declare_c2_type(MinReductionVNode, ReductionNode)                       \\\n@@ -1850,0 +1857,21 @@\n+  declare_c2_type(LoadVectorGatherNode, LoadVectorNode)                   \\\n+  declare_c2_type(StoreVectorScatterNode, StoreVectorNode)                \\\n+  declare_c2_type(VectorLoadMaskNode, VectorNode)                         \\\n+  declare_c2_type(VectorLoadShuffleNode, VectorNode)                      \\\n+  declare_c2_type(VectorStoreMaskNode, VectorNode)                        \\\n+  declare_c2_type(VectorBlendNode, VectorNode)                            \\\n+  declare_c2_type(VectorRearrangeNode, VectorNode)                        \\\n+  declare_c2_type(VectorMaskWrapperNode, VectorNode)                      \\\n+  declare_c2_type(VectorMaskCmpNode, VectorNode)                          \\\n+  declare_c2_type(VectorCastB2XNode, VectorNode)                          \\\n+  declare_c2_type(VectorCastS2XNode, VectorNode)                          \\\n+  declare_c2_type(VectorCastI2XNode, VectorNode)                          \\\n+  declare_c2_type(VectorCastL2XNode, VectorNode)                          \\\n+  declare_c2_type(VectorCastF2XNode, VectorNode)                          \\\n+  declare_c2_type(VectorCastD2XNode, VectorNode)                          \\\n+  declare_c2_type(VectorInsertNode, VectorNode)                           \\\n+  declare_c2_type(VectorUnboxNode, VectorNode)                            \\\n+  declare_c2_type(VectorReinterpretNode, VectorNode)                      \\\n+  declare_c2_type(VectorBoxNode, Node)                                    \\\n+  declare_c2_type(VectorBoxAllocateNode, CallStaticJavaNode)              \\\n+  declare_c2_type(VectorTestNode, Node)                                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -0,0 +1,468 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.internal.vm.vector;\n+\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n+import jdk.internal.misc.Unsafe;\n+import jdk.internal.vm.annotation.ForceInline;\n+\n+import java.nio.Buffer;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+import java.util.function.*;\n+\n+public class VectorSupport {\n+    static {\n+        registerNatives();\n+    }\n+\n+    private static final Unsafe U = Unsafe.getUnsafe();\n+\n+    \/\/ Unary\n+    public static final int VECTOR_OP_ABS  = 0;\n+    public static final int VECTOR_OP_NEG  = 1;\n+    public static final int VECTOR_OP_SQRT = 2;\n+\n+    \/\/ Binary\n+    public static final int VECTOR_OP_ADD  = 4;\n+    public static final int VECTOR_OP_SUB  = 5;\n+    public static final int VECTOR_OP_MUL  = 6;\n+    public static final int VECTOR_OP_DIV  = 7;\n+    public static final int VECTOR_OP_MIN  = 8;\n+    public static final int VECTOR_OP_MAX  = 9;\n+\n+    public static final int VECTOR_OP_AND  = 10;\n+    public static final int VECTOR_OP_OR   = 11;\n+    public static final int VECTOR_OP_XOR  = 12;\n+\n+    \/\/ Ternary\n+    public static final int VECTOR_OP_FMA  = 13;\n+\n+    \/\/ Broadcast int\n+    public static final int VECTOR_OP_LSHIFT  = 14;\n+    public static final int VECTOR_OP_RSHIFT  = 15;\n+    public static final int VECTOR_OP_URSHIFT = 16;\n+\n+    public static final int VECTOR_OP_CAST        = 17;\n+    public static final int VECTOR_OP_REINTERPRET = 18;\n+\n+    \/\/ enum BoolTest\n+    public static final int BT_eq = 0;\n+    public static final int BT_ne = 4;\n+    public static final int BT_le = 5;\n+    public static final int BT_ge = 7;\n+    public static final int BT_lt = 3;\n+    public static final int BT_gt = 1;\n+    public static final int BT_overflow = 2;\n+    public static final int BT_no_overflow = 6;\n+\n+    \/\/ BasicType codes, for primitives only:\n+    public static final int\n+        T_FLOAT   = 6,\n+        T_DOUBLE  = 7,\n+        T_BYTE    = 8,\n+        T_SHORT   = 9,\n+        T_INT     = 10,\n+        T_LONG    = 11;\n+\n+    \/* ============================================================================ *\/\n+\n+    public static class VectorSpecies<E> {}\n+\n+    public static class VectorPayload {\n+        private final Object payload; \/\/ array of primitives\n+\n+        public VectorPayload(Object payload) {\n+            this.payload = payload;\n+        }\n+\n+        protected final Object getPayload() {\n+            return VectorSupport.maybeRebox(this).payload;\n+        }\n+    }\n+\n+    public static class Vector<E> extends VectorPayload {\n+        public Vector(Object payload) {\n+            super(payload);\n+        }\n+    }\n+\n+    public static class VectorShuffle<E> extends VectorPayload {\n+        public VectorShuffle(Object payload) {\n+            super(payload);\n+        }\n+    }\n+    public static class VectorMask<E> extends VectorPayload {\n+        public VectorMask(Object payload) {\n+            super(payload);\n+        }\n+    }\n+\n+    \/* ============================================================================ *\/\n+    public interface BroadcastOperation<VM, E, S extends VectorSpecies<E>> {\n+        VM broadcast(long l, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM, E, S extends VectorSpecies<E>>\n+    VM broadcastCoerced(Class<? extends VM> vmClass, Class<E> E, int length,\n+                                  long bits, S s,\n+                                  BroadcastOperation<VM, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.broadcast(bits, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+    public interface ShuffleIotaOperation<E, S extends VectorSpecies<E>> {\n+        VectorShuffle<E> apply(int length, int start, int step, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <E, S extends VectorSpecies<E>>\n+    VectorShuffle<E> shuffleIota(Class<?> E, Class<?> ShuffleClass, S s, int length,\n+                     int start, int step, int wrap, ShuffleIotaOperation<E, S> defaultImpl) {\n+       assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+       return defaultImpl.apply(length, start, step, s);\n+    }\n+\n+    public interface ShuffleToVectorOperation<VM, Sh, E> {\n+       VM apply(Sh s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM ,Sh extends VectorShuffle<E>, E>\n+    VM shuffleToVector(Class<?> VM, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n+                       ShuffleToVectorOperation<VM,Sh,E> defaultImpl) {\n+      assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+      return defaultImpl.apply(s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+    public interface IndexOperation<V extends Vector<E>, E, S extends VectorSpecies<E>> {\n+        V index(V v, int step, S s);\n+    }\n+\n+    \/\/FIXME @IntrinsicCandidate\n+    public static\n+    <V extends Vector<E>, E, S extends VectorSpecies<E>>\n+    V indexVector(Class<? extends V> vClass, Class<E> E, int length,\n+                  V v, int step, S s,\n+                  IndexOperation<V, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.index(v, step, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int length,\n+                          V v,\n+                          Function<V,Long> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VecExtractOp<V> {\n+        long apply(V v1, int idx);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n+                 V vec, int ix,\n+                 VecExtractOp<V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vec, ix);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VecInsertOp<V> {\n+        V apply(V v1, int idx, long val);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n+             V vec, int ix, long val,\n+             VecInsertOp<V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vec, ix, val);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+               VM vm,\n+               Function<VM, VM> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+                VM vm1, VM vm2,\n+                BiFunction<VM, VM, VM> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm1, vm2);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface TernaryOperation<V> {\n+        V apply(V v1, V v2, V v3);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+                 VM vm1, VM vm2, VM vm3,\n+                 TernaryOperation<VM> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm1, vm2, vm3);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    \/\/ Memory operations\n+\n+    public interface LoadOperation<C, V, E, S extends VectorSpecies<E>> {\n+        V load(C container, int index, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, VM, E, S extends VectorSpecies<E>>\n+    VM load(Class<? extends VM> vmClass, Class<E> E, int length,\n+           Object base, long offset,    \/\/ Unsafe addressing\n+           C container, int index, S s,     \/\/ Arguments for default implementation\n+           LoadOperation<C, VM, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.load(container, index, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>> {\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>, W extends Vector<Integer>, E, S extends VectorSpecies<E>>\n+    V loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n+                  Object base, long offset, \/\/ Unsafe addressing\n+                  W index_vector,\n+                  C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+                  LoadVectorOperationWithMap<C, V, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface StoreVectorOperation<C, V extends Vector<?>> {\n+        void store(C container, int index, V v);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>>\n+    void store(Class<?> vectorClass, Class<?> elementType, int length,\n+               Object base, long offset,    \/\/ Unsafe addressing\n+               V v,\n+               C container, int index,      \/\/ Arguments for default implementation\n+               StoreVectorOperation<C, V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface StoreVectorOperationWithMap<C, V extends Vector<?>> {\n+        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>, W extends Vector<Integer>>\n+    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n+                      Object base, long offset,    \/\/ Unsafe addressing\n+                      W index_vector, V v,\n+                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n+                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.storeWithMap(container, index, v, indexMap, indexM);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    boolean test(int cond, Class<?> vmClass, Class<?> elementType, int length,\n+                 VM vm1, VM vm2,\n+                 BiFunction<VM, VM, Boolean> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm1, vm2);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorCompareOp<V,M> {\n+        M apply(int cond, V v1, V v2);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static <V extends Vector<E>,\n+                   M extends VectorMask<E>,\n+                   E>\n+    M compare(int cond, Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n+              V v1, V v2,\n+              VectorCompareOp<V,M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(cond, v1, v2);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorRearrangeOp<V extends Vector<E>,\n+            Sh extends VectorShuffle<E>,\n+            E> {\n+        V apply(V v1, Sh shuffle);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<E>,\n+            Sh extends VectorShuffle<E>,\n+            E>\n+    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<?> elementType, int vlen,\n+                  V v1, Sh sh,\n+                  VectorRearrangeOp<V,Sh, E> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v1, sh);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorBlendOp<V extends Vector<E>,\n+            M extends VectorMask<E>,\n+            E> {\n+        V apply(V v1, V v2, M mask);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V blend(Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n+            V v1, V v2, M m,\n+            VectorBlendOp<V,M, E> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v1, v2, m);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorBroadcastIntOp<V extends Vector<?>> {\n+        V apply(V v, int n);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<?> elementType, int length,\n+                   V v, int n,\n+                   VectorBroadcastIntOp<V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v, n);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorConvertOp<VOUT, VIN, S> {\n+        VOUT apply(VIN v, S species);\n+    }\n+\n+    \/\/ Users of this intrinsic assume that it respects\n+    \/\/ REGISTER_ENDIAN, which is currently ByteOrder.LITTLE_ENDIAN.\n+    \/\/ See javadoc for REGISTER_ENDIAN.\n+\n+    @IntrinsicCandidate\n+    public static <VOUT extends VectorPayload,\n+                    VIN extends VectorPayload,\n+                      S extends VectorSpecies<?>>\n+    VOUT convert(int oprId,\n+              Class<?> fromVectorClass, Class<?> fromElementType, int fromVLen,\n+              Class<?>   toVectorClass, Class<?>   toElementType, int   toVLen,\n+              VIN v, S s,\n+              VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static <V> V maybeRebox(V v) {\n+        \/\/ The fence is added here to avoid memory aliasing problems in C2 between scalar & vector accesses.\n+        \/\/ TODO: move the fence generation into C2. Generate only when reboxing is taking place.\n+        U.loadFence();\n+        return v;\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    \/\/ query the JVM's supported vector sizes and types\n+    public static native int getMaxLaneCount(Class<?> etype);\n+\n+    \/* ============================================================================ *\/\n+\n+    public static boolean isNonCapturingLambda(Object o) {\n+        return o.getClass().getDeclaredFields().length == 0;\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    private static native int registerNatives();\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":468,"deletions":0,"binary":false,"changes":468,"status":"added"},{"patch":"@@ -141,1 +141,1 @@\n-    exports jdk.internal to\n+    exports jdk.internal to \/\/ for @HotSpotIntrinsicCandidate\n@@ -143,0 +143,1 @@\n+        jdk.incubator.vector,\n@@ -198,0 +199,1 @@\n+        jdk.incubator.vector,\n@@ -231,0 +233,1 @@\n+        jdk.incubator.vector,\n@@ -234,0 +237,2 @@\n+    exports jdk.internal.vm.vector to\n+        jdk.incubator.vector;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"}]}