{"files":[{"patch":"@@ -1464,4 +1464,11 @@\n-void C2_MacroAssembler::load_vector_mask64(KRegister dst, XMMRegister src, XMMRegister xtmp, Register scratch) {\n-  vpmovsxbd(xtmp, src, Assembler::AVX_512bit);\n-  evpcmpd(dst, k0, xtmp, ExternalAddress(StubRoutines::x86::vector_int_mask_cmp_bits()),\n-          Assembler::eq, true, Assembler::AVX_512bit, scratch);\n+void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp,\n+                                         Register tmp, bool novlbwdq, int vlen_enc) {\n+  if (novlbwdq) {\n+    vpmovsxbd(xtmp, src, vlen_enc);\n+    evpcmpd(dst, k0, xtmp, ExternalAddress(StubRoutines::x86::vector_int_mask_cmp_bits()),\n+            Assembler::eq, true, vlen_enc, tmp);\n+  } else {\n+    vpxor(xtmp, xtmp, xtmp, vlen_enc);\n+    vpsubb(xtmp, xtmp, src, vlen_enc);\n+    evpmovb2m(dst, xtmp, vlen_enc);\n+  }\n@@ -4004,12 +4011,0 @@\n-    case Op_SqrtVF:\n-      evsqrtps(dst, mask, src1, src2, merge, vlen_enc); break;\n-    case Op_SqrtVD:\n-      evsqrtpd(dst, mask, src1, src2, merge, vlen_enc); break;\n-    case Op_AbsVB:\n-      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n-    case Op_AbsVS:\n-      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n-    case Op_AbsVI:\n-      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n-    case Op_AbsVL:\n-      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":11,"deletions":16,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -145,1 +145,1 @@\n-  void load_vector_mask64(KRegister dst, XMMRegister src, XMMRegister xtmp, Register scratch);\n+  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, Register tmp, bool novlbwdq, int vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4004,1 +4004,0 @@\n-    StubRoutines::x86::_vector_mask_cmp_bits = generate_vector_mask(\"vector_mask_cmp_bits\", 0x01010101);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -7584,1 +7584,0 @@\n-    StubRoutines::x86::_vector_mask_cmp_bits = generate_vector_mask(\"vector_mask_cmp_bits\", 0x0101010101010101);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-address StubRoutines::x86::_vector_mask_cmp_bits = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -168,1 +168,0 @@\n-  static address _vector_mask_cmp_bits;\n@@ -296,4 +295,0 @@\n-  static address vector_mask_cmp_bits() {\n-    return _vector_mask_cmp_bits;\n-  }\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1377,1 +1377,0 @@\n-  static address vector_mask_cmp_bits() { return StubRoutines::x86::vector_mask_cmp_bits(); }\n@@ -7869,1 +7868,1 @@\n-  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  format %{ \"vector_loadmask_byte $dst, $src\\n\\t\" %}\n@@ -7873,1 +7872,0 @@\n-\n@@ -7879,1 +7877,1 @@\n-instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI scratch) %{\n+instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI tmp) %{\n@@ -7882,2 +7880,2 @@\n-  effect(TEMP xtmp, TEMP scratch);\n-  format %{ \"vector_loadmask_64byte $dst,$src\\n\\t\" %}\n+  effect(TEMP xtmp, TEMP tmp);\n+  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp and $tmp as TEMP\" %}\n@@ -7885,1 +7883,2 @@\n-    __ load_vector_mask64($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister, $scratch$$Register);\n+    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                        $tmp$$Register, true, Assembler::AVX_512bit);\n@@ -7890,2 +7889,1 @@\n-\n-instruct loadMask_evex(kReg dst, vec src, rRegP scratch, kReg kscratch) %{\n+instruct loadMask_evex(kReg dst, vec src,  vec xtmp) %{\n@@ -7894,2 +7892,2 @@\n-  effect(TEMP scratch, TEMP kscratch);\n-  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  effect(TEMP xtmp);\n+  format %{ \"vector_loadmask_byte $dst, $src\\t! using $xtmp as TEMP\" %}\n@@ -7898,2 +7896,2 @@\n-    __ evpcmp(T_BYTE, $dst$$KRegister, k0, $src$$XMMRegister, ExternalAddress(vector_mask_cmp_bits()),\n-              Assembler::eq, vlen_enc, $scratch$$Register);\n+    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                        noreg, false, vlen_enc);\n@@ -7906,3 +7904,2 @@\n-instruct storeMask1B(vec dst, vec src, immI_1 size) %{\n-  predicate((Matcher::vector_length(n) < 64 || VM_Version::supports_avx512vlbw()) &&\n-            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n+instruct vstoreMask1B(vec dst, vec src, immI_1 size) %{\n+  predicate(Matcher::vector_length(n) < 64 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7910,1 +7907,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7912,2 +7909,3 @@\n-    assert(UseSSE >= 3, \"required\");\n-    if (Matcher::vector_length_in_bytes(this) <= 16) {\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 16 && UseAVX <= 2) {\n+      assert(UseSSE >= 3, \"required\");\n@@ -7916,1 +7914,1 @@\n-      assert(UseAVX >= 2, \"required\");\n+      assert(UseAVX > 0, \"required\");\n@@ -7924,13 +7922,0 @@\n-instruct storeMask2B(vec dst, vec src, immI_2 size) %{\n-  predicate(Matcher::vector_length(n) <= 8 &&\n-            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\n\\t\" %}\n-  ins_encode %{\n-    assert(UseSSE >= 3, \"required\");\n-    __ pabsw($dst$$XMMRegister, $src$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -7938,2 +7923,1 @@\n-  predicate(Matcher::vector_length(n) == 16 && !VM_Version::supports_avx512bw() &&\n-            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n+  predicate(Matcher::vector_length(n) <= 16 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7941,2 +7925,2 @@\n-  effect(TEMP dst);\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  effect(TEMP_DEF dst);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7945,16 +7929,11 @@\n-    __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n-    __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister,vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct storeMask4B(vec dst, vec src, immI_4 size) %{\n-  predicate(Matcher::vector_length(n) <= 4 && UseAVX <= 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  ins_encode %{\n-    assert(UseSSE >= 3, \"required\");\n-    __ pabsd($dst$$XMMRegister, $src$$XMMRegister);\n-    __ packssdw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 8 && UseAVX <= 2) {\n+      assert(UseSSE >= 3, \"required\");\n+      __ pabsw($dst$$XMMRegister, $src$$XMMRegister);\n+      __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 0, \"required\");\n+      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n+      __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -7966,1 +7945,1 @@\n-  predicate(Matcher::vector_length(n) == 8 && UseAVX <= 2);\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) <= 8 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7968,2 +7947,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  effect(TEMP dst);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n+  effect(TEMP_DEF dst);\n@@ -7972,4 +7951,13 @@\n-    __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n-    __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 4 && UseAVX <= 2) {\n+      assert(UseSSE >= 3, \"required\");\n+      __ pabsd($dst$$XMMRegister, $src$$XMMRegister);\n+      __ packusdw($dst$$XMMRegister, $dst$$XMMRegister);\n+      __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 0, \"required\");\n+      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n+      __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -7981,1 +7969,1 @@\n-  predicate(Matcher::vector_length(n) == 2 && UseAVX <= 2);\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 2);\n@@ -7983,1 +7971,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7994,2 +7982,2 @@\n-instruct storeMask8B_avx(vec dst, vec src, immI_8 size, legVec vtmp) %{\n-  predicate(Matcher::vector_length(n) == 4 && UseAVX <= 2);\n+instruct storeMask8B_avx(vec dst, vec src, immI_8 size, vec vtmp) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 4);\n@@ -7997,1 +7985,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t! using $vtmp as TEMP\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s], using $vtmp as TEMP\" %}\n@@ -8011,15 +7999,1 @@\n-instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n-  predicate(VM_Version::supports_avx512bw() &&\n-            n->in(1)->bottom_type()->isa_vectmask() == NULL);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  ins_encode %{\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n+instruct vstoreMask4B_evex_novectmask(vec dst, vec src, immI_4 size) %{\n@@ -8028,1 +8002,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -8041,1 +8015,1 @@\n-instruct vstoreMask8B_evex(vec dst, vec src, immI_8 size) %{\n+instruct vstoreMask8B_evex_novectmask(vec dst, vec src, immI_8 size) %{\n@@ -8044,1 +8018,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -8057,1 +8031,1 @@\n-instruct vstoreMask64(vec dst, kReg mask, immI size, rRegI scratch) %{\n+instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size, rRegI tmp) %{\n@@ -8060,2 +8034,2 @@\n-  effect(TEMP_DEF dst, TEMP scratch);\n-  format %{ \"vector_store_mask64 $dst,$mask\\t!\" %}\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vector_store_mask $dst, $mask \\t! elem size is $size byte[s]\" %}\n@@ -8063,0 +8037,1 @@\n+    assert(Matcher::vector_length_in_bytes(this, $mask) == 64, \"\");\n@@ -8064,1 +8039,1 @@\n-                 false, Assembler::AVX_512bit, $scratch$$Register);\n+                 false, Assembler::AVX_512bit, $tmp$$Register);\n@@ -8074,1 +8049,1 @@\n-  format %{ \"vector_store_mask $dst,$mask\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $mask \\t! elem size is $size byte[s]\" %}\n@@ -8776,6 +8751,6 @@\n-  match(Set dst (AddVB (Binary dst src2) mask));\n-  match(Set dst (AddVS (Binary dst src2) mask));\n-  match(Set dst (AddVI (Binary dst src2) mask));\n-  match(Set dst (AddVL (Binary dst src2) mask));\n-  match(Set dst (AddVF (Binary dst src2) mask));\n-  match(Set dst (AddVD (Binary dst src2) mask));\n+  match(Set dst (AddVB (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVD (Binary dst (LoadVector src2)) mask));\n@@ -8807,1 +8782,1 @@\n-  match(Set dst (XorV (Binary dst src2) mask));\n+  match(Set dst (XorV (Binary dst (LoadVector src2)) mask));\n@@ -8833,1 +8808,1 @@\n-  match(Set dst (OrV (Binary dst src2) mask));\n+  match(Set dst (OrV (Binary dst (LoadVector src2)) mask));\n@@ -8859,1 +8834,1 @@\n-  match(Set dst (AndV (Binary dst src2) mask));\n+  match(Set dst (AndV (Binary dst (LoadVector src2)) mask));\n@@ -8890,6 +8865,6 @@\n-  match(Set dst (SubVB (Binary dst src2) mask));\n-  match(Set dst (SubVS (Binary dst src2) mask));\n-  match(Set dst (SubVI (Binary dst src2) mask));\n-  match(Set dst (SubVL (Binary dst src2) mask));\n-  match(Set dst (SubVF (Binary dst src2) mask));\n-  match(Set dst (SubVD (Binary dst src2) mask));\n+  match(Set dst (SubVB (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVD (Binary dst (LoadVector src2)) mask));\n@@ -8925,5 +8900,5 @@\n-  match(Set dst (MulVS (Binary dst src2) mask));\n-  match(Set dst (MulVI (Binary dst src2) mask));\n-  match(Set dst (MulVL (Binary dst src2) mask));\n-  match(Set dst (MulVF (Binary dst src2) mask));\n-  match(Set dst (MulVD (Binary dst src2) mask));\n+  match(Set dst (MulVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVD (Binary dst (LoadVector src2)) mask));\n@@ -8956,16 +8931,0 @@\n-instruct vsqrt_mem_masked(vec dst, memory src, kReg mask) %{\n-  match(Set dst (SqrtVF src mask));\n-  match(Set dst (SqrtVD src mask));\n-  ins_cost(100);\n-  format %{ \"vpsqrt_masked $dst, $src, $mask\\t! sqrt masked operation\" %}\n-  ins_encode %{\n-    int vlen_enc = vector_length_encoding(this);\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    int opc = this->ideal_Opcode();\n-    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n-                   $dst$$XMMRegister, $src$$Address, true, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n@@ -8987,2 +8946,2 @@\n-  match(Set dst (DivVF (Binary dst src2) mask));\n-  match(Set dst (DivVD (Binary dst src2) mask));\n+  match(Set dst (DivVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (DivVD (Binary dst (LoadVector src2)) mask));\n@@ -9000,0 +8959,1 @@\n+\n@@ -9060,3 +9020,3 @@\n-  match(Set dst (LShiftVS (Binary dst src2) mask));\n-  match(Set dst (LShiftVI (Binary dst src2) mask));\n-  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  match(Set dst (LShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (LShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (LShiftVL (Binary dst (LoadVector src2)) mask));\n@@ -9106,3 +9066,3 @@\n-  match(Set dst (RShiftVS (Binary dst src2) mask));\n-  match(Set dst (RShiftVI (Binary dst src2) mask));\n-  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  match(Set dst (RShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (RShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (RShiftVL (Binary dst (LoadVector src2)) mask));\n@@ -9152,3 +9112,3 @@\n-  match(Set dst (URShiftVS (Binary dst src2) mask));\n-  match(Set dst (URShiftVI (Binary dst src2) mask));\n-  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  match(Set dst (URShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (URShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (URShiftVL (Binary dst (LoadVector src2)) mask));\n@@ -9180,1 +9140,1 @@\n-  match(Set dst (MaxV (Binary dst src2) mask));\n+  match(Set dst (MaxV (Binary dst (LoadVector src2)) mask));\n@@ -9206,1 +9166,1 @@\n-  match(Set dst (MinV (Binary dst src2) mask));\n+  match(Set dst (MinV (Binary dst (LoadVector src2)) mask));\n@@ -9263,2 +9223,2 @@\n-  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n-  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVF (Binary dst src2) (Binary (LoadVector src3) mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary (LoadVector src3) mask)));\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":99,"deletions":139,"binary":false,"changes":238,"status":"modified"}]}