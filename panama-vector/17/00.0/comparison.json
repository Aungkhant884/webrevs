{"files":[{"patch":"@@ -617,3 +617,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -2625,5 +2623,0 @@\n-\/\/ No-op on amd64\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n-  Unimplemented();\n-}\n-\n@@ -3789,0 +3782,4 @@\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n@@ -3794,1 +3791,4 @@\n-\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n@@ -3802,4 +3802,2 @@\n-    if (call == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    } else if (UseSVE > 0 && Compile::current()->max_vector_size() >= 16) {\n+\n+    if (UseSVE > 0 && Compile::current()->max_vector_size() >= 16) {\n@@ -4107,3 +4105,0 @@\n-  \/\/ Method Register when calling interpreter.\n-  interpreter_method_reg(R12);\n-\n@@ -5697,10 +5692,0 @@\n-operand interpreter_method_RegP(iRegP reg)\n-%{\n-  constraint(ALLOC_IN_RC(method_reg)); \/\/ interpreter_method_reg\n-  match(reg);\n-  match(iRegPNoSp);\n-  op_cost(0);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n@@ -14762,1 +14747,5 @@\n-    __ zero_words($base$$Register, $cnt$$Register);\n+    address tpc = __ zero_words($base$$Register, $cnt$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16040,2 +16029,2 @@\n-  ins_encode( aarch64_enc_java_static_call(meth),\n-              aarch64_enc_call_epilog );\n+  ins_encode(aarch64_enc_java_static_call(meth),\n+             aarch64_enc_call_epilog);\n@@ -16059,2 +16048,2 @@\n-  ins_encode( aarch64_enc_java_dynamic_call(meth),\n-               aarch64_enc_call_epilog );\n+  ins_encode(aarch64_enc_java_dynamic_call(meth),\n+             aarch64_enc_call_epilog);\n@@ -16526,4 +16515,8 @@\n-    __ arrays_equals($ary1$$Register, $ary2$$Register,\n-                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                     $result$$Register, $tmp$$Register, 1);\n-    %}\n+    address tpc = __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                                   $result$$Register, $tmp$$Register, 1);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n@@ -16543,3 +16536,7 @@\n-    __ arrays_equals($ary1$$Register, $ary2$$Register,\n-                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                     $result$$Register, $tmp$$Register, 2);\n+    address tpc = __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                                   $result$$Register, $tmp$$Register, 2);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16556,1 +16553,5 @@\n-    __ has_negatives($ary1$$Register, $len$$Register, $result$$Register);\n+    address tpc = __ has_negatives($ary1$$Register, $len$$Register, $result$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16589,2 +16590,7 @@\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$FloatRegister, $tmp2$$FloatRegister, $tmp3$$FloatRegister, $tmp4$$Register);\n+    address tpc = __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                                        $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                                        $tmp3$$FloatRegister, $tmp4$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":47,"deletions":41,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -179,0 +179,2 @@\n+      case Op_VectorMaskWrapper:\n+      case Op_VectorLoadConst:\n@@ -182,1 +184,0 @@\n-      case Op_VectorMaskWrapper:\n@@ -877,1 +878,1 @@\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n@@ -918,1 +919,1 @@\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -936,1 +937,1 @@\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -166,0 +166,2 @@\n+      case Op_VectorMaskWrapper:\n+      case Op_VectorLoadConst:\n@@ -169,1 +171,0 @@\n-      case Op_VectorMaskWrapper:\n@@ -544,1 +545,1 @@\n-            ELEMENT_SHORT_CHAR($6, n->in(2)));\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n@@ -567,1 +568,1 @@\n-            ELEMENT_SHORT_CHAR($6, n->in(2)));\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1092,14 +1092,0 @@\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) { Unimplemented(); }\n-\n@@ -1271,4 +1257,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they block out GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1278,12 +1263,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -1557,5 +1530,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -1834,5 +1802,5 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n+    \/\/ Now set thread in native\n+    __ mov(rscratch1, _thread_in_native);\n+    __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+    __ stlrw(rscratch1, rscratch2);\n+  }\n@@ -1867,0 +1835,15 @@\n+  Label safepoint_in_progress, safepoint_in_progress_done;\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, false \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n+    __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n+    __ cbnzw(rscratch1, needs_safepoint);\n+    __ b(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -1887,1 +1870,0 @@\n-  Label safepoint_in_progress, safepoint_in_progress_done;\n@@ -1905,1 +1887,0 @@\n-  Label after_transition;\n@@ -2110,5 +2091,1 @@\n-    if (!is_critical_native) {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2120,6 +2097,0 @@\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic above.\n-      __ b(after_transition);\n-    }\n-\n@@ -2174,5 +2145,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":24,"deletions":58,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -1176,4 +1176,0 @@\n-\/\/ No-op on ARM.\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n-}\n-\n@@ -1684,1 +1680,0 @@\n-  interpreter_method_reg(R_Rmethod);     \/\/ Method Register when calling interpreter\n@@ -2544,8 +2539,0 @@\n-operand interpreter_method_regP(iRegP reg) %{\n-  constraint(ALLOC_IN_RC(Rmethod_regP));\n-  match(reg);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -538,3 +538,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( as defined in frame section )\n-\/\/ 2) reg_class interpreter_method_reg     ( as defined in frame section )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -2361,4 +2359,0 @@\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n- Unimplemented();\n-}\n-\n@@ -3876,3 +3870,0 @@\n-  \/\/ Method Register when calling interpreter.\n-  interpreter_method_reg(R19); \/\/ R19_method\n-\n@@ -4784,14 +4775,0 @@\n-%}\n-\n-operand compiler_method_regP(iRegPdst reg) %{\n-  constraint(ALLOC_IN_RC(rscratch1_bits64_reg)); \/\/ compiler_method_reg\n-  match(reg);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand interpreter_method_regP(iRegPdst reg) %{\n-  constraint(ALLOC_IN_RC(r19_bits64_reg)); \/\/ interpreter_method_reg\n-  match(reg);\n-  format %{ %}\n-  interface(REG_INTER);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":1,"deletions":24,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1541,150 +1541,0 @@\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ If map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Save down double word first.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_FloatRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after DOUBLE stack slot)\");\n-      if (map != NULL) {\n-        __ stfd(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-      } else {\n-        __ lfd(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-      }\n-    } else if (in_regs[i].first()->is_Register() &&\n-        (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      if (map != NULL) {\n-        __ std(in_regs[i].first()->as_Register(), offset, R1_SP);\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));\n-        }\n-      } else {\n-        __ ld(in_regs[i].first()->as_Register(), offset, R1_SP);\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after LONG\/ARRAY stack slot)\");\n-    }\n-  }\n-  \/\/ Save or restore single word registers.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      \/\/ Value lives in an input register. Save it on stack.\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ stw(in_regs[i].first()->as_Register(), offset, R1_SP);\n-          } else {\n-            __ lwa(in_regs[i].first()->as_Register(), offset, R1_SP);\n-          }\n-          slot++;\n-          assert(slot <= stack_slots, \"overflow (after INT or smaller stack slot)\");\n-          break;\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_FloatRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow (after FLOAT stack slot)\");\n-        if (map != NULL) {\n-          __ stfs(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-        } else {\n-          __ lfs(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true. This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced. Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               const int stack_slots,\n-                                               const int total_in_args,\n-                                               const int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt,\n-                                               Register tmp_reg ) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ lbz(tmp_reg, (RegisterOrConstant)(intptr_t)GCLocker::needs_gc_address());\n-  __ cmplwi(CCR0, tmp_reg, 0);\n-  __ beq(CCR0, cont);\n-\n-  \/\/ Save down any values that are live in registers and call into the\n-  \/\/ runtime to halt for a GC.\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  __ mr(R3_ARG1, R16_thread);\n-  __ set_last_Java_frame(R1_SP, noreg);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  address entry_point = CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical);\n-#if defined(ABI_ELFv2)\n-  __ call_c(entry_point, relocInfo::runtime_call_type);\n-#else\n-  __ call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, entry_point), relocInfo::runtime_call_type);\n-#endif\n-  address start           = __ pc() - __ offset(),\n-          calls_return_pc = __ last_calls_return_pc();\n-  oop_maps->add_gc_map(calls_return_pc - start, map);\n-\n-  __ reset_last_Java_frame();\n-\n-  \/\/ Reload all the register arguments.\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ BIND(cont);\n-\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving.\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers.\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ neg(reg, reg);\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        __ fneg(in_regs[i].first()->as_FloatRegister(), in_regs[i].first()->as_FloatRegister());\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1832,4 +1682,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1839,12 +1688,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/   if (GCLocker::needs_gc())\n-\/\/     SharedRuntime::block_for_jni_critical();\n-\/\/   tranistion to thread_in_native\n-\/\/   unpack arrray arguments and call native entry point\n-\/\/   check for safepoint in progress\n-\/\/   check if any thread suspend flags are set\n-\/\/     call into JVM and possible unlock the JNI critical\n-\/\/     if a GC was suppressed while in the critical native.\n-\/\/   transition back to thread_in_Java\n-\/\/   return to caller\n-\/\/\n@@ -2157,5 +1994,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_in_args, oop_handle_slot_offset,\n-                                       oop_maps, in_regs, in_sig_bt, r_temp_1);\n-  }\n-\n@@ -2362,4 +2194,0 @@\n-\n-  \/\/ Publish thread state\n-  \/\/ --------------------------------------------------------------------------\n-\n@@ -2369,5 +2197,10 @@\n-  \/\/ Transition from _thread_in_Java to _thread_in_native.\n-  __ li(R0, _thread_in_native);\n-  __ release();\n-  \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n-  __ stw(R0, thread_(thread_state));\n+  if (!is_critical_native) {\n+    \/\/ Publish thread state\n+    \/\/ --------------------------------------------------------------------------\n+\n+    \/\/ Transition from _thread_in_Java to _thread_in_native.\n+    __ li(R0, _thread_in_native);\n+    __ release();\n+    \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n+    __ stw(R0, thread_(thread_state));\n+  }\n@@ -2433,0 +2266,16 @@\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    Register sync_state      = r_temp_5;\n+    __ safepoint_poll(needs_safepoint, sync_state);\n+\n+    Register suspend_flags   = r_temp_6;\n+    __ lwz(suspend_flags, thread_(suspend_flags));\n+    __ cmpwi(CCR1, suspend_flags, 0);\n+    __ beq(CCR1, after_transition);\n+    __ bind(needs_safepoint);\n+  }\n@@ -2460,1 +2309,0 @@\n-  Label after_transition;\n@@ -2488,3 +2336,2 @@\n-    address entry_point = is_critical_native\n-      ? CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)\n-      : CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n+    address entry_point =\n+      CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n@@ -2495,6 +2342,2 @@\n-    if (is_critical_native) {\n-      __ b(after_transition); \/\/ No thread state transition here.\n-    }\n-  }\n-  \/\/ Publish thread state.\n-  \/\/ --------------------------------------------------------------------------\n+    \/\/ Publish thread state.\n+    \/\/ --------------------------------------------------------------------------\n@@ -2504,2 +2347,2 @@\n-  \/\/ Thread state is thread_in_native_trans. Any safepoint blocking has\n-  \/\/ already happened so we can now change state to _thread_in_Java.\n+    \/\/ Thread state is thread_in_native_trans. Any safepoint blocking has\n+    \/\/ already happened so we can now change state to _thread_in_Java.\n@@ -2507,6 +2350,7 @@\n-  \/\/ Transition from _thread_in_native_trans to _thread_in_Java.\n-  __ li(R0, _thread_in_Java);\n-  __ lwsync(); \/\/ Acquire safepoint and suspend state, release thread state.\n-  \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n-  __ stw(R0, thread_(thread_state));\n-  __ bind(after_transition);\n+    \/\/ Transition from _thread_in_native_trans to _thread_in_Java.\n+    __ li(R0, _thread_in_Java);\n+    __ lwsync(); \/\/ Acquire safepoint and suspend state, release thread state.\n+    \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n+    __ stw(R0, thread_(thread_state));\n+    __ bind(after_transition);\n+  }\n@@ -2669,4 +2513,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":42,"deletions":202,"binary":false,"changes":244,"status":"modified"},{"patch":"@@ -281,3 +281,1 @@\n-\/\/ 2) reg_class compiler_method_reg        (as defined in frame section)\n-\/\/ 2) reg_class interpreter_method_reg     (as defined in frame section)\n-\/\/ 3) reg_class stack_slots(\/* one chunk of stack-based \"registers\" *\/)\n+\/\/ 2) reg_class stack_slots(\/* one chunk of stack-based \"registers\" *\/)\n@@ -2483,6 +2481,0 @@\n-  \/\/ Temporary in compiled entry-points\n-  \/\/ compiler_method_reg(Z_R1);\/\/Z_R1_scratch\n-\n-  \/\/ Method Register when calling interpreter\n-  interpreter_method_reg(Z_R9);\/\/Z_method\n-\n@@ -3552,14 +3544,0 @@\n-operand compiler_method_regP(iRegP reg) %{\n-  constraint(ALLOC_IN_RC(z_r1_RegP)); \/\/ compiler_method_reg\n-  match(reg);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand interpreter_method_regP(iRegP reg) %{\n-  constraint(ALLOC_IN_RC(z_r9_regP)); \/\/ interpreter_method_reg\n-  match(reg);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":1,"deletions":23,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1296,157 +1296,0 @@\n-static void save_or_restore_arguments(MacroAssembler *masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap *map,\n-                                      VMRegPair *in_regs,\n-                                      BasicType *in_sig_bt) {\n-\n-  \/\/ If map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Handle double words first.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_FloatRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after DOUBLE stack slot)\");\n-      const FloatRegister   freg = in_regs[i].first()->as_FloatRegister();\n-      Address   stackaddr(Z_SP, offset);\n-      if (map != NULL) {\n-        __ freg2mem_opt(freg, stackaddr);\n-      } else {\n-        __ mem2freg_opt(freg, stackaddr);\n-      }\n-    } else if (in_regs[i].first()->is_Register() &&\n-               (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      const Register   reg = in_regs[i].first()->as_Register();\n-      if (map != NULL) {\n-        __ z_stg(reg, offset, Z_SP);\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));\n-        }\n-      } else {\n-        __ z_lg(reg, offset, Z_SP);\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after LONG\/ARRAY stack slot)\");\n-    }\n-  }\n-\n-  \/\/ Save or restore single word registers.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      \/\/ Value lives in an input register. Save it on stack.\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT: {\n-          const Register   reg = in_regs[i].first()->as_Register();\n-          Address   stackaddr(Z_SP, offset);\n-          if (map != NULL) {\n-            __ z_st(reg, stackaddr);\n-          } else {\n-            __ z_lgf(reg, stackaddr);\n-          }\n-          slot++;\n-          assert(slot <= stack_slots, \"overflow (after INT or smaller stack slot)\");\n-          break;\n-        }\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_FloatRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow (after FLOAT stack slot)\");\n-        const FloatRegister   freg = in_regs[i].first()->as_FloatRegister();\n-        Address   stackaddr(Z_SP, offset);\n-        if (map != NULL) {\n-          __ freg2mem_opt(freg, stackaddr, false);\n-        } else {\n-          __ mem2freg_opt(freg, stackaddr, false);\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack() &&\n-               in_sig_bt[i] == T_ARRAY && map != NULL) {\n-      int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-      map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-    }\n-  }\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true. This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced. Save down any oops in registers and describe them in an OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler   *masm,\n-                                                const int stack_slots,\n-                                                const int total_in_args,\n-                                                const int arg_save_area,\n-                                                OopMapSet *oop_maps,\n-                                                VMRegPair *in_regs,\n-                                                BasicType *in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-\n-  \/\/ Check GCLocker::_needs_gc flag.\n-  __ load_const_optimized(Z_R1_scratch, (long) GCLocker::needs_gc_address());\n-  __ z_cli(0, Z_R1_scratch, 0);\n-  __ z_bre(cont);\n-\n-  \/\/ Save down any values that are live in registers and call into the\n-  \/\/ runtime to halt for a GC.\n-  OopMap *map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-  address the_pc = __ pc();\n-  __ set_last_Java_frame(Z_SP, noreg);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ z_lgr(Z_ARG1, Z_thread);\n-\n-  address entry_point = CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical);\n-  __ call_c(entry_point);\n-  oop_maps->add_gc_map(__ offset(), map);\n-\n-  __ reset_last_Java_frame();\n-\n-  \/\/ Reload all the register arguments.\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ bind(cont);\n-\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap *map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-\n-    \/\/ Destroy argument registers.\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        \/\/ Don't set CC.\n-        __ clear_reg(in_regs[i].first()->as_Register(), true, false);\n-      } else {\n-        if (in_regs[i].first()->is_FloatRegister()) {\n-          FloatRegister fr = in_regs[i].first()->as_FloatRegister();\n-          __ z_lcdbr(fr, fr);\n-        }\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-}\n-\n@@ -1869,6 +1712,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_in_args,\n-                                       oop_handle_slot_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n-\n@@ -2103,3 +1940,4 @@\n-  \/\/ Transition from _thread_in_Java to _thread_in_native.\n-  __ set_thread_state(_thread_in_native);\n-\n+  if (!is_critical_native) {\n+    \/\/ Transition from _thread_in_Java to _thread_in_native.\n+    __ set_thread_state(_thread_in_native);\n+  }\n@@ -2151,0 +1989,13 @@\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    \/\/ Does this need to save_native_result and fences?\n+    __ safepoint_poll(needs_safepoint, Z_R1);\n+    __ load_and_test_int(Z_R0, Address(Z_thread, JavaThread::suspend_flags_offset()));\n+    __ z_bre(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n@@ -2170,1 +2021,0 @@\n-  Label after_transition;\n@@ -2192,2 +2042,1 @@\n-    address entry_point = is_critical_native ? CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)\n-                                             : CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n+    address entry_point = CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n@@ -2197,4 +2046,0 @@\n-    if (is_critical_native) {\n-      restore_native_result(masm, ret_type, workspace_slot_offset);\n-      __ z_bru(after_transition); \/\/ No thread state transition here.\n-    }\n@@ -2213,1 +2058,0 @@\n-\n@@ -2396,4 +2240,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":18,"deletions":178,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -1225,259 +1225,0 @@\n-\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ if map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int handle_index = 0;\n-  \/\/ Save down double word first\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_XMMRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int slot = handle_index * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      handle_index += 2;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movdbl(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-      } else {\n-        __ movdbl(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-      }\n-    }\n-    if (in_regs[i].first()->is_Register() && in_sig_bt[i] == T_LONG) {\n-      int slot = handle_index * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      handle_index += 2;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movl(Address(rsp, offset), in_regs[i].first()->as_Register());\n-        if (in_regs[i].second()->is_Register()) {\n-          __ movl(Address(rsp, offset + 4), in_regs[i].second()->as_Register());\n-        }\n-      } else {\n-        __ movl(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        if (in_regs[i].second()->is_Register()) {\n-          __ movl(in_regs[i].second()->as_Register(), Address(rsp, offset + 4));\n-        }\n-      }\n-    }\n-  }\n-  \/\/ Save or restore single word registers\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        map->set_oop(VMRegImpl::stack2reg(slot));;\n-      }\n-\n-      \/\/ Value is in an input register pass we must flush it to the stack\n-      const Register reg = in_regs[i].first()->as_Register();\n-      switch (in_sig_bt[i]) {\n-        case T_ARRAY:\n-          if (map != NULL) {\n-            __ movptr(Address(rsp, offset), reg);\n-          } else {\n-            __ movptr(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ movl(Address(rsp, offset), reg);\n-          } else {\n-            __ movl(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_XMMRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        assert(handle_index <= stack_slots, \"overflow\");\n-        if (map != NULL) {\n-          __ movflt(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-        } else {\n-          __ movflt(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Registers need to be saved for runtime call\n-static Register caller_saved_registers[] = {\n-  rcx, rdx, rsi, rdi\n-};\n-\n-\/\/ Save caller saved registers except r1 and r2\n-static void save_registers_except(MacroAssembler* masm, Register r1, Register r2) {\n-  int reg_len = (int)(sizeof(caller_saved_registers) \/ sizeof(Register));\n-  for (int index = 0; index < reg_len; index ++) {\n-    Register this_reg = caller_saved_registers[index];\n-    if (this_reg != r1 && this_reg != r2) {\n-      __ push(this_reg);\n-    }\n-  }\n-}\n-\n-\/\/ Restore caller saved registers except r1 and r2\n-static void restore_registers_except(MacroAssembler* masm, Register r1, Register r2) {\n-  int reg_len = (int)(sizeof(caller_saved_registers) \/ sizeof(Register));\n-  for (int index = reg_len - 1; index >= 0; index --) {\n-    Register this_reg = caller_saved_registers[index];\n-    if (this_reg != r1 && this_reg != r2) {\n-      __ pop(this_reg);\n-    }\n-  }\n-}\n-\n-\/\/ Pin object, return pinned object or null in rax\n-static void gen_pin_object(MacroAssembler* masm,\n-                           Register thread, VMRegPair reg) {\n-  __ block_comment(\"gen_pin_object {\");\n-\n-  Label is_null;\n-  Register tmp_reg = rax;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    simple_move32(masm, reg, tmp);\n-    reg = tmp;\n-  } else {\n-    __ movl(tmp_reg, reg.first()->as_Register());\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-\n-  \/\/ Save registers that may be used by runtime call\n-  Register arg = reg.first()->is_Register() ? reg.first()->as_Register() : noreg;\n-  save_registers_except(masm, arg, thread);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),\n-    thread, reg.first()->as_Register());\n-\n-  \/\/ Restore saved registers\n-  restore_registers_except(masm, arg, thread);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_pin_object\");\n-}\n-\n-\/\/ Unpin object\n-static void gen_unpin_object(MacroAssembler* masm,\n-                             Register thread, VMRegPair reg) {\n-  __ block_comment(\"gen_unpin_object {\");\n-  Label is_null;\n-\n-  \/\/ temp register\n-  __ push(rax);\n-  Register tmp_reg = rax;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-\n-  simple_move32(masm, reg, tmp);\n-\n-  __ testptr(rax, rax);\n-  __ jccb(Assembler::equal, is_null);\n-\n-  \/\/ Save registers that may be used by runtime call\n-  Register arg = reg.first()->is_Register() ? reg.first()->as_Register() : noreg;\n-  save_registers_except(masm, arg, thread);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),\n-    thread, rax);\n-\n-  \/\/ Restore saved registers\n-  restore_registers_except(masm, arg, thread);\n-  __ bind(is_null);\n-  __ pop(rax);\n-  __ block_comment(\"} gen_unpin_object\");\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               Register thread,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);\n-  __ jcc(Assembler::equal, cont);\n-\n-  \/\/ Save down any incoming oops and call into the runtime to halt for a GC\n-\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  address the_pc = __ pc();\n-  oop_maps->add_gc_map( __ offset(), map);\n-  __ set_last_Java_frame(thread, rsp, noreg, the_pc);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ push(thread);\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));\n-  __ increment(rsp, wordSize);\n-\n-  __ get_thread(thread);\n-  __ reset_last_Java_frame(thread, false);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ bind(cont);\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers\n-    for (int i = 0; i < total_in_args - 1; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ xorptr(reg, reg);\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        __ xorpd(in_regs[i].first()->as_XMMRegister(), in_regs[i].first()->as_XMMRegister());\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      } else if (in_regs[i].first()->is_stack()) {\n-        \/\/ Nothing to do\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {\n-        i++;\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1608,4 +1349,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1615,11 +1355,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n@@ -1937,5 +1666,0 @@\n-  if (is_critical_native && !Universe::heap()->supports_object_pinning()) {\n-    check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -1975,5 +1699,0 @@\n-  \/\/ Inbound arguments that need to be pinned for critical natives\n-  GrowableArray<int> pinned_args(total_in_args);\n-  \/\/ Current stack slot for storing register based array argument\n-  int pinned_slot = oop_handle_offset;\n-\n@@ -1992,20 +1711,0 @@\n-          if (Universe::heap()->supports_object_pinning()) {\n-            \/\/ gen_pin_object handles save and restore\n-            \/\/ of any clobbered registers\n-            gen_pin_object(masm, thread, in_arg);\n-            pinned_args.append(i);\n-\n-            \/\/ rax has pinned array\n-            VMRegPair result_reg(rax->as_VMReg());\n-            if (!in_arg.first()->is_stack()) {\n-              assert(pinned_slot <= stack_slots, \"overflow\");\n-              simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));\n-              pinned_slot += VMRegImpl::slots_per_word;\n-            } else {\n-              \/\/ Write back pinned value, it will be used to unpin this argument\n-              __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()->as_Register());\n-            }\n-            \/\/ We have the array in register, use it\n-            in_arg = result_reg;\n-          }\n-\n@@ -2166,1 +1865,0 @@\n-\n@@ -2171,3 +1869,3 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -2205,18 +1903,11 @@\n-  \/\/ unpin pinned arguments\n-  pinned_slot = oop_handle_offset;\n-  if (pinned_args.length() > 0) {\n-    \/\/ save return value that may be overwritten otherwise.\n-    save_native_result(masm, ret_type, stack_slots);\n-    for (int index = 0; index < pinned_args.length(); index ++) {\n-      int i = pinned_args.at(index);\n-      assert(pinned_slot <= stack_slots, \"overflow\");\n-      if (!in_regs[i].first()->is_stack()) {\n-        int offset = pinned_slot * VMRegImpl::stack_slot_size;\n-        __ movl(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        pinned_slot += VMRegImpl::slots_per_word;\n-      }\n-      \/\/ gen_pin_object handles save and restore\n-      \/\/ of any other clobbered registers\n-      gen_unpin_object(masm, thread, in_regs[i]);\n-    }\n-    restore_native_result(masm, ret_type, stack_slots);\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n@@ -2244,2 +1935,0 @@\n-  Label after_transition;\n-\n@@ -2265,2 +1954,1 @@\n-    if (!is_critical_native) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n+    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n@@ -2268,4 +1956,0 @@\n-    } else {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n-                                              JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n@@ -2275,7 +1959,0 @@\n-\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic below.\n-      __ jmpb(after_transition);\n-    }\n-\n@@ -2522,4 +2199,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":18,"deletions":345,"binary":false,"changes":363,"status":"modified"},{"patch":"@@ -1406,216 +1406,0 @@\n-\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ if map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Save down double word first\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_XMMRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movdbl(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-      } else {\n-        __ movdbl(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-      }\n-    }\n-    if (in_regs[i].first()->is_Register() &&\n-        (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      if (map != NULL) {\n-        __ movq(Address(rsp, offset), in_regs[i].first()->as_Register());\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));;\n-        }\n-      } else {\n-        __ movq(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-    }\n-  }\n-  \/\/ Save or restore single word registers\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot++;\n-      assert(slot <= stack_slots, \"overflow\");\n-\n-      \/\/ Value is in an input register pass we must flush it to the stack\n-      const Register reg = in_regs[i].first()->as_Register();\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ movl(Address(rsp, offset), reg);\n-          } else {\n-            __ movl(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_XMMRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow\");\n-        if (map != NULL) {\n-          __ movflt(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-        } else {\n-          __ movflt(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Pin object, return pinned object or null in rax\n-static void gen_pin_object(MacroAssembler* masm,\n-                           VMRegPair reg) {\n-  __ block_comment(\"gen_pin_object {\");\n-\n-  \/\/ rax always contains oop, either incoming or\n-  \/\/ pinned.\n-  Register tmp_reg = rax;\n-\n-  Label is_null;\n-  VMRegPair tmp;\n-  VMRegPair in_reg = reg;\n-\n-  tmp.set_ptr(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    move_ptr(masm, reg, tmp);\n-    reg = tmp;\n-  } else {\n-    __ movptr(rax, reg.first()->as_Register());\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-\n-  if (reg.first()->as_Register() != c_rarg1) {\n-    __ movptr(c_rarg1, reg.first()->as_Register());\n-  }\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),\n-    r15_thread, c_rarg1);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_pin_object\");\n-}\n-\n-\/\/ Unpin object\n-static void gen_unpin_object(MacroAssembler* masm,\n-                             VMRegPair reg) {\n-  __ block_comment(\"gen_unpin_object {\");\n-  Label is_null;\n-\n-  if (reg.first()->is_stack()) {\n-    __ movptr(c_rarg1, Address(rbp, reg2offset_in(reg.first())));\n-  } else if (reg.first()->as_Register() != c_rarg1) {\n-    __ movptr(c_rarg1, reg.first()->as_Register());\n-  }\n-\n-  __ testptr(c_rarg1, c_rarg1);\n-  __ jccb(Assembler::equal, is_null);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),\n-    r15_thread, c_rarg1);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_unpin_object\");\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);\n-  __ jcc(Assembler::equal, cont);\n-\n-  \/\/ Save down any incoming oops and call into the runtime to halt for a GC\n-\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  address the_pc = __ pc();\n-  oop_maps->add_gc_map( __ offset(), map);\n-  __ set_last_Java_frame(rsp, noreg, the_pc);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ movptr(c_rarg0, r15_thread);\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  __ reset_last_Java_frame(false);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-  __ bind(cont);\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers\n-    for (int i = 0; i < total_in_args - 1; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ xorptr(reg, reg);\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        __ xorpd(in_regs[i].first()->as_XMMRegister(), in_regs[i].first()->as_XMMRegister());\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      } else if (in_regs[i].first()->is_stack()) {\n-        \/\/ Nothing to do\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {\n-        i++;\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1926,4 +1710,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1933,12 +1716,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -2245,5 +2016,0 @@\n-  if (is_critical_native && !Universe::heap()->supports_object_pinning()) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -2302,4 +2068,0 @@\n-  \/\/ Inbound arguments that need to be pinned for critical natives\n-  GrowableArray<int> pinned_args(total_in_args);\n-  \/\/ Current stack slot for storing register based array argument\n-  int pinned_slot = oop_handle_offset;\n@@ -2354,17 +2116,0 @@\n-          \/\/ pin before unpack\n-          if (Universe::heap()->supports_object_pinning()) {\n-            save_args(masm, total_c_args, 0, out_regs);\n-            gen_pin_object(masm, in_regs[i]);\n-            pinned_args.append(i);\n-            restore_args(masm, total_c_args, 0, out_regs);\n-\n-            \/\/ rax has pinned array\n-            VMRegPair result_reg;\n-            result_reg.set_ptr(rax->as_VMReg());\n-            move_ptr(masm, result_reg, in_regs[i]);\n-            if (!in_regs[i].first()->is_stack()) {\n-              assert(pinned_slot <= stack_slots, \"overflow\");\n-              move_ptr(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));\n-              pinned_slot += VMRegImpl::slots_per_word;\n-            }\n-          }\n@@ -2549,1 +2294,0 @@\n-\n@@ -2552,1 +2296,0 @@\n-\n@@ -2556,3 +2299,3 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -2585,16 +2328,11 @@\n-  \/\/ unpin pinned arguments\n-  pinned_slot = oop_handle_offset;\n-  if (pinned_args.length() > 0) {\n-    \/\/ save return value that may be overwritten otherwise.\n-    save_native_result(masm, ret_type, stack_slots);\n-    for (int index = 0; index < pinned_args.length(); index ++) {\n-      int i = pinned_args.at(index);\n-      assert(pinned_slot <= stack_slots, \"overflow\");\n-      if (!in_regs[i].first()->is_stack()) {\n-        int offset = pinned_slot * VMRegImpl::stack_slot_size;\n-        __ movq(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        pinned_slot += VMRegImpl::slots_per_word;\n-      }\n-      gen_unpin_object(masm, in_regs[i]);\n-    }\n-    restore_native_result(masm, ret_type, stack_slots);\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, r15_thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n@@ -2617,2 +2355,0 @@\n-  Label after_transition;\n-\n@@ -2642,5 +2378,1 @@\n-    if (!is_critical_native) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2651,7 +2383,0 @@\n-\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic below.\n-      __ jmpb(after_transition);\n-    }\n-\n@@ -2881,5 +2606,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":18,"deletions":298,"binary":false,"changes":316,"status":"modified"},{"patch":"@@ -134,3 +134,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -153,1 +151,0 @@\n-\/\/ This register class can be used for implicit null checks on win95.\n@@ -1463,51 +1460,0 @@\n-\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n-  \/\/ Get the memory operand from the node\n-  uint numopnds = node->num_opnds();        \/\/ Virtual call for number of operands\n-  uint skipped  = node->oper_input_base();  \/\/ Sum of leaves skipped so far\n-  assert( idx >= skipped, \"idx too low in pd_implicit_null_fixup\" );\n-  uint opcnt     = 1;                 \/\/ First operand\n-  uint num_edges = node->_opnds[1]->num_edges(); \/\/ leaves for first operand\n-  while( idx >= skipped+num_edges ) {\n-    skipped += num_edges;\n-    opcnt++;                          \/\/ Bump operand count\n-    assert( opcnt < numopnds, \"Accessing non-existent operand\" );\n-    num_edges = node->_opnds[opcnt]->num_edges(); \/\/ leaves for next operand\n-  }\n-\n-  MachOper *memory = node->_opnds[opcnt];\n-  MachOper *new_memory = NULL;\n-  switch (memory->opcode()) {\n-  case DIRECT:\n-  case INDOFFSET32X:\n-    \/\/ No transformation necessary.\n-    return;\n-  case INDIRECT:\n-    new_memory = new indirect_win95_safeOper( );\n-    break;\n-  case INDOFFSET8:\n-    new_memory = new indOffset8_win95_safeOper(memory->disp(NULL, NULL, 0));\n-    break;\n-  case INDOFFSET32:\n-    new_memory = new indOffset32_win95_safeOper(memory->disp(NULL, NULL, 0));\n-    break;\n-  case INDINDEXOFFSET:\n-    new_memory = new indIndexOffset_win95_safeOper(memory->disp(NULL, NULL, 0));\n-    break;\n-  case INDINDEXSCALE:\n-    new_memory = new indIndexScale_win95_safeOper(memory->scale());\n-    break;\n-  case INDINDEXSCALEOFFSET:\n-    new_memory = new indIndexScaleOffset_win95_safeOper(memory->scale(), memory->disp(NULL, NULL, 0));\n-    break;\n-  case LOAD_LONG_INDIRECT:\n-  case LOAD_LONG_INDOFFSET32:\n-    \/\/ Does not use EBP as address register, use { EDX, EBX, EDI, ESI}\n-    return;\n-  default:\n-    assert(false, \"unexpected memory operand in pd_implicit_null_fixup()\");\n-    return;\n-  }\n-  node->_opnds[opcnt] = new_memory;\n-}\n-\n@@ -3207,1 +3153,0 @@\n-  interpreter_method_reg(EBX);          \/\/ Method Register when calling interpreter\n@@ -4421,92 +4366,0 @@\n-\/\/----------Memory Operands - Win95 Implicit Null Variants----------------\n-\/\/ Indirect Memory Operand\n-operand indirect_win95_safe(eRegP_no_EBP reg)\n-%{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(reg);\n-\n-  op_cost(100);\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Short Offset Operand\n-operand indOffset8_win95_safe(eRegP_no_EBP reg, immI8 off)\n-%{\n-  match(AddP reg off);\n-\n-  op_cost(100);\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand indOffset32_win95_safe(eRegP_no_EBP reg, immI off)\n-%{\n-  match(AddP reg off);\n-\n-  op_cost(100);\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndexOffset_win95_safe(eRegP_no_EBP reg, rRegI ireg, immI off)\n-%{\n-  match(AddP (AddP reg ireg) off);\n-\n-  op_cost(100);\n-  format %{\"[$reg + $off + $ireg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register\n-operand indIndexScale_win95_safe(eRegP_no_EBP reg, rRegI ireg, immI2 scale)\n-%{\n-  match(AddP reg (LShiftI ireg scale));\n-\n-  op_cost(100);\n-  format %{\"[$reg + $ireg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale($scale);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n-operand indIndexScaleOffset_win95_safe(eRegP_no_EBP reg, immI off, rRegI ireg, immI2 scale)\n-%{\n-  match(AddP (AddP reg (LShiftI ireg scale)) off);\n-\n-  op_cost(100);\n-  format %{\"[$reg + $off + $ireg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":1,"deletions":148,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -164,3 +164,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -1681,3 +1679,0 @@\n-\/\/ No-op on amd64\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {}\n-\n@@ -2762,2 +2757,0 @@\n-  interpreter_method_reg(RBX);          \/\/ Method Register when\n-                                        \/\/ calling interpreter\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -796,1 +796,7 @@\n-          \"Do not use subtype check macro node\")\n+          \"Do not use subtype check macro node\")                            \\\n+                                                                            \\\n+  develop(uintx, StressLongCountedLoop, 0,                                  \\\n+          \"if > 0, convert int counted loops to long counted loops\"         \\\n+          \"to stress handling of long counted loops: run inner loop\"        \\\n+          \"for at most jint_max \/ StressLongCountedLoop\")                   \\\n+          range(0, max_juint)                                               \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -983,0 +983,40 @@\n+\n+void CallJavaNode::copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt) {\n+  \/\/ Copy debug information and adjust JVMState information\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain()->cnt();\n+  int jvms_adj  = new_dbg_start - old_dbg_start;\n+  assert (new_dbg_start == req(), \"argument count mismatch\");\n+  Compile* C = phase->C;\n+\n+  \/\/ SafePointScalarObject node could be referenced several times in debug info.\n+  \/\/ Use Dict to record cloned nodes.\n+  Dict* sosn_map = new Dict(cmpkey,hashkey);\n+  for (uint i = old_dbg_start; i < sfpt->req(); i++) {\n+    Node* old_in = sfpt->in(i);\n+    \/\/ Clone old SafePointScalarObjectNodes, adjusting their field contents.\n+    if (old_in != NULL && old_in->is_SafePointScalarObject()) {\n+      SafePointScalarObjectNode* old_sosn = old_in->as_SafePointScalarObject();\n+      bool new_node;\n+      Node* new_in = old_sosn->clone(sosn_map, new_node);\n+      if (new_node) { \/\/ New node?\n+        new_in->set_req(0, C->root()); \/\/ reset control edge\n+        new_in = phase->transform(new_in); \/\/ Register new node.\n+      }\n+      old_in = new_in;\n+    }\n+    add_req(old_in);\n+  }\n+\n+  \/\/ JVMS may be shared so clone it before we modify it\n+  set_jvms(sfpt->jvms() != NULL ? sfpt->jvms()->clone_deep(C) : NULL);\n+  for (JVMState *jvms = this->jvms(); jvms != NULL; jvms = jvms->caller()) {\n+    jvms->set_map(this);\n+    jvms->set_locoff(jvms->locoff()+jvms_adj);\n+    jvms->set_stkoff(jvms->stkoff()+jvms_adj);\n+    jvms->set_monoff(jvms->monoff()+jvms_adj);\n+    jvms->set_scloff(jvms->scloff()+jvms_adj);\n+    jvms->set_endoff(jvms->endoff()+jvms_adj);\n+  }\n+}\n+\n@@ -1237,1 +1277,3 @@\n-  if( in(0)->is_Proj() ) {\n+  \/\/ Transforming long counted loops requires a safepoint node. Do not\n+  \/\/ eliminate a safepoint until loop opts are over.\n+  if (in(0)->is_Proj() && !phase->C->major_progress()) {\n@@ -1410,1 +1452,1 @@\n-SafePointScalarObjectNode::clone(Dict* sosn_map) const {\n+SafePointScalarObjectNode::clone(Dict* sosn_map, bool& new_node) const {\n@@ -1413,0 +1455,1 @@\n+    new_node = false;\n@@ -1415,0 +1458,1 @@\n+  new_node = true;\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":46,"deletions":2,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -333,1 +333,2 @@\n-      _adr_type(adr_type)\n+      _adr_type(adr_type),\n+      _has_ea_local_in_scope(false)\n@@ -341,0 +342,1 @@\n+  bool            _has_ea_local_in_scope; \/\/ NoEscape or ArgEscape objects in JVM States\n@@ -460,0 +462,6 @@\n+  void set_has_ea_local_in_scope(bool b) {\n+    _has_ea_local_in_scope = b;\n+  }\n+  bool has_ea_local_in_scope() const {\n+    return _has_ea_local_in_scope;\n+  }\n@@ -531,1 +539,1 @@\n-  SafePointScalarObjectNode* clone(Dict* sosn_map) const;\n+  SafePointScalarObjectNode* clone(Dict* sosn_map, bool& new_node) const;\n@@ -639,0 +647,2 @@\n+  virtual void copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt) {}\n+\n@@ -660,0 +670,1 @@\n+  bool    _arg_escape;             \/\/ ArgEscape in parameter list\n@@ -667,1 +678,2 @@\n-      _method(method), _bci(bci)\n+      _method(method),\n+      _arg_escape(false), _bci(bci)\n@@ -681,0 +693,3 @@\n+  void  set_arg_escape(bool f)             { _arg_escape = f; }\n+  bool  arg_escape() const                 { return _arg_escape; }\n+  void copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":19,"deletions":4,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -122,3 +122,3 @@\n-  for (int i = 1; i < _intrinsics->length(); i++) {\n-    CallGenerator* cg1 = _intrinsics->at(i-1);\n-    CallGenerator* cg2 = _intrinsics->at(i);\n+  for (int i = 1; i < _intrinsics.length(); i++) {\n+    CallGenerator* cg1 = _intrinsics.at(i-1);\n+    CallGenerator* cg2 = _intrinsics.at(i);\n@@ -132,1 +132,1 @@\n-  return _intrinsics->find_sorted<IntrinsicDescPair*, IntrinsicDescPair::compare>(&pair, found);\n+  return _intrinsics.find_sorted<IntrinsicDescPair*, IntrinsicDescPair::compare>(&pair, found);\n@@ -136,4 +136,0 @@\n-  if (_intrinsics == NULL) {\n-    _intrinsics = new (comp_arena())GrowableArray<CallGenerator*>(comp_arena(), 60, 0, NULL);\n-  }\n-  int len = _intrinsics->length();\n@@ -143,1 +139,1 @@\n-  _intrinsics->insert_before(index, cg);\n+  _intrinsics.insert_before(index, cg);\n@@ -149,1 +145,1 @@\n-  if (_intrinsics != NULL) {\n+  if (_intrinsics.length() > 0) {\n@@ -153,1 +149,1 @@\n-      return _intrinsics->at(index);\n+      return _intrinsics.at(index);\n@@ -171,3 +167,1 @@\n-\/\/ Compile:: register_library_intrinsics and make_vm_intrinsic are defined\n-\/\/ in library_call.cpp.\n-\n+\/\/ Compile::make_vm_intrinsic is defined in library_call.cpp.\n@@ -372,29 +366,6 @@\n-void Compile::remove_useless_node(Node* dead) {\n-  remove_modified_node(dead);\n-\n-  \/\/ Constant node that has no out-edges and has only one in-edge from\n-  \/\/ root is usually dead. However, sometimes reshaping walk makes\n-  \/\/ it reachable by adding use edges. So, we will NOT count Con nodes\n-  \/\/ as dead to be conservative about the dead node count at any\n-  \/\/ given time.\n-  if (!dead->is_Con()) {\n-    record_dead_node(dead->_idx);\n-  }\n-  if (dead->is_macro()) {\n-    remove_macro_node(dead);\n-  }\n-  if (dead->is_expensive()) {\n-    remove_expensive_node(dead);\n-  }\n-  CastIINode* cast = dead->isa_CastII();\n-  if (cast != NULL && cast->has_range_check()) {\n-    remove_range_check_cast(cast);\n-  }\n-  if (dead->Opcode() == Op_Opaque4) {\n-    remove_opaque4_node(dead);\n-  }\n-  if (dead->is_Call()) {\n-    remove_useless_late_inlines(&_string_late_inlines, dead);\n-    remove_useless_late_inlines(&_boxing_late_inlines, dead);\n-    remove_useless_late_inlines(&_late_inlines, dead);\n-    remove_useless_late_inlines(&_vector_reboxing_late_inlines, dead);\n+void Compile::remove_useless_nodes(GrowableArray<Node*>& node_list, Unique_Node_List& useful) {\n+  for (int i = node_list.length() - 1; i >= 0; i--) {\n+    Node* n = node_list.at(i);\n+    if (!useful.member(n)) {\n+      node_list.remove_if_existing(n);\n+    }\n@@ -402,2 +373,0 @@\n-  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-  bs->unregister_potential_barrier_node(dead);\n@@ -420,1 +389,1 @@\n-      if (! useful.member(child)) {\n+      if (!useful.member(child)) {\n@@ -433,28 +402,5 @@\n-  \/\/ Remove useless macro and predicate opaq nodes\n-  for (int i = C->macro_count()-1; i >= 0; i--) {\n-    Node* n = C->macro_node(i);\n-    if (!useful.member(n)) {\n-      remove_macro_node(n);\n-    }\n-  }\n-  \/\/ Remove useless CastII nodes with range check dependency\n-  for (int i = range_check_cast_count() - 1; i >= 0; i--) {\n-    Node* cast = range_check_cast_node(i);\n-    if (!useful.member(cast)) {\n-      remove_range_check_cast(cast);\n-    }\n-  }\n-  \/\/ Remove useless expensive nodes\n-  for (int i = C->expensive_count()-1; i >= 0; i--) {\n-    Node* n = C->expensive_node(i);\n-    if (!useful.member(n)) {\n-      remove_expensive_node(n);\n-    }\n-  }\n-  \/\/ Remove useless Opaque4 nodes\n-  for (int i = opaque4_count() - 1; i >= 0; i--) {\n-    Node* opaq = opaque4_node(i);\n-    if (!useful.member(opaq)) {\n-      remove_opaque4_node(opaq);\n-    }\n-  }\n+\n+  remove_useless_nodes(_macro_nodes,        useful); \/\/ remove useless macro and predicate opaq nodes\n+  remove_useless_nodes(_expensive_nodes,    useful); \/\/ remove useless expensive nodes\n+  remove_useless_nodes(_for_post_loop_igvn, useful); \/\/ remove useless node recorded for post loop opts IGVN pass\n+\n@@ -570,0 +516,1 @@\n+                  _post_loop_opts_phase(false),\n@@ -587,0 +534,5 @@\n+                  _intrinsics        (comp_arena(), 0, 0, NULL),\n+                  _macro_nodes       (comp_arena(), 8, 0, NULL),\n+                  _predicate_opaqs   (comp_arena(), 8, 0, NULL),\n+                  _expensive_nodes   (comp_arena(), 8, 0, NULL),\n+                  _for_post_loop_igvn(comp_arena(), 8, 0, NULL),\n@@ -871,0 +823,1 @@\n+    _post_loop_opts_phase(false),\n@@ -1069,7 +1022,0 @@\n-  _intrinsics = NULL;\n-  _macro_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _predicate_opaqs = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _expensive_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _range_check_casts = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);\n-  register_library_intrinsics();\n@@ -1846,12 +1792,5 @@\n-void Compile::add_range_check_cast(Node* n) {\n-  assert(n->isa_CastII()->has_range_check(), \"CastII should have range check dependency\");\n-  assert(!_range_check_casts->contains(n), \"duplicate entry in range check casts\");\n-  _range_check_casts->append(n);\n-}\n-\n-\/\/ Remove all range check dependent CastIINodes.\n-void Compile::remove_range_check_casts(PhaseIterGVN &igvn) {\n-  for (int i = range_check_cast_count(); i > 0; i--) {\n-    Node* cast = range_check_cast_node(i-1);\n-    assert(cast->isa_CastII()->has_range_check(), \"CastII should have range check dependency\");\n-    igvn.replace_node(cast, cast->in(1));\n+void Compile::record_for_post_loop_opts_igvn(Node* n) {\n+  if (!n->for_post_loop_opts_igvn()) {\n+    assert(!_for_post_loop_igvn.contains(n), \"duplicate\");\n+    n->add_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n+    _for_post_loop_igvn.append(n);\n@@ -1859,1 +1798,0 @@\n-  assert(range_check_cast_count() == 0, \"should be empty\");\n@@ -1862,4 +1800,3 @@\n-void Compile::add_opaque4_node(Node* n) {\n-  assert(n->Opcode() == Op_Opaque4, \"Opaque4 only\");\n-  assert(!_opaque4_nodes->contains(n), \"duplicate entry in Opaque4 list\");\n-  _opaque4_nodes->append(n);\n+void Compile::remove_from_post_loop_opts_igvn(Node* n) {\n+  n->remove_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n+  _for_post_loop_igvn.remove(n);\n@@ -1868,16 +1805,8 @@\n-\/\/ Remove all Opaque4 nodes.\n-void Compile::remove_opaque4_nodes(PhaseIterGVN &igvn) {\n-  for (int i = opaque4_count(); i > 0; i--) {\n-    Node* opaq = opaque4_node(i-1);\n-    assert(opaq->Opcode() == Op_Opaque4, \"Opaque4 only\");\n-    \/\/ With Opaque4 nodes, the expectation is that the test of input 1\n-    \/\/ is always equal to the constant value of input 2. So we can\n-    \/\/ remove the Opaque4 and replace it by input 2. In debug builds,\n-    \/\/ leave the non constant test in instead to sanity check that it\n-    \/\/ never fails (if it does, that subgraph was constructed so, at\n-    \/\/ runtime, a Halt node is executed).\n-#ifdef ASSERT\n-    igvn.replace_node(opaq, opaq->in(1));\n-#else\n-    igvn.replace_node(opaq, opaq->in(2));\n-#endif\n+void Compile::process_for_post_loop_opts_igvn(PhaseIterGVN& igvn) {\n+  if (_for_post_loop_igvn.length() == 0) {\n+    return; \/\/ no work to do\n+  }\n+  while (_for_post_loop_igvn.length() > 0) {\n+    Node* n = _for_post_loop_igvn.pop();\n+    n->remove_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n+    igvn._worklist.push(n);\n@@ -1885,1 +1814,2 @@\n-  assert(opaque4_count() == 0, \"should be empty\");\n+  igvn.optimize();\n+  assert(_for_post_loop_igvn.length() == 0, \"no more delayed nodes allowed\");\n@@ -2056,1 +1986,1 @@\n-  if(_loop_opts_cnt > 0) {\n+  if (_loop_opts_cnt > 0) {\n@@ -2058,1 +1988,1 @@\n-    while(major_progress() && (_loop_opts_cnt > 0)) {\n+    while (major_progress() && (_loop_opts_cnt > 0)) {\n@@ -2189,0 +2119,1 @@\n+    Unique_Node_List* save_for_igvn = for_igvn();\n@@ -2301,6 +2232,0 @@\n-  if (range_check_cast_count() > 0) {\n-    \/\/ No more loop optimizations. Remove all range check dependent CastIINodes.\n-    C->remove_range_check_casts(igvn);\n-    igvn.optimize();\n-  }\n-\n@@ -2330,4 +2255,3 @@\n-  if (opaque4_count() > 0) {\n-    C->remove_opaque4_nodes(igvn);\n-    igvn.optimize();\n-  }\n+  C->set_post_loop_opts_phase(); \/\/ no more loop opts allowed\n+\n+  process_for_post_loop_opts_igvn(igvn);\n@@ -2366,0 +2290,1 @@\n+  assert(igvn._worklist.size() == 0, \"not empty\");\n@@ -3514,0 +3439,1 @@\n+    assert(!n->as_Loop()->is_transformed_long_loop() || _loop_opts_cnt == 0, \"should have been turned into a counted loop\");\n@@ -3768,1 +3694,1 @@\n-    _expensive_nodes->at(i)->set_req(0, NULL);\n+    _expensive_nodes.at(i)->set_req(0, NULL);\n@@ -4189,2 +4115,0 @@\n-    \/\/ Save CastII node to remove it after loop optimizations.\n-    phase->C->add_range_check_cast(value);\n@@ -4422,1 +4346,1 @@\n-    _expensive_nodes->sort(cmp_expensive_nodes);\n+    _expensive_nodes.sort(cmp_expensive_nodes);\n@@ -4427,2 +4351,2 @@\n-  for (int i = 1; i < _expensive_nodes->length(); i++) {\n-    if (cmp_expensive_nodes(_expensive_nodes->adr_at(i), _expensive_nodes->adr_at(i-1)) < 0) {\n+  for (int i = 1; i < _expensive_nodes.length(); i++) {\n+    if (cmp_expensive_nodes(_expensive_nodes.adr_at(i), _expensive_nodes.adr_at(i-1)) < 0) {\n@@ -4436,1 +4360,1 @@\n-  if (_expensive_nodes->length() == 0) {\n+  if (_expensive_nodes.length() == 0) {\n@@ -4444,2 +4368,2 @@\n-  for (int i = 0; i < _expensive_nodes->length(); i++) {\n-    Node* n = _expensive_nodes->at(i);\n+  for (int i = 0; i < _expensive_nodes.length(); i++) {\n+    Node* n = _expensive_nodes.at(i);\n@@ -4448,1 +4372,1 @@\n-      _expensive_nodes->at_put(j, n);\n+      _expensive_nodes.at_put(j, n);\n@@ -4452,1 +4376,1 @@\n-  _expensive_nodes->trunc_to(j);\n+  _expensive_nodes.trunc_to(j);\n@@ -4459,2 +4383,2 @@\n-  for (int i = 0; i < _expensive_nodes->length()-1; i++) {\n-    if (cmp_expensive_nodes(_expensive_nodes->adr_at(i), _expensive_nodes->adr_at(i+1)) == 0) {\n+  for (int i = 0; i < _expensive_nodes.length()-1; i++) {\n+    if (cmp_expensive_nodes(_expensive_nodes.adr_at(i), _expensive_nodes.adr_at(i+1)) == 0) {\n@@ -4469,1 +4393,1 @@\n-  if (_expensive_nodes->length() == 0) {\n+  if (_expensive_nodes.length() == 0) {\n@@ -4483,1 +4407,1 @@\n-  for (; i < _expensive_nodes->length()-1; i++) {\n+  for (; i < _expensive_nodes.length()-1; i++) {\n@@ -4485,1 +4409,1 @@\n-    if (_expensive_nodes->at(i)->Opcode() == _expensive_nodes->at(i+1)->Opcode()) {\n+    if (_expensive_nodes.at(i)->Opcode() == _expensive_nodes.at(i+1)->Opcode()) {\n@@ -4487,1 +4411,1 @@\n-      _expensive_nodes->at_put(j++, _expensive_nodes->at(i));\n+      _expensive_nodes.at_put(j++, _expensive_nodes.at(i));\n@@ -4491,1 +4415,1 @@\n-      _expensive_nodes->at_put(j++, _expensive_nodes->at(i));\n+      _expensive_nodes.at_put(j++, _expensive_nodes.at(i));\n@@ -4494,1 +4418,1 @@\n-      Node* n = _expensive_nodes->at(i);\n+      Node* n = _expensive_nodes.at(i);\n@@ -4501,3 +4425,3 @@\n-    _expensive_nodes->at_put(j++, _expensive_nodes->at(i));\n-  } else if (_expensive_nodes->length() >= 1) {\n-    Node* n = _expensive_nodes->at(i);\n+    _expensive_nodes.at_put(j++, _expensive_nodes.at(i));\n+  } else if (_expensive_nodes.length() >= 1) {\n+    Node* n = _expensive_nodes.at(i);\n@@ -4508,1 +4432,1 @@\n-  _expensive_nodes->trunc_to(j);\n+  _expensive_nodes.trunc_to(j);\n@@ -4515,1 +4439,1 @@\n-  assert(!_expensive_nodes->contains(n), \"duplicate entry in expensive list\");\n+  assert(!_expensive_nodes.contains(n), \"duplicate entry in expensive list\");\n@@ -4519,1 +4443,1 @@\n-    _expensive_nodes->append(n);\n+    _expensive_nodes.append(n);\n@@ -4697,2 +4621,2 @@\n-        _macro_nodes->at_put(allocates, n);\n-        _macro_nodes->at_put(i, tmp);\n+        _macro_nodes.at_put(allocates, n);\n+        _macro_nodes.at_put(i, tmp);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":78,"deletions":154,"binary":false,"changes":232,"status":"modified"},{"patch":"@@ -265,0 +265,2 @@\n+  bool                  _post_loop_opts_phase;  \/\/ Loop opts are finished.\n+\n@@ -312,6 +314,5 @@\n-  GrowableArray<CallGenerator*>* _intrinsics;   \/\/ List of intrinsics.\n-  GrowableArray<Node*>* _macro_nodes;           \/\/ List of nodes which need to be expanded before matching.\n-  GrowableArray<Node*>* _predicate_opaqs;       \/\/ List of Opaque1 nodes for the loop predicates.\n-  GrowableArray<Node*>* _expensive_nodes;       \/\/ List of nodes that are expensive to compute and that we'd better not let the GVN freely common\n-  GrowableArray<Node*>* _range_check_casts;     \/\/ List of CastII nodes with a range check dependency\n-  GrowableArray<Node*>* _opaque4_nodes;         \/\/ List of Opaque4 nodes that have a default value\n+  GrowableArray<CallGenerator*> _intrinsics;    \/\/ List of intrinsics.\n+  GrowableArray<Node*>  _macro_nodes;           \/\/ List of nodes which need to be expanded before matching.\n+  GrowableArray<Node*>  _predicate_opaqs;       \/\/ List of Opaque1 nodes for the loop predicates.\n+  GrowableArray<Node*>  _expensive_nodes;       \/\/ List of nodes that are expensive to compute and that we'd better not let the GVN freely common\n+  GrowableArray<Node*>  _for_post_loop_igvn;    \/\/ List of nodes for IGVN after loop opts are over\n@@ -379,2 +380,1 @@\n-  GrowableArray<CallGenerator*> _late_inlines;        \/\/ List of CallGenerators to be revisited after\n-                                                      \/\/ main parsing has finished.\n+  GrowableArray<CallGenerator*> _late_inlines;        \/\/ List of CallGenerators to be revisited after main parsing has finished.\n@@ -382,1 +382,0 @@\n-\n@@ -662,6 +661,8 @@\n-  int           macro_count()             const { return _macro_nodes->length(); }\n-  int           predicate_count()         const { return _predicate_opaqs->length();}\n-  int           expensive_count()         const { return _expensive_nodes->length(); }\n-  Node*         macro_node(int idx)       const { return _macro_nodes->at(idx); }\n-  Node*         predicate_opaque1_node(int idx) const { return _predicate_opaqs->at(idx);}\n-  Node*         expensive_node(int idx)   const { return _expensive_nodes->at(idx); }\n+  int           macro_count()             const { return _macro_nodes.length(); }\n+  int           predicate_count()         const { return _predicate_opaqs.length();}\n+  int           expensive_count()         const { return _expensive_nodes.length(); }\n+\n+  Node*         macro_node(int idx)       const { return _macro_nodes.at(idx); }\n+  Node*         predicate_opaque1_node(int idx) const { return _predicate_opaqs.at(idx);}\n+  Node*         expensive_node(int idx)   const { return _expensive_nodes.at(idx); }\n+\n@@ -672,2 +673,2 @@\n-    assert(!_macro_nodes->contains(n), \"duplicate entry in expand list\");\n-    _macro_nodes->append(n);\n+    assert(!_macro_nodes.contains(n), \"duplicate entry in expand list\");\n+    _macro_nodes.append(n);\n@@ -678,1 +679,1 @@\n-    _macro_nodes->remove_if_existing(n);\n+    _macro_nodes.remove_if_existing(n);\n@@ -681,1 +682,1 @@\n-      _predicate_opaqs->remove_if_existing(n);\n+      _predicate_opaqs.remove_if_existing(n);\n@@ -686,1 +687,1 @@\n-    _expensive_nodes->remove_if_existing(n);\n+    _expensive_nodes.remove_if_existing(n);\n@@ -689,3 +690,3 @@\n-    assert(!_predicate_opaqs->contains(n), \"duplicate entry in predicate opaque1\");\n-    assert(_macro_nodes->contains(n), \"should have already been in macro list\");\n-    _predicate_opaqs->append(n);\n+    assert(!_predicate_opaqs.contains(n), \"duplicate entry in predicate opaque1\");\n+    assert(_macro_nodes.contains(n), \"should have already been in macro list\");\n+    _predicate_opaqs.append(n);\n@@ -694,17 +695,6 @@\n-  \/\/ Range check dependent CastII nodes that can be removed after loop optimizations\n-  void add_range_check_cast(Node* n);\n-  void remove_range_check_cast(Node* n) {\n-    _range_check_casts->remove_if_existing(n);\n-  }\n-  Node* range_check_cast_node(int idx) const { return _range_check_casts->at(idx);  }\n-  int   range_check_cast_count()       const { return _range_check_casts->length(); }\n-  \/\/ Remove all range check dependent CastIINodes.\n-  void  remove_range_check_casts(PhaseIterGVN &igvn);\n-\n-  void add_opaque4_node(Node* n);\n-  void remove_opaque4_node(Node* n) {\n-    _opaque4_nodes->remove_if_existing(n);\n-  }\n-  Node* opaque4_node(int idx) const { return _opaque4_nodes->at(idx);  }\n-  int   opaque4_count()       const { return _opaque4_nodes->length(); }\n-  void  remove_opaque4_nodes(PhaseIterGVN &igvn);\n+  bool     post_loop_opts_phase() { return _post_loop_opts_phase; }\n+  void set_post_loop_opts_phase() { _post_loop_opts_phase = true; }\n+\n+  void record_for_post_loop_opts_igvn(Node* n);\n+  void remove_from_post_loop_opts_igvn(Node* n);\n+  void process_for_post_loop_opts_igvn(PhaseIterGVN& igvn);\n@@ -717,2 +707,2 @@\n-  bool is_predicate_opaq(Node * n) {\n-    return _predicate_opaqs->contains(n);\n+  bool is_predicate_opaq(Node* n) {\n+    return _predicate_opaqs.contains(n);\n@@ -928,2 +918,0 @@\n-  void              remove_useless_node(Node* dead);\n-\n@@ -958,0 +946,1 @@\n+  void remove_useless_nodes       (GrowableArray<Node*>&        node_list, Unique_Node_List &useful);\n@@ -1090,1 +1079,0 @@\n-  void           register_library_intrinsics();                            \/\/ initializer\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":33,"deletions":45,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -126,0 +126,1 @@\n+  GrowableArray<SafePointNode*>  sfn_worklist;\n@@ -191,0 +192,3 @@\n+    if (n-> is_SafePoint()) {\n+      sfn_worklist.append(n->as_SafePoint());\n+    }\n@@ -320,0 +324,19 @@\n+\n+  \/\/ Annotate at safepoints if they have <= ArgEscape objects in their scope and at\n+  \/\/ java calls if they pass ArgEscape objects as parameters.\n+  if (has_non_escaping_obj &&\n+      (C->env()->should_retain_local_variables() ||\n+       C->env()->jvmti_can_get_owned_monitor_info() ||\n+       C->env()->jvmti_can_walk_any_space() ||\n+       DeoptimizeObjectsALot)) {\n+    int sfn_length = sfn_worklist.length();\n+    for (int next = 0; next < sfn_length; next++) {\n+      SafePointNode* sfn = sfn_worklist.at(next);\n+      sfn->set_has_ea_local_in_scope(has_ea_local_in_scope(sfn));\n+      if (sfn->is_CallJava()) {\n+        CallJavaNode* call = sfn->as_CallJava();\n+        call->set_arg_escape(has_arg_escape(call));\n+      }\n+    }\n+  }\n+\n@@ -323,0 +346,61 @@\n+\/\/ Returns true if there is an object in the scope of sfn that does not escape globally.\n+bool ConnectionGraph::has_ea_local_in_scope(SafePointNode* sfn) {\n+  Compile* C = _compile;\n+  for (JVMState* jvms = sfn->jvms(); jvms != NULL; jvms = jvms->caller()) {\n+    if (C->env()->should_retain_local_variables() || C->env()->jvmti_can_walk_any_space() ||\n+        DeoptimizeObjectsALot) {\n+      \/\/ Jvmti agents can access locals. Must provide info about local objects at runtime.\n+      int num_locs = jvms->loc_size();\n+      for (int idx = 0; idx < num_locs; idx++) {\n+        Node* l = sfn->local(jvms, idx);\n+        if (not_global_escape(l)) {\n+          return true;\n+        }\n+      }\n+    }\n+    if (C->env()->jvmti_can_get_owned_monitor_info() ||\n+        C->env()->jvmti_can_walk_any_space() || DeoptimizeObjectsALot) {\n+      \/\/ Jvmti agents can read monitors. Must provide info about locked objects at runtime.\n+      int num_mon = jvms->nof_monitors();\n+      for (int idx = 0; idx < num_mon; idx++) {\n+        Node* m = sfn->monitor_obj(jvms, idx);\n+        if (m != NULL && not_global_escape(m)) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ Returns true if at least one of the arguments to the call is an object\n+\/\/ that does not escape globally.\n+bool ConnectionGraph::has_arg_escape(CallJavaNode* call) {\n+  if (call->method() != NULL) {\n+    uint max_idx = TypeFunc::Parms + call->method()->arg_size();\n+    for (uint idx = TypeFunc::Parms; idx < max_idx; idx++) {\n+      Node* p = call->in(idx);\n+      if (not_global_escape(p)) {\n+        return true;\n+      }\n+    }\n+  } else {\n+    const char* name = call->as_CallStaticJava()->_name;\n+    assert(name != NULL, \"no name\");\n+    \/\/ no arg escapes through uncommon traps\n+    if (strcmp(name, \"uncommon_trap\") != 0) {\n+      \/\/ process_call_arguments() assumes that all arguments escape globally\n+      const TypeTuple* d = call->tf()->domain();\n+      for (uint i = TypeFunc::Parms; i < d->cnt(); i++) {\n+        const Type* at = d->field_at(i);\n+        if (at->isa_oopptr() != NULL) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\n+\n@@ -1005,0 +1089,2 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"sha3_implCompress\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"sha3_implCompressMB\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":86,"deletions":0,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -282,1 +282,1 @@\n-                                        bool long_state, address stubAddr, const char *stubName,\n+                                        const char* state_type, address stubAddr, const char *stubName,\n@@ -284,2 +284,2 @@\n-  Node* get_state_from_digest_object(Node *digestBase_object);\n-  Node* get_long_state_from_digest_object(Node *digestBase_object);\n+  Node* get_state_from_digest_object(Node *digestBase_object, const char* state_type);\n+  Node* get_digest_length_from_digest_object(Node *digestBase_object);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1254,0 +1254,1 @@\n+      mcall_java->_arg_escape = call_java->arg_escape();\n@@ -1278,0 +1279,1 @@\n+  msfpt->_has_ea_local_in_scope = sfpt->has_ea_local_in_scope();\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -421,7 +421,0 @@\n-  static OptoReg::Name  interpreter_method_reg();\n-  static int            interpreter_method_reg_encode();\n-\n-  static OptoReg::Name  compiler_method_reg();\n-  static const RegMask &compiler_method_reg_mask();\n-  static int            compiler_method_reg_encode();\n-\n@@ -545,4 +538,0 @@\n-  \/\/ Perform a platform dependent implicit null fixup.  This is needed\n-  \/\/ on windows95 to take care of some unusual register constraints.\n-  void pd_implicit_null_fixup(MachNode *load, uint idx);\n-\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -519,1 +519,1 @@\n-  if (is_macro())\n+  if (is_macro()) {\n@@ -521,1 +521,2 @@\n-  if (is_expensive())\n+  }\n+  if (is_expensive()) {\n@@ -523,8 +524,4 @@\n-  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-  bs->register_potential_barrier_node(n);\n-  \/\/ If the cloned node is a range check dependent CastII, add it to the list.\n-  CastIINode* cast = n->isa_CastII();\n-  if (cast != NULL && cast->has_range_check()) {\n-    C->add_range_check_cast(cast);\n-  if (n->Opcode() == Op_Opaque4) {\n-    C->add_opaque4_node(n);\n+  if (for_post_loop_opts_igvn()) {\n+    \/\/ Don't add cloned node to Compile::_for_post_loop_opts_igvn list automatically.\n+    \/\/ If it is applicable, it will happen anyway when the cloned node is registered with IGVN.\n+    n->remove_flag(Node::NodeFlags::Flag_for_post_loop_opts_igvn);\n@@ -533,0 +530,2 @@\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bs->register_potential_barrier_node(n);\n@@ -645,6 +644,2 @@\n-  CastIINode* cast = isa_CastII();\n-  if (cast != NULL && cast->has_range_check()) {\n-    compile->remove_range_check_cast(cast);\n-  }\n-  if (Opcode() == Op_Opaque4) {\n-    compile->remove_opaque4_node(this);\n+  if (for_post_loop_opts_igvn()) {\n+    compile->remove_from_post_loop_opts_igvn(this);\n@@ -905,0 +900,8 @@\n+  \/\/ the layout of Node::_in\n+  \/\/ r: a required input, null is allowed\n+  \/\/ p: a precedence, null values are all at the end\n+  \/\/ -----------------------------------\n+  \/\/ |r|...|r|p|...|p|null|...|null|\n+  \/\/         |                     |\n+  \/\/         req()                 len()\n+  \/\/ -----------------------------------\n@@ -913,2 +916,2 @@\n-  for (uint i = req(); i < len(); ++i) {\n-    set_prec(i, nullptr);\n+  for (uint i = len(); i > req(); ) {\n+    rm_prec(--i);  \/\/ no-op if _in[i] is nullptr\n@@ -917,0 +920,7 @@\n+#ifdef ASSERT\n+  \/\/ sanity check\n+  for (uint i = 0; i < len(); ++i) {\n+    assert(_in[i] == nullptr, \"disconnect_inputs() failed!\");\n+  }\n+#endif\n+\n@@ -1050,1 +1060,1 @@\n-  assert(max_flags() <= max_jushort, \"too many NodeProperty flags\");\n+  assert(max_flags() <= max_juint, \"too many NodeProperty flags\");\n@@ -1407,6 +1417,2 @@\n-      CastIINode* cast = dead->isa_CastII();\n-      if (cast != NULL && cast->has_range_check()) {\n-        igvn->C->remove_range_check_cast(cast);\n-      }\n-      if (dead->Opcode() == Op_Opaque4) {\n-        igvn->C->remove_opaque4_node(dead);\n+      if (dead->for_post_loop_opts_igvn()) {\n+        igvn->C->remove_from_post_loop_opts_igvn(dead);\n@@ -2430,0 +2436,23 @@\n+bool Node::is_dead_loop_safe() const {\n+  if (is_Phi()) {\n+    return true;\n+  }\n+  if (is_Proj() && in(0) == NULL)  {\n+    return true;\n+  }\n+  if ((_flags & (Flag_is_dead_loop_safe | Flag_is_Con)) != 0) {\n+    if (!is_Proj()) {\n+      return true;\n+    }\n+    if (in(0)->is_Allocate()) {\n+      return false;\n+    }\n+    \/\/ MemNode::can_see_stored_value() peeks through the boxing call\n+    if (in(0)->is_CallStaticJava() && in(0)->as_CallStaticJava()->is_boxing_method()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":54,"deletions":25,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -442,0 +442,1 @@\n+\/\/ 'worklist' is cleared upon returning.\n@@ -513,0 +514,3 @@\n+\n+  \/\/ Clear the original worklist\n+  worklist->clear();\n@@ -1395,1 +1399,20 @@\n-      C->remove_useless_node(dead);\n+      C->remove_modified_node(dead);\n+      \/\/ Constant node that has no out-edges and has only one in-edge from\n+      \/\/ root is usually dead. However, sometimes reshaping walk makes\n+      \/\/ it reachable by adding use edges. So, we will NOT count Con nodes\n+      \/\/ as dead to be conservative about the dead node count at any\n+      \/\/ given time.\n+      if (!dead->is_Con()) {\n+        C->record_dead_node(dead->_idx);\n+      }\n+      if (dead->is_macro()) {\n+        C->remove_macro_node(dead);\n+      }\n+      if (dead->is_expensive()) {\n+        C->remove_expensive_node(dead);\n+      }\n+      if (dead->for_post_loop_opts_igvn()) {\n+        C->remove_from_post_loop_opts_igvn(dead);\n+      }\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      bs->unregister_potential_barrier_node(dead);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -983,1 +983,1 @@\n-const TypeFunc* OptoRuntime::digestBase_implCompress_Type() {\n+const TypeFunc* OptoRuntime::digestBase_implCompress_Type(bool is_sha3) {\n@@ -985,1 +985,1 @@\n-  int num_args = 2;\n+  int num_args = is_sha3 ? 3 : 2;\n@@ -991,0 +991,1 @@\n+  if (is_sha3) fields[argp++] = TypeInt::INT; \/\/ digest_length\n@@ -1004,1 +1005,1 @@\n-const TypeFunc* OptoRuntime::digestBase_implCompressMB_Type() {\n+const TypeFunc* OptoRuntime::digestBase_implCompressMB_Type(bool is_sha3) {\n@@ -1006,1 +1007,1 @@\n-  int num_args = 4;\n+  int num_args = is_sha3 ? 5 : 4;\n@@ -1012,0 +1013,1 @@\n+  if (is_sha3) fields[argp++] = TypeInt::INT; \/\/ digest_length\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -282,2 +282,2 @@\n-  static const TypeFunc* digestBase_implCompress_Type();\n-  static const TypeFunc* digestBase_implCompressMB_Type();\n+  static const TypeFunc* digestBase_implCompress_Type(bool is_sha3);\n+  static const TypeFunc* digestBase_implCompressMB_Type(bool is_sha3);\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -626,1 +626,2 @@\n-    if (!arch_supports_vector(is_store ? Op_StoreVector : Op_LoadVector, byte_num_elem, T_BYTE, VecMaskNotUsed)) {\n+    if (!arch_supports_vector(is_store ? Op_StoreVector : Op_LoadVector, byte_num_elem, T_BYTE, VecMaskNotUsed)\n+        || !arch_supports_vector(Op_VectorReinterpret, byte_num_elem, T_BYTE, VecMaskNotUsed)) {\n@@ -649,3 +650,3 @@\n-         if (!arch_supports_vector(Op_StoreVector, num_elem, elem_bt, VecMaskUseStore)) {\n-           return false; \/\/ not supported\n-         }\n+      if (!arch_supports_vector(Op_StoreVector, num_elem, elem_bt, VecMaskUseStore)) {\n+        return false; \/\/ not supported\n+      }\n@@ -697,3 +698,3 @@\n-          vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, num_elem, T_BOOLEAN));\n-          const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n-          vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n+        vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, num_elem, T_BOOLEAN));\n+        const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+        vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n@@ -701,1 +702,1 @@\n-          vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, num_elem, elem_bt));\n+        vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, num_elem, elem_bt));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -534,0 +534,1 @@\n+  { \"CriticalJNINatives\",                  JDK_Version::jdk(16), JDK_Version::jdk(17), JDK_Version::jdk(18) },\n@@ -3903,0 +3904,16 @@\n+static void apply_debugger_ergo() {\n+  if (UseDebuggerErgo) {\n+    \/\/ Turn on sub-flags\n+    FLAG_SET_ERGO_IF_DEFAULT(UseDebuggerErgo1, true);\n+    FLAG_SET_ERGO_IF_DEFAULT(UseDebuggerErgo2, true);\n+  }\n+\n+  if (UseDebuggerErgo2) {\n+    \/\/ Debugging with limited number of CPUs\n+    FLAG_SET_ERGO_IF_DEFAULT(UseNUMA, false);\n+    FLAG_SET_ERGO_IF_DEFAULT(ConcGCThreads, 1);\n+    FLAG_SET_ERGO_IF_DEFAULT(ParallelGCThreads, 1);\n+    FLAG_SET_ERGO_IF_DEFAULT(CICompilerCount, 2);\n+  }\n+}\n+\n@@ -4099,0 +4116,2 @@\n+  apply_debugger_ergo();\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -493,7 +493,0 @@\n-  \/\/ Block before entering a JNI critical method\n-  static void block_for_jni_critical(JavaThread* thread);\n-\n-  \/\/ Pin\/Unpin object\n-  static oopDesc* pin_object(JavaThread* thread, oopDesc* obj);\n-  static void unpin_object(JavaThread* thread, oopDesc* obj);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -149,0 +149,2 @@\n+address StubRoutines::_sha3_implCompress     = NULL;\n+address StubRoutines::_sha3_implCompressMB   = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -231,0 +231,2 @@\n+  static address _sha3_implCompress;\n+  static address _sha3_implCompressMB;\n@@ -576,0 +578,2 @@\n+  static address sha3_implCompress()     { return _sha3_implCompress; }\n+  static address sha3_implCompressMB()   { return _sha3_implCompressMB; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-#include \"memory\/metaspace\/metablock.hpp\"\n@@ -470,1 +469,1 @@\n-     static_field(vmSymbols,                   _symbols[0],                                   Symbol*)                               \\\n+     static_field(Symbol,                      _vm_symbols[0],                                Symbol*)                               \\\n@@ -907,1 +906,1 @@\n-  c2_nonstatic_field(Node,                     _flags,                                        jushort)                               \\\n+  c2_nonstatic_field(Node,                     _flags,                                        juint)                                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -449,0 +449,6 @@\n+  void swap(GrowableArrayWithAllocator<E, Derived>* other) {\n+    ::swap(this->_data, other->_data);\n+    ::swap(this->_len, other->_len);\n+    ::swap(this->_max, other->_max);\n+  }\n+\n@@ -701,1 +707,1 @@\n-  GrowableArrayCHeap(int initial_max) :\n+  GrowableArrayCHeap(int initial_max = 0) :\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -426,1 +426,6 @@\n-     * {@inheritDoc} <!--workaround-->\n+     * Returns a vector of the given species\n+     * where all lane elements are set to\n+     * zero, the default primitive value.\n+     *\n+     * @param species species of the desired zero vector\n+     * @return a zero vector\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -426,1 +426,6 @@\n-     * {@inheritDoc} <!--workaround-->\n+     * Returns a vector of the given species\n+     * where all lane elements are set to\n+     * zero, the default primitive value.\n+     *\n+     * @param species species of the desired zero vector\n+     * @return a zero vector\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -230,1 +230,1 @@\n-     * unary operators,\n+     * unary (one-argument) operators,\n@@ -246,1 +246,1 @@\n-     * binary operators,\n+     * binary (two-argument) operators,\n@@ -262,1 +262,1 @@\n-     * ternary operators,\n+     * ternary (three-argument) operators,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorOperators.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -430,1 +430,6 @@\n-     * {@inheritDoc} <!--workaround-->\n+     * Returns a vector of the given species\n+     * where all lane elements are set to\n+     * zero, the default primitive value.\n+     *\n+     * @param species species of the desired zero vector\n+     * @return a zero vector\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"}]}