{"files":[{"patch":"@@ -188,121 +188,0 @@\n-class C2AccessFence: public StackObj {\n-  C2Access& _access;\n-  Node* _leading_membar;\n-\n-public:\n-  C2AccessFence(C2Access& access) :\n-    _access(access), _leading_membar(NULL) {\n-    GraphKit* kit = NULL;\n-    if (access.is_parse_access()) {\n-      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n-      kit = parse_access.kit();\n-    }\n-    DecoratorSet decorators = access.decorators();\n-\n-    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n-    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n-    bool is_atomic = is_read && is_write;\n-\n-    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n-    bool is_release = (decorators & MO_RELEASE) != 0;\n-\n-    if (is_atomic) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      \/\/ Memory-model-wise, a LoadStore acts like a little synchronized\n-      \/\/ block, so needs barriers on each side.  These don't translate\n-      \/\/ into actual barriers on most machines, but we still need rest of\n-      \/\/ compiler to respect ordering.\n-      if (is_release) {\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-      } else if (is_volatile) {\n-        if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n-        } else {\n-          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-        }\n-      }\n-    } else if (is_write) {\n-      \/\/ If reference is volatile, prevent following memory ops from\n-      \/\/ floating down past the volatile write.  Also prevents commoning\n-      \/\/ another volatile read.\n-      if (is_volatile || is_release) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-      }\n-    } else {\n-      \/\/ Memory barrier to prevent normal and 'unsafe' accesses from\n-      \/\/ bypassing each other.  Happens after null checks, so the\n-      \/\/ exception paths do not take memory state from the memory barrier,\n-      \/\/ so there's no problems making a strong assert about mixing users\n-      \/\/ of safe & unsafe memory.\n-      if (is_volatile && support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n-      }\n-    }\n-\n-    if (access.needs_cpu_membar()) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    }\n-\n-    if (is_atomic) {\n-      \/\/ 4984716: MemBars must be inserted before this\n-      \/\/          memory node in order to avoid a false\n-      \/\/          dependency which will confuse the scheduler.\n-      access.set_memory();\n-    }\n-  }\n-\n-  ~C2AccessFence() {\n-    GraphKit* kit = NULL;\n-    if (_access.is_parse_access()) {\n-      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(_access);\n-      kit = parse_access.kit();\n-    }\n-    DecoratorSet decorators = _access.decorators();\n-\n-    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n-    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n-    bool is_atomic = is_read && is_write;\n-\n-    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n-    bool is_acquire = (decorators & MO_ACQUIRE) != 0;\n-\n-    \/\/ If reference is volatile, prevent following volatiles ops from\n-    \/\/ floating up before the volatile access.\n-    if (_access.needs_cpu_membar()) {\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    }\n-\n-    if (is_atomic) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      if (is_acquire || is_volatile) {\n-        Node* n = _access.raw_access();\n-        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n-        if (_leading_membar != NULL) {\n-          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n-        }\n-      }\n-    } else if (is_write) {\n-      \/\/ If not multiple copy atomic, we do the MemBarVolatile before the load.\n-      if (is_volatile && !support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        Node* n = _access.raw_access();\n-        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); \/\/ Use fat membar\n-        if (_leading_membar != NULL) {\n-          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n-        }\n-      }\n-    } else {\n-      if (is_volatile || is_acquire) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        Node* n = _access.raw_access();\n-        assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, \"no leading membar expected\");\n-        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n-        mb->as_MemBar()->set_trailing_load();\n-      }\n-    }\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":0,"deletions":121,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -911,0 +911,121 @@\n+class C2AccessFence: public StackObj {\n+  C2Access& _access;\n+  Node* _leading_membar;\n+\n+public:\n+  C2AccessFence(C2Access& access) :\n+    _access(access), _leading_membar(NULL) {\n+    GraphKit* kit = NULL;\n+    if (access.is_parse_access()) {\n+      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n+      kit = parse_access.kit();\n+    }\n+    DecoratorSet decorators = access.decorators();\n+\n+    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n+    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n+    bool is_atomic = is_read && is_write;\n+\n+    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n+    bool is_release = (decorators & MO_RELEASE) != 0;\n+\n+    if (is_atomic) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      \/\/ Memory-model-wise, a LoadStore acts like a little synchronized\n+      \/\/ block, so needs barriers on each side.  These don't translate\n+      \/\/ into actual barriers on most machines, but we still need rest of\n+      \/\/ compiler to respect ordering.\n+      if (is_release) {\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+      } else if (is_volatile) {\n+        if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n+        } else {\n+          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+        }\n+      }\n+    } else if (is_write) {\n+      \/\/ If reference is volatile, prevent following memory ops from\n+      \/\/ floating down past the volatile write.  Also prevents commoning\n+      \/\/ another volatile read.\n+      if (is_volatile || is_release) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+      }\n+    } else {\n+      \/\/ Memory barrier to prevent normal and 'unsafe' accesses from\n+      \/\/ bypassing each other.  Happens after null checks, so the\n+      \/\/ exception paths do not take memory state from the memory barrier,\n+      \/\/ so there's no problems making a strong assert about mixing users\n+      \/\/ of safe & unsafe memory.\n+      if (is_volatile && support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n+      }\n+    }\n+\n+    if (access.needs_cpu_membar()) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    }\n+\n+    if (is_atomic) {\n+      \/\/ 4984716: MemBars must be inserted before this\n+      \/\/          memory node in order to avoid a false\n+      \/\/          dependency which will confuse the scheduler.\n+      access.set_memory();\n+    }\n+  }\n+\n+  ~C2AccessFence() {\n+    GraphKit* kit = NULL;\n+    if (_access.is_parse_access()) {\n+      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(_access);\n+      kit = parse_access.kit();\n+    }\n+    DecoratorSet decorators = _access.decorators();\n+\n+    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n+    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n+    bool is_atomic = is_read && is_write;\n+\n+    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n+    bool is_acquire = (decorators & MO_ACQUIRE) != 0;\n+\n+    \/\/ If reference is volatile, prevent following volatiles ops from\n+    \/\/ floating up before the volatile access.\n+    if (_access.needs_cpu_membar()) {\n+      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    }\n+\n+    if (is_atomic) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      if (is_acquire || is_volatile) {\n+        Node* n = _access.raw_access();\n+        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n+        if (_leading_membar != NULL) {\n+          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n+        }\n+      }\n+    } else if (is_write) {\n+      \/\/ If not multiple copy atomic, we do the MemBarVolatile before the load.\n+      if (is_volatile && !support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        Node* n = _access.raw_access();\n+        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); \/\/ Use fat membar\n+        if (_leading_membar != NULL) {\n+          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n+        }\n+      }\n+    } else {\n+      if (is_volatile || is_acquire) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        Node* n = _access.raw_access();\n+        assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, \"no leading membar expected\");\n+        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n+        mb->as_MemBar()->set_trailing_load();\n+      }\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -2212,7 +2212,3 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n-  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n-  DecoratorSet decorators = C2_UNSAFE_ACCESS;\n-  guarantee(!is_store || kind != Acquire, \"Acquire accesses can be produced only for loads\");\n-  guarantee( is_store || kind != Release, \"Release accesses can be produced only for stores\");\n-  assert(type != T_OBJECT || !unaligned, \"unaligned access not supported with object type\");\n-\n+bool LibraryCallKit::prepare_unsafe_access(const BasicType type, const bool unaligned, Node* base, Node* offset, const AccessKind kind,\n+  DecoratorSet& decorators, Node *&heap_base_oop, Node *&adr, bool &can_access_non_heap) {\n+  decorators = C2_UNSAFE_ACCESS;\n@@ -2227,0 +2223,33 @@\n+  \/\/ Build address expression.\n+  heap_base_oop = top();\n+\n+  adr = make_unsafe_address(base, offset, type, kind == Relaxed);\n+\n+  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {\n+    if (type != T_OBJECT) {\n+      decorators |= IN_NATIVE; \/\/ off-heap primitive access\n+    } else {\n+      return false; \/\/ off-heap oop accesses are not supported\n+    }\n+  } else {\n+    heap_base_oop = base; \/\/ on-heap or mixed access\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));\n+\n+  if (!can_access_non_heap) {\n+    decorators |= IN_HEAP;\n+  }\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n+\n+  guarantee(!is_store || kind != Acquire, \"Acquire accesses can be produced only for loads\");\n+  guarantee( is_store || kind != Release, \"Release accesses can be produced only for stores\");\n+  assert(type != T_OBJECT || !unaligned, \"unaligned access not supported with object type\");\n+\n+\n@@ -2257,3 +2286,0 @@\n-  \/\/ Build address expression.\n-  Node* heap_base_oop = top();\n-\n@@ -2276,1 +2302,4 @@\n-  Node* adr = make_unsafe_address(base, offset, type, kind == Relaxed);\n+  DecoratorSet decorators = DECORATORS_NONE;\n+  Node* heap_base_oop;\n+  Node* adr;\n+  bool can_access_non_heap;\n@@ -2278,17 +2307,5 @@\n-  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {\n-    if (type != T_OBJECT) {\n-      decorators |= IN_NATIVE; \/\/ off-heap primitive access\n-    } else {\n-      set_map(old_map);\n-      set_sp(old_sp);\n-      return false; \/\/ off-heap oop accesses are not supported\n-    }\n-  } else {\n-    heap_base_oop = base; \/\/ on-heap or mixed access\n-  }\n-\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));\n-\n-  if (!can_access_non_heap) {\n-    decorators |= IN_HEAP;\n+  if (!prepare_unsafe_access(type, unaligned, base, offset, kind, decorators, heap_base_oop, adr, can_access_non_heap)) {\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    assert(false, \"performance issue\");\n+    return false;\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":45,"deletions":28,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -226,0 +226,2 @@\n+  bool prepare_unsafe_access(const BasicType type, const bool unaligned, Node* base, Node* offset, const AccessKind kind,\n+    DecoratorSet& decorators, Node *&heap_base_oop, Node *&adr, bool &can_access_non_heap);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -762,1 +762,13 @@\n-  Node* addr = make_unsafe_address(base, offset, (is_mask ? T_BOOLEAN : elem_bt), true);\n+  DecoratorSet decorators = DECORATORS_NONE;\n+  Node* heap_base_oop;\n+  Node* addr;\n+  bool can_access_non_heap;\n+  const BasicType access_type = is_mask ? T_BOOLEAN : elem_bt;\n+  \/\/ The unaligned param does not matter \"in fact\"\n+  \/\/ Here's a bit of \"work around\", actually we are not going ot load access_type, but vector of access_types\n+  if (!prepare_unsafe_access(access_type, \/* unaligned *\/ true, base, offset, Relaxed, decorators, heap_base_oop, addr, can_access_non_heap)) {\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    assert(false, \"performance issue\");\n+    return false;\n+  }\n@@ -764,2 +776,0 @@\n-  \/\/ This check is repetition of some checks from inline_unsafe_access(), used to determine if barriers are needed\n-  \/\/ Not full scope of checks is performed, we check only if access can be mixed\n@@ -767,10 +777,0 @@\n-\n-  \/\/ Is off heap access (true implies can_access_non_heap = true)\n-  const bool off_heap_access = TypePtr::NULL_PTR == base_type;\n-\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  const bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(base_type);\n-\n-  \/\/ Not determined access base can and can not be null.\n-  const bool mixed_access = !off_heap_access && can_access_non_heap;\n-\n@@ -837,3 +837,2 @@\n-  if (mixed_access) {\n-    insert_mem_bar(Op_MemBarCPUOrder);\n-  }\n+  C2AccessValuePtr adr(addr, addr_type);\n+  C2ParseAccess access(this, decorators | (is_store ? C2_WRITE_ACCESS : C2_READ_ACCESS), access_type, heap_base_oop, adr);\n@@ -842,0 +841,2 @@\n+    \/\/ Constructor sets pre-barrier, destructor post barrier\n+    C2AccessFence fence(access);\n@@ -861,0 +862,3 @@\n+    \/\/ Constructor sets pre-barrier, destructor post barrier\n+    C2AccessFence fence(access);\n+\n@@ -884,4 +888,0 @@\n-  if (mixed_access) {\n-    insert_mem_bar(Op_MemBarCPUOrder);\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -412,7 +412,1 @@\n-            if (base == null) {\n-              return VectorSupport.load(vmClass, e, length,\n-                      null, BufferAccess.bufferAddress(bb, offset),\n-                      bb, offset, s,\n-                      defaultImpl);\n-            } else {\n-              return VectorSupport.load(vmClass, e, length,\n+            return VectorSupport.load(vmClass, e, length,\n@@ -422,1 +416,0 @@\n-            }\n@@ -463,13 +456,5 @@\n-            if (base == null) {\n-              VectorSupport.store(vmClass, e, length,\n-                                  null, BufferAccess.bufferAddress(bb, offset),\n-                                  v,\n-                                  bb, offset,\n-                                  defaultImpl);\n-            } else {\n-              VectorSupport.store(vmClass, e, length,\n-                                  base, BufferAccess.bufferAddress(bb, offset),\n-                                  v,\n-                                  bb, offset,\n-                                  defaultImpl);\n-            }\n+            VectorSupport.store(vmClass, e, length,\n+                                base, BufferAccess.bufferAddress(bb, offset),\n+                                v,\n+                                bb, offset,\n+                                defaultImpl);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/X-ScopedMemoryAccess.java.template","additions":6,"deletions":21,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -3461,0 +3461,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3086,0 +3086,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3073,0 +3073,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3182,0 +3182,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3085,0 +3085,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3448,0 +3448,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4398,0 +4398,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}