{"files":[{"patch":"@@ -4,0 +4,2 @@\n+\/**\/.idea\n+\/**\/*.iml\n@@ -5,0 +7,1 @@\n+\/**\/.vscode\n@@ -21,0 +24,4 @@\n+\n+# Downloaded binaries\n+\/src\/utils\/hsdis\/binutils*\n+\/src\/utils\/hsdis\/build\/\n","filename":".gitignore","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -188,121 +188,0 @@\n-class C2AccessFence: public StackObj {\n-  C2Access& _access;\n-  Node* _leading_membar;\n-\n-public:\n-  C2AccessFence(C2Access& access) :\n-    _access(access), _leading_membar(NULL) {\n-    GraphKit* kit = NULL;\n-    if (access.is_parse_access()) {\n-      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n-      kit = parse_access.kit();\n-    }\n-    DecoratorSet decorators = access.decorators();\n-\n-    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n-    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n-    bool is_atomic = is_read && is_write;\n-\n-    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n-    bool is_release = (decorators & MO_RELEASE) != 0;\n-\n-    if (is_atomic) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      \/\/ Memory-model-wise, a LoadStore acts like a little synchronized\n-      \/\/ block, so needs barriers on each side.  These don't translate\n-      \/\/ into actual barriers on most machines, but we still need rest of\n-      \/\/ compiler to respect ordering.\n-      if (is_release) {\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-      } else if (is_volatile) {\n-        if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n-        } else {\n-          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-        }\n-      }\n-    } else if (is_write) {\n-      \/\/ If reference is volatile, prevent following memory ops from\n-      \/\/ floating down past the volatile write.  Also prevents commoning\n-      \/\/ another volatile read.\n-      if (is_volatile || is_release) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-      }\n-    } else {\n-      \/\/ Memory barrier to prevent normal and 'unsafe' accesses from\n-      \/\/ bypassing each other.  Happens after null checks, so the\n-      \/\/ exception paths do not take memory state from the memory barrier,\n-      \/\/ so there's no problems making a strong assert about mixing users\n-      \/\/ of safe & unsafe memory.\n-      if (is_volatile && support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n-      }\n-    }\n-\n-    if (access.needs_cpu_membar()) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    }\n-\n-    if (is_atomic) {\n-      \/\/ 4984716: MemBars must be inserted before this\n-      \/\/          memory node in order to avoid a false\n-      \/\/          dependency which will confuse the scheduler.\n-      access.set_memory();\n-    }\n-  }\n-\n-  ~C2AccessFence() {\n-    GraphKit* kit = NULL;\n-    if (_access.is_parse_access()) {\n-      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(_access);\n-      kit = parse_access.kit();\n-    }\n-    DecoratorSet decorators = _access.decorators();\n-\n-    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n-    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n-    bool is_atomic = is_read && is_write;\n-\n-    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n-    bool is_acquire = (decorators & MO_ACQUIRE) != 0;\n-\n-    \/\/ If reference is volatile, prevent following volatiles ops from\n-    \/\/ floating up before the volatile access.\n-    if (_access.needs_cpu_membar()) {\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    }\n-\n-    if (is_atomic) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      if (is_acquire || is_volatile) {\n-        Node* n = _access.raw_access();\n-        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n-        if (_leading_membar != NULL) {\n-          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n-        }\n-      }\n-    } else if (is_write) {\n-      \/\/ If not multiple copy atomic, we do the MemBarVolatile before the load.\n-      if (is_volatile && !support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        Node* n = _access.raw_access();\n-        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); \/\/ Use fat membar\n-        if (_leading_membar != NULL) {\n-          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n-        }\n-      }\n-    } else {\n-      if (is_volatile || is_acquire) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        Node* n = _access.raw_access();\n-        assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, \"no leading membar expected\");\n-        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n-        mb->as_MemBar()->set_trailing_load();\n-      }\n-    }\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":0,"deletions":121,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -911,0 +911,121 @@\n+class C2AccessFence: public StackObj {\n+  C2Access& _access;\n+  Node* _leading_membar;\n+\n+public:\n+  C2AccessFence(C2Access& access) :\n+    _access(access), _leading_membar(NULL) {\n+    GraphKit* kit = NULL;\n+    if (access.is_parse_access()) {\n+      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n+      kit = parse_access.kit();\n+    }\n+    DecoratorSet decorators = access.decorators();\n+\n+    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n+    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n+    bool is_atomic = is_read && is_write;\n+\n+    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n+    bool is_release = (decorators & MO_RELEASE) != 0;\n+\n+    if (is_atomic) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      \/\/ Memory-model-wise, a LoadStore acts like a little synchronized\n+      \/\/ block, so needs barriers on each side.  These don't translate\n+      \/\/ into actual barriers on most machines, but we still need rest of\n+      \/\/ compiler to respect ordering.\n+      if (is_release) {\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+      } else if (is_volatile) {\n+        if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n+        } else {\n+          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+        }\n+      }\n+    } else if (is_write) {\n+      \/\/ If reference is volatile, prevent following memory ops from\n+      \/\/ floating down past the volatile write.  Also prevents commoning\n+      \/\/ another volatile read.\n+      if (is_volatile || is_release) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+      }\n+    } else {\n+      \/\/ Memory barrier to prevent normal and 'unsafe' accesses from\n+      \/\/ bypassing each other.  Happens after null checks, so the\n+      \/\/ exception paths do not take memory state from the memory barrier,\n+      \/\/ so there's no problems making a strong assert about mixing users\n+      \/\/ of safe & unsafe memory.\n+      if (is_volatile && support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n+      }\n+    }\n+\n+    if (access.needs_cpu_membar()) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    }\n+\n+    if (is_atomic) {\n+      \/\/ 4984716: MemBars must be inserted before this\n+      \/\/          memory node in order to avoid a false\n+      \/\/          dependency which will confuse the scheduler.\n+      access.set_memory();\n+    }\n+  }\n+\n+  ~C2AccessFence() {\n+    GraphKit* kit = NULL;\n+    if (_access.is_parse_access()) {\n+      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(_access);\n+      kit = parse_access.kit();\n+    }\n+    DecoratorSet decorators = _access.decorators();\n+\n+    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n+    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n+    bool is_atomic = is_read && is_write;\n+\n+    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n+    bool is_acquire = (decorators & MO_ACQUIRE) != 0;\n+\n+    \/\/ If reference is volatile, prevent following volatiles ops from\n+    \/\/ floating up before the volatile access.\n+    if (_access.needs_cpu_membar()) {\n+      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    }\n+\n+    if (is_atomic) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      if (is_acquire || is_volatile) {\n+        Node* n = _access.raw_access();\n+        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n+        if (_leading_membar != NULL) {\n+          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n+        }\n+      }\n+    } else if (is_write) {\n+      \/\/ If not multiple copy atomic, we do the MemBarVolatile before the load.\n+      if (is_volatile && !support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        Node* n = _access.raw_access();\n+        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); \/\/ Use fat membar\n+        if (_leading_membar != NULL) {\n+          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n+        }\n+      }\n+    } else {\n+      if (is_volatile || is_acquire) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        Node* n = _access.raw_access();\n+        assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, \"no leading membar expected\");\n+        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n+        mb->as_MemBar()->set_trailing_load();\n+      }\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -2212,7 +2212,3 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n-  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n-  DecoratorSet decorators = C2_UNSAFE_ACCESS;\n-  guarantee(!is_store || kind != Acquire, \"Acquire accesses can be produced only for loads\");\n-  guarantee( is_store || kind != Release, \"Release accesses can be produced only for stores\");\n-  assert(type != T_OBJECT || !unaligned, \"unaligned access not supported with object type\");\n-\n+bool LibraryCallKit::prepare_unsafe_access(const BasicType type, const bool unaligned, Node* base, Node* offset, const AccessKind kind,\n+  DecoratorSet& decorators, Node *&heap_base_oop, Node *&adr, bool &can_access_non_heap) {\n+  decorators = C2_UNSAFE_ACCESS;\n@@ -2227,0 +2223,33 @@\n+  \/\/ Build address expression.\n+  heap_base_oop = top();\n+\n+  adr = make_unsafe_address(base, offset, type, kind == Relaxed);\n+\n+  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {\n+    if (type != T_OBJECT) {\n+      decorators |= IN_NATIVE; \/\/ off-heap primitive access\n+    } else {\n+      return false; \/\/ off-heap oop accesses are not supported\n+    }\n+  } else {\n+    heap_base_oop = base; \/\/ on-heap or mixed access\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));\n+\n+  if (!can_access_non_heap) {\n+    decorators |= IN_HEAP;\n+  }\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n+\n+  guarantee(!is_store || kind != Acquire, \"Acquire accesses can be produced only for loads\");\n+  guarantee( is_store || kind != Release, \"Release accesses can be produced only for stores\");\n+  assert(type != T_OBJECT || !unaligned, \"unaligned access not supported with object type\");\n+\n+\n@@ -2257,3 +2286,0 @@\n-  \/\/ Build address expression.\n-  Node* heap_base_oop = top();\n-\n@@ -2276,1 +2302,4 @@\n-  Node* adr = make_unsafe_address(base, offset, type, kind == Relaxed);\n+  DecoratorSet decorators = DECORATORS_NONE;\n+  Node* heap_base_oop;\n+  Node* adr;\n+  bool can_access_non_heap;\n@@ -2278,17 +2307,5 @@\n-  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {\n-    if (type != T_OBJECT) {\n-      decorators |= IN_NATIVE; \/\/ off-heap primitive access\n-    } else {\n-      set_map(old_map);\n-      set_sp(old_sp);\n-      return false; \/\/ off-heap oop accesses are not supported\n-    }\n-  } else {\n-    heap_base_oop = base; \/\/ on-heap or mixed access\n-  }\n-\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));\n-\n-  if (!can_access_non_heap) {\n-    decorators |= IN_HEAP;\n+  if (!prepare_unsafe_access(type, unaligned, base, offset, kind, decorators, heap_base_oop, adr, can_access_non_heap)) {\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    assert(false, \"performance issue\");\n+    return false;\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":45,"deletions":28,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -226,0 +226,2 @@\n+  bool prepare_unsafe_access(const BasicType type, const bool unaligned, Node* base, Node* offset, const AccessKind kind,\n+    DecoratorSet& decorators, Node *&heap_base_oop, Node *&adr, bool &can_access_non_heap);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -762,3 +762,13 @@\n-  Node* addr = make_unsafe_address(base, offset, (is_mask ? T_BOOLEAN : elem_bt), true);\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  DecoratorSet decorators = DECORATORS_NONE;\n+  Node* heap_base_oop;\n+  Node* addr;\n+  bool can_access_non_heap;\n+  const BasicType access_type = is_mask ? T_BOOLEAN : elem_bt;\n+  \/\/ The unaligned param does not matter \"in fact\"\n+  \/\/ Here's a bit of \"work around\", actually we are not going ot load access_type, but vector of access_types\n+  if (!prepare_unsafe_access(access_type, \/* unaligned *\/ true, base, offset, Relaxed, decorators, heap_base_oop, addr, can_access_non_heap)) {\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    assert(false, \"performance issue\");\n+    return false;\n+  }\n@@ -766,0 +776,1 @@\n+  const Type *const base_type = gvn().type(base);\n@@ -826,3 +837,2 @@\n-  if (can_access_non_heap) {\n-    insert_mem_bar(Op_MemBarCPUOrder);\n-  }\n+  C2AccessValuePtr adr(addr, addr_type);\n+  C2ParseAccess access(this, decorators | (is_store ? C2_WRITE_ACCESS : C2_READ_ACCESS), access_type, heap_base_oop, adr);\n@@ -831,0 +841,2 @@\n+    \/\/ Constructor sets pre-barrier, destructor post barrier\n+    C2AccessFence fence(access);\n@@ -850,0 +862,3 @@\n+    \/\/ Constructor sets pre-barrier, destructor post barrier\n+    C2AccessFence fence(access);\n+\n@@ -873,4 +888,0 @@\n-  if (can_access_non_heap) {\n-    insert_mem_bar(Op_MemBarCPUOrder);\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":21,"deletions":10,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.util.Objects;\n@@ -409,0 +410,2 @@\n+            byte[] base = (byte[]) BufferAccess.bufferBase(bb);\n+\n@@ -410,3 +413,3 @@\n-                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset),\n-                    bb, offset, s,\n-                    defaultImpl);\n+                      base, BufferAccess.bufferAddress(bb, offset),\n+                      bb, offset, s,\n+                      defaultImpl);\n@@ -451,0 +454,2 @@\n+            final byte[] base = (byte[]) BufferAccess.bufferBase(bb);\n+\n@@ -452,4 +457,4 @@\n-                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset),\n-                    v,\n-                    bb, offset,\n-                    defaultImpl);\n+                                base, BufferAccess.bufferAddress(bb, offset),\n+                                v,\n+                                bb, offset,\n+                                defaultImpl);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/X-ScopedMemoryAccess.java.template","additions":12,"deletions":7,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -3461,0 +3461,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3086,0 +3086,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3073,0 +3073,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3182,0 +3182,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3085,0 +3085,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3448,0 +3448,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4398,0 +4398,1 @@\n+        \/\/TODO Optimize: polymorphic call can lead to lack of inlining, and strange unswitch\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ *  Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *  Copyright (c) 2021, Rado Smogura. All rights reserved.\n+ *\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary Test if memory ordering is preserved\n+ *\n+ * @run main\/othervm -XX:-TieredCompilation -XX:+UnlockDiagnosticVMOptions -XX:+AbortVMOnCompilationFailure\n+ *      -XX:CompileThreshold=100 -XX:CompileCommand=dontinline,compiler.vectorapi.VectorMemoryAlias::test\n+ *      compiler.vectorapi.VectorMemoryAlias\n+ * @modules jdk.incubator.vector\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.VectorSpecies;\n+\n+public class VectorMemoryAlias {\n+\n+  public static void main(String[] args) {\n+    for (int i=0; i < 30000; i++) {\n+      if (test() != 1) {\n+        throw new AssertionError();\n+      }\n+    }\n+  }\n+\n+  public static int test() {\n+    byte arr[] = new byte[256];\n+    final var bb = ByteBuffer.wrap(arr);\n+    final var vs = VectorSpecies.ofLargestShape(byte.class);\n+    final var ones = ByteVector.broadcast(vs, 1);\n+    var res = ByteVector.zero(vs);\n+\n+    int result = 0;\n+    result += arr[2];\n+    res.add(ones).intoByteBuffer(bb, 0, ByteOrder.nativeOrder());\n+    result += arr[2];\n+\n+    return result;\n+  }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMemoryAlias.java","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,113 @@\n+\/*\n+ *  Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *  Copyright (c) 2021, Rado Smogura. All rights reserved.\n+ *\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+package org.openjdk.bench.jdk.incubator.vector;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.concurrent.TimeUnit;\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.VectorSpecies;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.CompilerControl;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@Warmup(iterations = 5, time = 500, timeUnit = TimeUnit.MILLISECONDS)\n+@Measurement(iterations = 10, time = 500, timeUnit = TimeUnit.MILLISECONDS)\n+@State(org.openjdk.jmh.annotations.Scope.Thread)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@Fork(value = 1, jvmArgsAppend = {\n+    \"--add-modules=jdk.incubator.foreign,jdk.incubator.vector\",\n+    \"-Dforeign.restricted=permit\",\n+    \"--enable-native-access\", \"ALL-UNNAMED\"})\n+public class ByteBufferVectorAccess {\n+  private static final VectorSpecies<Byte> SPECIES = VectorSpecies.ofLargestShape(byte.class);\n+\n+  @Param(\"1024\")\n+  private int size;\n+\n+  ByteBuffer directIn, directOut;\n+  ByteBuffer heapIn, heapOut;\n+\n+  ByteBuffer directInRo, directOutRo;\n+  ByteBuffer heapInRo, heapOutRo;\n+\n+  @Setup\n+  public void setup() {\n+    directIn = ByteBuffer.allocateDirect(size);\n+    directOut = ByteBuffer.allocateDirect(size);\n+\n+    heapIn = ByteBuffer.wrap(new byte[size]);\n+    heapOut = ByteBuffer.wrap(new byte[size]);\n+\n+    directInRo = directIn.asReadOnlyBuffer();\n+    directOutRo = directOut.asReadOnlyBuffer();\n+\n+    heapInRo = heapIn.asReadOnlyBuffer();\n+    heapOutRo = heapOut.asReadOnlyBuffer();\n+  }\n+\n+  @Benchmark\n+  public void directBuffers() {\n+    copyMemory(directIn, directOut);\n+  }\n+\n+  @Benchmark\n+  public void heapBuffers() {\n+    copyMemory(heapIn, heapOut);\n+  }\n+\n+  @Benchmark\n+  public void pollutedBuffers2() {\n+    copyMemory(directIn, directOut);\n+    copyMemory(heapIn, heapOut);\n+  }\n+\n+  @Benchmark\n+  public void pollutedBuffers3() {\n+    copyMemory(directIn, directOut);\n+    copyMemory(heapIn, heapOut);\n+\n+    copyMemory(directInRo, directOut);\n+    copyMemory(heapInRo, heapOut);\n+  }\n+\n+  @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+  protected void copyMemory(ByteBuffer in, ByteBuffer out) {\n+    for (int i=0; i < SPECIES.loopBound(in.limit()); i += SPECIES.vectorByteSize()) {\n+      final var v = ByteVector.fromByteBuffer(SPECIES, in, i, ByteOrder.nativeOrder());\n+      v.intoByteBuffer(out, i, ByteOrder.nativeOrder());\n+    }\n+  }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/ByteBufferVectorAccess.java","additions":113,"deletions":0,"binary":false,"changes":113,"status":"added"}]}