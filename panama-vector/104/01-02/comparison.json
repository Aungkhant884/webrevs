{"files":[{"patch":"@@ -188,0 +188,121 @@\n+class C2AccessFence: public StackObj {\n+  C2Access& _access;\n+  Node* _leading_membar;\n+\n+public:\n+  C2AccessFence(C2Access& access) :\n+    _access(access), _leading_membar(NULL) {\n+    GraphKit* kit = NULL;\n+    if (access.is_parse_access()) {\n+      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n+      kit = parse_access.kit();\n+    }\n+    DecoratorSet decorators = access.decorators();\n+\n+    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n+    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n+    bool is_atomic = is_read && is_write;\n+\n+    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n+    bool is_release = (decorators & MO_RELEASE) != 0;\n+\n+    if (is_atomic) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      \/\/ Memory-model-wise, a LoadStore acts like a little synchronized\n+      \/\/ block, so needs barriers on each side.  These don't translate\n+      \/\/ into actual barriers on most machines, but we still need rest of\n+      \/\/ compiler to respect ordering.\n+      if (is_release) {\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+      } else if (is_volatile) {\n+        if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n+        } else {\n+          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+        }\n+      }\n+    } else if (is_write) {\n+      \/\/ If reference is volatile, prevent following memory ops from\n+      \/\/ floating down past the volatile write.  Also prevents commoning\n+      \/\/ another volatile read.\n+      if (is_volatile || is_release) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n+      }\n+    } else {\n+      \/\/ Memory barrier to prevent normal and 'unsafe' accesses from\n+      \/\/ bypassing each other.  Happens after null checks, so the\n+      \/\/ exception paths do not take memory state from the memory barrier,\n+      \/\/ so there's no problems making a strong assert about mixing users\n+      \/\/ of safe & unsafe memory.\n+      if (is_volatile && support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n+      }\n+    }\n+\n+    if (access.needs_cpu_membar()) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    }\n+\n+    if (is_atomic) {\n+      \/\/ 4984716: MemBars must be inserted before this\n+      \/\/          memory node in order to avoid a false\n+      \/\/          dependency which will confuse the scheduler.\n+      access.set_memory();\n+    }\n+  }\n+\n+  ~C2AccessFence() {\n+    GraphKit* kit = NULL;\n+    if (_access.is_parse_access()) {\n+      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(_access);\n+      kit = parse_access.kit();\n+    }\n+    DecoratorSet decorators = _access.decorators();\n+\n+    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n+    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n+    bool is_atomic = is_read && is_write;\n+\n+    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n+    bool is_acquire = (decorators & MO_ACQUIRE) != 0;\n+\n+    \/\/ If reference is volatile, prevent following volatiles ops from\n+    \/\/ floating up before the volatile access.\n+    if (_access.needs_cpu_membar()) {\n+      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    }\n+\n+    if (is_atomic) {\n+      assert(kit != NULL, \"unsupported at optimization time\");\n+      if (is_acquire || is_volatile) {\n+        Node* n = _access.raw_access();\n+        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n+        if (_leading_membar != NULL) {\n+          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n+        }\n+      }\n+    } else if (is_write) {\n+      \/\/ If not multiple copy atomic, we do the MemBarVolatile before the load.\n+      if (is_volatile && !support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        Node* n = _access.raw_access();\n+        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); \/\/ Use fat membar\n+        if (_leading_membar != NULL) {\n+          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n+        }\n+      }\n+    } else {\n+      if (is_volatile || is_acquire) {\n+        assert(kit != NULL, \"unsupported at optimization time\");\n+        Node* n = _access.raw_access();\n+        assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, \"no leading membar expected\");\n+        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n+        mb->as_MemBar()->set_trailing_load();\n+      }\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -911,121 +911,0 @@\n-class C2AccessFence: public StackObj {\n-  C2Access& _access;\n-  Node* _leading_membar;\n-\n-public:\n-  C2AccessFence(C2Access& access) :\n-    _access(access), _leading_membar(NULL) {\n-    GraphKit* kit = NULL;\n-    if (access.is_parse_access()) {\n-      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n-      kit = parse_access.kit();\n-    }\n-    DecoratorSet decorators = access.decorators();\n-\n-    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n-    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n-    bool is_atomic = is_read && is_write;\n-\n-    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n-    bool is_release = (decorators & MO_RELEASE) != 0;\n-\n-    if (is_atomic) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      \/\/ Memory-model-wise, a LoadStore acts like a little synchronized\n-      \/\/ block, so needs barriers on each side.  These don't translate\n-      \/\/ into actual barriers on most machines, but we still need rest of\n-      \/\/ compiler to respect ordering.\n-      if (is_release) {\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-      } else if (is_volatile) {\n-        if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n-        } else {\n-          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-        }\n-      }\n-    } else if (is_write) {\n-      \/\/ If reference is volatile, prevent following memory ops from\n-      \/\/ floating down past the volatile write.  Also prevents commoning\n-      \/\/ another volatile read.\n-      if (is_volatile || is_release) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);\n-      }\n-    } else {\n-      \/\/ Memory barrier to prevent normal and 'unsafe' accesses from\n-      \/\/ bypassing each other.  Happens after null checks, so the\n-      \/\/ exception paths do not take memory state from the memory barrier,\n-      \/\/ so there's no problems making a strong assert about mixing users\n-      \/\/ of safe & unsafe memory.\n-      if (is_volatile && support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);\n-      }\n-    }\n-\n-    if (access.needs_cpu_membar()) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    }\n-\n-    if (is_atomic) {\n-      \/\/ 4984716: MemBars must be inserted before this\n-      \/\/          memory node in order to avoid a false\n-      \/\/          dependency which will confuse the scheduler.\n-      access.set_memory();\n-    }\n-  }\n-\n-  ~C2AccessFence() {\n-    GraphKit* kit = NULL;\n-    if (_access.is_parse_access()) {\n-      C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(_access);\n-      kit = parse_access.kit();\n-    }\n-    DecoratorSet decorators = _access.decorators();\n-\n-    bool is_write = (decorators & C2_WRITE_ACCESS) != 0;\n-    bool is_read = (decorators & C2_READ_ACCESS) != 0;\n-    bool is_atomic = is_read && is_write;\n-\n-    bool is_volatile = (decorators & MO_SEQ_CST) != 0;\n-    bool is_acquire = (decorators & MO_ACQUIRE) != 0;\n-\n-    \/\/ If reference is volatile, prevent following volatiles ops from\n-    \/\/ floating up before the volatile access.\n-    if (_access.needs_cpu_membar()) {\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    }\n-\n-    if (is_atomic) {\n-      assert(kit != NULL, \"unsupported at optimization time\");\n-      if (is_acquire || is_volatile) {\n-        Node* n = _access.raw_access();\n-        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n-        if (_leading_membar != NULL) {\n-          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n-        }\n-      }\n-    } else if (is_write) {\n-      \/\/ If not multiple copy atomic, we do the MemBarVolatile before the load.\n-      if (is_volatile && !support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        Node* n = _access.raw_access();\n-        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); \/\/ Use fat membar\n-        if (_leading_membar != NULL) {\n-          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());\n-        }\n-      }\n-    } else {\n-      if (is_volatile || is_acquire) {\n-        assert(kit != NULL, \"unsupported at optimization time\");\n-        Node* n = _access.raw_access();\n-        assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, \"no leading membar expected\");\n-        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);\n-        mb->as_MemBar()->set_trailing_load();\n-      }\n-    }\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":0,"deletions":121,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -2212,3 +2212,7 @@\n-bool LibraryCallKit::prepare_unsafe_access(const BasicType type, const bool unaligned, Node* base, Node* offset, const AccessKind kind,\n-  DecoratorSet& decorators, Node *&heap_base_oop, Node *&adr, bool &can_access_non_heap) {\n-  decorators = C2_UNSAFE_ACCESS;\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS;\n+  guarantee(!is_store || kind != Acquire, \"Acquire accesses can be produced only for loads\");\n+  guarantee( is_store || kind != Release, \"Release accesses can be produced only for stores\");\n+  assert(type != T_OBJECT || !unaligned, \"unaligned access not supported with object type\");\n+\n@@ -2223,33 +2227,0 @@\n-  \/\/ Build address expression.\n-  heap_base_oop = top();\n-\n-  adr = make_unsafe_address(base, offset, type, kind == Relaxed);\n-\n-  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {\n-    if (type != T_OBJECT) {\n-      decorators |= IN_NATIVE; \/\/ off-heap primitive access\n-    } else {\n-      return false; \/\/ off-heap oop accesses are not supported\n-    }\n-  } else {\n-    heap_base_oop = base; \/\/ on-heap or mixed access\n-  }\n-\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));\n-\n-  if (!can_access_non_heap) {\n-    decorators |= IN_HEAP;\n-  }\n-\n-  return true;\n-}\n-\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n-  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n-\n-  guarantee(!is_store || kind != Acquire, \"Acquire accesses can be produced only for loads\");\n-  guarantee( is_store || kind != Release, \"Release accesses can be produced only for stores\");\n-  assert(type != T_OBJECT || !unaligned, \"unaligned access not supported with object type\");\n-\n-\n@@ -2286,0 +2257,3 @@\n+  \/\/ Build address expression.\n+  Node* heap_base_oop = top();\n+\n@@ -2302,4 +2276,1 @@\n-  DecoratorSet decorators = DECORATORS_NONE;\n-  Node* heap_base_oop;\n-  Node* adr;\n-  bool can_access_non_heap;\n+  Node* adr = make_unsafe_address(base, offset, type, kind == Relaxed);\n@@ -2307,5 +2278,17 @@\n-  if (!prepare_unsafe_access(type, unaligned, base, offset, kind, decorators, heap_base_oop, adr, can_access_non_heap)) {\n-    set_map(old_map);\n-    set_sp(old_sp);\n-    assert(false, \"performance issue\");\n-    return false;\n+  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {\n+    if (type != T_OBJECT) {\n+      decorators |= IN_NATIVE; \/\/ off-heap primitive access\n+    } else {\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false; \/\/ off-heap oop accesses are not supported\n+    }\n+  } else {\n+    heap_base_oop = base; \/\/ on-heap or mixed access\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));\n+\n+  if (!can_access_non_heap) {\n+    decorators |= IN_HEAP;\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":28,"deletions":45,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -226,2 +226,0 @@\n-  bool prepare_unsafe_access(const BasicType type, const bool unaligned, Node* base, Node* offset, const AccessKind kind,\n-    DecoratorSet& decorators, Node *&heap_base_oop, Node *&adr, bool &can_access_non_heap);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -762,13 +762,1 @@\n-  DecoratorSet decorators = DECORATORS_NONE;\n-  Node* heap_base_oop;\n-  Node* addr;\n-  bool can_access_non_heap;\n-  const BasicType access_type = is_mask ? T_BOOLEAN : elem_bt;\n-  \/\/ The unaligned param does not matter \"in fact\"\n-  \/\/ Here's a bit of \"work around\", actually we are not going ot load access_type, but vector of access_types\n-  if (!prepare_unsafe_access(access_type, \/* unaligned *\/ true, base, offset, Relaxed, decorators, heap_base_oop, addr, can_access_non_heap)) {\n-    set_map(old_map);\n-    set_sp(old_sp);\n-    assert(false, \"performance issue\");\n-    return false;\n-  }\n+  Node* addr = make_unsafe_address(base, offset, (is_mask ? T_BOOLEAN : elem_bt), true);\n@@ -776,0 +764,2 @@\n+  \/\/ This check is repetition of some checks from inline_unsafe_access(), used to determine if barriers are needed\n+  \/\/ Not full scope of checks is performed, we check only if access can be mixed\n@@ -777,0 +767,10 @@\n+\n+  \/\/ Is off heap access (true implies can_access_non_heap = true)\n+  const bool off_heap_access = TypePtr::NULL_PTR == base_type;\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  const bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(base_type);\n+\n+  \/\/ Not determined access base can and can not be null.\n+  const bool mixed_access = !off_heap_access && can_access_non_heap;\n+\n@@ -837,2 +837,3 @@\n-  C2AccessValuePtr adr(addr, addr_type);\n-  C2ParseAccess access(this, decorators | (is_store ? C2_WRITE_ACCESS : C2_READ_ACCESS), access_type, heap_base_oop, adr);\n+  if (mixed_access) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n@@ -841,2 +842,0 @@\n-    \/\/ Constructor sets pre-barrier, destructor post barrier\n-    C2AccessFence fence(access);\n@@ -862,3 +861,0 @@\n-    \/\/ Constructor sets pre-barrier, destructor post barrier\n-    C2AccessFence fence(access);\n-\n@@ -888,0 +884,4 @@\n+  if (mixed_access) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"}]}