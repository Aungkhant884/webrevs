{"files":[{"patch":"@@ -2077,1 +2077,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && dst_hi != OptoReg::Bad) {\n@@ -2198,0 +2198,3 @@\n+      } else if (dst_lo_rc == rc_predicate) {\n+        __ unspill_sve_predicate(as_PRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo),\n+                                 Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n@@ -2200,2 +2203,18 @@\n-        __ unspill(rscratch1, is64, src_offset);\n-        __ spill(rscratch1, is64, dst_offset);\n+        if (ideal_reg() == Op_RegVMask) {\n+          __ spill_copy_sve_predicate_stack_to_stack(src_offset, dst_offset,\n+                                                     Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+        } else {\n+          __ unspill(rscratch1, is64, src_offset);\n+          __ spill(rscratch1, is64, dst_offset);\n+        }\n+      }\n+      break;\n+    case rc_predicate:\n+      if (dst_lo_rc == rc_predicate) {\n+        __ sve_mov(as_PRegister(Matcher::_regEncode[dst_lo]), as_PRegister(Matcher::_regEncode[src_lo]));\n+      } else if (dst_lo_rc == rc_stack) {\n+        __ spill_sve_predicate(as_PRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo),\n+                               Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+      } else {\n+        assert(false, \"bad src and dst rc_class combination.\");\n+        ShouldNotReachHere();\n@@ -2239,0 +2258,4 @@\n+    } else if (ideal_reg() == Op_RegVMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# predicate spill size = %d\", vsize);\n@@ -2398,0 +2421,11 @@\n+    case Op_LoadVectorMask:\n+    case Op_StoreVectorMask:\n+    case Op_StoreVectorMasked:\n+    case Op_MaskAll:\n+    case Op_MaskToVector:\n+    case Op_VectorToMask:\n+    case Op_VectorCmpMaskGen:\n+      if (UseSVE == 0) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -2438,0 +2472,12 @@\n+const bool Matcher::match_rule_supported_masked_vector(int opcode, int vlen, BasicType bt) {\n+  if (!match_rule_supported(opcode) || !vector_size_supported(bt, vlen)) {\n+    return false;\n+  }\n+\n+  if (UseSVE > 0) {\n+    return masked_op_sve_supported(opcode);\n+  }\n+  \/\/ Only SVE support masked operations.\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":49,"deletions":3,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+  bool masked_op_sve_supported(int opcode);\n@@ -128,0 +129,5 @@\n+  static inline int vector_mask_element_size(const MachNode* n) {\n+    const TypeVMask* vmt = n->bottom_type()->is_vmask();\n+    return vmt->element_size_in_bytes();\n+  }\n+\n@@ -185,27 +191,29 @@\n-  static void sve_compare(C2_MacroAssembler masm, PRegister pd, BasicType bt,\n-                          PRegister pg, FloatRegister zn, FloatRegister zm, int cond) {\n-    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n-    if (bt == T_FLOAT || bt == T_DOUBLE) {\n-      switch (cond) {\n-        case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n-        case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n-        case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n-        case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n-        case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n-        case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n-        default:\n-          assert(false, \"unsupported\");\n-          ShouldNotReachHere();\n-      }\n-    } else {\n-      switch (cond) {\n-        case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n-        case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n-        case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n-        case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n-        case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n-        case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n-        default:\n-          assert(false, \"unsupported\");\n-          ShouldNotReachHere();\n-      }\n+  static void sve_compare_integral(C2_MacroAssembler masm, PRegister pd,\n+                                   Assembler::SIMD_RegVariant size, PRegister pg,\n+                                   FloatRegister zn, FloatRegister zm, int cond) {\n+    switch (cond) {\n+      case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n+      case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n+      case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n+      case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n+      case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n+      case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  static void sve_compare_fp(C2_MacroAssembler masm, PRegister pd,\n+                             Assembler::SIMD_RegVariant size, PRegister pg,\n+                             FloatRegister zn, FloatRegister zm, int cond) {\n+    switch (cond) {\n+      case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n+      case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n+      case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n+      case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n+      case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n+      case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n@@ -247,0 +255,5 @@\n+  bool masked_op_sve_supported(int opcode) {\n+    \/\/ TODO: list opcodes that are not supported with predicate feature\n+    return true;\n+  }\n+\n@@ -342,0 +355,102 @@\n+instruct storeV_mask(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t # vector (sve)\" %}\n+  ins_encode %{\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg, as_PRegister($pg$$reg),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector to mask\n+\n+instruct vector2vmask(pRegGov pg, vReg src, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set pg (VectorToMask src));\n+  effect(KILL cr, DEF pg, USE src);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmpne  $pg, $src, 0\\t# vector mask (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    __ sve_cmpne(as_PRegister($pg$$reg), elemType_to_regVariant(bt),\n+                 ptrue, as_FloatRegister($src$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector compare mask\n+\n+instruct vcmpmask_integral(pRegGov pg, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            is_integral_type(n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set pg (VectorCmpMaskGen (Binary src1 src2) cond));\n+  effect(KILL cr, DEF pg);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $pg, $src1, $src2\\t# vector cmp (sve)\" %}\n+  ins_encode %{\n+    int esize = vector_mask_element_size(this);\n+    Assembler::SIMD_RegVariant variant = elemBytes_to_regVariant(esize);\n+    sve_compare_integral(C2_MacroAssembler(&cbuf), as_PRegister($pg$$reg),\n+                   variant, ptrue, as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmpmask_fp(pRegGov pg, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            is_floating_point_type(n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set pg (VectorCmpMaskGen (Binary src1 src2) cond));\n+  effect(KILL cr, DEF pg);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $pg, $src1, $src2\\t# vector cmp (sve)\" %}\n+  ins_encode %{\n+    int esize = vector_mask_element_size(this);\n+    Assembler::SIMD_RegVariant variant = elemBytes_to_regVariant(esize);\n+    sve_compare_fp(C2_MacroAssembler(&cbuf), as_PRegister($pg$$reg),\n+                   variant, ptrue, as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ maskAll\n+\n+instruct vmaskAll(pRegGov pg, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set pg (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue $pg\\t# mask all (sve)\" %}\n+  ins_encode %{\n+    long value = $src$$constant;\n+    if (value == -1) {\n+      __ sve_ptrue(as_PRegister($pg$$reg), __ B);\n+    } else {\n+      assert(value == 0, \"Unsupported value\");\n+      __ sve_pfalse(as_PRegister($pg$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask to vector\n+\n+instruct vmask2vector(vReg dst, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskToVector pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cpy $dst, $pg, 1\\t# mask to vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pg$$reg), 1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -461,0 +576,86 @@\n+\/\/ vector abs - predicated\n+\n+instruct vabsB_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (AbsVB src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst, $pg, $src\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsS_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (AbsVS src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst, $pg, $src\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsI_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (AbsVI src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst, $pg, $src\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsL_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (AbsVL src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst, $pg, $src\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsF_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (AbsVF src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst, $pg, $src\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsD_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (AbsVD src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst, $pg, $src\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -541,0 +742,80 @@\n+\/\/ vector add - predicated\n+\n+instruct vaddB_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddS_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddI_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddL_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddF_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddD_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -586,1 +867,52 @@\n-\/\/ vector float div\n+\/\/ vector and - predicated\n+\n+instruct vand_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (AndV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ sve_and(as_FloatRegister($dst_src1$$reg), size,\n+         as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or - predicated\n+\n+instruct vor_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (OrV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_or $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ sve_or(as_FloatRegister($dst_src1$$reg), size,\n+         as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor - predicated\n+\n+instruct vxor_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (XorV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ sve_eor(as_FloatRegister($dst_src1$$reg), size,\n+         as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fdiv\n@@ -612,0 +944,28 @@\n+\/\/ vector fdiv - predicated\n+\n+instruct vfdivF_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vfdivD_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -654,0 +1014,42 @@\n+\/\/ vector min\/max - predicated\n+\n+instruct vmin_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MinV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"Unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmax_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MaxV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"Unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -890,1 +1292,0 @@\n-\n@@ -909,1 +1310,78 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst_src1 (MulVI dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n+  match(Set dst_src1 (MulVL dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst (MulVF src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n+  match(Set dst (MulVD src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n@@ -912,1 +1390,2 @@\n-         ptrue, as_FloatRegister($src2$$reg));\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -917,3 +1396,3 @@\n-instruct vmulI(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n-  match(Set dst_src1 (MulVI dst_src1 src2));\n+instruct vmulI_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) pg));\n@@ -921,1 +1400,1 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n@@ -924,1 +1403,2 @@\n-         ptrue, as_FloatRegister($src2$$reg));\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -929,3 +1409,3 @@\n-instruct vmulL(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n-  match(Set dst_src1 (MulVL dst_src1 src2));\n+instruct vmulL_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) pg));\n@@ -933,1 +1413,1 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n@@ -936,1 +1416,2 @@\n-         ptrue, as_FloatRegister($src2$$reg));\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -941,3 +1422,3 @@\n-instruct vmulF(vReg dst, vReg src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n-  match(Set dst (MulVF src1 src2));\n+instruct vmulF_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) pg));\n@@ -945,1 +1426,1 @@\n-  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n@@ -947,3 +1428,3 @@\n-    __ sve_fmul(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -954,3 +1435,3 @@\n-instruct vmulD(vReg dst, vReg src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n-  match(Set dst (MulVD src1 src2));\n+instruct vmulD_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) pg));\n@@ -958,1 +1439,1 @@\n-  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n@@ -960,3 +1441,3 @@\n-    __ sve_fmul(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -967,1 +1448,15 @@\n-\/\/ vector fneg\n+\/\/ vector neg\n+\n+instruct vnegI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (NegVI src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant element_size =\n+      elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_neg(as_FloatRegister($dst$$reg), element_size,\n+               ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -993,1 +1488,1 @@\n-\/\/ popcount vector\n+\/\/ vector fneg - predicated\n@@ -995,4 +1490,6 @@\n-instruct vpopcountI(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n-  match(Set dst (PopCountVI src));\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n+instruct vnegF_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (NegVF src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst, $pg, $src\\t # vector (sve) (S)\" %}\n@@ -1000,1 +1497,3 @@\n-     __ sve_cnt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_fneg(as_FloatRegister($dst$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n@@ -1005,9 +1504,6 @@\n-\/\/ vector mask compare\n-\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t # vector mask cmp (sve)\" %}\n+instruct vnegD_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (NegVD src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst, $pg, $src\\t # vector (sve) (D)\" %}\n@@ -1015,6 +1511,3 @@\n-    BasicType bt = vector_element_basic_type(this);\n-    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n-                ptrue, as_FloatRegister($src1$$reg),\n-                as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n+    __ sve_fneg(as_FloatRegister($dst$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n@@ -1025,1 +1518,1 @@\n-\/\/ vector blend\n+\/\/ popcount vector\n@@ -1027,7 +1520,4 @@\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+instruct vpopcountI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst (PopCountVI src));\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n@@ -1035,6 +1525,1 @@\n-    Assembler::SIMD_RegVariant size =\n-              elemType_to_regVariant(vector_element_basic_type(this));\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+     __ sve_cnt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg));\n@@ -1045,1 +1530,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -1047,2 +1532,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -1050,5 +1534,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t # vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t # vector blend (sve)\" %}\n@@ -1056,7 +1538,4 @@\n-    BasicType bt = vector_element_basic_type(this);\n-    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n-                ptrue, as_FloatRegister($src3$$reg),\n-                as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n@@ -1073,2 +1552,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_neg $dst, $src\\t # vector load mask (B)\" %}\n+  ins_cost(0);\n+  format %{ \"vloadmaskB (elided)\\t# vector load mask (sve) (B) - do nothing\" %}\n@@ -1076,2 +1555,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    \/\/ empty\n@@ -1086,3 +1564,2 @@\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector load mask (B to H)\" %}\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\t# vector load mask (sve) (B to H)\" %}\n@@ -1092,2 +1569,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1103,1 +1578,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -1105,2 +1580,1 @@\n-            \"sve_uunpklo $dst, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector load mask (B to S)\" %}\n+            \"sve_uunpklo $dst, $dst\\t# vector load mask (sve) (B to S)\" %}\n@@ -1112,2 +1586,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1123,1 +1595,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -1126,2 +1598,1 @@\n-            \"sve_uunpklo $dst, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector load mask (B to D)\" %}\n+            \"sve_uunpklo $dst, $dst\\t# vector load mask (sve) (B to D)\" %}\n@@ -1135,2 +1606,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1146,2 +1615,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_neg $dst, $src\\t # vector store mask (B)\" %}\n+  ins_cost(0);\n+  format %{ \"vstoremaskB (elided)\\t# vector store mask (sve) (B) - do nothing\" %}\n@@ -1149,2 +1618,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    \/\/ empty\n@@ -1159,1 +1627,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -1161,2 +1629,1 @@\n-            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector store mask (sve) (H to B)\" %}\n+            \"sve_uzp1 $dst, $src, $tmp\\t# vector store mask (sve) (H to B)\" %}\n@@ -1167,3 +1634,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n@@ -1178,1 +1642,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -1181,2 +1645,1 @@\n-            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector store mask (sve) (S to B)\" %}\n+            \"sve_uzp1 $dst, $dst, $tmp\\t# vector store mask (sve) (S to B)\" %}\n@@ -1189,2 +1652,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1199,1 +1660,1 @@\n-  ins_cost(5 * SVE_COST);\n+  ins_cost(4 * SVE_COST);\n@@ -1203,2 +1664,1 @@\n-            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector store mask (sve) (D to B)\" %}\n+            \"sve_uzp1 $dst, $dst, $tmp\\t# vector store mask (sve) (D to B)\" %}\n@@ -1213,2 +1673,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -1221,7 +1679,5 @@\n-instruct vloadmask_loadV(vReg dst, vmemA mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n-            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # load vector mask (sve)\" %}\n+instruct loadVMask(vReg dst, vmemA mem) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LoadVectorMask mem));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ld1b $dst, $mem\\t# load vector mask (sve)\" %}\n@@ -1229,0 +1685,2 @@\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type.\n@@ -1230,2 +1688,2 @@\n-    Assembler::SIMD_RegVariant to_vect_size =\n-              elemType_to_regVariant(vector_element_basic_type(this));\n+    Assembler::SIMD_RegVariant element_size =\n+      elemType_to_regVariant(vector_element_basic_type(this));\n@@ -1233,1 +1691,1 @@\n-                         T_BOOLEAN, to_vect_size, $mem->opcode(),\n+                         T_BOOLEAN, element_size, $mem->opcode(),\n@@ -1235,1 +1693,0 @@\n-    __ sve_neg(dst_reg, to_vect_size, ptrue, dst_reg);\n@@ -1240,15 +1697,12 @@\n-instruct storeV_vstoremask(vmemA mem, vReg src, vReg tmp, immI size) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= 2 &&\n-            n->as_StoreVector()->vect_type()->element_basic_type() == T_BOOLEAN);\n-  match(Set mem (StoreVector mem (VectorStoreMask src size)));\n-  effect(TEMP tmp);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t # store vector mask (sve)\" %}\n-  ins_encode %{\n-    Assembler::SIMD_RegVariant from_vect_size =\n-              elemBytes_to_regVariant((int)$size$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_size, ptrue,\n-               as_FloatRegister($src$$reg));\n-    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n-                         ptrue, T_BOOLEAN, from_vect_size, $mem->opcode(),\n+instruct storeVMask(vReg src, vmemA mem) %{\n+  predicate(UseSVE > 0);\n+  match(Set mem (StoreVectorMask mem src));\n+  format %{ \"sve_st1b $src, $mem\\t # store vector mask (sve)\" %}\n+  ins_cost(SVE_COST);\n+  ins_encode %{\n+    \/\/ Store the src vector elements as boolean values.\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    Assembler::SIMD_RegVariant element_size =\n+      elemType_to_regVariant(vector_element_basic_type(this, $src));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg, ptrue,\n+                         T_BOOLEAN, element_size, $mem->opcode(),\n@@ -2379,0 +2833,158 @@\n+\/\/ vector shift - predicated\n+\n+instruct vasrB_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (RShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (RShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (RShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (RShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (LShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (LShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (LShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (LShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (URShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (URShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrI_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (URShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (URShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2405,0 +3017,30 @@\n+\/\/ vector sqrt - predicated\n+\n+instruct vsqrtF_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (SqrtVF src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst, $pg, $src\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsqrtD_mask(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (SqrtVD src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst, $pg, $src\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2485,0 +3127,80 @@\n+\/\/ vector sub - predicated\n+\n+instruct vsubB_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubS_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubI_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubL_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubF_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubD_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":893,"deletions":171,"binary":false,"changes":1064,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+  bool masked_op_sve_supported(int opcode);\n@@ -124,0 +125,5 @@\n+  static inline int vector_mask_element_size(const MachNode* n) {\n+    const TypeVMask* vmt = n->bottom_type()->is_vmask();\n+    return vmt->element_size_in_bytes();\n+  }\n+\n@@ -181,27 +187,29 @@\n-  static void sve_compare(C2_MacroAssembler masm, PRegister pd, BasicType bt,\n-                          PRegister pg, FloatRegister zn, FloatRegister zm, int cond) {\n-    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n-    if (bt == T_FLOAT || bt == T_DOUBLE) {\n-      switch (cond) {\n-        case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n-        case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n-        case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n-        case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n-        case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n-        case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n-        default:\n-          assert(false, \"unsupported\");\n-          ShouldNotReachHere();\n-      }\n-    } else {\n-      switch (cond) {\n-        case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n-        case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n-        case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n-        case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n-        case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n-        case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n-        default:\n-          assert(false, \"unsupported\");\n-          ShouldNotReachHere();\n-      }\n+  static void sve_compare_integral(C2_MacroAssembler masm, PRegister pd,\n+                                   Assembler::SIMD_RegVariant size, PRegister pg,\n+                                   FloatRegister zn, FloatRegister zm, int cond) {\n+    switch (cond) {\n+      case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n+      case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n+      case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n+      case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n+      case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n+      case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  static void sve_compare_fp(C2_MacroAssembler masm, PRegister pd,\n+                             Assembler::SIMD_RegVariant size, PRegister pg,\n+                             FloatRegister zn, FloatRegister zm, int cond) {\n+    switch (cond) {\n+      case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n+      case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n+      case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n+      case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n+      case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n+      case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n@@ -243,0 +251,5 @@\n+  bool masked_op_sve_supported(int opcode) {\n+    \/\/ TODO: list opcodes that are not supported with predicate feature\n+    return true;\n+  }\n+\n@@ -343,1 +356,19 @@\n-%}dnl\n+%}\n+\n+instruct storeV_mask(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t # vector (sve)\" %}\n+  ins_encode %{\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg, as_PRegister($pg$$reg),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector to mask\n@@ -345,0 +376,72 @@\n+instruct vector2vmask(pRegGov pg, vReg src, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set pg (VectorToMask src));\n+  effect(KILL cr, DEF pg, USE src);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmpne  $pg, $src, 0\\t# vector mask (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    __ sve_cmpne(as_PRegister($pg$$reg), elemType_to_regVariant(bt),\n+                 ptrue, as_FloatRegister($src$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+dnl\n+dnl VECTOR_CMP_PREDICATE($1,          $2,         $3)\n+dnl VECTOR_CMP_PREDICATE(name_suffix, type_check, insn)\n+define(`VECTOR_CMP_PREDICATE', `\n+instruct vcmpmask_$1(pRegGov pg, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            $2(n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set pg (VectorCmpMaskGen (Binary src1 src2) cond));\n+  effect(KILL cr, DEF pg);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $pg, $src1, $src2\\t# vector cmp (sve)\" %}\n+  ins_encode %{\n+    int esize = vector_mask_element_size(this);\n+    Assembler::SIMD_RegVariant variant = elemBytes_to_regVariant(esize);\n+    $3(C2_MacroAssembler(&cbuf), as_PRegister($pg$$reg),\n+                   variant, ptrue, as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector compare mask\n+VECTOR_CMP_PREDICATE(integral, is_integral_type, sve_compare_integral)\n+VECTOR_CMP_PREDICATE(fp, is_floating_point_type, sve_compare_fp)\n+\n+\/\/ maskAll\n+\n+instruct vmaskAll(pRegGov pg, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set pg (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue $pg\\t# mask all (sve)\" %}\n+  ins_encode %{\n+    long value = $src$$constant;\n+    if (value == -1) {\n+      __ sve_ptrue(as_PRegister($pg$$reg), __ B);\n+    } else {\n+      assert(value == 0, \"Unsupported value\");\n+      __ sve_pfalse(as_PRegister($pg$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask to vector\n+\n+instruct vmask2vector(vReg dst, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskToVector pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cpy $dst, $pg, 1\\t# mask to vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pg$$reg), 1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -408,0 +511,25 @@\n+\n+dnl UNARY_OP_PREDICATE($1,        $2,      $3,   $4,    $5  )\n+dnl UNARY_OP_PREDICATE(insn_name, op_name, size, etype, insn)\n+define(`UNARY_OP_PREDICATE', `\n+instruct $1(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == $4);\n+  match(Set dst ($2 src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$5 $dst, $pg, $src\\t # vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $5(as_FloatRegister($dst$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\/\/ vector abs - predicated\n+UNARY_OP_PREDICATE(vabsB_mask, AbsVB, B, T_BYTE,   sve_abs)\n+UNARY_OP_PREDICATE(vabsS_mask, AbsVS, H, T_SHORT,  sve_abs)\n+UNARY_OP_PREDICATE(vabsI_mask, AbsVI, S, T_INT,    sve_abs)\n+UNARY_OP_PREDICATE(vabsL_mask, AbsVL, D, T_LONG,   sve_abs)\n+UNARY_OP_PREDICATE(vabsF_mask, AbsVF, S, T_FLOAT,  sve_fabs)\n+UNARY_OP_PREDICATE(vabsD_mask, AbsVD, D, T_DOUBLE, sve_fabs)\n+\n@@ -409,3 +537,3 @@\n-dnl BINARY_OP_UNPREDICATED($1,        $2       $3,   $4           $5  )\n-dnl BINARY_OP_UNPREDICATED(insn_name, op_name, size, min_vec_len, insn)\n-define(`BINARY_OP_UNPREDICATED', `\n+dnl BINARY_OP_UNPREDICATE($1,        $2       $3,   $4           $5  )\n+dnl BINARY_OP_UNPREDICATE(insn_name, op_name, size, min_vec_len, insn)\n+define(`BINARY_OP_UNPREDICATE', `\n@@ -424,1 +552,18 @@\n-\n+dnl\n+dnl\n+dnl BINARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl BINARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`BINARY_OP_PREDICATE', `\n+instruct $1(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src1$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -426,6 +571,15 @@\n-BINARY_OP_UNPREDICATED(vaddB, AddVB, B, 16, sve_add)\n-BINARY_OP_UNPREDICATED(vaddS, AddVS, H, 8,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddI, AddVI, S, 4,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddL, AddVL, D, 2,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddF, AddVF, S, 4,  sve_fadd)\n-BINARY_OP_UNPREDICATED(vaddD, AddVD, D, 2,  sve_fadd)\n+BINARY_OP_UNPREDICATE(vaddB, AddVB, B, 16, sve_add)\n+BINARY_OP_UNPREDICATE(vaddS, AddVS, H, 8,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddI, AddVI, S, 4,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddL, AddVL, D, 2,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddF, AddVF, S, 4,  sve_fadd)\n+BINARY_OP_UNPREDICATE(vaddD, AddVD, D, 2,  sve_fadd)\n+\n+\/\/ vector add - predicated\n+BINARY_OP_PREDICATE(vaddB_mask, AddVB, B, sve_add)\n+BINARY_OP_PREDICATE(vaddS_mask, AddVS, H, sve_add)\n+BINARY_OP_PREDICATE(vaddI_mask, AddVI, S, sve_add)\n+BINARY_OP_PREDICATE(vaddL_mask, AddVL, D, sve_add)\n+BINARY_OP_PREDICATE(vaddF_mask, AddVF, S, sve_fadd)\n+BINARY_OP_PREDICATE(vaddD_mask, AddVD, D, sve_fadd)\n+\n@@ -433,2 +587,2 @@\n-dnl BINARY_OP_UNSIZED($1,        $2,      $3,          $4  )\n-dnl BINARY_OP_UNSIZED(insn_name, op_name, min_vec_len, insn)\n+dnl BINARY_OP_UNSIZED($1,        $2,      $3  )\n+dnl BINARY_OP_UNSIZED(insn_name, op_name, insn)\n@@ -437,1 +591,1 @@\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= $3);\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n@@ -440,1 +594,1 @@\n-  format %{ \"$4  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"$3  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -442,1 +596,1 @@\n-    __ $4(as_FloatRegister($dst$$reg),\n+    __ $3(as_FloatRegister($dst$$reg),\n@@ -448,1 +602,1 @@\n-\n+dnl\n@@ -450,1 +604,1 @@\n-BINARY_OP_UNSIZED(vand, AndV, 16, sve_and)\n+BINARY_OP_UNSIZED(vand, AndV, sve_and)\n@@ -453,1 +607,1 @@\n-BINARY_OP_UNSIZED(vor, OrV, 16, sve_orr)\n+BINARY_OP_UNSIZED(vor, OrV, sve_orr)\n@@ -456,1 +610,29 @@\n-BINARY_OP_UNSIZED(vxor, XorV, 16, sve_eor)\n+BINARY_OP_UNSIZED(vxor, XorV, sve_eor)\n+\n+dnl BINARY_LOGIC_OP_PREDICATE($1,        $2,      $3  )\n+dnl BINARY_LOGIC_OP_PREDICATE(insn_name, op_name, insn)\n+define(`BINARY_LOGIC_OP_PREDICATE', `\n+instruct $1(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ $3(as_FloatRegister($dst_src1$$reg), size,\n+         as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector and - predicated\n+BINARY_LOGIC_OP_PREDICATE(vand_mask, AndV, sve_and)\n+\n+\/\/ vector or - predicated\n+BINARY_LOGIC_OP_PREDICATE(vor_mask, OrV, sve_or)\n+\n+\/\/ vector xor - predicated\n+BINARY_LOGIC_OP_PREDICATE(vxor_mask, XorV, sve_eor)\n+\n@@ -472,2 +654,2 @@\n-\n-\/\/ vector float div\n+dnl\n+\/\/ vector fdiv\n@@ -477,1 +659,3 @@\n-\/\/ vector min\/max\n+\/\/ vector fdiv - predicated\n+BINARY_OP_PREDICATE(vfdivF_mask, DivVF, S, sve_fdiv)\n+BINARY_OP_PREDICATE(vfdivD_mask, DivVD, D, sve_fdiv)\n@@ -479,1 +663,5 @@\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX($1     , $2, $3   , $4  )\n+dnl VMINMAX(op_name, op, finsn, insn)\n+define(`VMINMAX', `\n+instruct v$1(vReg dst_src1, vReg src2) %{\n@@ -481,1 +669,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src1 ($2 dst_src1 src2));\n@@ -483,1 +671,1 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -488,1 +676,1 @@\n-      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n@@ -492,1 +680,1 @@\n-      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n@@ -497,1 +685,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max\n+VMINMAX(min, MinV, sve_fmin, sve_smin)\n+VMINMAX(max, MaxV, sve_fmax, sve_smax)\n@@ -499,1 +691,5 @@\n-instruct vmax(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX_PREDICATE($1     , $2, $3   , $4  )\n+dnl VMINMAX_PREDICATE(op_name, op, finsn, insn)\n+define(`VMINMAX_PREDICATE', `\n+instruct v$1_mask(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -501,1 +697,1 @@\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n@@ -503,1 +699,1 @@\n-  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -508,2 +704,2 @@\n-      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -512,2 +708,2 @@\n-      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -517,1 +713,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max - predicated\n+VMINMAX_PREDICATE(min, MinV, sve_fmin, sve_smin)\n+VMINMAX_PREDICATE(max, MaxV, sve_fmax, sve_smax)\n@@ -670,1 +870,1 @@\n-\n+dnl\n@@ -676,2 +876,12 @@\n-BINARY_OP_UNPREDICATED(vmulF, MulVF, S, 4, sve_fmul)\n-BINARY_OP_UNPREDICATED(vmulD, MulVD, D, 2, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulF, MulVF, S, 4, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulD, MulVD, D, 2, sve_fmul)\n+\n+\/\/ vector mul - predicated\n+BINARY_OP_PREDICATE(vmulB_mask, MulVB, B, sve_mul)\n+BINARY_OP_PREDICATE(vmulS_mask, MulVS, H, sve_mul)\n+BINARY_OP_PREDICATE(vmulI_mask, MulVI, S, sve_mul)\n+BINARY_OP_PREDICATE(vmulL_mask, MulVL, D, sve_mul)\n+BINARY_OP_PREDICATE(vmulF_mask, MulVF, S, sve_fmul)\n+BINARY_OP_PREDICATE(vmulD_mask, MulVD, D, sve_fmul)\n+\n+\/\/ vector neg\n@@ -679,0 +889,13 @@\n+instruct vnegI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (NegVI src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant element_size =\n+      elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_neg(as_FloatRegister($dst$$reg), element_size,\n+               ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -695,1 +918,0 @@\n-\/\/ vector fneg\n@@ -699,0 +921,4 @@\n+\/\/ vector fneg - predicated\n+UNARY_OP_PREDICATE(vnegF_mask, NegVF, S, T_FLOAT,  sve_fneg)\n+UNARY_OP_PREDICATE(vnegD_mask, NegVD, D, T_DOUBLE, sve_fneg)\n+\n@@ -711,20 +937,0 @@\n-\/\/ vector mask compare\n-\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t # vector mask cmp (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = vector_element_basic_type(this);\n-    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n-                ptrue, as_FloatRegister($src1$$reg),\n-                as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -733,1 +939,1 @@\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -735,5 +941,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t # vector blend (sve)\" %}\n@@ -743,3 +947,1 @@\n-    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n-                 as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n@@ -751,22 +953,0 @@\n-\/\/ vector blend with compare\n-\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t # vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n-  ins_encode %{\n-    BasicType bt = vector_element_basic_type(this);\n-    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n-                ptrue, as_FloatRegister($src3$$reg),\n-                as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -779,2 +959,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_neg $dst, $src\\t # vector load mask (B)\" %}\n+  ins_cost(0);\n+  format %{ \"vloadmaskB (elided)\\t# vector load mask (sve) (B) - do nothing\" %}\n@@ -782,2 +962,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    \/\/ empty\n@@ -792,3 +971,2 @@\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector load mask (B to H)\" %}\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\t# vector load mask (sve) (B to H)\" %}\n@@ -798,2 +976,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -809,1 +985,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -811,2 +987,1 @@\n-            \"sve_uunpklo $dst, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector load mask (B to S)\" %}\n+            \"sve_uunpklo $dst, $dst\\t# vector load mask (sve) (B to S)\" %}\n@@ -818,2 +993,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -829,1 +1002,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -832,2 +1005,1 @@\n-            \"sve_uunpklo $dst, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector load mask (B to D)\" %}\n+            \"sve_uunpklo $dst, $dst\\t# vector load mask (sve) (B to D)\" %}\n@@ -841,2 +1013,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -852,2 +1022,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_neg $dst, $src\\t # vector store mask (B)\" %}\n+  ins_cost(0);\n+  format %{ \"vstoremaskB (elided)\\t# vector store mask (sve) (B) - do nothing\" %}\n@@ -855,2 +1025,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    \/\/ empty\n@@ -865,1 +1034,1 @@\n-  ins_cost(3 * SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -867,2 +1036,1 @@\n-            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector store mask (sve) (H to B)\" %}\n+            \"sve_uzp1 $dst, $src, $tmp\\t# vector store mask (sve) (H to B)\" %}\n@@ -873,3 +1041,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n@@ -884,1 +1049,1 @@\n-  ins_cost(4 * SVE_COST);\n+  ins_cost(3 * SVE_COST);\n@@ -887,2 +1052,1 @@\n-            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector store mask (sve) (S to B)\" %}\n+            \"sve_uzp1 $dst, $dst, $tmp\\t# vector store mask (sve) (S to B)\" %}\n@@ -895,2 +1059,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -905,1 +1067,1 @@\n-  ins_cost(5 * SVE_COST);\n+  ins_cost(4 * SVE_COST);\n@@ -909,2 +1071,1 @@\n-            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # vector store mask (sve) (D to B)\" %}\n+            \"sve_uzp1 $dst, $dst, $tmp\\t# vector store mask (sve) (D to B)\" %}\n@@ -919,2 +1080,0 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n@@ -927,7 +1086,5 @@\n-instruct vloadmask_loadV(vReg dst, vmemA mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n-            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t # load vector mask (sve)\" %}\n+instruct loadVMask(vReg dst, vmemA mem) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LoadVectorMask mem));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ld1b $dst, $mem\\t# load vector mask (sve)\" %}\n@@ -935,0 +1092,2 @@\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type.\n@@ -936,2 +1095,2 @@\n-    Assembler::SIMD_RegVariant to_vect_size =\n-              elemType_to_regVariant(vector_element_basic_type(this));\n+    Assembler::SIMD_RegVariant element_size =\n+      elemType_to_regVariant(vector_element_basic_type(this));\n@@ -939,1 +1098,1 @@\n-                         T_BOOLEAN, to_vect_size, $mem->opcode(),\n+                         T_BOOLEAN, element_size, $mem->opcode(),\n@@ -941,1 +1100,0 @@\n-    __ sve_neg(dst_reg, to_vect_size, ptrue, dst_reg);\n@@ -946,8 +1104,5 @@\n-instruct storeV_vstoremask(vmemA mem, vReg src, vReg tmp, immI size) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= 2 &&\n-            n->as_StoreVector()->vect_type()->element_basic_type() == T_BOOLEAN);\n-  match(Set mem (StoreVector mem (VectorStoreMask src size)));\n-  effect(TEMP tmp);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t # store vector mask (sve)\" %}\n+instruct storeVMask(vReg src, vmemA mem) %{\n+  predicate(UseSVE > 0);\n+  match(Set mem (StoreVectorMask mem src));\n+  format %{ \"sve_st1b $src, $mem\\t # store vector mask (sve)\" %}\n+  ins_cost(SVE_COST);\n@@ -955,6 +1110,6 @@\n-    Assembler::SIMD_RegVariant from_vect_size =\n-              elemBytes_to_regVariant((int)$size$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_size, ptrue,\n-               as_FloatRegister($src$$reg));\n-    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n-                         ptrue, T_BOOLEAN, from_vect_size, $mem->opcode(),\n+    \/\/ Store the src vector elements as boolean values.\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    Assembler::SIMD_RegVariant element_size =\n+      elemType_to_regVariant(vector_element_basic_type(this, $src));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg, ptrue,\n+                         T_BOOLEAN, element_size, $mem->opcode(),\n@@ -1444,0 +1599,14 @@\n+\/\/ vector shift - predicated\n+BINARY_OP_PREDICATE(vasrB_mask, RShiftVB,  B, sve_asr)\n+BINARY_OP_PREDICATE(vasrS_mask, RShiftVS,  H, sve_asr)\n+BINARY_OP_PREDICATE(vasrI_mask, RShiftVI,  S, sve_asr)\n+BINARY_OP_PREDICATE(vasrL_mask, RShiftVL,  D, sve_asr)\n+BINARY_OP_PREDICATE(vlslB_mask, LShiftVB,  B, sve_lsl)\n+BINARY_OP_PREDICATE(vlslS_mask, LShiftVS,  H, sve_lsl)\n+BINARY_OP_PREDICATE(vlslI_mask, LShiftVI,  S, sve_lsl)\n+BINARY_OP_PREDICATE(vlslL_mask, LShiftVL,  D, sve_lsl)\n+BINARY_OP_PREDICATE(vlsrB_mask, URShiftVB, B, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrS_mask, URShiftVS, H, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrI_mask, URShiftVI, S, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrL_mask, URShiftVL, D, sve_lsr)\n+\n@@ -1448,0 +1617,4 @@\n+\/\/ vector sqrt - predicated\n+UNARY_OP_PREDICATE(vsqrtF_mask, SqrtVF, S, T_FLOAT,  sve_fsqrt)\n+UNARY_OP_PREDICATE(vsqrtD_mask, SqrtVD, D, T_DOUBLE, sve_fsqrt)\n+\n@@ -1449,6 +1622,14 @@\n-BINARY_OP_UNPREDICATED(vsubB, SubVB, B, 16, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubS, SubVS, H, 8, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubI, SubVI, S, 4, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubL, SubVL, D, 2, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubF, SubVF, S, 4, sve_fsub)\n-BINARY_OP_UNPREDICATED(vsubD, SubVD, D, 2, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubB, SubVB, B, 16, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubS, SubVS, H, 8, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubI, SubVI, S, 4, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubL, SubVL, D, 2, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubF, SubVF, S, 4, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubD, SubVD, D, 2, sve_fsub)\n+\n+\/\/ vector sub - predicated\n+BINARY_OP_PREDICATE(vsubB_mask, SubVB, B, sve_sub)\n+BINARY_OP_PREDICATE(vsubS_mask, SubVS, H, sve_sub)\n+BINARY_OP_PREDICATE(vsubI_mask, SubVI, S, sve_sub)\n+BINARY_OP_PREDICATE(vsubL_mask, SubVL, D, sve_sub)\n+BINARY_OP_PREDICATE(vsubF_mask, SubVF, S, sve_fsub)\n+BINARY_OP_PREDICATE(vsubD_mask, SubVD, D, sve_fsub)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":370,"deletions":189,"binary":false,"changes":559,"status":"modified"},{"patch":"@@ -2992,0 +2992,1 @@\n+  INSN(sve_and,  0b00000100, 0b011010000); \/\/ vector and\n@@ -2994,1 +2995,1 @@\n-  INSN(sve_cnt,  0b00000100, 0b011010101)  \/\/ count non-zero bits\n+  INSN(sve_cnt,  0b00000100, 0b011010101); \/\/ count non-zero bits\n@@ -2996,0 +2997,1 @@\n+  INSN(sve_eor,  0b00000100, 0b011001000); \/\/ vector eor\n@@ -3002,0 +3004,1 @@\n+  INSN(sve_or,   0b00000100, 0b011000000); \/\/ vector or\n@@ -3235,1 +3238,28 @@\n-   \/\/ SVE cpy immediate\n+  void sve_pfalse(PRegister pd) {\n+    starti;\n+    f(0b00100101, 31, 24), f(0b00, 23, 22), f(0b011000111001, 21, 10);\n+    f(0b000000, 9, 4), prf(pd, 0);\n+  }\n+\n+\/\/ SVE load\/store predicate register\n+#define INSN(NAME, op1)                                                  \\\n+  void NAME(PRegister Pt, const Address &a)  {                           \\\n+    starti;                                                              \\\n+    assert(a.index() == noreg, \"invalid address variant\");               \\\n+    f(op1, 31, 29), f(0b0010110, 28, 22), sf(a.offset() >> 3, 21, 16),   \\\n+    f(0b000, 15, 13), f(a.offset() & 0x7, 12, 10), srf(a.base(), 5),     \\\n+    f(0, 4), prf(Pt, 0);                                                 \\\n+  }\n+\n+  INSN(sve_ldr, 0b100); \/\/ LDR (predicate)\n+  INSN(sve_str, 0b111); \/\/ STR (predicate)\n+#undef INSN\n+\n+  \/\/ SVE move predicate register\n+  void sve_mov(PRegister Pd, PRegister Pn) {\n+    starti;\n+    f(0b001001011000, 31, 20), prf(Pn, 16), f(0b01, 15, 14), prf(Pn, 10);\n+    f(0, 9), prf(Pn, 5), f(0, 4), prf(Pd, 0);\n+  }\n+\n+  \/\/ SVE cpy immediate\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":32,"deletions":2,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -318,0 +318,1 @@\n+  PRegSet               _p_regs;\n@@ -331,0 +332,2 @@\n+        } else if (vm_reg->is_PRegister()) {\n+          _p_regs += PRegSet::of(vm_reg->as_PRegister());\n@@ -344,1 +347,2 @@\n-      _fp_regs() {\n+      _fp_regs(),\n+      _p_regs() {\n@@ -352,0 +356,1 @@\n+    __ push_p(_p_regs, sp);\n@@ -356,0 +361,1 @@\n+    __ pop_p(_p_regs, sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2215,0 +2215,76 @@\n+\/\/ Return the number of dwords pushed\n+int MacroAssembler::push_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  int num_of_regs = PRegisterImpl::number_of_saved_registers;\n+  unsigned char regs[num_of_regs];\n+  int count = 0;\n+  for (int reg = 0; reg < num_of_regs; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_push_bytes = align_up(sve_predicate_size_in_slots *\n+                                  VMRegImpl::stack_slot_size * count, 16);\n+  sub(stack, stack, total_push_bytes);\n+  for (int i = 0; i < count; i++) {\n+    sve_str(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  return total_push_bytes \/ 8;\n+}\n+\n+\/\/ Return the number of dwords poped\n+int MacroAssembler::pop_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  int num_of_regs = PRegisterImpl::number_of_saved_registers;\n+  unsigned char regs[num_of_regs];\n+  int count = 0;\n+  for (int reg = 0; reg < num_of_regs; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_pop_bytes = align_up(sve_predicate_size_in_slots *\n+                                 VMRegImpl::stack_slot_size * count, 16);\n+  for (int i = count - 1; i >= 0; i--) {\n+    sve_ldr(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  add(stack, stack, total_pop_bytes);\n+  return total_pop_bytes \/ 8;\n+}\n+\n@@ -2673,1 +2749,1 @@\n-                                    int sve_vector_size_in_bytes) {\n+                                    int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n@@ -2690,0 +2766,6 @@\n+  if (save_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    sub(sp, sp, total_predicate_in_bytes);\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      sve_str(as_PRegister(i), Address(sp, i));\n+    }\n+  }\n@@ -2693,1 +2775,7 @@\n-                                   int sve_vector_size_in_bytes) {\n+                                   int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n+  if (restore_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    for (int i = PRegisterImpl::number_of_saved_registers - 1; i >= 0; i--) {\n+      sve_ldr(as_PRegister(i), Address(sp, i));\n+    }\n+    add(sp, sp, total_predicate_in_bytes);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":90,"deletions":2,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -471,0 +471,3 @@\n+  int push_p(unsigned int bitset, Register stack);\n+  int pop_p(unsigned int bitset, Register stack);\n+\n@@ -482,0 +485,3 @@\n+  void push_p(PRegSet regs, Register stack) { if (regs.bits()) push_p(regs.bits(), stack); }\n+  void pop_p(PRegSet regs, Register stack) { if (regs.bits()) pop_p(regs.bits(), stack); }\n+\n@@ -913,1 +919,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                      int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -915,1 +921,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                     int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -1387,0 +1393,1 @@\n+\n@@ -1390,0 +1397,4 @@\n+  void spill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_str(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1400,0 +1411,1 @@\n+\n@@ -1403,0 +1415,4 @@\n+  void unspill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_ldr(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1425,0 +1441,6 @@\n+  void spill_copy_sve_predicate_stack_to_stack(int src_offset, int dst_offset,\n+                                               int sve_predicate_reg_size_in_bytes) {\n+    sve_ldr(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, src_offset));\n+    sve_str(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, dst_offset));\n+    reinitialize_ptrue();\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -37,1 +37,2 @@\n-  = ConcreteRegisterImpl::max_fpr + PRegisterImpl::number_of_registers;\n+  = ConcreteRegisterImpl::max_fpr +\n+    PRegisterImpl::number_of_registers * PRegisterImpl::max_slots_per_register;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -245,0 +245,5 @@\n+    number_of_governing_registers = 8,\n+    \/\/ AArch64 has 8 governing predicate registers, but p7 is used as an\n+    \/\/ all-1s register so the predicates to save are from p0 to p6 if we\n+    \/\/ don't have non-governing predicate registers support.\n+    number_of_saved_registers = number_of_governing_registers - 1,\n@@ -378,0 +383,1 @@\n+typedef AbstractRegSet<PRegister> PRegSet;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -102,1 +102,4 @@\n-  int v0_offset_in_bytes(void)   { return 0; }\n+  int v0_offset_in_bytes();\n+\n+  \/\/ Total stack size in bytes for saving sve predicate registers.\n+  int total_sve_predicate_in_bytes();\n@@ -140,1 +143,1 @@\n-  int r0_offset = (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n+  int r0_offset = v0_offset_in_bytes() + (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n@@ -144,0 +147,20 @@\n+int RegisterSaver::v0_offset_in_bytes() {\n+  \/\/ The floating point registers are located above the predicate registers if\n+  \/\/ they are present in the stack frame pushed by save_live_registers(). So the\n+  \/\/ offset depends on the saved total predicate vectors in the stack frame.\n+  return (total_sve_predicate_in_bytes() \/ VMRegImpl::stack_slot_size) * BytesPerInt;\n+}\n+\n+int RegisterSaver::total_sve_predicate_in_bytes() {\n+#if COMPILER2\n+  if (_save_vectors && Matcher::supports_scalable_vector()) {\n+    \/\/ The number of total predicate bytes is unlikely to be a multiple\n+    \/\/ of 16 bytes so we manually align it up.\n+    return align_up(Matcher::scalable_predicate_reg_slots() *\n+                    VMRegImpl::stack_slot_size *\n+                    PRegisterImpl::number_of_saved_registers, 16);\n+  }\n+#endif\n+  return 0;\n+}\n+\n@@ -148,0 +171,3 @@\n+  int sve_predicate_size_in_slots = 0;\n+  int total_predicate_in_bytes = total_sve_predicate_in_bytes();\n+  int total_predicate_in_slots = total_predicate_in_bytes \/ VMRegImpl::stack_slot_size;\n@@ -151,2 +177,5 @@\n-  sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n-  sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+  if (use_sve) {\n+    sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n+    sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n@@ -157,1 +186,0 @@\n-    int vect_words = 0;\n@@ -165,3 +193,4 @@\n-    vect_words = FloatRegisterImpl::number_of_registers * extra_save_slots_per_register \/\n-                 VMRegImpl::slots_per_word;\n-    additional_frame_words += vect_words;\n+    int extra_vector_bytes = extra_save_slots_per_register *\n+                             VMRegImpl::stack_slot_size *\n+                             FloatRegisterImpl::number_of_registers;\n+    additional_frame_words += ((extra_vector_bytes + total_predicate_in_bytes) \/ wordSize);\n@@ -185,1 +214,1 @@\n-  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes);\n+  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes, total_predicate_in_bytes);\n@@ -202,2 +231,1 @@\n-      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots),\n-                                r->as_VMReg());\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots), r->as_VMReg());\n@@ -211,1 +239,1 @@\n-      sp_offset = use_sve ? (sve_vector_size_in_slots * i) :\n+      sp_offset = use_sve ? (total_predicate_in_slots + sve_vector_size_in_slots * i) :\n@@ -216,2 +244,9 @@\n-    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n-                              r->as_VMReg());\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+  }\n+\n+  if (_save_vectors && use_sve) {\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      PRegister r = as_PRegister(i);\n+      int sp_offset = sve_predicate_size_in_slots * i;\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+    }\n@@ -226,1 +261,1 @@\n-                   Matcher::scalable_vector_reg_size(T_BYTE));\n+                   Matcher::scalable_vector_reg_size(T_BYTE), total_sve_predicate_in_bytes());\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":50,"deletions":15,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -42,2 +42,1 @@\n-  assert( is_Register(), \"must be\");\n-  \/\/ Yuk\n+  assert(is_Register(), \"must be\");\n@@ -48,2 +47,1 @@\n-  assert( is_FloatRegister() && is_even(value()), \"must be\" );\n-  \/\/ Yuk\n+  assert(is_FloatRegister() && is_even(value()), \"must be\");\n@@ -55,1 +53,1 @@\n-  assert( is_PRegister(), \"must be\" );\n+  assert(is_PRegister(), \"must be\");\n","filename":"src\/hotspot\/cpu\/aarch64\/vmreg_aarch64.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -992,0 +992,4 @@\n+const bool Matcher::match_rule_supported_masked_vector(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2159,0 +2159,4 @@\n+const bool Matcher::match_rule_supported_masked_vector(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1545,0 +1545,4 @@\n+const bool Matcher::match_rule_supported_masked_vector(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1768,0 +1768,4 @@\n+const bool Matcher::match_rule_supported_masked_vector(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -949,1 +949,1 @@\n-    return \"Type::BOTTOM\";\n+    return \"TypeVMask::VMASK\";\n","filename":"src\/hotspot\/share\/adlc\/archDesc.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,0 +273,1 @@\n+  if( strcmp(opType,\"LoadVectorMask\")==0 )  return Form::idealV;\n@@ -291,0 +292,1 @@\n+  if( strcmp(opType,\"StoreVectorMask\")==0 )  return Form::idealV;\n","filename":"src\/hotspot\/share\/adlc\/forms.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -786,1 +786,2 @@\n-       !strcmp(_matrule->_rChild->_opType,\"VectorMaskGen\")||\n+       !strcmp(_matrule->_rChild->_opType,\"VectorMaskGen\") ||\n+       !strcmp(_matrule->_rChild->_opType,\"VectorCmpMaskGen\") ||\n@@ -2277,0 +2278,1 @@\n+  if (strcmp(name, \"RegVMask\") == 0) size = 1;\n@@ -3510,1 +3512,1 @@\n-    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\", \"LoadVectorMask\", \"StoreVectorMask\",\n@@ -4085,0 +4087,1 @@\n+        strcmp(opType,\"MaskToVector\")==0 ||\n@@ -4202,1 +4205,1 @@\n-    \"VectorMaskWrapper\", \"VectorMaskCmp\", \"VectorReinterpret\",\"LoadVectorMasked\",\"StoreVectorMasked\",\n+    \"VectorMaskWrapper\", \"VectorMaskCmp\", \"VectorReinterpret\", \"LoadVectorMasked\",\"StoreVectorMasked\",\n@@ -4206,1 +4209,2 @@\n-    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\"\n+    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\",\n+    \"MaskToVector\", \"LoadVectorMask\", \"StoreVectorMask\"\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -821,0 +821,5 @@\n+  do_intrinsic(_VectorBinaryMaskOp, jdk_internal_vm_vector_VectorSupport, vector_binary_mask_op_name, vector_binary_mask_op_sig, F_S)          \\\n+   do_signature(vector_binary_mask_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"        \\\n+                                            \"Ljava\/lang\/Object;Ljdk\/internal\/vm\/vector\/VectorSupport$BinaryMaskOperation;)Ljava\/lang\/Object;\") \\\n+   do_name(vector_binary_mask_op_name,     \"binaryMaskOp\")                                                                                     \\\n+                                                                                                                                               \\\n@@ -850,0 +855,6 @@\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorStoreMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_store_masked_op_name, vector_store_masked_op_sig, F_S)             \\\n+   do_signature(vector_store_masked_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$Vector;\"    \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;I\"                            \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorMaskedOperation;)V\")                            \\\n+   do_name(vector_store_masked_op_name,     \"storeMasked\")                                                                                     \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -663,0 +663,1 @@\n+  case vmIntrinsics::_VectorBinaryMaskOp:\n@@ -668,0 +669,1 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,0 +80,1 @@\n+  if( _is_predicate ) tty->print(\"Predicate \");\n@@ -487,0 +488,1 @@\n+\n@@ -641,1 +643,1 @@\n-      } else if (lrg.num_regs() == 1) {\n+      } else if (lrg.num_regs() == 1 && !lrg.is_scalable()) {\n@@ -656,9 +658,13 @@\n-          OptoReg::Name lo = OptoReg::add(hi, (1-num_regs)); \/\/ Find lo\n-          \/\/ We have to use pair [lo,lo+1] even for wide vectors because\n-          \/\/ the rest of code generation works only with pairs. It is safe\n-          \/\/ since for registers encoding only 'lo' is used.\n-          \/\/ Second reg from pair is used in ScheduleAndBundle on SPARC where\n-          \/\/ vector max size is 8 which corresponds to registers pair.\n-          \/\/ It is also used in BuildOopMaps but oop operations are not\n-          \/\/ vectorized.\n-          set2(i, lo);\n+          if (num_regs > 1) {\n+            OptoReg::Name lo = OptoReg::add(hi, (1-num_regs)); \/\/ Find lo\n+            \/\/ We have to use pair [lo,lo+1] even for wide vectors\/vmasks because\n+            \/\/ the rest of code generation works only with pairs. It is safe\n+            \/\/ since for registers encoding only 'lo' is used.\n+            \/\/ Second reg from pair is used in ScheduleAndBundle on SPARC where\n+            \/\/ vector max size is 8 which corresponds to registers pair.\n+            \/\/ It is also used in BuildOopMaps but oop operations are not\n+            \/\/ vectorized.\n+            set2(i, lo);\n+          } else {\n+            set1(i, hi);\n+          }\n@@ -827,0 +833,13 @@\n+        if (ireg == Op_RegVMask) {\n+          assert(Matcher::has_predicated_vectors(), \"predicated vector should be supported\");\n+          lrg._is_predicate = 1;\n+          if (Matcher::supports_scalable_vector()) {\n+            lrg._is_scalable = 1;\n+            \/\/ For scalable predicate, when it is allocated in physical register,\n+            \/\/ num_regs is RegMask::SlotsPerRegVmask for reg mask,\n+            \/\/ which may not be the actual physical register size.\n+            \/\/ If it is allocated in stack, we need to get the actual\n+            \/\/ physical length of scalable predicate register.\n+            lrg.set_scalable_reg_slots(Matcher::scalable_predicate_reg_slots());\n+          }\n+        }\n@@ -969,0 +988,6 @@\n+        case Op_RegVMask:\n+          assert(Matcher::has_predicated_vectors(), \"sanity\");\n+          assert(RegMask::num_registers(Op_RegVMask) == RegMask::SlotsPerRegVmask, \"sanity\");\n+          lrg.set_num_regs(RegMask::SlotsPerRegVmask);\n+          lrg.set_reg_pressure(0);\n+          break;\n@@ -1369,0 +1394,5 @@\n+    } else if (lrg._is_predicate) {\n+      assert(num_regs == RegMask::SlotsPerRegVmask, \"scalable predicate register\");\n+      num_regs = lrg.scalable_reg_slots();\n+      mask.clear_to_sets(num_regs);\n+      return mask.find_first_set(lrg, num_regs);\n@@ -1415,1 +1445,1 @@\n-  if (lrg._is_vector || lrg.num_regs() == 2) {\n+  if (lrg._is_vector || lrg.num_regs() == 2 || lrg.is_scalable()) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":42,"deletions":12,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -166,2 +166,2 @@\n-      \/\/ Should only be a vector for now, but it could also be a RegVMask in future.\n-      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA), \"unexpected scalable reg\");\n+      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA) ||\n+             _is_predicate && (_num_regs == RegMask::SlotsPerRegVmask), \"unexpected scalable reg\");\n@@ -198,0 +198,1 @@\n+         _is_predicate:1,       \/\/ True if in mask\/predicate registers\n","filename":"src\/hotspot\/share\/opto\/chaitin.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -414,0 +414,1 @@\n+macro(LoadVectorMask)\n@@ -415,0 +416,1 @@\n+macro(StoreVectorMask)\n@@ -448,0 +450,1 @@\n+macro(VectorMask)\n@@ -466,0 +469,4 @@\n+macro(VectorToMask)\n+macro(VectorCmpMaskGen)\n+macro(MaskAll)\n+macro(MaskToVector)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3465,0 +3465,2 @@\n+  case Op_LoadVectorMask:\n+  case Op_StoreVectorMask:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -691,0 +691,1 @@\n+        case Op_StoreVectorMask:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -637,0 +637,2 @@\n+  case vmIntrinsics::_VectorBinaryMaskOp:\n+    return inline_vector_nary_mask_operation(2);\n@@ -647,0 +649,2 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store=*\/true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -314,0 +314,1 @@\n+  bool inline_vector_nary_mask_operation(int n);\n@@ -318,0 +319,1 @@\n+  bool inline_vector_mem_masked_operation(bool is_store);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -97,0 +97,1 @@\n+  idealreg2spillmask  [Op_RegVMask] = NULL;\n@@ -111,0 +112,1 @@\n+  idealreg2debugmask  [Op_RegVMask] = NULL;\n@@ -125,0 +127,1 @@\n+  idealreg2mhdebugmask[Op_RegVMask] = NULL;\n@@ -433,1 +436,19 @@\n-#define NOF_STACK_MASKS (3*12)\n+const int Matcher::scalable_predicate_reg_slots() {\n+  assert(Matcher::has_predicated_vectors() && Matcher::supports_scalable_vector(),\n+        \"scalable predicate vector should be supported\");\n+  int vector_reg_bit_size = Matcher::scalable_vector_reg_size(T_BYTE) << LogBitsPerByte;\n+  \/\/ We assume each predicate register is one-eighth of the size of\n+  \/\/ scalable vector register, one mask bit per vector byte.\n+  int predicate_reg_bit_size = vector_reg_bit_size >> 3;\n+  \/\/ Compute number of slots which is required when scalable predicate\n+  \/\/ register is spilled. E.g. if scalable vector register is 640 bits,\n+  \/\/ predicate register is 80 bits, which is 2.5 * slots.\n+  \/\/ We will round up the slot number to power of 2, which is required\n+  \/\/ by find_first_set().\n+  int slots = predicate_reg_bit_size & (BitsPerInt - 1)\n+              ? (predicate_reg_bit_size >> LogBitsPerInt) + 1\n+              : predicate_reg_bit_size >> LogBitsPerInt;\n+  return round_up_power_of_2(slots);\n+}\n+\n+#define NOF_STACK_MASKS (3*13)\n@@ -475,14 +496,17 @@\n-\n-  idealreg2debugmask  [Op_VecA] = &rms[24];\n-  idealreg2debugmask  [Op_VecS] = &rms[25];\n-  idealreg2debugmask  [Op_VecD] = &rms[26];\n-  idealreg2debugmask  [Op_VecX] = &rms[27];\n-  idealreg2debugmask  [Op_VecY] = &rms[28];\n-  idealreg2debugmask  [Op_VecZ] = &rms[29];\n-\n-  idealreg2mhdebugmask[Op_VecA] = &rms[30];\n-  idealreg2mhdebugmask[Op_VecS] = &rms[31];\n-  idealreg2mhdebugmask[Op_VecD] = &rms[32];\n-  idealreg2mhdebugmask[Op_VecX] = &rms[33];\n-  idealreg2mhdebugmask[Op_VecY] = &rms[34];\n-  idealreg2mhdebugmask[Op_VecZ] = &rms[35];\n+  idealreg2spillmask  [Op_RegVMask] = &rms[24];\n+\n+  idealreg2debugmask  [Op_VecA] = &rms[25];\n+  idealreg2debugmask  [Op_VecS] = &rms[26];\n+  idealreg2debugmask  [Op_VecD] = &rms[27];\n+  idealreg2debugmask  [Op_VecX] = &rms[28];\n+  idealreg2debugmask  [Op_VecY] = &rms[29];\n+  idealreg2debugmask  [Op_VecZ] = &rms[30];\n+  idealreg2debugmask  [Op_RegVMask] = &rms[31];\n+\n+  idealreg2mhdebugmask[Op_VecA] = &rms[32];\n+  idealreg2mhdebugmask[Op_VecS] = &rms[33];\n+  idealreg2mhdebugmask[Op_VecD] = &rms[34];\n+  idealreg2mhdebugmask[Op_VecX] = &rms[35];\n+  idealreg2mhdebugmask[Op_VecY] = &rms[36];\n+  idealreg2mhdebugmask[Op_VecZ] = &rms[37];\n+  idealreg2mhdebugmask[Op_RegVMask] = &rms[38];\n@@ -604,0 +628,13 @@\n+    \/\/ Exclude last input arg stack slots to avoid spilling vector register there,\n+    \/\/ otherwise RegVMask spills could stomp over stack slots in caller frame.\n+    for (; (in >= init_in) && (k < scalable_predicate_reg_slots()); k++) {\n+      scalable_stack_mask.Remove(in);\n+      in = OptoReg::add(in, -1);\n+    }\n+\n+    \/\/ For RegVMask\n+    scalable_stack_mask.clear_to_sets(scalable_predicate_reg_slots());\n+    assert(scalable_stack_mask.is_AllStack(), \"should be infinite stack\");\n+    *idealreg2spillmask[Op_RegVMask] = *idealreg2regmask[Op_RegVMask];\n+    idealreg2spillmask[Op_RegVMask]->OR(scalable_stack_mask);\n+\n@@ -618,0 +655,1 @@\n+    *idealreg2spillmask[Op_RegVMask] = RegMask::Empty;\n@@ -659,0 +697,1 @@\n+  *idealreg2debugmask[Op_RegVMask] = *idealreg2spillmask[Op_RegVMask];\n@@ -673,0 +712,1 @@\n+  *idealreg2mhdebugmask[Op_RegVMask] = *idealreg2spillmask[Op_RegVMask];\n@@ -693,0 +733,1 @@\n+  idealreg2debugmask[Op_RegVMask]->SUBTRACT(*caller_save_mask);\n@@ -707,0 +748,1 @@\n+  idealreg2mhdebugmask[Op_RegVMask]->SUBTRACT(*mh_caller_save_mask);\n@@ -968,0 +1010,8 @@\n+  \/\/ Walkaround for vector api.\n+  if (Matcher::has_predicated_vectors() && Matcher::match_rule_supported(Op_VectorToMask)) {\n+    const int length = Matcher::max_vector_size(T_BYTE);\n+    Node* con = new ConINode(TypeInt::ZERO);\n+    Node* in = new ReplicateBNode(con, TypeVect::make(T_BYTE, length));\n+    MachNode *spillRegVMask = match_tree(new VectorToMaskNode(in, TypeVMask::make(T_BYTE, length)));\n+    idealreg2regmask[Op_RegVMask] = &spillRegVMask->out_RegMask();\n+  }\n@@ -2274,0 +2324,11 @@\n+  if (n->is_Vector() && (n->req() == 4) && n->in(3)->is_VectorMask()) {\n+    \/\/ TODO: unary op\n+    \/\/ Handle op with mask\n+    Node* pair = new BinaryNode(n->in(1), n->in(2));\n+    Node* mask = n->in(3);\n+    n->set_req(1, pair);\n+    n->set_req(2, mask);\n+    n->del_req(3);\n+    return;\n+  }\n+\n@@ -2413,1 +2474,2 @@\n-    case Op_VectorMaskCmp: {\n+    case Op_VectorMaskCmp:\n+    case Op_VectorCmpMaskGen: {\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":79,"deletions":17,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -320,0 +320,2 @@\n+  static const bool match_rule_supported_masked_vector(int opcode, int vlen, BasicType bt);\n+\n@@ -344,0 +346,2 @@\n+  \/\/ Actual max scalable predicate register length.\n+  static const int scalable_predicate_reg_slots();\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-class StoreVectorMaskedNode;\n@@ -169,0 +168,1 @@\n+class StoreVectorMaskedNode;\n@@ -171,0 +171,1 @@\n+class VectorMaskNode;\n@@ -695,0 +696,1 @@\n+      DEFINE_CLASS_ID(VectorMask, Type, 7)\n@@ -925,0 +927,1 @@\n+  DEFINE_CLASS_QUERY(VectorMask)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -312,1 +312,1 @@\n-      assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      assert(val->ideal_reg() == Op_VecA || val->ideal_reg() == Op_RegVMask, \"scalable register\");\n@@ -316,1 +316,1 @@\n-        n_regs = RegMask::SlotsPerVecA;\n+        n_regs = lrgs(val_idx)._is_predicate ? RegMask::SlotsPerRegVmask : RegMask::SlotsPerVecA;\n@@ -321,2 +321,1 @@\n-      if (lrgs(val_idx).is_scalable()) {\n-        assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      if (lrgs(val_idx).is_scalable() && val->ideal_reg() == Op_VecA) {\n","filename":"src\/hotspot\/share\/opto\/postaloc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -231,1 +231,1 @@\n-  if (lrg.is_scalable()) {\n+  if (lrg.is_scalable() && lrg._is_vector) {\n","filename":"src\/hotspot\/share\/opto\/regmask.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -108,1 +108,1 @@\n-         };\n+         SlotsPerRegVmask = X86_ONLY(2) NOT_X86(1) };\n","filename":"src\/hotspot\/share\/opto\/regmask.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+  { Bad,             T_ILLEGAL,    \"vmask:\",        false, Op_RegVMask,          relocInfo::none          },  \/\/ VMask\n@@ -682,0 +683,4 @@\n+  if (Matcher::has_predicated_vectors()) {\n+    TypeVMask::VMASK = TypeVMask::make(T_BYTE, Matcher::max_vector_size(T_BYTE));\n+  }\n+\n@@ -688,0 +693,1 @@\n+  mreg2type[Op_RegVMask] = TypeVMask::VMASK;\n@@ -1009,0 +1015,1 @@\n+  Bad,          \/\/ VMask - handled in v-call\n@@ -1309,0 +1316,49 @@\n+\/\/===============================TypeVMask=====================================\n+bool TypeVMask::empty(void) const {\n+  return false;\n+}\n+\n+\/\/------------------------------eq---------------------------------\n+\/\/ Structural equality check for Type representations\n+bool TypeVMask::eq(const Type *t) const {\n+  const TypeVMask* vt = t->is_vmask();\n+  return (_elem_size == vt->_elem_size) && (_length == vt->_length);\n+}\n+\n+\/\/------------------------------hash-------------------------------\n+\/\/ Type-specific hashing function.\n+int TypeVMask::hash(void) const {\n+  return _elem_size + _length * _elem_size;\n+}\n+\n+bool TypeVMask::singleton(void) const {\n+  return false;\n+}\n+\n+const TypeVMask* TypeVMask::VMASK = NULL;\n+\n+\/\/------------------------------xdual-------------------------------\n+\/\/ Dual: compute field-by-field dual\n+const Type* TypeVMask::xdual() const {\n+  return this;\n+}\n+\n+\/\/ Compute the MEET of two types. It returns a new Type object.\n+const Type* TypeVMask::xmeet(const Type* t) const {\n+  if (this == t) return this;  \/\/ Meeting same type-rep?\n+  switch (t->base()) {          \/\/ switch on original type\n+  case Bottom:                  \/\/ Ye Olde Default\n+    return t;\n+  case VMask: {                 \/\/ Meeting 2 vmask?\n+    const TypeVMask* vmask = t->is_vmask();\n+    assert(base() == vmask->base(), \"basic type\");\n+    return TypeVMask::make(_elem_size, _length);\n+  }\n+  case Top:\n+    return this;\n+  default:                      \/\/ All else is a mistake\n+    typerr(t);\n+  }\n+  return this;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":56,"deletions":0,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+class   TypeVMask;\n@@ -98,0 +99,1 @@\n+    VMask,                      \/\/ Vector mask\/predicate\n@@ -302,0 +304,2 @@\n+  const TypeVMask  *is_vmask() const;            \/\/ VMask\n+  const TypeVMask  *isa_vmask() const;           \/\/ Returns NULL if not a VMask\n@@ -848,0 +852,33 @@\n+class TypeVMask : public Type {\n+  const uint _elem_size;  \/\/ Element size in bytes of the masked vector\n+  const uint _length;     \/\/ Number of elements in the masked vector\n+public:\n+  TypeVMask(uint elem_size, uint length) :\n+    Type(VMask), _elem_size(elem_size), _length(length) {}\n+  TypeVMask(const BasicType elem_bt, uint length) : Type(VMask),\n+    _elem_size(type2aelembytes(elem_bt)), _length(length) {}\n+\n+  static const TypeVMask* make(const BasicType elem_bt, uint length) {\n+    return (TypeVMask*)(new TypeVMask(elem_bt, length))->hashcons();\n+  }\n+\n+  static const TypeVMask* make(uint elem_size, uint length) {\n+    return (TypeVMask*)(new TypeVMask(elem_size, length))->hashcons();\n+  }\n+\n+  static const TypeVMask* VMASK;\n+\n+  uint element_size_in_bytes(void) const {\n+    return _elem_size;\n+  }\n+  uint length(void) const {\n+    return _length;\n+  }\n+  virtual bool empty(void) const;\n+  virtual bool eq(const Type* t) const;\n+  virtual int hash() const;  \/\/ Type specific hashing\n+  virtual bool singleton(void) const;\n+  virtual const Type* xdual() const;\n+  virtual const Type* xmeet(const Type* t) const;\n+};\n+\n@@ -1694,0 +1731,9 @@\n+inline const TypeVMask* Type::is_vmask() const {\n+  assert( _base == VMask, \"Not a VectorMask\" );\n+  return (TypeVMask*)this;\n+}\n+\n+inline const TypeVMask* Type::isa_vmask() const {\n+  return (_base == VMask) ? (TypeVMask*)this : NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -379,0 +379,221 @@\n+\/\/ public static\n+\/\/ <V, M>\n+\/\/ V binaryMaskOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/                int length, V v1, V v2, M m,\n+\/\/                BinaryMaskOperation<V, M> defaultImpl) {\n+\/\/\n+\/\/ TODO: add the mask support for unary\/ternay mask op. After then, the original intrinsics and above method\n+\/\/ \"LibraryCallKit::inline_vector_nary_operation\" could be removed.\n+\/\/\n+\/\/ The prototype intrinsics might be:\n+\/\/\n+\/\/ public static\n+\/\/ <V, M>\n+\/\/ V unaryMaskOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/               int length, V v, M m,\n+\/\/               UnaryMaskOperation<V, M> defaultImpl) {\n+\/\/\n+\/\/ public static\n+\/\/ <V, M>\n+\/\/ V ternaryMaskOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<?> elementType,\n+\/\/                 int length, V v1, V v2, V v3, M m,\n+\/\/                 TernaryMaskOperation<V, M> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_nary_mask_operation(int n) {\n+  const TypeInt*     opr          = gvn().type(argument(0))->isa_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n+\n+  if (opr == NULL || vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      !opr->is_con() || vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass=%s maskclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int opc = VectorSupport::vop2ideal(opr->get_con(), elem_bt);\n+  int sopc = VectorNode::opcode(opc, elem_bt);\n+  if ((opc != Op_CallLeafVector) && (sopc == 0)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** operation not supported: opc=%s bt=%s\", NodeClassNames[opc], type2name(elem_bt));\n+    }\n+    return false; \/\/ operation not supported\n+  }\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  \/\/ \"argument(n + 5)\" should be the mask object. We assume it is \"null\" when no mask\n+  \/\/ is used to control this operation.\n+  bool use_mask = gvn().type(argument(n + 5)) != TypePtr::NULL_PTR;\n+  if (is_vector_mask(vbox_klass)) {\n+    assert(!use_mask, \"mask operations do not need mask to control\");\n+  }\n+\n+  if (use_mask && !is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (opc == Op_CallLeafVector) {\n+    if (!UseVectorStubs) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** vector stubs support is disabled\");\n+      }\n+      return false;\n+    }\n+    if (!Matcher::supports_vector_calling_convention()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** no vector calling conventions supported\");\n+      }\n+      return false;\n+    }\n+    if (!Matcher::vector_size_supported(elem_bt, num_elem)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** vector size (vlen=%d, etype=%s) is not supported\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false;\n+    }\n+  }\n+\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  VectorMaskUseType mask_use_type = is_vector_mask(vbox_klass) ? VecMaskUseAll\n+                                      : use_mask ? VecMaskUseLoad : VecMaskNotUsed;\n+  if ((sopc != 0) && !arch_supports_vector(sopc, num_elem, elem_bt, mask_use_type)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+                    n, sopc, num_elem, type2name(elem_bt),\n+                    is_vector_mask(vbox_klass) ? 1 : 0);\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* opd1 = NULL; Node* opd2 = NULL; Node* opd3 = NULL;\n+  switch (n) {\n+    case 3: {\n+      opd3 = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+      if (opd3 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v3=%s\",\n+                        NodeClassNames[argument(7)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      \/\/ fall-through\n+    }\n+    case 2: {\n+      opd2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      if (opd2 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v2=%s\",\n+                        NodeClassNames[argument(6)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      \/\/ fall-through\n+    }\n+    case 1: {\n+      opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      if (opd1 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v1=%s\",\n+                        NodeClassNames[argument(5)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      break;\n+    }\n+    default: fatal(\"unsupported arity: %d\", n);\n+  }\n+\n+  Node* mask = NULL;\n+  if (use_mask) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(n + 5), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed mask=%s\",\n+                        NodeClassNames[argument(n + 5)->Opcode()]);\n+        }\n+        return false;\n+      }\n+  }\n+\n+  Node* operation = NULL;\n+  if (opc == Op_CallLeafVector) {\n+    assert(UseVectorStubs, \"sanity\");\n+    operation = gen_call_to_svml(opr->get_con(), elem_bt, num_elem, opd1, opd2);\n+    if (operation == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** svml call failed\");\n+      }\n+      return false;\n+     }\n+  } else {\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    switch (n) {\n+      case 1:\n+      case 2: {\n+        operation = VectorNode::make(sopc, opd1, opd2, vt);\n+        break;\n+      }\n+      case 3: {\n+        operation = VectorNode::make(sopc, opd1, opd2, opd3, vt);\n+        break;\n+      }\n+      default: fatal(\"unsupported arity: %d\", n);\n+    }\n+  }\n+\n+  \/\/ Currently just focus on binary mask operation.\n+  if (n == 2 && mask != NULL) {\n+    if (sopc != 0 &&\n+        Matcher::match_rule_supported_masked_vector(sopc, num_elem, elem_bt) &&\n+        Matcher::match_rule_supported(Op_VectorToMask)) {\n+      const TypeVMask* vmask_type = TypeVMask::make(elem_bt, num_elem);\n+      mask = gvn().transform(new VectorToMaskNode(mask, vmask_type));\n+      operation->add_req(mask);\n+    } else {\n+      \/\/ TODO: arch match rule support check for \"Op_VectorBlend\"\n+      operation = VectorBlendNode::make(gvn(), opd1, operation, mask);\n+    }\n+  } else {\n+    assert(mask == NULL, \"unsupported mask operation\");\n+  }\n+  operation = gvn().transform(operation);\n+\n+  \/\/ Wrap it up in VectorBox to keep object type information.\n+  Node* vbox = box_vector(operation, vbox_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n@@ -464,1 +685,1 @@\n-    res = gvn().transform(new VectorBlendNode(biased_val, res, mask));\n+    res = gvn().transform(VectorBlendNode::make(gvn(), biased_val, res, mask));\n@@ -585,23 +806,36 @@\n-\n-  Node* elem = NULL;\n-  switch (elem_bt) {\n-    case T_BOOLEAN: \/\/ fall-through\n-    case T_BYTE:    \/\/ fall-through\n-    case T_SHORT:   \/\/ fall-through\n-    case T_CHAR:    \/\/ fall-through\n-    case T_INT: {\n-      elem = gvn().transform(new ConvL2INode(bits));\n-      break;\n-    }\n-    case T_DOUBLE: {\n-      elem = gvn().transform(new MoveL2DNode(bits));\n-      break;\n-    }\n-    case T_FLOAT: {\n-      bits = gvn().transform(new ConvL2INode(bits));\n-      elem = gvn().transform(new MoveI2FNode(bits));\n-      break;\n-    }\n-    case T_LONG: {\n-      elem = bits; \/\/ no conversion needed\n-      break;\n+  Node* node = NULL;\n+  const TypeLong* value = gvn().type(bits)->is_long();\n+  if (is_vector_mask(vbox_klass) &&\n+     value->is_con() && (value->get_con() == -1 || value->get_con() == 0) &&\n+     arch_supports_vector(Op_MaskAll, num_elem, elem_bt, VecMaskNotUsed)) {\n+     ConLNode* con = (ConLNode*)gvn().makecon(value);\n+     node = gvn().transform(new MaskAllNode(con, TypeVMask::make(elem_bt, num_elem)));\n+     \/\/ TODO: remove the conversion once reboxing for predicate is supported.\n+     node = gvn().transform(new MaskToVectorNode(node, TypeVect::make(elem_bt, num_elem)));\n+  }\n+\n+  if (node == NULL) {\n+    Node* elem = NULL;\n+    switch (elem_bt) {\n+      case T_BOOLEAN: \/\/ fall-through\n+      case T_BYTE:    \/\/ fall-through\n+      case T_SHORT:   \/\/ fall-through\n+      case T_CHAR:    \/\/ fall-through\n+      case T_INT: {\n+        elem = gvn().transform(new ConvL2INode(bits));\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        elem = gvn().transform(new MoveL2DNode(bits));\n+        break;\n+      }\n+      case T_FLOAT: {\n+        bits = gvn().transform(new ConvL2INode(bits));\n+        elem = gvn().transform(new MoveI2FNode(bits));\n+        break;\n+      }\n+      case T_LONG: {\n+        elem = bits; \/\/ no conversion needed\n+        break;\n+      }\n+      default: fatal(\"%s\", type2name(elem_bt));\n@@ -609,2 +843,0 @@\n-    default: fatal(\"%s\", type2name(elem_bt));\n-  }\n@@ -612,2 +844,3 @@\n-  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n-  broadcast = gvn().transform(broadcast);\n+    node = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n+    node = gvn().transform(node);\n+  }\n@@ -615,1 +848,1 @@\n-  Node* box = box_vector(broadcast, vbox_type, elem_bt, num_elem);\n+  Node* box = box_vector(node, vbox_type, elem_bt, num_elem);\n@@ -802,0 +1035,133 @@\n+\/\/    <C, V extends Vector<?>,\n+\/\/     M extends VectorMask<?>>\n+\/\/    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                     int length, Object base, long offset,   \/\/ Unsafe addressing\n+\/\/                     V v, M m,\n+\/\/                     C container, int index,      \/\/ Arguments for default implementation\n+\/\/                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+\/\/\n+\/\/    TODO: Handle special case where load\/store happens from\/to byte array but element type\n+\/\/    is not byte. And also add the mask support for vector load. The intrinsic looks like:\n+\/\/\n+\/\/    <C, V extends Vector<?>,\n+\/\/     M extends VectorMask<?>>\n+\/\/    V loadMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+\/\/                 int vlen, Object base, long offset,\n+\/\/                 M m,   \/\/ mask arguemnt\n+\/\/                 Object container, int index,\n+\/\/                 LoadMaskedOperation<C, V, M> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_mem_masked_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+\n+  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int sopc = is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked;\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUseLoad) ||\n+      !Matcher::match_rule_supported_masked_vector(sopc, num_elem, elem_bt)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=yes\",\n+                    is_store, is_store ? \"storeMask\" : \"loadMask\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS;\n+  Node* addr = make_unsafe_address(base, offset, decorators, elem_bt, true);\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+  if (arr_type != NULL && elem_bt != arr_type->elem()->array_element_basic_type()) {\n+    return false;\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(!is_vector_mask(vbox_klass) && is_vector_mask(mbox_klass), \"Invalid class type\");\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      return false; \/\/ operand unboxing failed\n+    }\n+    Node* mask = unbox_vector(argument(8), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(8)->Opcode()]);\n+      }\n+      return false;\n+    }\n+\n+    if (Matcher::match_rule_supported(Op_VectorToMask)) {\n+      const TypeVMask* vmask_type = TypeVMask::make(elem_bt, num_elem);\n+      mask = gvn().transform(new VectorToMaskNode(mask, vmask_type));\n+    }\n+\n+    set_all_memory(reset_memory());\n+    Node* vstore = gvn().transform(new StoreVectorMaskedNode(control(), memory(addr), addr, val, addr_type, mask));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    \/\/ TODO: mask support for load op.\n+    assert(is_store, \"unimplemented load mask\");\n+  }\n+\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n@@ -1146,1 +1512,1 @@\n-  Node* blend = gvn().transform(new VectorBlendNode(v1, v2, mask));\n+  Node* blend = gvn().transform(VectorBlendNode::make(gvn(), v1, v2, mask));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":396,"deletions":30,"binary":false,"changes":426,"status":"modified"},{"patch":"@@ -723,0 +723,25 @@\n+Node* StoreVectorNode::Identity(PhaseGVN* phase) {\n+  \/\/ StoreVectorNode (VectorStoreMask src)  ==>  (StoreVectorMask src).\n+  Node* value = in(MemNode::ValueIn);\n+  if (value->Opcode() == Op_VectorStoreMask) {\n+    assert(this->vect_type()->element_basic_type() == T_BOOLEAN, \"Invalid basic type to store mask\");\n+    if (Matcher::match_rule_supported(Op_StoreVectorMask)) {\n+      const TypeVect* type = value->in(1)->bottom_type()->is_vect();\n+      const TypeVect* mem_type = TypeVect::make(T_BOOLEAN, type->length());\n+      return phase->transform(new StoreVectorMaskNode(in(MemNode::Control),\n+                                                      in(MemNode::Memory),\n+                                                      in(MemNode::Address),\n+                                                      adr_type(), value->in(1), mem_type));\n+    }\n+  }\n+  return StoreNode::Identity(phase);\n+}\n+\n+Node* NegVINode::Identity(PhaseGVN* phase) {\n+  \/\/ NegVI (NegVI src)  ==> src\n+  if (in(1)->Opcode() == Op_NegVI) {\n+    return in(1)->in(1);\n+  }\n+  return this;\n+}\n+\n@@ -755,1 +780,27 @@\n-  return NULL;\n+  return StoreNode::Ideal(phase, can_reshape);\n+}\n+\n+const TypeVect* StoreVectorMaskedNode::vect_type() const {\n+  Node* value = in(MemNode::ValueIn);\n+  if (value->bottom_type()->isa_vect() != NULL) {\n+    return value->bottom_type()->is_vect();\n+  }\n+  \/\/ The value input is actually a BinaryNode, which has two inputs value and mask.\n+  return value->in(1)->bottom_type()->is_vect();\n+}\n+\n+Node* VectorToMaskNode::Identity(PhaseGVN* phase) {\n+  \/\/ VectorToMask (MaskToVector mask)  ==> mask\n+  if (in(1)->Opcode() == Op_MaskToVector) {\n+    return in(1)->in(1);\n+  }\n+  return this;\n+}\n+\n+VectorBlendNode* VectorBlendNode::make(PhaseGVN& gvn, Node* vec1, Node* vec2, Node* mask) {\n+  if (Matcher::match_rule_supported(Op_VectorToMask) && !mask->is_VectorMask()) {\n+    const TypeVect* vtype = mask->bottom_type()->is_vect();\n+    const TypeVMask* vmask_type = TypeVMask::make(vtype->element_basic_type(), vtype->length());\n+    mask = gvn.transform(new VectorToMaskNode(mask, vmask_type));\n+  }\n+  return new VectorBlendNode(vec1, vec2, mask);\n@@ -975,0 +1026,2 @@\n+  \/\/ VectorLoadMask (LoadVector src)  ==> (LoadVectorMask src).\n+  LoadVectorNode* load = this->in(1)->isa_LoadVector();\n@@ -976,0 +1029,9 @@\n+  if (load != NULL && Matcher::match_rule_supported(Op_LoadVectorMask)) {\n+    const TypeVect* mem_type = TypeVect::make(T_BOOLEAN, length());\n+    const TypeVect* type = TypeVect::make(out_bt, length());\n+    return phase->transform(new LoadVectorMaskNode(load->in(MemNode::Control),\n+                                                   load->in(MemNode::Memory),\n+                                                   load->in(MemNode::Address),\n+                                                   load->adr_type(), type, mem_type));\n+  }\n+\n@@ -1192,0 +1254,13 @@\n+Node* VectorMaskCmpNode::Identity(PhaseGVN* phase) {\n+  \/\/ Generate the mask specific compare node if backend supported.\n+  \/\/ (VectorMaskCmp src1 src2 cond)  ==> MaskToVector (VectorCmpMaskGen src1 src2 cond)\n+  if (Matcher::match_rule_supported(Op_VectorCmpMaskGen) &&\n+      Matcher::match_rule_supported(Op_MaskToVector)) {\n+    const TypeVect* vtype = vect_type();\n+    const TypeVMask* vmask_type = TypeVMask::make(vtype->element_basic_type(), vtype->length());\n+    Node* cmp = phase->transform(new VectorCmpMaskGenNode(in(1), in(2), (ConINode*) in(3), vmask_type));\n+    return phase->transform(new MaskToVectorNode(cmp, vtype));\n+  }\n+  return this;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":76,"deletions":1,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -468,0 +468,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -749,1 +750,1 @@\n-  const TypeVect* vect_type() const { return in(MemNode::ValueIn)->bottom_type()->is_vect(); }\n+  virtual const TypeVect* vect_type() const { return in(MemNode::ValueIn)->bottom_type()->is_vect(); }\n@@ -761,0 +762,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -779,2 +781,2 @@\n-                                                     idx == MemNode::ValueIn ||\n-                                                     idx == MemNode::ValueIn + 1; }\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1; }\n@@ -787,1 +789,1 @@\n-    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    assert(mask->bottom_type()->isa_long() != NULL || mask->is_Vector() || mask->is_VectorMask(), \"sanity\");\n@@ -794,0 +796,1 @@\n+  virtual const TypeVect* vect_type() const;\n@@ -833,0 +836,116 @@\n+\/\/ ==========================Mask feature specific==============================\n+\n+class LoadVectorMaskNode : public LoadVectorNode {\n+ private:\n+  \/**\n+   * The type of the accessed memory, whose basic element type is T_BOOLEAN for mask vector.\n+   * It is different with the basic element type of the node, which can be T_BYTE, T_SHORT,\n+   * T_INT, T_LONG, T_FLOAT or T_DOUBLE.\n+   **\/\n+  const TypeVect* _mem_type;\n+\n+ public:\n+  LoadVectorMaskNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, const TypeVect* mt)\n+   : LoadVectorNode(c, mem, adr, at, vt), _mem_type(mt) {\n+    assert(_mem_type->element_basic_type() == T_BOOLEAN, \"Memory type must be T_BOOLEAN\");\n+    init_class_id(Class_LoadVector);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual int memory_size() const { return _mem_type->length_in_bytes(); }\n+  virtual int store_Opcode() const { return Op_StoreVectorMask; }\n+  virtual uint ideal_reg() const  { return Matcher::vector_ideal_reg(vect_type()->length_in_bytes()); }\n+  virtual uint size_of() const { return sizeof(LoadVectorMaskNode); }\n+};\n+\n+class StoreVectorMaskNode : public StoreVectorNode {\n+ private:\n+  \/**\n+   * The type of the accessed memory, whose basic element type is T_BOOLEAN for mask vector.\n+   * It is different with the basic element type of the src value, which can be T_BYTE, T_SHORT,\n+   * T_INT, T_LONG, T_FLOAT or T_DOUBLE.\n+   **\/\n+  const TypeVect* _mem_type;\n+\n+ public:\n+  StoreVectorMaskNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* src, const TypeVect* mt)\n+   : StoreVectorNode(c, mem, adr, at, src), _mem_type(mt) {\n+    assert(_mem_type->element_basic_type() == T_BOOLEAN, \"Memory type must be T_BOOLEAN\");\n+    init_class_id(Class_StoreVector);\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual int memory_size() const { return _mem_type->length_in_bytes(); }\n+  virtual uint ideal_reg() const  { return Matcher::vector_ideal_reg(vect_type()->length_in_bytes()); }\n+  virtual uint size_of() const { return sizeof(StoreVectorMaskNode); }\n+};\n+\n+class VectorMaskNode : public TypeNode {\n+ public:\n+  VectorMaskNode(Node* in1, const TypeVMask* vmask_type) :\n+    TypeNode(vmask_type, 2) {\n+    init_class_id(Class_VectorMask);\n+    init_req(1, in1);\n+  }\n+\n+  VectorMaskNode(Node* in1, Node* in2, Node* in3, const TypeVMask* vmask_type) :\n+    TypeNode(vmask_type, 4) {\n+    init_class_id(Class_VectorMask);\n+    init_req(1, in1);\n+    init_req(2, in2);\n+    init_req(3, in3);\n+  }\n+\n+  virtual int Opcode() const;\n+};\n+\n+class VectorToMaskNode : public VectorMaskNode {\n+ public:\n+  VectorToMaskNode(Node* in, const TypeVMask* vmask_type) : VectorMaskNode(in, vmask_type) {\n+    assert(in->bottom_type()->is_vect()->length_in_bytes() ==\n+           vmask_type->length() * vmask_type->element_size_in_bytes(), \"wrong type\");\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual Node* Identity(PhaseGVN* phase);\n+};\n+\n+class MaskAllNode : public VectorMaskNode {\n+ public:\n+  MaskAllNode(ConLNode* in, const TypeVMask* vmask_type) : VectorMaskNode(in, vmask_type) {\n+    assert(in->get_long() == 0 || in->get_long() == -1, \"Unsupported value to mask all\");\n+  }\n+\n+  virtual int Opcode() const;\n+};\n+\n+\/\/ Vector compare node with a TypeVMask bottom_type. It is specially generated for platforms\n+\/\/ that have mask hardware feature. The main difference with \"VectorMaskCmpNode\" is that this\n+\/\/ is a kind of mask node, while \"VectorMaskCmpNode\" is a vector node.\n+class VectorCmpMaskGenNode : public VectorMaskNode {\n+ public:\n+  VectorCmpMaskGenNode(Node* in1, Node* in2, ConINode* predicate_node, const TypeVMask* vmask_type) :\n+    VectorMaskNode(in1, in2, predicate_node, vmask_type) {\n+    assert(in1->bottom_type()->is_vect()->element_basic_type() == in2->bottom_type()->is_vect()->element_basic_type(),\n+           \"VectorCmpMaskGen inputs must have the same type for elements\");\n+    assert(in1->bottom_type()->is_vect()->length() == in2->bottom_type()->is_vect()->length(),\n+           \"VectorCmpMaskGen inputs must have the same number of elements\");\n+    assert(in1->bottom_type()->is_vect()->length_in_bytes() ==\n+           vmask_type->length() * vmask_type->element_size_in_bytes(), \"wrong type\");\n+  }\n+\n+  virtual int Opcode() const;\n+};\n+\n+class MaskToVectorNode : public VectorNode {\n+ public:\n+  MaskToVectorNode(Node* mask, const TypeVect* vt) : VectorNode(mask, vt) {\n+    assert(mask->is_VectorMask(), \"input must be a VectorMask\");\n+    const TypeVMask* vmask_type = mask->as_VectorMask()->bottom_type()->is_vmask();\n+    assert(vmask_type->length() * vmask_type->element_size_in_bytes() ==\n+           vt->length_in_bytes(), \"wrong type\");\n+  }\n+\n+  virtual int Opcode() const;\n+};\n+\n@@ -1124,0 +1243,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1165,1 +1285,1 @@\n- public:\n+ protected:\n@@ -1168,1 +1288,3 @@\n-    \/\/ assert(mask->is_VectorMask(), \"VectorBlendNode requires that third argument be a mask\");\n+    if (Matcher::match_rule_supported(Op_VectorToMask)) {\n+      assert(mask->is_VectorMask(), \"VectorBlendNode requires that third argument be a mask\");\n+    }\n@@ -1171,0 +1293,2 @@\n+ public:\n+  static VectorBlendNode* make(PhaseGVN& gvn, Node* vec1, Node* vec2, Node* mask);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":131,"deletions":7,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -1868,0 +1868,2 @@\n+  declare_c2_type(LoadVectorMaskNode, LoadVectorNode)                     \\\n+  declare_c2_type(StoreVectorMaskNode, StoreVectorNode)                   \\\n@@ -1885,0 +1887,1 @@\n+  declare_c2_type(MaskToVectorNode, VectorNode)                           \\\n@@ -1888,0 +1891,4 @@\n+  declare_c2_type(VectorMaskNode, TypeNode)                               \\\n+  declare_c2_type(VectorCmpMaskGenNode, VectorMaskNode)                   \\\n+  declare_c2_type(VectorToMaskNode, VectorMaskNode)                       \\\n+  declare_c2_type(MaskAllNode, VectorMaskNode)                            \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -269,0 +269,14 @@\n+    @IntrinsicCandidate\n+    public static\n+    <V, M>\n+    V binaryMaskOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass,\n+                   Class<?> elementType, int length, V v1, V v2, M m,\n+                   BinaryMaskOperation<V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v1, v2, m);\n+    }\n+\n+    public interface BinaryMaskOperation<V, M> {\n+        V apply(V v1, V v2, M mask);\n+    }\n+\n@@ -340,0 +354,17 @@\n+    public interface StoreVectorMaskedOperation<C, V extends Vector<?>, M extends VectorMask<?>> {\n+        void store(C container, int index, V v, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>,\n+     M extends VectorMask<?>>\n+    void storeMasked(Class<?> vectorClass, Class<M> maskClass, Class<?> elementType,\n+                     int length, Object base, long offset,   \/\/ Unsafe addressing\n+                     V v, M m,\n+                     C container, int index,      \/\/ Arguments for default implementation\n+                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v, m);\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte128Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Byte128Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v, (Byte128Mask) m);  \/\/ specialize\n@@ -830,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte256Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Byte256Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v, (Byte256Mask) m);  \/\/ specialize\n@@ -862,0 +862,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte512Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Byte512Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v, (Byte512Mask) m);  \/\/ specialize\n@@ -926,0 +926,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Byte64Vector lanewise(Binary op, Vector<Byte> v) {\n-        return (Byte64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Byte64Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v, (Byte64Mask) m);  \/\/ specialize\n@@ -814,0 +814,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public ByteMaxVector lanewise(Binary op, Vector<Byte> v) {\n-        return (ByteMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public ByteMaxVector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v, (ByteMaxMask) m);  \/\/ specialize\n@@ -800,0 +800,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -553,1 +555,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -556,1 +558,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -593,0 +595,11 @@\n+    @ForceInline\n+    public final\n+    ByteVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Byte> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,byte,VectorMask)\n+     *\/\n@@ -596,1 +609,3 @@\n-                                  Vector<Byte> v);\n+                                  Vector<Byte> v,\n+                                  VectorMask<Byte> m);\n+\n@@ -600,1 +615,2 @@\n-                                          Vector<Byte> v) {\n+                                          Class<? extends VectorMask<Byte>> maskType,\n+                                          Vector<Byte> v, VectorMask<Byte> m) {\n@@ -620,1 +636,1 @@\n-                return hi.lanewise(OR, lo);\n+                return m != null ? blend(hi.lanewise(OR, lo), m) : hi.lanewise(OR, lo);\n@@ -627,2 +643,10 @@\n-                if (eqz.anyTrue()) {\n-                    throw that.divZeroException();\n+                if (m != null) {\n+                    if (eqz.and(m).anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n+                    \/\/ suppress div\/0 exceptions in unset lanes\n+                    that = that.lanewise(NOT, eqz);\n+                } else {\n+                    if (eqz.anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n@@ -633,4 +657,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), byte.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, byte.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -638,24 +662,24 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (byte)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (byte)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (byte)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n@@ -666,1 +690,1 @@\n-    ImplCache<Binary,BinaryOperator<ByteVector>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<ByteVector, VectorMask<Byte>>> BIN_MASK_IMPL\n@@ -669,21 +693,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,byte,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    ByteVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Byte> v,\n-                                  VectorMask<Byte> m) {\n-        ByteVector that = (ByteVector) v;\n-        if (op == DIV) {\n-            VectorMask<Byte> eqz = that.eq((byte)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n-            }\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        }\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -718,7 +721,1 @@\n-        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n-            return lanewiseShift(op, (int) e);\n-        }\n-        if (op == AND_NOT) {\n-            op = AND; e = (byte) ~e;\n-        }\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -752,1 +749,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n+            ByteVector shift = lanewiseShift(op, (int) e);\n+            return m != null ? blend(shift, m) : shift;\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (byte) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -769,8 +773,1 @@\n-        byte e1 = (byte) e;\n-        if ((long)e1 != e\n-            \/\/ allow shift ops to clip down their int parameters\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n-            vspecies().checkValue(e);  \/\/ for exception\n-        }\n-        return lanewise(op, e1);\n+        return lanewise(op, e, null);\n@@ -793,1 +790,8 @@\n-        return blend(lanewise(op, e), m);\n+        byte e1 = (byte) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -2983,1 +2987,0 @@\n-            \/\/ FIXME: optimize\n@@ -2986,1 +2989,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3231,0 +3234,16 @@\n+    abstract\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":96,"deletions":77,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double128Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Double128Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v, (Double128Mask) m);  \/\/ specialize\n@@ -798,0 +798,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double256Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Double256Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v, (Double256Mask) m);  \/\/ specialize\n@@ -802,0 +802,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double512Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Double512Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v, (Double512Mask) m);  \/\/ specialize\n@@ -810,0 +810,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Double64Vector lanewise(Binary op, Vector<Double> v) {\n-        return (Double64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Double64Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v, (Double64Mask) m);  \/\/ specialize\n@@ -796,0 +796,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public DoubleMaxVector lanewise(Binary op, Vector<Double> v) {\n-        return (DoubleMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public DoubleMaxVector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v, (DoubleMaxMask) m);  \/\/ specialize\n@@ -795,0 +795,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -619,0 +621,11 @@\n+    @ForceInline\n+    public final\n+    DoubleVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Double> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,double,VectorMask)\n+     *\/\n@@ -622,1 +635,3 @@\n-                                  Vector<Double> v);\n+                                  Vector<Double> v,\n+                                  VectorMask<Double> m);\n+\n@@ -626,1 +641,2 @@\n-                                          Vector<Double> v) {\n+                                          Class<? extends VectorMask<Double>> maskType,\n+                                          Vector<Double> v, VectorMask<Double> m) {\n@@ -637,1 +653,1 @@\n-                return this.viewAsIntegralLanes()\n+                that = this.viewAsIntegralLanes()\n@@ -640,0 +656,1 @@\n+                return m != null ? blend(that, m) : that;\n@@ -643,4 +660,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), double.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, double.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -648,20 +665,20 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.min(a, b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double)Math.min(a, b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+                case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (double) Math.hypot(a, b));\n@@ -672,1 +689,1 @@\n-    ImplCache<Binary,BinaryOperator<DoubleVector>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<DoubleVector, VectorMask<Double>>> BIN_MASK_IMPL\n@@ -675,11 +692,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,double,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    DoubleVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Double> v,\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -714,1 +720,1 @@\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -742,1 +748,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -759,6 +765,1 @@\n-        double e1 = (double) e;\n-        if ((long)e1 != e\n-            ) {\n-            vspecies().checkValue(e);  \/\/ for exception\n-        }\n-        return lanewise(op, e1);\n+        return lanewise(op, e, null);\n@@ -781,1 +782,6 @@\n-        return blend(lanewise(op, e), m);\n+        double e1 = (double) e;\n+        if ((long)e1 != e\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -2896,1 +2902,0 @@\n-            \/\/ FIXME: optimize\n@@ -2899,1 +2904,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3185,0 +3190,16 @@\n+    abstract\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":72,"deletions":51,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float128Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Float128Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v, (Float128Mask) m);  \/\/ specialize\n@@ -802,0 +802,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float256Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Float256Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v, (Float256Mask) m);  \/\/ specialize\n@@ -810,0 +810,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float512Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Float512Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v, (Float512Mask) m);  \/\/ specialize\n@@ -826,0 +826,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Float64Vector lanewise(Binary op, Vector<Float> v) {\n-        return (Float64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Float64Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v, (Float64Mask) m);  \/\/ specialize\n@@ -798,0 +798,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public FloatMaxVector lanewise(Binary op, Vector<Float> v) {\n-        return (FloatMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public FloatMaxVector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v, (FloatMaxMask) m);  \/\/ specialize\n@@ -795,0 +795,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -619,0 +621,11 @@\n+    @ForceInline\n+    public final\n+    FloatVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Float> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,float,VectorMask)\n+     *\/\n@@ -622,1 +635,3 @@\n-                                  Vector<Float> v);\n+                                  Vector<Float> v,\n+                                  VectorMask<Float> m);\n+\n@@ -626,1 +641,2 @@\n-                                          Vector<Float> v) {\n+                                          Class<? extends VectorMask<Float>> maskType,\n+                                          Vector<Float> v, VectorMask<Float> m) {\n@@ -637,1 +653,1 @@\n-                return this.viewAsIntegralLanes()\n+                that = this.viewAsIntegralLanes()\n@@ -640,0 +656,1 @@\n+                return m != null ? blend(that, m) : that;\n@@ -643,4 +660,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), float.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, float.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -648,20 +665,20 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.min(a, b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float)Math.min(a, b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+                case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (float) Math.hypot(a, b));\n@@ -672,1 +689,1 @@\n-    ImplCache<Binary,BinaryOperator<FloatVector>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<FloatVector, VectorMask<Float>>> BIN_MASK_IMPL\n@@ -675,11 +692,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,float,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    FloatVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Float> v,\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -714,1 +720,1 @@\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -742,1 +748,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -759,6 +765,1 @@\n-        float e1 = (float) e;\n-        if ((long)e1 != e\n-            ) {\n-            vspecies().checkValue(e);  \/\/ for exception\n-        }\n-        return lanewise(op, e1);\n+        return lanewise(op, e, null);\n@@ -781,1 +782,6 @@\n-        return blend(lanewise(op, e), m);\n+        float e1 = (float) e;\n+        if ((long)e1 != e\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -2902,1 +2908,0 @@\n-            \/\/ FIXME: optimize\n@@ -2905,1 +2910,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3172,0 +3177,16 @@\n+    abstract\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":72,"deletions":51,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int128Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Int128Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v, (Int128Mask) m);  \/\/ specialize\n@@ -806,0 +806,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int256Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Int256Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v, (Int256Mask) m);  \/\/ specialize\n@@ -814,0 +814,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int512Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Int512Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v, (Int512Mask) m);  \/\/ specialize\n@@ -830,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Int64Vector lanewise(Binary op, Vector<Integer> v) {\n-        return (Int64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Int64Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v, (Int64Mask) m);  \/\/ specialize\n@@ -802,0 +802,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public IntMaxVector lanewise(Binary op, Vector<Integer> v) {\n-        return (IntMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public IntMaxVector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v, (IntMaxMask) m);  \/\/ specialize\n@@ -811,0 +811,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -553,1 +555,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -556,1 +558,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -593,0 +595,11 @@\n+    @ForceInline\n+    public final\n+    IntVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Integer> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,int,VectorMask)\n+     *\/\n@@ -596,1 +609,3 @@\n-                                  Vector<Integer> v);\n+                                  Vector<Integer> v,\n+                                  VectorMask<Integer> m);\n+\n@@ -600,1 +615,2 @@\n-                                          Vector<Integer> v) {\n+                                          Class<? extends VectorMask<Integer>> maskType,\n+                                          Vector<Integer> v, VectorMask<Integer> m) {\n@@ -620,1 +636,1 @@\n-                return hi.lanewise(OR, lo);\n+                return m != null ? blend(hi.lanewise(OR, lo), m) : hi.lanewise(OR, lo);\n@@ -627,2 +643,10 @@\n-                if (eqz.anyTrue()) {\n-                    throw that.divZeroException();\n+                if (m != null) {\n+                    if (eqz.and(m).anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n+                    \/\/ suppress div\/0 exceptions in unset lanes\n+                    that = that.lanewise(NOT, eqz);\n+                } else {\n+                    if (eqz.anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n@@ -633,4 +657,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), int.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, int.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -638,24 +662,24 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (int)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (int)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (int)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n@@ -666,1 +690,1 @@\n-    ImplCache<Binary,BinaryOperator<IntVector>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<IntVector, VectorMask<Integer>>> BIN_MASK_IMPL\n@@ -669,21 +693,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,int,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    IntVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Integer> v,\n-                                  VectorMask<Integer> m) {\n-        IntVector that = (IntVector) v;\n-        if (op == DIV) {\n-            VectorMask<Integer> eqz = that.eq((int)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n-            }\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        }\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -718,7 +721,1 @@\n-        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n-            return lanewiseShift(op, (int) e);\n-        }\n-        if (op == AND_NOT) {\n-            op = AND; e = (int) ~e;\n-        }\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -752,1 +749,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n+            IntVector shift = lanewiseShift(op, (int) e);\n+            return m != null ? blend(shift, m) : shift;\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (int) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -769,8 +773,1 @@\n-        int e1 = (int) e;\n-        if ((long)e1 != e\n-            \/\/ allow shift ops to clip down their int parameters\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n-            vspecies().checkValue(e);  \/\/ for exception\n-        }\n-        return lanewise(op, e1);\n+        return lanewise(op, e, null);\n@@ -793,1 +790,8 @@\n-        return blend(lanewise(op, e), m);\n+        int e1 = (int) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -3005,1 +3009,0 @@\n-            \/\/ FIXME: optimize\n@@ -3008,1 +3011,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3275,0 +3278,16 @@\n+    abstract\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":96,"deletions":77,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long128Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Long128Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v, (Long128Mask) m);  \/\/ specialize\n@@ -792,0 +792,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long256Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Long256Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v, (Long256Mask) m);  \/\/ specialize\n@@ -796,0 +796,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long512Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Long512Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v, (Long512Mask) m);  \/\/ specialize\n@@ -804,0 +804,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public Long64Vector lanewise(Binary op, Vector<Long> v) {\n-        return (Long64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Long64Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v, (Long64Mask) m);  \/\/ specialize\n@@ -790,0 +790,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -273,2 +273,2 @@\n-    public LongMaxVector lanewise(Binary op, Vector<Long> v) {\n-        return (LongMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public LongMaxVector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v, (LongMaxMask) m);  \/\/ specialize\n@@ -790,0 +790,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -511,1 +513,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -514,1 +516,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -551,0 +553,11 @@\n+    @ForceInline\n+    public final\n+    LongVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Long> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,long,VectorMask)\n+     *\/\n@@ -554,1 +567,3 @@\n-                                  Vector<Long> v);\n+                                  Vector<Long> v,\n+                                  VectorMask<Long> m);\n+\n@@ -558,1 +573,2 @@\n-                                          Vector<Long> v) {\n+                                          Class<? extends VectorMask<Long>> maskType,\n+                                          Vector<Long> v, VectorMask<Long> m) {\n@@ -578,1 +594,1 @@\n-                return hi.lanewise(OR, lo);\n+                return m != null ? blend(hi.lanewise(OR, lo), m) : hi.lanewise(OR, lo);\n@@ -585,2 +601,10 @@\n-                if (eqz.anyTrue()) {\n-                    throw that.divZeroException();\n+                if (m != null) {\n+                    if (eqz.and(m).anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n+                    \/\/ suppress div\/0 exceptions in unset lanes\n+                    that = that.lanewise(NOT, eqz);\n+                } else {\n+                    if (eqz.anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n@@ -591,4 +615,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), long.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, long.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -596,24 +620,24 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (long)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (long)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (long)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n@@ -624,1 +648,1 @@\n-    ImplCache<Binary,BinaryOperator<LongVector>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<LongVector, VectorMask<Long>>> BIN_MASK_IMPL\n@@ -627,21 +651,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,long,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    LongVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Long> v,\n-                                  VectorMask<Long> m) {\n-        LongVector that = (LongVector) v;\n-        if (op == DIV) {\n-            VectorMask<Long> eqz = that.eq((long)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n-            }\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        }\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -676,7 +679,1 @@\n-        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n-            return lanewiseShift(op, (int) e);\n-        }\n-        if (op == AND_NOT) {\n-            op = AND; e = (long) ~e;\n-        }\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -710,1 +707,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n+            LongVector shift = lanewiseShift(op, (int) e);\n+            return m != null ? blend(shift, m) : shift;\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (long) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -2889,1 +2893,0 @@\n-            \/\/ FIXME: optimize\n@@ -2892,1 +2895,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3178,0 +3181,16 @@\n+    abstract\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":87,"deletions":68,"binary":false,"changes":155,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short128Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short128Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Short128Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v, (Short128Mask) m);  \/\/ specialize\n@@ -820,0 +820,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short256Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short256Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Short256Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v, (Short256Mask) m);  \/\/ specialize\n@@ -836,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short512Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short512Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Short512Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v, (Short512Mask) m);  \/\/ specialize\n@@ -868,0 +868,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public Short64Vector lanewise(Binary op, Vector<Short> v) {\n-        return (Short64Vector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public Short64Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v, (Short64Mask) m);  \/\/ specialize\n@@ -812,0 +812,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -278,2 +278,2 @@\n-    public ShortMaxVector lanewise(Binary op, Vector<Short> v) {\n-        return (ShortMaxVector) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public ShortMaxVector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v, (ShortMaxMask) m);  \/\/ specialize\n@@ -806,0 +806,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -218,0 +217,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -553,1 +555,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -556,1 +558,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -593,0 +595,11 @@\n+    @ForceInline\n+    public final\n+    ShortVector lanewise(VectorOperators.Binary op,\n+                                  Vector<Short> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,short,VectorMask)\n+     *\/\n@@ -596,1 +609,3 @@\n-                                  Vector<Short> v);\n+                                  Vector<Short> v,\n+                                  VectorMask<Short> m);\n+\n@@ -600,1 +615,2 @@\n-                                          Vector<Short> v) {\n+                                          Class<? extends VectorMask<Short>> maskType,\n+                                          Vector<Short> v, VectorMask<Short> m) {\n@@ -620,1 +636,1 @@\n-                return hi.lanewise(OR, lo);\n+                return m != null ? blend(hi.lanewise(OR, lo), m) : hi.lanewise(OR, lo);\n@@ -627,2 +643,10 @@\n-                if (eqz.anyTrue()) {\n-                    throw that.divZeroException();\n+                if (m != null) {\n+                    if (eqz.and(m).anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n+                    \/\/ suppress div\/0 exceptions in unset lanes\n+                    that = that.lanewise(NOT, eqz);\n+                } else {\n+                    if (eqz.anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n@@ -633,4 +657,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), short.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, short.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -638,24 +662,24 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)Math.min(a, b));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> (short)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (short)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (short)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n@@ -666,1 +690,1 @@\n-    ImplCache<Binary,BinaryOperator<ShortVector>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<ShortVector, VectorMask<Short>>> BIN_MASK_IMPL\n@@ -669,21 +693,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,short,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    ShortVector lanewise(VectorOperators.Binary op,\n-                                  Vector<Short> v,\n-                                  VectorMask<Short> m) {\n-        ShortVector that = (ShortVector) v;\n-        if (op == DIV) {\n-            VectorMask<Short> eqz = that.eq((short)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n-            }\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        }\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -718,7 +721,1 @@\n-        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n-            return lanewiseShift(op, (int) e);\n-        }\n-        if (op == AND_NOT) {\n-            op = AND; e = (short) ~e;\n-        }\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -752,1 +749,8 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n+            ShortVector shift = lanewiseShift(op, (int) e);\n+            return m != null ? blend(shift, m) : shift;\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (short) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -769,8 +773,1 @@\n-        short e1 = (short) e;\n-        if ((long)e1 != e\n-            \/\/ allow shift ops to clip down their int parameters\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n-            vspecies().checkValue(e);  \/\/ for exception\n-        }\n-        return lanewise(op, e1);\n+        return lanewise(op, e, null);\n@@ -793,1 +790,8 @@\n-        return blend(lanewise(op, e), m);\n+        short e1 = (short) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -3138,1 +3142,0 @@\n-            \/\/ FIXME: optimize\n@@ -3141,1 +3144,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3551,0 +3554,16 @@\n+    abstract\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":96,"deletions":77,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -222,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -566,1 +568,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -569,1 +571,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -641,0 +643,11 @@\n+    @ForceInline\n+    public final\n+    $abstractvectortype$ lanewise(VectorOperators.Binary op,\n+                                  Vector<$Boxtype$> v) {\n+        return lanewise(op, v, null);\n+    }\n+\n+    \/**\n+     * {@inheritDoc} <!--workaround-->\n+     * @see #lanewise(VectorOperators.Binary,$type$,VectorMask)\n+     *\/\n@@ -644,1 +657,3 @@\n-                                  Vector<$Boxtype$> v);\n+                                  Vector<$Boxtype$> v,\n+                                  VectorMask<$Boxtype$> m);\n+\n@@ -648,1 +663,2 @@\n-                                          Vector<$Boxtype$> v) {\n+                                          Class<? extends VectorMask<$Boxtype$>> maskType,\n+                                          Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n@@ -660,1 +676,1 @@\n-                return this.viewAsIntegralLanes()\n+                that = this.viewAsIntegralLanes()\n@@ -663,0 +679,1 @@\n+                return m != null ? blend(that, m) : that;\n@@ -677,1 +694,1 @@\n-                return hi.lanewise(OR, lo);\n+                return m != null ? blend(hi.lanewise(OR, lo), m) : hi.lanewise(OR, lo);\n@@ -684,2 +701,10 @@\n-                if (eqz.anyTrue()) {\n-                    throw that.divZeroException();\n+                if (m != null) {\n+                    if (eqz.and(m).anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n+                    \/\/ suppress div\/0 exceptions in unset lanes\n+                    that = that.lanewise(NOT, eqz);\n+                } else {\n+                    if (eqz.anyTrue()) {\n+                        throw that.divZeroException();\n+                    }\n@@ -691,4 +716,4 @@\n-        return VectorSupport.binaryOp(\n-            opc, getClass(), $type$.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n+        return VectorSupport.binaryMaskOp(\n+            opc, getClass(), maskType, $type$.class, length(),\n+            this, that, m,\n+            BIN_MASK_IMPL.find(op, opc, (opc_) -> {\n@@ -696,12 +721,12 @@\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.min(a, b));\n+                case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a + b));\n+                case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a - b));\n+                case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a * b));\n+                case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a \/ b));\n+                case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.max(a, b));\n+                case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.min(a, b));\n@@ -709,12 +734,12 @@\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+                case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a & b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a | b));\n+                case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$)(a ^ b));\n+                case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> ($type$)(a << n));\n+                case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> ($type$)(a >> n));\n+                case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n@@ -723,8 +748,8 @@\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n+                case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+                case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                        v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.hypot(a, b));\n@@ -736,1 +761,1 @@\n-    ImplCache<Binary,BinaryOperator<$abstractvectortype$>> BIN_IMPL\n+    ImplCache<Binary, BinaryMaskOperation<$abstractvectortype$, VectorMask<$Boxtype$>>> BIN_MASK_IMPL\n@@ -739,23 +764,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     * @see #lanewise(VectorOperators.Binary,$type$,VectorMask)\n-     *\/\n-    @ForceInline\n-    public final\n-    $abstractvectortype$ lanewise(VectorOperators.Binary op,\n-                                  Vector<$Boxtype$> v,\n-                                  VectorMask<$Boxtype$> m) {\n-#if[BITWISE]\n-        $abstractvectortype$ that = ($abstractvectortype$) v;\n-        if (op == DIV) {\n-            VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n-            }\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n-        }\n-#end[BITWISE]\n-        return blend(lanewise(op, v), m);\n-    }\n@@ -790,9 +792,1 @@\n-#if[BITWISE]\n-        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n-            return lanewiseShift(op, (int) e);\n-        }\n-        if (op == AND_NOT) {\n-            op = AND; e = ($type$) ~e;\n-        }\n-#end[BITWISE]\n-        return lanewise(op, broadcast(e));\n+        return lanewise(op, e, null);\n@@ -826,1 +820,10 @@\n-        return blend(lanewise(op, e), m);\n+#if[BITWISE]\n+        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n+            $abstractvectortype$ shift = lanewiseShift(op, (int) e);\n+            return m != null ? blend(shift, m) : shift;\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = ($type$) ~e;\n+        }\n+#end[BITWISE]\n+        return lanewise(op, broadcast(e), m);\n@@ -844,10 +847,1 @@\n-        $type$ e1 = ($type$) e;\n-        if ((long)e1 != e\n-#if[BITWISE]\n-            \/\/ allow shift ops to clip down their int parameters\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-#end[BITWISE]\n-            ) {\n-            vspecies().checkValue(e);  \/\/ for exception\n-        }\n-        return lanewise(op, e1);\n+        return lanewise(op, e, null);\n@@ -870,1 +864,10 @@\n-        return blend(lanewise(op, e), m);\n+        $type$ e1 = ($type$) e;\n+        if ((long)e1 != e\n+#if[BITWISE]\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+#end[BITWISE]\n+            ) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -3684,1 +3687,0 @@\n-            \/\/ FIXME: optimize\n@@ -3687,1 +3689,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -4176,0 +4178,16 @@\n+    abstract\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":110,"deletions":92,"binary":false,"changes":202,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -280,2 +280,2 @@\n-    public $vectortype$ lanewise(Binary op, Vector<$Boxtype$> v) {\n-        return ($vectortype$) super.lanewiseTemplate(op, v);  \/\/ specialize\n+    public $vectortype$ lanewise(Binary op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v, ($masktype$) m);  \/\/ specialize\n@@ -1096,0 +1096,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"}]}