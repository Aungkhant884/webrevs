{"files":[{"patch":"@@ -423,9 +423,16 @@\n-\t  $$(call ExecuteWithLog, $$@, $$(call MakeCommandRelative, \\\n-\t      $$($1_COMPILER) -showIncludes $$($1_COMPILE_OPTIONS))) \\\n-\t      | $(TR) -d '\\r' | $(GREP) -v -e \"^Note: including file:\" \\\n-\t          -e \"^$$($1_FILENAME)$$$$\" || test \"$$$$?\" = \"1\" ; \\\n-\t  $(ECHO) $$@: \\\\ > $$($1_DEPS_FILE) ; \\\n-\t  $(SED) $(WINDOWS_SHOWINCLUDE_SED_PATTERN) $$($1_OBJ).log \\\n-\t      | $(SORT) -u >> $$($1_DEPS_FILE) ; \\\n-\t  $(ECHO) >> $$($1_DEPS_FILE) ; \\\n-\t  $(SED) $(DEPENDENCY_TARGET_SED_PATTERN) $$($1_DEPS_FILE) > $$($1_DEPS_TARGETS_FILE)\n+          ifeq ($$(filter %.s, $$($1_FILENAME)), )\n+\t    $$(call ExecuteWithLog, $$@, $$(call MakeCommandRelative, \\\n+\t        $$($1_COMPILER) -showIncludes $$($1_COMPILE_OPTIONS))) \\\n+\t        | $(TR) -d '\\r' | $(GREP) -v -e \"^Note: including file:\" \\\n+\t            -e \"^$$($1_FILENAME)$$$$\" || test \"$$$$?\" = \"1\" ; \\\n+\t    $(ECHO) $$@: \\\\ > $$($1_DEPS_FILE) ; \\\n+\t    $(SED) $(WINDOWS_SHOWINCLUDE_SED_PATTERN) $$($1_OBJ).log \\\n+\t        | $(SORT) -u >> $$($1_DEPS_FILE) ; \\\n+\t    $(ECHO) >> $$($1_DEPS_FILE) ; \\\n+\t    $(SED) $(DEPENDENCY_TARGET_SED_PATTERN) $$($1_DEPS_FILE) > $$($1_DEPS_TARGETS_FILE)\n+          else\n+            # For assembler calls, no need to build dependency list.\n+\t    $$(call ExecuteWithLog, $$@, $$(call MakeCommandRelative, \\\n+\t        $$($1_COMPILER) $$($1_FLAGS) \\\n+\t          $(CC_OUT_OPTION)$$($1_OBJ) \/Ta $$($1_SRC_FILE)))\n+          endif\n","filename":"make\/common\/NativeCompilation.gmk","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2439,0 +2439,14 @@\n+\/\/ Vector calling convention not yet implemented.\n+const bool Matcher::supports_vector_calling_convention(void) {\n+  return false;\n+}\n+\n+void Matcher::vector_calling_convention(VMRegPair *regs, uint num_bits, uint total_args_passed) {\n+  (void) SharedRuntime::vector_calling_convention(regs, num_bits, total_args_passed);\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  Unimplemented();\n+  return OptoRegPair(0, 0);\n+}\n+\n@@ -2483,0 +2497,4 @@\n+    if (bt == T_BOOLEAN) {\n+      \/\/ To support vector api load\/store mask.\n+      return MaxVectorSize \/ 8;\n+    }\n@@ -2510,1 +2528,1 @@\n-  if (UseSVE > 0 && 16 <= len && len <= 256) {\n+  if (UseSVE > 0 && 2 <= len && len <= 256) {\n@@ -16581,1 +16599,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 4);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 4);\n@@ -16592,1 +16610,1 @@\n-  predicate(n->as_LoadVector()->memory_size() == 8);\n+  predicate(UseSVE == 0 && n->as_LoadVector()->memory_size() == 8);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -104,0 +104,12 @@\n+  static inline uint vector_length(const MachNode* n) {\n+    const TypeVect* vt = n->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static inline uint vector_length(const MachNode* use, const MachOper* opnd) {\n+    int def_idx = use->operand_index(opnd);\n+    Node* def = use->in(def_idx);\n+    const TypeVect* vt = def->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n@@ -129,2 +141,3 @@\n-  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store,\n-                                   FloatRegister reg, PRegister pg, BasicType bt,\n+  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+                                   PRegister pg, BasicType mem_elem_bt,\n+                                   Assembler::SIMD_RegVariant vector_elem_size,\n@@ -133,2 +146,1 @@\n-    Assembler::SIMD_RegVariant type;\n-    int esize = type2aelembytes(bt);\n+    int esize = type2aelembytes(mem_elem_bt);\n@@ -140,1 +152,0 @@\n-        type = Assembler::B;\n@@ -144,1 +155,0 @@\n-        type = Assembler::H;\n@@ -148,1 +158,0 @@\n-        type = Assembler::S;\n@@ -152,1 +161,0 @@\n-        type = Assembler::D;\n@@ -158,1 +166,1 @@\n-      (masm.*insn)(reg, type, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n+      (masm.*insn)(reg, vector_elem_size, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n@@ -165,0 +173,30 @@\n+  static void sve_compare(C2_MacroAssembler masm, PRegister pd, BasicType bt,\n+                          PRegister pg, FloatRegister zn, FloatRegister zm, int cond) {\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (bt == T_FLOAT || bt == T_DOUBLE) {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    } else {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n@@ -184,5 +222,0 @@\n-      case Op_AndReductionV:\n-      case Op_OrReductionV:\n-      case Op_XorReductionV:\n-      case Op_MaxReductionV:\n-      case Op_MinReductionV:\n@@ -191,1 +224,0 @@\n-      case Op_VectorBlend:\n@@ -201,2 +233,0 @@\n-      case Op_VectorLoadMask:\n-      case Op_VectorMaskCmp:\n@@ -206,1 +236,0 @@\n-      case Op_VectorStoreMask:\n@@ -222,1 +251,0 @@\n-\n@@ -227,1 +255,1 @@\n-\/\/ Use predicated vector load\/store\n+\/\/ Unpredicated vector load\/store\n@@ -229,1 +257,2 @@\n-  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n@@ -235,0 +264,1 @@\n+    BasicType bt = vector_element_basic_type(this);\n@@ -236,1 +266,1 @@\n-                         vector_element_basic_type(this), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -243,1 +273,2 @@\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n@@ -249,0 +280,1 @@\n+    BasicType bt = vector_element_basic_type(this, $src);\n@@ -250,1 +282,31 @@\n-                         vector_element_basic_type(this, $src), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Predicated vector load\/store, based on the vector length of the node.\n+\/\/ Only load\/store values in the range of the memory_size. This is needed\n+\/\/ when the memory_size is lower than the hardware supported max vector size.\n+\/\/ And this might happen for Vector API mask vector load\/store.\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                       iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_LoadVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_LoadVector()->memory_size() != MaxVectorSize);\n+  match(Set dst (LoadVector mem));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_ldr $dst, $pTmp, $mem\\t # load vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n@@ -256,0 +318,25 @@\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                          iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_StoreVector()->memory_size() != MaxVectorSize);\n+  match(Set mem (StoreVector mem src));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_str $src, $pTmp, $mem\\t # store vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this, $src));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -874,1 +961,1 @@\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\"  %}\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n@@ -881,0 +968,255 @@\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n+            \"sve_cpy $dst, $pTmp, -1\\t # vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), -1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend\n+\n+instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) src3));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n+                 as_FloatRegister($src3$$reg), -1);\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend with compare\n+\n+instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n+                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src3, $src4\\t # vector cmp (sve)\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src3$$reg),\n+                as_FloatRegister($src4$$reg), (int)$cond$$constant);\n+    __ sve_sel(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector load mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskS(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to H)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to S)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskL(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 16);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector store mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 8);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (H to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (S to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(5 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (D to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ load\/store mask vector\n+\n+instruct vloadmask_loadV(vReg dst, vmemA mem) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # load vector mask (sve)\" %}\n+  ins_encode %{\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    Assembler::SIMD_RegVariant to_vect_size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n+                         T_BOOLEAN, to_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_neg(dst_reg, to_vect_size, ptrue, dst_reg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_vstoremask(vmemA mem, vReg src, vReg tmp, immI size) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= 2 &&\n+            n->as_StoreVector()->vect_type()->element_basic_type() == T_BOOLEAN);\n+  match(Set mem (StoreVector mem (VectorStoreMask src size)));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_neg $tmp, $src\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t # store vector mask (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant from_vect_size =\n+              elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_size, ptrue,\n+               as_FloatRegister($src$$reg));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n+                         ptrue, T_BOOLEAN, from_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -983,0 +1325,234 @@\n+\/\/ vector and reduction\n+\n+instruct reduce_andB(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_andv $tmp, $src2\\t# vector (sve) (B)\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"andw  $dst, $dst, $src1\\n\\t\"\n+            \"sxtb  $dst, $dst\\t # and reduction B\" %}\n+  ins_encode %{\n+    __ sve_andv(as_FloatRegister($tmp$$reg), __ B,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andS(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_andv $tmp, $src2\\t# vector (sve) (H)\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"andw  $dst, $dst, $src1\\n\\t\"\n+            \"sxth  $dst, $dst\\t # and reduction H\" %}\n+  ins_encode %{\n+    __ sve_andv(as_FloatRegister($tmp$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_andv $tmp, $src2\\t# vector (sve) (S)\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"andw  $dst, $dst, $src1\\t # and reduction S\" %}\n+  ins_encode %{\n+    __ sve_andv(as_FloatRegister($tmp$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_andv $tmp, $src2\\t# vector (sve) (D)\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"andr  $dst, $dst, $src1\\t # and reduction D\" %}\n+  ins_encode %{\n+    __ sve_andv(as_FloatRegister($tmp$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reduction\n+\n+instruct reduce_orB(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orv $tmp, $src2\\t# vector (sve) (B)\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"orrw  $dst, $dst, $src1\\n\\t\"\n+            \"sxtb  $dst, $dst\\t # or reduction B\" %}\n+  ins_encode %{\n+    __ sve_orv(as_FloatRegister($tmp$$reg), __ B,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orS(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orv $tmp, $src2\\t# vector (sve) (H)\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"orrw  $dst, $dst, $src1\\n\\t\"\n+            \"sxth  $dst, $dst\\t # or reduction H\" %}\n+  ins_encode %{\n+    __ sve_orv(as_FloatRegister($tmp$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orv $tmp, $src2\\t# vector (sve) (S)\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"orrw  $dst, $dst, $src1\\t # or reduction S\" %}\n+  ins_encode %{\n+    __ sve_orv(as_FloatRegister($tmp$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orv $tmp, $src2\\t# vector (sve) (D)\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"orr  $dst, $dst, $src1\\t # or reduction D\" %}\n+  ins_encode %{\n+    __ sve_orv(as_FloatRegister($tmp$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reduction\n+\n+instruct reduce_eorB(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eorv $tmp, $src2\\t# vector (sve) (B)\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"eorw  $dst, $dst, $src1\\n\\t\"\n+            \"sxtb  $dst, $dst\\t # eor reduction B\" %}\n+  ins_encode %{\n+    __ sve_eorv(as_FloatRegister($tmp$$reg), __ B,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorS(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eorv $tmp, $src2\\t# vector (sve) (H)\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"eorw  $dst, $dst, $src1\\n\\t\"\n+            \"sxth  $dst, $dst\\t # eor reduction H\" %}\n+  ins_encode %{\n+    __ sve_eorv(as_FloatRegister($tmp$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eorv $tmp, $src2\\t# vector (sve) (S)\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"eorw  $dst, $dst, $src1\\t # eor reduction S\" %}\n+  ins_encode %{\n+    __ sve_eorv(as_FloatRegister($tmp$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eorv $tmp, $src2\\t# vector (sve) (D)\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"eor  $dst, $dst, $src1\\t # eor reduction D\" %}\n+  ins_encode %{\n+    __ sve_eorv(as_FloatRegister($tmp$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -985,0 +1561,80 @@\n+instruct reduce_maxB(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_smaxv $tmp, $src2\\t# vector (sve) (B)\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 GT\\t# max reduction B\" %}\n+  ins_encode %{\n+    __ sve_smaxv(as_FloatRegister($tmp$$reg), __ B,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxS(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_smaxv $tmp, $src2\\t# vector (sve) (H)\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 GT\\t# max reduction H\" %}\n+  ins_encode %{\n+    __ sve_smaxv(as_FloatRegister($tmp$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_smaxv $tmp, $src2\\t# vector (sve) (S)\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 GT\\t# max reduction S\" %}\n+  ins_encode %{\n+    __ sve_smaxv(as_FloatRegister($tmp$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_smaxv $tmp, $src2\\t# vector (sve) (D)\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"cmp  $dst, $src1\\n\\t\"\n+            \"csel $dst, $dst, $src1 GT\\t# max reduction D\" %}\n+  ins_encode %{\n+    __ sve_smaxv(as_FloatRegister($tmp$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ cmp($dst$$Register, $src1$$Register);\n+    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1007,1 +1663,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (S)\\n\\t\"\n+  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (D)\\n\\t\"\n@@ -1019,0 +1675,80 @@\n+instruct reduce_minB(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sminv $tmp, $src2\\t# vector (sve) (B)\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 LT\\t# min reduction B\" %}\n+  ins_encode %{\n+    __ sve_sminv(as_FloatRegister($tmp$$reg), __ B,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minS(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sminv $tmp, $src2\\t# vector (sve) (H)\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 LT\\t# min reduction H\" %}\n+  ins_encode %{\n+    __ sve_sminv(as_FloatRegister($tmp$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sminv $tmp, $src2\\t# vector (sve) (S)\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 LT\\t# min reduction S\" %}\n+  ins_encode %{\n+    __ sve_sminv(as_FloatRegister($tmp$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sminv $tmp, $src2\\t# vector (sve) (D)\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"cmp  $dst, $src1\\n\\t\"\n+            \"csel $dst, $dst, $src1 LT\\t# min reduction D\" %}\n+  ins_encode %{\n+    __ sve_sminv(as_FloatRegister($tmp$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ cmp($dst$$Register, $src1$$Register);\n+    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1041,1 +1777,1 @@\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (S)\\n\\t\"\n+  format %{ \"sve_fminv $dst, $src2 # vector (sve) (D)\\n\\t\"\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":763,"deletions":27,"binary":false,"changes":790,"status":"modified"},{"patch":"@@ -91,0 +91,12 @@\n+  static inline uint vector_length(const MachNode* n) {\n+    const TypeVect* vt = n->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n+  static inline uint vector_length(const MachNode* use, const MachOper* opnd) {\n+    int def_idx = use->operand_index(opnd);\n+    Node* def = use->in(def_idx);\n+    const TypeVect* vt = def->bottom_type()->is_vect();\n+    return vt->length();\n+  }\n+\n@@ -116,2 +128,3 @@\n-  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store,\n-                                   FloatRegister reg, PRegister pg, BasicType bt,\n+  static void loadStoreA_predicate(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+                                   PRegister pg, BasicType mem_elem_bt,\n+                                   Assembler::SIMD_RegVariant vector_elem_size,\n@@ -120,2 +133,1 @@\n-    Assembler::SIMD_RegVariant type;\n-    int esize = type2aelembytes(bt);\n+    int esize = type2aelembytes(mem_elem_bt);\n@@ -127,1 +139,0 @@\n-        type = Assembler::B;\n@@ -131,1 +142,0 @@\n-        type = Assembler::H;\n@@ -135,1 +145,0 @@\n-        type = Assembler::S;\n@@ -139,1 +148,0 @@\n-        type = Assembler::D;\n@@ -145,1 +153,1 @@\n-      (masm.*insn)(reg, type, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n+      (masm.*insn)(reg, vector_elem_size, pg, Address(base, disp \/ Matcher::scalable_vector_reg_size(T_BYTE)));\n@@ -152,0 +160,30 @@\n+  static void sve_compare(C2_MacroAssembler masm, PRegister pd, BasicType bt,\n+                          PRegister pg, FloatRegister zn, FloatRegister zm, int cond) {\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (bt == T_FLOAT || bt == T_DOUBLE) {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_fcmeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_fcmne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_fcmge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_fcmgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_fcmge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_fcmgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    } else {\n+      switch (cond) {\n+        case BoolTest::eq: masm.sve_cmpeq(pd, size, pg, zn, zm); break;\n+        case BoolTest::ne: masm.sve_cmpne(pd, size, pg, zn, zm); break;\n+        case BoolTest::ge: masm.sve_cmpge(pd, size, pg, zn, zm); break;\n+        case BoolTest::gt: masm.sve_cmpgt(pd, size, pg, zn, zm); break;\n+        case BoolTest::le: masm.sve_cmpge(pd, size, pg, zm, zn); break;\n+        case BoolTest::lt: masm.sve_cmpgt(pd, size, pg, zm, zn); break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n@@ -171,5 +209,0 @@\n-      case Op_AndReductionV:\n-      case Op_OrReductionV:\n-      case Op_XorReductionV:\n-      case Op_MaxReductionV:\n-      case Op_MinReductionV:\n@@ -178,1 +211,0 @@\n-      case Op_VectorBlend:\n@@ -188,2 +220,0 @@\n-      case Op_VectorLoadMask:\n-      case Op_VectorMaskCmp:\n@@ -193,1 +223,0 @@\n-      case Op_VectorStoreMask:\n@@ -207,1 +236,0 @@\n-\n@@ -221,1 +249,1 @@\n-\/\/ Use predicated vector load\/store\n+\/\/ Unpredicated vector load\/store\n@@ -223,1 +251,2 @@\n-  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_LoadVector()->memory_size() >= 16 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n@@ -229,0 +258,1 @@\n+    BasicType bt = vector_element_basic_type(this);\n@@ -230,1 +260,1 @@\n-                         vector_element_basic_type(this), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -237,1 +267,2 @@\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16);\n+  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() >= 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n@@ -243,0 +274,1 @@\n+    BasicType bt = vector_element_basic_type(this, $src);\n@@ -244,1 +276,1 @@\n-                         vector_element_basic_type(this, $src), $mem->opcode(),\n+                         bt, elemType_to_regVariant(bt), $mem->opcode(),\n@@ -250,0 +282,56 @@\n+\/\/ Predicated vector load\/store, based on the vector length of the node.\n+\/\/ Only load\/store values in the range of the memory_size. This is needed\n+\/\/ when the memory_size is lower than the hardware supported max vector size.\n+\/\/ And this might happen for Vector API mask vector load\/store.\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                       iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_LoadVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_LoadVector()->memory_size() != MaxVectorSize);\n+  match(Set dst (LoadVector mem));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_ldr $dst, $pTmp, $mem\\t # load vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, iRegINoSp tmp1,\n+                          iRegINoSp tmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= MaxVectorSize \/ 8 &&\n+            n->as_StoreVector()->memory_size() != MaxVectorSize);\n+  match(Set mem (StoreVector mem src));\n+  effect(TEMP pTmp, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"mov $tmp1, 0\\n\\t\"\n+            \"mov $tmp2, vector_length\\n\\t\"\n+            \"sve_whilelo $pTmp, $tmp1, $tmp2\\n\\t\"\n+            \"sve_str $src, $pTmp, $mem\\t # store vector mask\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    __ mov(as_Register($tmp1$$reg), 0);\n+    __ mov(as_Register($tmp2$$reg), vector_length(this, $src));\n+    __ sve_whilelo(as_PRegister($pTmp$$reg), size,\n+                   as_Register($tmp1$$reg), as_Register($tmp2$$reg));\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, src_reg,\n+                         as_PRegister($pTmp$$reg), bt, size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}dnl\n+\n@@ -570,1 +658,1 @@\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\"  %}\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n@@ -575,0 +663,255 @@\n+%}\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n+            \"sve_cpy $dst, $pTmp, -1\\t # vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src1$$reg),\n+                as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), -1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend\n+\n+instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) src3));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    __ sve_cmpeq(as_PRegister($pTmp$$reg), size, ptrue,\n+                 as_FloatRegister($src3$$reg), -1);\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend with compare\n+\n+instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n+                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n+  effect(TEMP pTmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_cmp $pTmp, $src3, $src4\\t # vector cmp (sve)\\n\\t\"\n+            \"sve_sel $dst, $pTmp, $src2, $src1\\t # vector blend (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    sve_compare(C2_MacroAssembler(&cbuf), as_PRegister($pTmp$$reg), bt,\n+                ptrue, as_FloatRegister($src3$$reg),\n+                as_FloatRegister($src4$$reg), (int)$cond$$constant);\n+    __ sve_sel(as_FloatRegister($dst$$reg), elemType_to_regVariant(bt),\n+               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n+               as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector load mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskS(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to H)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to S)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmaskL(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_uunpklo $dst, $src\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_uunpklo $dst, $dst\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector load mask (B to D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H,\n+                   as_FloatRegister($src$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D,\n+                   as_FloatRegister($dst$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 16);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_neg $dst, $src\\t # vector store mask (B)\" %}\n+  ins_encode %{\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 8);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (H to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (S to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(5 * SVE_COST);\n+  format %{ \"sve_dup $tmp, 0\\n\\t\"\n+            \"sve_uzp1 $dst, $src, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_uzp1 $dst, $dst, $tmp\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # vector store mask (sve) (D to B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n+                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n+                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n+               as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ load\/store mask vector\n+\n+instruct vloadmask_loadV(vReg dst, vmemA mem) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16 &&\n+            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n+            \"sve_neg $dst, $dst\\t # load vector mask (sve)\" %}\n+  ins_encode %{\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    Assembler::SIMD_RegVariant to_vect_size =\n+              elemType_to_regVariant(vector_element_basic_type(this));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n+                         T_BOOLEAN, to_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_neg(dst_reg, to_vect_size, ptrue, dst_reg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_vstoremask(vmemA mem, vReg src, vReg tmp, immI size) %{\n+  predicate(UseSVE > 0 && n->as_StoreVector()->length() >= 2 &&\n+            n->as_StoreVector()->vect_type()->element_basic_type() == T_BOOLEAN);\n+  match(Set mem (StoreVector mem (VectorStoreMask src size)));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_neg $tmp, $src\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t # store vector mask (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant from_vect_size =\n+              elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_size, ptrue,\n+               as_FloatRegister($src$$reg));\n+    loadStoreA_predicate(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n+                         ptrue, T_BOOLEAN, from_vect_size, $mem->opcode(),\n+                         as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n@@ -638,1 +981,1 @@\n-REDUCE_ADD_EXT(reduce_addB, AddReductionVI, iRegINoSp, iRegIorL2I, B, T_BYTE,  sxtb)\n+REDUCE_ADD_EXT(reduce_addB, AddReductionVI, iRegINoSp, iRegIorL2I, B, T_BYTE, sxtb)\n@@ -644,0 +987,196 @@\n+dnl\n+dnl REDUCE_AND_EXT($1,        $2,      $3,      $4,      $5,   $6,        $7   )\n+dnl REDUCE_AND_EXT(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1)\n+define(`REDUCE_AND_EXT', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_andv $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"smov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"andw  $dst, $dst, $src1\\n\\t\"\n+            \"$7  $dst, $dst\\t # and reduction $5\" %}\n+  ins_encode %{\n+    __ sve_andv(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ $7($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_AND($1,        $2,      $3,      $4,      $5,   $6,        $7   )\n+dnl REDUCE_AND(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1)\n+define(`REDUCE_AND', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_andv $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"umov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"$7  $dst, $dst, $src1\\t # and reduction $5\" %}\n+  ins_encode %{\n+    __ sve_andv(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ $7($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+\/\/ vector and reduction\n+REDUCE_AND_EXT(reduce_andB, AndReductionV, iRegINoSp, iRegIorL2I, B, T_BYTE, sxtb)\n+REDUCE_AND_EXT(reduce_andS, AndReductionV, iRegINoSp, iRegIorL2I, H, T_SHORT, sxth)\n+REDUCE_AND(reduce_andI, AndReductionV, iRegINoSp, iRegIorL2I, S, T_INT, andw)\n+REDUCE_AND(reduce_andL, AndReductionV, iRegLNoSp, iRegL, D, T_LONG, andr)\n+dnl\n+dnl REDUCE_OR_EXT($1,        $2,      $3,      $4,      $5,   $6,        $7   )\n+dnl REDUCE_OR_EXT(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1)\n+define(`REDUCE_OR_EXT', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orv $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"smov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"orrw  $dst, $dst, $src1\\n\\t\"\n+            \"$7  $dst, $dst\\t # or reduction $5\" %}\n+  ins_encode %{\n+    __ sve_orv(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ $7($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_OR($1,        $2,      $3,      $4,      $5,   $6,        $7   )\n+dnl REDUCE_OR(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1)\n+define(`REDUCE_OR', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orv $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"umov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"$7  $dst, $dst, $src1\\t # or reduction $5\" %}\n+  ins_encode %{\n+    __ sve_orv(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ $7($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+\/\/ vector or reduction\n+REDUCE_OR_EXT(reduce_orB, OrReductionV, iRegINoSp, iRegIorL2I, B, T_BYTE, sxtb)\n+REDUCE_OR_EXT(reduce_orS, OrReductionV, iRegINoSp, iRegIorL2I, H, T_SHORT, sxth)\n+REDUCE_OR(reduce_orI, OrReductionV, iRegINoSp, iRegIorL2I, S, T_INT, orrw)\n+REDUCE_OR(reduce_orL, OrReductionV, iRegLNoSp, iRegL, D, T_LONG, orr)\n+dnl\n+dnl REDUCE_XOR_EXT($1,        $2,      $3,      $4,      $5,   $6,        $7   )\n+dnl REDUCE_XOR_EXT(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1)\n+define(`REDUCE_XOR_EXT', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eorv $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"smov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"eorw  $dst, $dst, $src1\\n\\t\"\n+            \"$7  $dst, $dst\\t # eor reduction $5\" %}\n+  ins_encode %{\n+    __ sve_eorv(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ $7($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_XOR($1,        $2,      $3,      $4,      $5,   $6,        $7   )\n+dnl REDUCE_XOR(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1)\n+define(`REDUCE_XOR', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eorv $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"umov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"$7  $dst, $dst, $src1\\t # eor reduction $5\" %}\n+  ins_encode %{\n+    __ sve_eorv(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ $7($dst$$Register, $dst$$Register, $src1$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+\/\/ vector xor reduction\n+REDUCE_XOR_EXT(reduce_eorB, XorReductionV, iRegINoSp, iRegIorL2I, B, T_BYTE, sxtb)\n+REDUCE_XOR_EXT(reduce_eorS, XorReductionV, iRegINoSp, iRegIorL2I, H, T_SHORT, sxth)\n+REDUCE_XOR(reduce_eorI, XorReductionV, iRegINoSp, iRegIorL2I, S, T_INT, eorw)\n+REDUCE_XOR(reduce_eorL, XorReductionV, iRegLNoSp, iRegL, D, T_LONG, eor)\n+dnl\n+dnl REDUCE_MAXMIN_EXT($1,        $2,      $3,      $4,      $5,   $6,        $7,  $8     )\n+dnl REDUCE_MAXMIN_EXT(insn_name, op_name, reg_dst, reg_src, size, elem_type, cmp, min_max)\n+define(`REDUCE_MAXMIN_EXT', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_s$8v $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"smov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"cmpw  $dst, $src1\\n\\t\"\n+            \"cselw $dst, $dst, $src1 $7\\t# $8 reduction $5\" %}\n+  ins_encode %{\n+    __ sve_s$8v(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ cmpw($dst$$Register, $src1$$Register);\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$7);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN($1,        $2,      $3,      $4,      $5,   $6,        $7,    $8,    $9 , $10    )\n+dnl REDUCE_MAXMIN(insn_name, op_name, reg_dst, reg_src, size, elem_type, insn1, insn2, cmp, min_max)\n+define(`REDUCE_MAXMIN', `\n+instruct $1($3 dst, $4 src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() >= 16 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $6);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_s$10v $tmp, $src2\\t# vector (sve) ($5)\\n\\t\"\n+            \"umov  $dst, $tmp, $5, 0\\n\\t\"\n+            \"$7  $dst, $src1\\n\\t\"\n+            \"$8 $dst, $dst, $src1 $9\\t# $10 reduction $5\" %}\n+  ins_encode %{\n+    __ sve_s$10v(as_FloatRegister($tmp$$reg), __ $5,\n+         ptrue, as_FloatRegister($src2$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ $5, 0);\n+    __ $7($dst$$Register, $src1$$Register);\n+    __ $8(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$9);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n@@ -655,1 +1194,1 @@\n-  format %{ \"sve_f$1v $dst, $src2 # vector (sve) (S)\\n\\t\"\n+  format %{ \"sve_f$1v $dst, $src2 # vector (sve) ($4)\\n\\t\"\n@@ -665,0 +1204,4 @@\n+REDUCE_MAXMIN_EXT(reduce_maxB, MaxReductionV, iRegINoSp, iRegIorL2I, B, T_BYTE, GT, max)\n+REDUCE_MAXMIN_EXT(reduce_maxS, MaxReductionV, iRegINoSp, iRegIorL2I, H, T_SHORT, GT, max)\n+REDUCE_MAXMIN(reduce_maxI, MaxReductionV, iRegINoSp, iRegIorL2I, S, T_INT, cmpw, cselw, GT, max)\n+REDUCE_MAXMIN(reduce_maxL, MaxReductionV, iRegLNoSp, iRegL, D, T_LONG, cmp, csel, GT, max)\n@@ -669,0 +1212,4 @@\n+REDUCE_MAXMIN_EXT(reduce_minB, MinReductionV, iRegINoSp, iRegIorL2I, B, T_BYTE, LT, min)\n+REDUCE_MAXMIN_EXT(reduce_minS, MinReductionV, iRegINoSp, iRegIorL2I, H, T_SHORT, LT, min)\n+REDUCE_MAXMIN(reduce_minI, MinReductionV, iRegINoSp, iRegIorL2I, S, T_INT, cmpw, cselw, LT, min)\n+REDUCE_MAXMIN(reduce_minL, MinReductionV, iRegLNoSp, iRegL, D, T_LONG, cmp, csel, LT, min)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":574,"deletions":27,"binary":false,"changes":601,"status":"modified"},{"patch":"@@ -3202,1 +3202,1 @@\n-  void sve_dup(FloatRegister Zd, SIMD_RegVariant T, int imm8) {\n+  void sve_dup(FloatRegister Zd, SIMD_RegVariant T, int imm16) {\n@@ -3206,1 +3206,2 @@\n-    if (imm8 <= 127 && imm8 >= -128) {\n+    unsigned imm = imm16;\n+    if (imm16 <= 127 && imm16 >= -128) {\n@@ -3208,1 +3209,1 @@\n-    } else if (T != B && imm8 <= 32512 && imm8 >= -32768 && (imm8 & 0xff) == 0) {\n+    } else if (T != B && imm16 <= 32512 && imm16 >= -32768 && (imm16 & 0xff) == 0) {\n@@ -3210,1 +3211,1 @@\n-      imm8 = (imm8 >> 8);\n+      imm = (imm >> 8);\n@@ -3214,0 +3215,2 @@\n+    unsigned mask = (1U << 8) - 1;\n+    imm &= mask;\n@@ -3215,1 +3218,1 @@\n-    f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+    f(sh, 13), f(imm, 12, 5), rf(Zd, 0);\n@@ -3224,0 +3227,124 @@\n+   \/\/ SVE cpy immediate\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm16, bool isMerge) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    int sh = 0;\n+    unsigned imm = imm16;\n+    if (imm16 <= 127 && imm16 >= -128) {\n+      sh = 0;\n+    } else if (T != B && imm16 <= 32512 && imm16 >= -32768 && (imm16 & 0xff) == 0) {\n+      sh = 1;\n+      imm = (imm >> 8);\n+    } else {\n+      guarantee(false, \"invalid immediate\");\n+    }\n+    unsigned mask = (1U << 8) - 1;\n+    imm &= mask;\n+    int m = isMerge ? 1 : 0;\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b01, 21, 20);\n+    prf(Pg, 16), f(0b0, 15), f(m, 14), f(sh, 13), f(imm, 12, 5), rf(Zd, 0);\n+  }\n+\n+  \/\/ SVE vector sel\n+  void sve_sel(FloatRegister Zd,\n+               SIMD_RegVariant T,\n+               PRegister Pg,\n+               FloatRegister Zn,\n+               FloatRegister Zm) {\n+    starti;\n+    assert(T != Q, \"invalid size\");\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);\n+    f(0b11, 15, 14), prf(Pg, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n+\/\/ SVE compare vector\n+#define INSN(NAME, op, cond, fp)  \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pg, FloatRegister Zn, FloatRegister Zm)  { \\\n+    starti;                                                                                       \\\n+    if (fp == 0) {                                                                                \\\n+      assert(T != Q, \"invalid size\");                                                             \\\n+    } else {                                                                                      \\\n+      assert(T != B && T != Q, \"invalid size\");                                                   \\\n+    }                                                                                             \\\n+    f(op, 31, 24), f(T, 23, 22), f(0b0, 21), rf(Zm, 16), f((cond >> 1) & 0x7, 15, 13);            \\\n+    pgrf(Pg, 10), rf(Zn, 5), f(cond & 0x1, 4), prf(Pd, 0);                                        \\\n+  }\n+\n+  INSN(sve_cmpeq, 0b00100100, 0b1010, 0);\n+  INSN(sve_cmpne, 0b00100100, 0b1011, 0);\n+  INSN(sve_cmpge, 0b00100100, 0b1000, 0);\n+  INSN(sve_cmpgt, 0b00100100, 0b1001, 0);\n+  INSN(sve_fcmeq, 0b01100101, 0b0110, 1);\n+  INSN(sve_fcmne, 0b01100101, 0b0111, 1);\n+  INSN(sve_fcmgt, 0b01100101, 0b0101, 1);\n+  INSN(sve_fcmge, 0b01100101, 0b0100, 1);\n+#undef INSN\n+\n+\/\/ SVE compare vector with immediate\n+#define INSN(NAME, cond)  \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pg, FloatRegister Zn, int imm5) { \\\n+    starti;                                                                              \\\n+    assert(T != Q, \"invalid size\");                                                      \\\n+    if (imm5 > 15 || imm5 < -16) {                                                       \\\n+      guarantee(false, \"invalid immediate\");                                             \\\n+    }                                                                                    \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(0b0, 21), sf(imm5, 20, 16),                   \\\n+    f((cond >> 1) & 0x7, 15, 13), pgrf(Pg, 10), rf(Zn, 5), f(cond & 0x1, 4), prf(Pd, 0); \\\n+  }\n+\n+  INSN(sve_cmpeq, 0b1000);\n+  INSN(sve_cmpne, 0b1001);\n+  INSN(sve_cmpgt, 0b0001);\n+  INSN(sve_cmpge, 0b0000);\n+  INSN(sve_cmplt, 0b0010);\n+  INSN(sve_cmple, 0b0011);\n+#undef INSN\n+\n+\/\/ SVE unpack and extend\n+#define INSN(NAME, op) \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn) { \\\n+    starti;                                                          \\\n+    assert(T != B && T != Q, \"invalid size\");                        \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1100, 21, 18);          \\\n+    f(op, 17, 16), f(0b001110, 15, 10), rf(Zn, 5), rf(Zd, 0);        \\\n+  }\n+\n+  INSN(sve_uunpkhi, 0b11);\n+  INSN(sve_uunpklo, 0b10);\n+  INSN(sve_sunpkhi, 0b01);\n+  INSN(sve_sunpklo, 0b00);\n+#undef INSN\n+\n+\/\/ SVE vector uzp1,uzp2\n+#define INSN(NAME, op) \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, FloatRegister Zn, FloatRegister Zm) { \\\n+    starti;                                                                            \\\n+    assert(T != Q, \"invalid size\");                                                    \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b1, 21), rf(Zm, 16);                       \\\n+    f(0b01101, 15, 11), f(op, 10), rf(Zn, 5), rf(Zd, 0);                               \\\n+  }\n+\n+  INSN(sve_uzp1, 0b0);\n+  INSN(sve_uzp2, 0b1);\n+#undef INSN\n+\n+\/\/ SVE while[cond]\n+#define INSN(NAME, decode, sf)                                            \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, Register Rn, Register Rm) {  \\\n+    starti;                                                               \\\n+    assert(T != Q, \"invalid register variant\");                           \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(1, 21),                        \\\n+    zrf(Rm, 16), f(0, 15, 13), f(sf, 12), f(decode >> 1, 11, 10),         \\\n+    zrf(Rn, 5), f(decode & 0b1, 4), prf(Pd, 0);                           \\\n+  }\n+\n+  INSN(sve_whilelt,  0b010, 1);\n+  INSN(sve_whileltw, 0b010, 0);\n+  INSN(sve_whilele,  0b011, 1);\n+  INSN(sve_whilelew, 0b011, 0);\n+  INSN(sve_whilelo,  0b110, 1);\n+  INSN(sve_whilelow, 0b110, 0);\n+  INSN(sve_whilels,  0b111, 1);\n+  INSN(sve_whilelsw, 0b111, 0);\n+#undef INSN\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":132,"deletions":5,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -855,0 +855,8 @@\n+int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n+                                             uint num_bits,\n+                                             uint total_args_passed) {\n+  assert(!Matcher::supports_vector_calling_convention(), \"not implemented\");\n+  Unimplemented();\n+  return 0;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -365,0 +365,8 @@\n+int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n+                                             uint num_bits,\n+                                             uint total_args_passed) {\n+  assert(!Matcher::supports_vector_calling_convention(), \"not implemented\");\n+  Unimplemented();\n+  return 0;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2158,0 +2158,14 @@\n+\/\/ Vector calling convention not yet implemented.\n+const bool Matcher::supports_vector_calling_convention(void) {\n+  return false;\n+}\n+\n+void Matcher::vector_calling_convention(VMRegPair *regs, uint num_bits, uint total_args_passed) {\n+  (void) SharedRuntime::vector_calling_convention(regs, num_bits, total_args_passed);\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  Unimplemented();\n+  return OptoRegPair(0, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -927,0 +927,8 @@\n+int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n+                                             uint num_bits,\n+                                             uint total_args_passed) {\n+  assert(!Matcher::supports_vector_calling_convention(), \"not implemented\");\n+  Unimplemented();\n+  return 0;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1551,0 +1551,14 @@\n+\/\/ Vector calling convention not yet implemented.\n+const bool Matcher::supports_vector_calling_convention(void) {\n+  return false;\n+}\n+\n+void Matcher::vector_calling_convention(VMRegPair *regs, uint num_bits, uint total_args_passed) {\n+  (void) SharedRuntime::vector_calling_convention(regs, num_bits, total_args_passed);\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  Unimplemented();\n+  return OptoRegPair(0, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -861,0 +861,8 @@\n+int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n+                                             uint num_bits,\n+                                             uint total_args_passed) {\n+  assert(!Matcher::supports_vector_calling_convention(), \"not implemented\");\n+  Unimplemented();\n+  return 0;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1031,0 +1031,8 @@\n+int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n+                                             uint num_bits,\n+                                             uint total_args_passed) {\n+  assert(!Matcher::supports_vector_calling_convention(), \"not implemented\");\n+  Unimplemented();\n+  return 0;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1136,0 +1136,25 @@\n+int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n+                                             uint num_bits,\n+                                             uint total_args_passed) {\n+  assert(num_bits == 64 || num_bits == 128 || num_bits == 256 || num_bits == 512,\n+         \"only certain vector sizes are supported for now\");\n+\n+  static const XMMRegister VEC_ArgReg[32] = {\n+     xmm0,  xmm1,  xmm2,  xmm3,  xmm4,  xmm5,  xmm6,  xmm7,\n+     xmm8,  xmm9, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15,\n+    xmm16, xmm17, xmm18, xmm19, xmm20, xmm21, xmm22, xmm23,\n+    xmm24, xmm25, xmm26, xmm27, xmm28, xmm29, xmm30, xmm31\n+  };\n+\n+  uint stk_args = 0;\n+  uint fp_args = 0;\n+\n+  for (uint i = 0; i < total_args_passed; i++) {\n+    VMReg vmreg = VEC_ArgReg[fp_args++]->as_VMReg();\n+    int next_val = num_bits == 64 ? 1 : (num_bits == 128 ? 3 : (num_bits  == 256 ? 7 : 15));\n+    regs[i].set_pair(vmreg->next(next_val), vmreg);\n+  }\n+\n+  return stk_args;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -53,0 +53,277 @@\n+#ifdef __VECTOR_API_MATH_INTRINSICS_COMMON\n+\/\/ Vector API SVML routines written in assembly\n+extern \"C\"\n+{\n+   float __svml_expf4_ha_ex(float a);\n+   double __svml_exp1_ha_ex(double a);\n+   double __svml_exp2_ha_ex(double a);\n+   float __svml_expf4_ha_l9(float a);\n+   float __svml_expf8_ha_l9(float a);\n+   float __svml_expf4_ha_e9(float a);\n+   float __svml_expf8_ha_e9(float a);\n+   float __svml_expf16_ha_z0(float a);\n+   double __svml_exp1_ha_l9(double a);\n+   double __svml_exp2_ha_l9(double a);\n+   double __svml_exp4_ha_l9(double a);\n+   double __svml_exp1_ha_e9(double a);\n+   double __svml_exp2_ha_e9(double a);\n+   double __svml_exp4_ha_e9(double a);\n+   double __svml_exp8_ha_z0(double a);\n+   float  __svml_expm1f4_ha_ex(float a);\n+   double __svml_expm11_ha_ex(double a);\n+   double __svml_expm12_ha_ex(double a);\n+   float  __svml_expm1f4_ha_l9(float a);\n+   float  __svml_expm1f8_ha_l9(float a);\n+   float  __svml_expm1f4_ha_e9(float a);\n+   float  __svml_expm1f8_ha_e9(float a);\n+   float __svml_expm1f16_ha_z0(float a);\n+   double __svml_expm11_ha_l9(double a);\n+   double __svml_expm12_ha_l9(double a);\n+   double __svml_expm14_ha_l9(double a);\n+   double __svml_expm11_ha_e9(double a);\n+   double __svml_expm12_ha_e9(double a);\n+   double __svml_expm14_ha_e9(double a);\n+   double __svml_expm18_ha_z0(double a);\n+   float __svml_log1pf4_ha_l9(float a);\n+   float __svml_log1pf8_ha_l9(float a);\n+   float __svml_log1pf4_ha_e9(float a);\n+   float __svml_log1pf8_ha_e9(float a);\n+   float __svml_log1pf16_ha_z0(float a);\n+   double __svml_log1p1_ha_l9(double a);\n+   double __svml_log1p2_ha_l9(double a);\n+   double __svml_log1p4_ha_l9(double a);\n+   double __svml_log1p1_ha_e9(double a);\n+   double __svml_log1p2_ha_e9(double a);\n+   double __svml_log1p4_ha_e9(double a);\n+   double __svml_log1p8_ha_z0(double a);\n+   float __svml_logf4_ha_l9(float a);\n+   float __svml_logf8_ha_l9(float a);\n+   float __svml_logf4_ha_e9(float a);\n+   float __svml_logf8_ha_e9(float a);\n+   float __svml_logf16_ha_z0(float a);\n+   double __svml_log1_ha_l9(double a);\n+   double __svml_log2_ha_l9(double a);\n+   double __svml_log4_ha_l9(double a);\n+   double __svml_log1_ha_e9(double a);\n+   double __svml_log2_ha_e9(double a);\n+   double __svml_log4_ha_e9(double a);\n+   double __svml_log8_ha_z0(double a);\n+   float __svml_log10f4_ha_l9(float a);\n+   float __svml_log10f8_ha_l9(float a);\n+   float __svml_log10f4_ha_e9(float a);\n+   float __svml_log10f8_ha_e9(float a);\n+   float __svml_log10f16_ha_z0(float a);\n+   double __svml_log101_ha_l9(double a);\n+   double __svml_log102_ha_l9(double a);\n+   double __svml_log104_ha_l9(double a);\n+   double __svml_log101_ha_e9(double a);\n+   double __svml_log102_ha_e9(double a);\n+   double __svml_log104_ha_e9(double a);\n+   double __svml_log108_ha_z0(double a);\n+   float __svml_sinf4_ha_l9(float a);\n+   float __svml_sinf8_ha_l9(float a);\n+   float __svml_sinf4_ha_e9(float a);\n+   float __svml_sinf8_ha_e9(float a);\n+   float __svml_sinf16_ha_z0(float a);\n+   double __svml_sin1_ha_l9(double a);\n+   double __svml_sin2_ha_l9(double a);\n+   double __svml_sin4_ha_l9(double a);\n+   double __svml_sin1_ha_e9(double a);\n+   double __svml_sin2_ha_e9(double a);\n+   double __svml_sin4_ha_e9(double a);\n+   double __svml_sin8_ha_z0(double a);\n+   float __svml_cosf4_ha_l9(float a);\n+   float __svml_cosf8_ha_l9(float a);\n+   float __svml_cosf4_ha_e9(float a);\n+   float __svml_cosf8_ha_e9(float a);\n+   float __svml_cosf16_ha_z0(float a);\n+   double  __svml_cos1_ha_l9(double a);\n+   double  __svml_cos2_ha_l9(double a);\n+   double __svml_cos4_ha_l9(double a);\n+   double  __svml_cos1_ha_e9(double a);\n+   double  __svml_cos2_ha_e9(double a);\n+   double __svml_cos4_ha_e9(double a);\n+   double  __svml_cos8_ha_z0(double a);\n+   float __svml_tanf4_ha_l9(float a);\n+   float __svml_tanf8_ha_l9(float a);\n+   float __svml_tanf4_ha_e9(float a);\n+   float __svml_tanf8_ha_e9(float a);\n+   float __svml_tanf16_ha_z0(float a);\n+   double __svml_tan1_ha_l9(double a);\n+   double __svml_tan2_ha_l9(double a);\n+   double __svml_tan4_ha_l9(double a);\n+   double __svml_tan1_ha_e9(double a);\n+   double __svml_tan2_ha_e9(double a);\n+   double __svml_tan4_ha_e9(double a);\n+   double __svml_tan8_ha_z0(double a);\n+   double __svml_sinh1_ha_l9(double a);\n+   double __svml_sinh2_ha_l9(double a);\n+   double __svml_sinh4_ha_l9(double a);\n+   double __svml_sinh1_ha_e9(double a);\n+   double __svml_sinh2_ha_e9(double a);\n+   double __svml_sinh4_ha_e9(double a);\n+   double __svml_sinh8_ha_z0(double a);\n+   float __svml_sinhf4_ha_l9(float a);\n+   float __svml_sinhf8_ha_l9(float a);\n+   float __svml_sinhf4_ha_e9(float a);\n+   float __svml_sinhf8_ha_e9(float a);\n+   float __svml_sinhf16_ha_z0(float a);\n+   double __svml_cosh1_ha_l9(double a);\n+   double __svml_cosh2_ha_l9(double a);\n+   double __svml_cosh4_ha_l9(double a);\n+   double __svml_cosh1_ha_e9(double a);\n+   double __svml_cosh2_ha_e9(double a);\n+   double __svml_cosh4_ha_e9(double a);\n+   double __svml_cosh8_ha_z0(double a);\n+   float __svml_coshf4_ha_l9(float a);\n+   float __svml_coshf8_ha_l9(float a);\n+   float __svml_coshf4_ha_e9(float a);\n+   float __svml_coshf8_ha_e9(float a);\n+   float __svml_coshf16_ha_z0(float a);\n+   double __svml_tanh1_ha_l9(double a);\n+   double __svml_tanh2_ha_l9(double a);\n+   double __svml_tanh4_ha_l9(double a);\n+   double __svml_tanh1_ha_e9(double a);\n+   double __svml_tanh2_ha_e9(double a);\n+   double __svml_tanh4_ha_e9(double a);\n+   double __svml_tanh8_ha_z0(double a);\n+   float __svml_tanhf4_ha_l9(float a);\n+   float __svml_tanhf8_ha_l9(float a);\n+   float __svml_tanhf4_ha_e9(float a);\n+   float __svml_tanhf8_ha_e9(float a);\n+   float __svml_tanhf16_ha_z0(float a);\n+   float __svml_acosf4_ha_ex(float a);\n+   float __svml_acosf4_ha_l9(float a);\n+   float __svml_acosf8_ha_l9(float a);\n+   float __svml_acosf4_ha_e9(float a);\n+   float __svml_acosf8_ha_e9(float a);\n+   float __svml_acosf16_ha_z0(float a);\n+   double __svml_acos1_ha_ex(double a);\n+   double __svml_acos2_ha_ex(double a);\n+   double __svml_acos1_ha_l9(double a);\n+   double __svml_acos2_ha_l9(double a);\n+   double __svml_acos4_ha_l9(double a);\n+   double __svml_acos1_ha_e9(double a);\n+   double __svml_acos2_ha_e9(double a);\n+   double __svml_acos4_ha_e9(double a);\n+   double __svml_acos8_ha_z0(double a);\n+   float __svml_asinf4_ha_ex(float a);\n+   double __svml_asin1_ha_ex(double a);\n+   double __svml_asin2_ha_ex(double a);\n+   double __svml_asin1_ha_l9(double a);\n+   double __svml_asin2_ha_l9(double a);\n+   double __svml_asin4_ha_l9(double a);\n+   double __svml_asin1_ha_e9(double a);\n+   double __svml_asin2_ha_e9(double a);\n+   double __svml_asin4_ha_e9(double a);\n+   double __svml_asin8_ha_z0(double a);\n+   float __svml_asinf4_ha_l9(float a);\n+   float __svml_asinf8_ha_l9(float a);\n+   float __svml_asinf4_ha_e9(float a);\n+   float __svml_asinf8_ha_e9(float a);\n+   float __svml_asinf16_ha_z0(float a);\n+   float __svml_atanf4_ha_ex(float a);\n+   double __svml_atan1_ha_ex(double a);\n+   double __svml_atan2_ha_ex(double a);\n+   double __svml_atan1_ha_l9(double a);\n+   double __svml_atan2_ha_l9(double a);\n+   double __svml_atan4_ha_l9(double a);\n+   double __svml_atan1_ha_e9(double a);\n+   double __svml_atan2_ha_e9(double a);\n+   double __svml_atan4_ha_e9(double a);\n+   double __svml_atan8_ha_z0(double a);\n+   float __svml_atanf4_ha_l9(float a);\n+   float __svml_atanf8_ha_l9(float a);\n+   float __svml_atanf4_ha_e9(float a);\n+   float __svml_atanf8_ha_e9(float a);\n+   float __svml_atanf16_ha_z0(float a);\n+   float __svml_powf4_ha_l9(float a, float b);\n+   float __svml_powf8_ha_l9(float a, float b);\n+   float __svml_powf4_ha_e9(float a, float b);\n+   float __svml_powf8_ha_e9(float a, float b);\n+   float __svml_powf16_ha_z0(float a, float b);\n+   double __svml_pow1_ha_l9(double a, double b);\n+   double __svml_pow2_ha_l9(double a, double b);\n+   double __svml_pow4_ha_l9(double a, double b);\n+   double __svml_pow1_ha_e9(double a, double b);\n+   double __svml_pow2_ha_e9(double a, double b);\n+   double __svml_pow4_ha_e9(double a, double b);\n+   double __svml_pow8_ha_z0(double a, double b);\n+   float __svml_hypotf4_ha_l9(float a, float b);\n+   float __svml_hypotf8_ha_l9(float a, float b);\n+   float __svml_hypotf4_ha_e9(float a, float b);\n+   float __svml_hypotf8_ha_e9(float a, float b);\n+   float __svml_hypotf16_ha_z0(float a, float b);\n+   double __svml_hypot1_ha_l9(double a, double b);\n+   double __svml_hypot2_ha_l9(double a, double b);\n+   double __svml_hypot4_ha_l9(double a, double b);\n+   double __svml_hypot1_ha_e9(double a, double b);\n+   double __svml_hypot2_ha_e9(double a, double b);\n+   double __svml_hypot4_ha_e9(double a, double b);\n+   double __svml_hypot8_ha_z0(double a, double b);\n+   float __svml_cbrtf4_ha_l9(float a);\n+   float __svml_cbrtf8_ha_l9(float a);\n+   float __svml_cbrtf4_ha_e9(float a);\n+   float __svml_cbrtf8_ha_e9(float a);\n+   float __svml_cbrtf16_ha_z0(float a);\n+   double __svml_cbrt1_ha_l9(double a);\n+   double __svml_cbrt2_ha_l9(double a);\n+   double __svml_cbrt4_ha_l9(double a);\n+   double __svml_cbrt1_ha_e9(double a);\n+   double __svml_cbrt2_ha_e9(double a);\n+   double __svml_cbrt4_ha_e9(double a);\n+   double __svml_cbrt8_ha_z0(double a);\n+   float __svml_atan2f4_ha_l9(float a, float b);\n+   float __svml_atan2f8_ha_l9(float a, float b);\n+   float __svml_atan2f4_ha_e9(float a, float b);\n+   float __svml_atan2f8_ha_e9(float a, float b);\n+   float __svml_atan2f16_ha_z0(float a, float b);\n+   double __svml_atan21_ha_l9(double a, double b);\n+   double __svml_atan22_ha_l9(double a, double b);\n+   double __svml_atan24_ha_l9(double a, double b);\n+   double __svml_atan28_ha_z0(double a, double b);\n+   double __svml_atan21_ha_e9(double a, double b);\n+   double __svml_atan22_ha_e9(double a, double b);\n+   double __svml_atan24_ha_e9(double a, double b);\n+   float __svml_sinf4_ha_ex(float a);\n+   double __svml_sin1_ha_ex(double a);\n+   double __svml_sin2_ha_ex(double a);\n+   float __svml_cosf4_ha_ex(float a);\n+   double __svml_cos1_ha_ex(double a);\n+   double __svml_cos2_ha_ex(double a);\n+   float __svml_tanf4_ha_ex(float a);\n+   double __svml_tan1_ha_ex(double a);\n+   double __svml_tan2_ha_ex(double a);\n+   float __svml_sinhf4_ha_ex(float a);\n+   double __svml_sinh1_ha_ex(double a);\n+   double __svml_sinh2_ha_ex(double a);\n+   float __svml_coshf4_ha_ex(float a);\n+   double __svml_cosh1_ha_ex(double a);\n+   double __svml_cosh2_ha_ex(double a);\n+   float __svml_tanhf4_ha_ex(float a);\n+   double __svml_tanh1_ha_ex(double a);\n+   double __svml_tanh2_ha_ex(double a);\n+   double __svml_log1_ha_ex(double a);\n+   double __svml_log2_ha_ex(double a);\n+   double __svml_log1p1_ha_ex(double a);\n+   double __svml_log1p2_ha_ex(double a);\n+   double __svml_log101_ha_ex(double a);\n+   double __svml_log102_ha_ex(double a);\n+   float __svml_logf4_ha_ex(float a);\n+   float __svml_log1pf4_ha_ex(float a);\n+   float __svml_log10f4_ha_ex(float a);\n+   double __svml_atan21_ha_ex(double a);\n+   double __svml_atan22_ha_ex(double a);\n+   float __svml_atan2f4_ha_ex(float a);\n+   float __svml_hypotf4_ha_ex(float a);\n+   double __svml_hypot1_ha_ex(double a);\n+   double __svml_hypot2_ha_ex(double a);\n+   double __svml_pow1_ha_ex(double a);\n+   double __svml_pow2_ha_ex(double a);\n+   float __svml_powf4_ha_ex(float a);\n+   double __svml_cbrt1_ha_ex(double a);\n+   double __svml_cbrt2_ha_ex(double a);\n+   float __svml_cbrtf4_ha_ex(float a);\n+}\n+#endif\n+\n@@ -6948,0 +7225,335 @@\n+#ifdef __VECTOR_API_MATH_INTRINSICS_COMMON\n+#ifdef __VECTOR_API_MATH_INTRINSICS_LINUX\n+    if (UseAVX > 2) {\n+      StubRoutines::_vector_exp_float512    = CAST_FROM_FN_PTR(address, __svml_expf16_ha_z0);\n+      StubRoutines::_vector_exp_double512   = CAST_FROM_FN_PTR(address, __svml_exp8_ha_z0);\n+      StubRoutines::_vector_expm1_float512  = CAST_FROM_FN_PTR(address, __svml_expm1f16_ha_z0);\n+      StubRoutines::_vector_expm1_double512 = CAST_FROM_FN_PTR(address, __svml_expm18_ha_z0);\n+      StubRoutines::_vector_log1p_float512  = CAST_FROM_FN_PTR(address, __svml_log1pf16_ha_z0);\n+      StubRoutines::_vector_log1p_double512 = CAST_FROM_FN_PTR(address, __svml_log1p8_ha_z0);\n+      StubRoutines::_vector_log_float512    = CAST_FROM_FN_PTR(address, __svml_logf16_ha_z0);\n+      StubRoutines::_vector_log_double512   = CAST_FROM_FN_PTR(address, __svml_log8_ha_z0);\n+      StubRoutines::_vector_log10_float512  = CAST_FROM_FN_PTR(address, __svml_log10f16_ha_z0);\n+      StubRoutines::_vector_log10_double512 = CAST_FROM_FN_PTR(address, __svml_log108_ha_z0);\n+      StubRoutines::_vector_sin_float512    = CAST_FROM_FN_PTR(address, __svml_sinf16_ha_z0);\n+      StubRoutines::_vector_sin_double512   = CAST_FROM_FN_PTR(address, __svml_sin8_ha_z0);\n+      StubRoutines::_vector_cos_float512    = CAST_FROM_FN_PTR(address, __svml_cosf16_ha_z0);\n+      StubRoutines::_vector_cos_double512   = CAST_FROM_FN_PTR(address, __svml_cos8_ha_z0);\n+      StubRoutines::_vector_tan_float512    = CAST_FROM_FN_PTR(address, __svml_tanf16_ha_z0);\n+      StubRoutines::_vector_tan_double512   = CAST_FROM_FN_PTR(address, __svml_tan8_ha_z0);\n+      StubRoutines::_vector_sinh_float512   = CAST_FROM_FN_PTR(address, __svml_sinhf16_ha_z0);\n+      StubRoutines::_vector_sinh_double512  = CAST_FROM_FN_PTR(address, __svml_sinh8_ha_z0);\n+      StubRoutines::_vector_cosh_float512   = CAST_FROM_FN_PTR(address, __svml_coshf16_ha_z0);\n+      StubRoutines::_vector_cosh_double512  = CAST_FROM_FN_PTR(address, __svml_cosh8_ha_z0);\n+      StubRoutines::_vector_tanh_float512   = CAST_FROM_FN_PTR(address, __svml_tanhf16_ha_z0);\n+      StubRoutines::_vector_tanh_double512  = CAST_FROM_FN_PTR(address, __svml_tanh8_ha_z0);\n+      StubRoutines::_vector_acos_float512   = CAST_FROM_FN_PTR(address, __svml_acosf16_ha_z0);\n+      StubRoutines::_vector_acos_double512  = CAST_FROM_FN_PTR(address, __svml_acos8_ha_z0);\n+      StubRoutines::_vector_asin_float512   = CAST_FROM_FN_PTR(address, __svml_asinf16_ha_z0);\n+      StubRoutines::_vector_asin_double512  = CAST_FROM_FN_PTR(address, __svml_asin8_ha_z0);\n+      StubRoutines::_vector_atan_float512   = CAST_FROM_FN_PTR(address, __svml_atanf16_ha_z0);\n+      StubRoutines::_vector_atan_double512  = CAST_FROM_FN_PTR(address, __svml_atan8_ha_z0);\n+      StubRoutines::_vector_pow_float512    = CAST_FROM_FN_PTR(address, __svml_powf16_ha_z0);\n+      StubRoutines::_vector_pow_double512   = CAST_FROM_FN_PTR(address, __svml_pow8_ha_z0);\n+      StubRoutines::_vector_hypot_float512  = CAST_FROM_FN_PTR(address, __svml_hypotf16_ha_z0);\n+      StubRoutines::_vector_hypot_double512 = CAST_FROM_FN_PTR(address, __svml_hypot8_ha_z0);\n+      StubRoutines::_vector_cbrt_float512   = CAST_FROM_FN_PTR(address, __svml_cbrtf16_ha_z0);\n+      StubRoutines::_vector_cbrt_double512  = CAST_FROM_FN_PTR(address, __svml_cbrt8_ha_z0);\n+      StubRoutines::_vector_atan2_float512  = CAST_FROM_FN_PTR(address, __svml_atan2f16_ha_z0);\n+      StubRoutines::_vector_atan2_double512 = CAST_FROM_FN_PTR(address, __svml_atan28_ha_z0);\n+    }\n+#endif\n+    if (UseAVX > 1) {\n+      StubRoutines::_vector_exp_float64     = CAST_FROM_FN_PTR(address, __svml_expf4_ha_l9);\n+      StubRoutines::_vector_exp_float128    = CAST_FROM_FN_PTR(address, __svml_expf4_ha_l9);\n+      StubRoutines::_vector_exp_float256    = CAST_FROM_FN_PTR(address, __svml_expf8_ha_l9);\n+      StubRoutines::_vector_exp_double64    = CAST_FROM_FN_PTR(address, __svml_exp1_ha_l9);\n+      StubRoutines::_vector_exp_double128   = CAST_FROM_FN_PTR(address, __svml_exp2_ha_l9);\n+      StubRoutines::_vector_exp_double256   = CAST_FROM_FN_PTR(address, __svml_exp4_ha_l9);\n+      StubRoutines::_vector_expm1_float64   = CAST_FROM_FN_PTR(address, __svml_expm1f4_ha_l9);\n+      StubRoutines::_vector_expm1_float128  = CAST_FROM_FN_PTR(address, __svml_expm1f4_ha_l9);\n+      StubRoutines::_vector_expm1_float256  = CAST_FROM_FN_PTR(address, __svml_expm1f8_ha_l9);\n+      StubRoutines::_vector_expm1_double64  = CAST_FROM_FN_PTR(address, __svml_expm11_ha_l9);\n+      StubRoutines::_vector_expm1_double128 = CAST_FROM_FN_PTR(address, __svml_expm12_ha_l9);\n+      StubRoutines::_vector_expm1_double256 = CAST_FROM_FN_PTR(address, __svml_expm14_ha_l9);\n+      StubRoutines::_vector_log1p_float64   = CAST_FROM_FN_PTR(address, __svml_log1pf4_ha_l9);\n+      StubRoutines::_vector_log1p_float128  = CAST_FROM_FN_PTR(address, __svml_log1pf4_ha_l9);\n+      StubRoutines::_vector_log1p_float256  = CAST_FROM_FN_PTR(address, __svml_log1pf8_ha_l9);\n+      StubRoutines::_vector_log1p_double64  = CAST_FROM_FN_PTR(address, __svml_log1p1_ha_l9);\n+      StubRoutines::_vector_log1p_double128 = CAST_FROM_FN_PTR(address, __svml_log1p2_ha_l9);\n+      StubRoutines::_vector_log1p_double256 = CAST_FROM_FN_PTR(address, __svml_log1p4_ha_l9);\n+      StubRoutines::_vector_log_float64     = CAST_FROM_FN_PTR(address, __svml_logf4_ha_l9);\n+      StubRoutines::_vector_log_float128    = CAST_FROM_FN_PTR(address, __svml_logf4_ha_l9);\n+      StubRoutines::_vector_log_float256    = CAST_FROM_FN_PTR(address, __svml_logf8_ha_l9);\n+      StubRoutines::_vector_log_double64    = CAST_FROM_FN_PTR(address, __svml_log1_ha_l9);\n+      StubRoutines::_vector_log_double128   = CAST_FROM_FN_PTR(address, __svml_log2_ha_l9);\n+      StubRoutines::_vector_log_double256   = CAST_FROM_FN_PTR(address, __svml_log4_ha_l9);\n+      StubRoutines::_vector_log10_float64   = CAST_FROM_FN_PTR(address, __svml_log10f4_ha_l9);\n+      StubRoutines::_vector_log10_float128  = CAST_FROM_FN_PTR(address, __svml_log10f4_ha_l9);\n+      StubRoutines::_vector_log10_float256  = CAST_FROM_FN_PTR(address, __svml_log10f8_ha_l9);\n+      StubRoutines::_vector_log10_double64  = CAST_FROM_FN_PTR(address, __svml_log101_ha_l9);\n+      StubRoutines::_vector_log10_double128 = CAST_FROM_FN_PTR(address, __svml_log102_ha_l9);\n+      StubRoutines::_vector_log10_double256 = CAST_FROM_FN_PTR(address, __svml_log104_ha_l9);\n+      StubRoutines::_vector_sin_float64     = CAST_FROM_FN_PTR(address, __svml_sinf4_ha_l9);\n+      StubRoutines::_vector_sin_float128    = CAST_FROM_FN_PTR(address, __svml_sinf4_ha_l9);\n+      StubRoutines::_vector_sin_float256    = CAST_FROM_FN_PTR(address, __svml_sinf8_ha_l9);\n+      StubRoutines::_vector_sin_double64    = CAST_FROM_FN_PTR(address, __svml_sin1_ha_l9);\n+      StubRoutines::_vector_sin_double128   = CAST_FROM_FN_PTR(address, __svml_sin2_ha_l9);\n+      StubRoutines::_vector_sin_double256   = CAST_FROM_FN_PTR(address, __svml_sin4_ha_l9);\n+      StubRoutines::_vector_cos_float64     = CAST_FROM_FN_PTR(address, __svml_cosf4_ha_l9);\n+      StubRoutines::_vector_cos_float128    = CAST_FROM_FN_PTR(address, __svml_cosf4_ha_l9);\n+      StubRoutines::_vector_cos_float256    = CAST_FROM_FN_PTR(address, __svml_cosf8_ha_l9);\n+      StubRoutines::_vector_cos_double64    = CAST_FROM_FN_PTR(address, __svml_cos1_ha_l9);\n+      StubRoutines::_vector_cos_double128   = CAST_FROM_FN_PTR(address, __svml_cos2_ha_l9);\n+      StubRoutines::_vector_cos_double256   = CAST_FROM_FN_PTR(address, __svml_cos4_ha_l9);\n+      StubRoutines::_vector_tan_float64     = CAST_FROM_FN_PTR(address, __svml_tanf4_ha_l9);\n+      StubRoutines::_vector_tan_float128    = CAST_FROM_FN_PTR(address, __svml_tanf4_ha_l9);\n+      StubRoutines::_vector_tan_float256    = CAST_FROM_FN_PTR(address, __svml_tanf8_ha_l9);\n+      StubRoutines::_vector_tan_double64    = CAST_FROM_FN_PTR(address, __svml_tan1_ha_l9);\n+      StubRoutines::_vector_tan_double128   = CAST_FROM_FN_PTR(address, __svml_tan2_ha_l9);\n+      StubRoutines::_vector_tan_double256   = CAST_FROM_FN_PTR(address, __svml_tan4_ha_l9);\n+      StubRoutines::_vector_sinh_float64    = CAST_FROM_FN_PTR(address, __svml_sinhf4_ha_l9);\n+      StubRoutines::_vector_sinh_float128   = CAST_FROM_FN_PTR(address, __svml_sinhf4_ha_l9);\n+      StubRoutines::_vector_sinh_float256   = CAST_FROM_FN_PTR(address, __svml_sinhf8_ha_l9);\n+      StubRoutines::_vector_sinh_double64   = CAST_FROM_FN_PTR(address, __svml_sinh1_ha_l9);\n+      StubRoutines::_vector_sinh_double128  = CAST_FROM_FN_PTR(address, __svml_sinh2_ha_l9);\n+      StubRoutines::_vector_sinh_double256  = CAST_FROM_FN_PTR(address, __svml_sinh4_ha_l9);\n+      StubRoutines::_vector_cosh_float64    = CAST_FROM_FN_PTR(address, __svml_coshf4_ha_l9);\n+      StubRoutines::_vector_cosh_float128   = CAST_FROM_FN_PTR(address, __svml_coshf4_ha_l9);\n+      StubRoutines::_vector_cosh_float256   = CAST_FROM_FN_PTR(address, __svml_coshf8_ha_l9);\n+      StubRoutines::_vector_cosh_double64   = CAST_FROM_FN_PTR(address, __svml_cosh1_ha_l9);\n+      StubRoutines::_vector_cosh_double128  = CAST_FROM_FN_PTR(address, __svml_cosh2_ha_l9);\n+      StubRoutines::_vector_cosh_double256  = CAST_FROM_FN_PTR(address, __svml_cosh4_ha_l9);\n+      StubRoutines::_vector_tanh_float64    = CAST_FROM_FN_PTR(address, __svml_tanhf4_ha_l9);\n+      StubRoutines::_vector_tanh_float128   = CAST_FROM_FN_PTR(address, __svml_tanhf4_ha_l9);\n+      StubRoutines::_vector_tanh_float256   = CAST_FROM_FN_PTR(address, __svml_tanhf8_ha_l9);\n+      StubRoutines::_vector_tanh_double64   = CAST_FROM_FN_PTR(address, __svml_tanh1_ha_l9);\n+      StubRoutines::_vector_tanh_double128  = CAST_FROM_FN_PTR(address, __svml_tanh2_ha_l9);\n+      StubRoutines::_vector_tanh_double256  = CAST_FROM_FN_PTR(address, __svml_tanh4_ha_l9);\n+      StubRoutines::_vector_acos_float64    = CAST_FROM_FN_PTR(address, __svml_acosf4_ha_l9);\n+      StubRoutines::_vector_acos_float128   = CAST_FROM_FN_PTR(address, __svml_acosf4_ha_l9);\n+      StubRoutines::_vector_acos_float256   = CAST_FROM_FN_PTR(address, __svml_acosf8_ha_l9);\n+      StubRoutines::_vector_acos_double64   = CAST_FROM_FN_PTR(address, __svml_acos1_ha_l9);\n+      StubRoutines::_vector_acos_double128  = CAST_FROM_FN_PTR(address, __svml_acos2_ha_l9);\n+      StubRoutines::_vector_acos_double256  = CAST_FROM_FN_PTR(address, __svml_acos4_ha_l9);\n+      StubRoutines::_vector_asin_float64    = CAST_FROM_FN_PTR(address, __svml_asinf4_ha_l9);\n+      StubRoutines::_vector_asin_float128   = CAST_FROM_FN_PTR(address, __svml_asinf4_ha_l9);\n+      StubRoutines::_vector_asin_float256   = CAST_FROM_FN_PTR(address, __svml_asinf8_ha_l9);\n+      StubRoutines::_vector_asin_double64   = CAST_FROM_FN_PTR(address, __svml_asin1_ha_l9);\n+      StubRoutines::_vector_asin_double128  = CAST_FROM_FN_PTR(address, __svml_asin2_ha_l9);\n+      StubRoutines::_vector_asin_double256  = CAST_FROM_FN_PTR(address, __svml_asin4_ha_l9);\n+      StubRoutines::_vector_atan_float64    = CAST_FROM_FN_PTR(address, __svml_atanf4_ha_l9);\n+      StubRoutines::_vector_atan_float128   = CAST_FROM_FN_PTR(address, __svml_atanf4_ha_l9);\n+      StubRoutines::_vector_atan_float256   = CAST_FROM_FN_PTR(address, __svml_atanf8_ha_l9);\n+      StubRoutines::_vector_atan_double64   = CAST_FROM_FN_PTR(address, __svml_atan1_ha_l9);\n+      StubRoutines::_vector_atan_double128  = CAST_FROM_FN_PTR(address, __svml_atan2_ha_l9);\n+      StubRoutines::_vector_atan_double256  = CAST_FROM_FN_PTR(address, __svml_atan4_ha_l9);\n+      StubRoutines::_vector_pow_float64     = CAST_FROM_FN_PTR(address, __svml_powf4_ha_l9);\n+      StubRoutines::_vector_pow_float128    = CAST_FROM_FN_PTR(address, __svml_powf4_ha_l9);\n+      StubRoutines::_vector_pow_float256    = CAST_FROM_FN_PTR(address, __svml_powf8_ha_l9);\n+      StubRoutines::_vector_pow_double64    = CAST_FROM_FN_PTR(address, __svml_pow1_ha_l9);\n+      StubRoutines::_vector_pow_double128   = CAST_FROM_FN_PTR(address, __svml_pow2_ha_l9);\n+      StubRoutines::_vector_pow_double256   = CAST_FROM_FN_PTR(address, __svml_pow4_ha_l9);\n+      StubRoutines::_vector_hypot_float64   = CAST_FROM_FN_PTR(address, __svml_hypotf4_ha_l9);\n+      StubRoutines::_vector_hypot_float128  = CAST_FROM_FN_PTR(address, __svml_hypotf4_ha_l9);\n+      StubRoutines::_vector_hypot_float256  = CAST_FROM_FN_PTR(address, __svml_hypotf8_ha_l9);\n+      StubRoutines::_vector_hypot_double64  = CAST_FROM_FN_PTR(address, __svml_hypot1_ha_l9);\n+      StubRoutines::_vector_hypot_double128 = CAST_FROM_FN_PTR(address, __svml_hypot2_ha_l9);\n+      StubRoutines::_vector_hypot_double256 = CAST_FROM_FN_PTR(address, __svml_hypot4_ha_l9);\n+      StubRoutines::_vector_cbrt_float64    = CAST_FROM_FN_PTR(address, __svml_cbrtf4_ha_l9);\n+      StubRoutines::_vector_cbrt_float128   = CAST_FROM_FN_PTR(address, __svml_cbrtf4_ha_l9);\n+      StubRoutines::_vector_cbrt_float256   = CAST_FROM_FN_PTR(address, __svml_cbrtf8_ha_l9);\n+      StubRoutines::_vector_cbrt_double64   = CAST_FROM_FN_PTR(address, __svml_cbrt1_ha_l9);\n+      StubRoutines::_vector_cbrt_double128  = CAST_FROM_FN_PTR(address, __svml_cbrt2_ha_l9);\n+      StubRoutines::_vector_cbrt_double256  = CAST_FROM_FN_PTR(address, __svml_cbrt4_ha_l9);\n+      StubRoutines::_vector_atan2_float64   = CAST_FROM_FN_PTR(address, __svml_atan2f4_ha_l9);\n+      StubRoutines::_vector_atan2_float128  = CAST_FROM_FN_PTR(address, __svml_atan2f4_ha_l9);\n+      StubRoutines::_vector_atan2_float256  = CAST_FROM_FN_PTR(address, __svml_atan2f8_ha_l9);\n+      StubRoutines::_vector_atan2_double64  = CAST_FROM_FN_PTR(address, __svml_atan21_ha_l9);\n+      StubRoutines::_vector_atan2_double128 = CAST_FROM_FN_PTR(address, __svml_atan22_ha_l9);\n+      StubRoutines::_vector_atan2_double256 = CAST_FROM_FN_PTR(address, __svml_atan24_ha_l9);\n+    } else if (UseAVX > 0) {\n+      StubRoutines::_vector_exp_float64     = CAST_FROM_FN_PTR(address, __svml_expf4_ha_e9);\n+      StubRoutines::_vector_exp_float128    = CAST_FROM_FN_PTR(address, __svml_expf4_ha_e9);\n+      StubRoutines::_vector_exp_float256    = CAST_FROM_FN_PTR(address, __svml_expf8_ha_e9);\n+      StubRoutines::_vector_exp_double64    = CAST_FROM_FN_PTR(address, __svml_exp1_ha_e9);\n+      StubRoutines::_vector_exp_double128   = CAST_FROM_FN_PTR(address, __svml_exp2_ha_e9);\n+      StubRoutines::_vector_exp_double256   = CAST_FROM_FN_PTR(address, __svml_exp4_ha_e9);\n+      StubRoutines::_vector_expm1_float64   = CAST_FROM_FN_PTR(address, __svml_expm1f4_ha_e9);\n+      StubRoutines::_vector_expm1_float128  = CAST_FROM_FN_PTR(address, __svml_expm1f4_ha_e9);\n+      StubRoutines::_vector_expm1_float256  = CAST_FROM_FN_PTR(address, __svml_expm1f8_ha_e9);\n+      StubRoutines::_vector_expm1_double64  = CAST_FROM_FN_PTR(address, __svml_expm11_ha_e9);\n+      StubRoutines::_vector_expm1_double128 = CAST_FROM_FN_PTR(address, __svml_expm12_ha_e9);\n+      StubRoutines::_vector_expm1_double256 = CAST_FROM_FN_PTR(address, __svml_expm14_ha_e9);\n+      StubRoutines::_vector_log1p_float64   = CAST_FROM_FN_PTR(address, __svml_log1pf4_ha_e9);\n+      StubRoutines::_vector_log1p_float128  = CAST_FROM_FN_PTR(address, __svml_log1pf4_ha_e9);\n+      StubRoutines::_vector_log1p_float256  = CAST_FROM_FN_PTR(address, __svml_log1pf8_ha_e9);\n+      StubRoutines::_vector_log1p_double64  = CAST_FROM_FN_PTR(address, __svml_log1p1_ha_e9);\n+      StubRoutines::_vector_log1p_double128 = CAST_FROM_FN_PTR(address, __svml_log1p2_ha_e9);\n+      StubRoutines::_vector_log1p_double256 = CAST_FROM_FN_PTR(address, __svml_log1p4_ha_e9);\n+      StubRoutines::_vector_log_float64     = CAST_FROM_FN_PTR(address, __svml_logf4_ha_e9);\n+      StubRoutines::_vector_log_float128    = CAST_FROM_FN_PTR(address, __svml_logf4_ha_e9);\n+      StubRoutines::_vector_log_float256    = CAST_FROM_FN_PTR(address, __svml_logf8_ha_e9);\n+      StubRoutines::_vector_log_double64    = CAST_FROM_FN_PTR(address, __svml_log1_ha_e9);\n+      StubRoutines::_vector_log_double128   = CAST_FROM_FN_PTR(address, __svml_log2_ha_e9);\n+      StubRoutines::_vector_log_double256   = CAST_FROM_FN_PTR(address, __svml_log4_ha_e9);\n+      StubRoutines::_vector_log10_float64   = CAST_FROM_FN_PTR(address, __svml_log10f4_ha_e9);\n+      StubRoutines::_vector_log10_float128  = CAST_FROM_FN_PTR(address, __svml_log10f4_ha_e9);\n+      StubRoutines::_vector_log10_float256  = CAST_FROM_FN_PTR(address, __svml_log10f8_ha_e9);\n+      StubRoutines::_vector_log10_double64  = CAST_FROM_FN_PTR(address, __svml_log101_ha_e9);\n+      StubRoutines::_vector_log10_double128 = CAST_FROM_FN_PTR(address, __svml_log102_ha_e9);\n+      StubRoutines::_vector_log10_double256 = CAST_FROM_FN_PTR(address, __svml_log104_ha_e9);\n+      StubRoutines::_vector_sin_float64     = CAST_FROM_FN_PTR(address, __svml_sinf4_ha_e9);\n+      StubRoutines::_vector_sin_float128    = CAST_FROM_FN_PTR(address, __svml_sinf4_ha_e9);\n+      StubRoutines::_vector_sin_float256    = CAST_FROM_FN_PTR(address, __svml_sinf8_ha_e9);\n+      StubRoutines::_vector_sin_double64    = CAST_FROM_FN_PTR(address, __svml_sin1_ha_e9);\n+      StubRoutines::_vector_sin_double128   = CAST_FROM_FN_PTR(address, __svml_sin2_ha_e9);\n+      StubRoutines::_vector_sin_double256   = CAST_FROM_FN_PTR(address, __svml_sin4_ha_e9);\n+      StubRoutines::_vector_cos_float64     = CAST_FROM_FN_PTR(address, __svml_cosf4_ha_e9);\n+      StubRoutines::_vector_cos_float128    = CAST_FROM_FN_PTR(address, __svml_cosf4_ha_e9);\n+      StubRoutines::_vector_cos_float256    = CAST_FROM_FN_PTR(address, __svml_cosf8_ha_e9);\n+      StubRoutines::_vector_cos_double64    = CAST_FROM_FN_PTR(address, __svml_cos1_ha_e9);\n+      StubRoutines::_vector_cos_double128   = CAST_FROM_FN_PTR(address, __svml_cos2_ha_e9);\n+      StubRoutines::_vector_cos_double256   = CAST_FROM_FN_PTR(address, __svml_cos4_ha_e9);\n+      StubRoutines::_vector_tan_float64     = CAST_FROM_FN_PTR(address, __svml_tanf4_ha_e9);\n+      StubRoutines::_vector_tan_float128    = CAST_FROM_FN_PTR(address, __svml_tanf4_ha_e9);\n+      StubRoutines::_vector_tan_float256    = CAST_FROM_FN_PTR(address, __svml_tanf8_ha_e9);\n+      StubRoutines::_vector_tan_double64    = CAST_FROM_FN_PTR(address, __svml_tan1_ha_e9);\n+      StubRoutines::_vector_tan_double128   = CAST_FROM_FN_PTR(address, __svml_tan2_ha_e9);\n+      StubRoutines::_vector_tan_double256   = CAST_FROM_FN_PTR(address, __svml_tan4_ha_e9);\n+      StubRoutines::_vector_sinh_float64    = CAST_FROM_FN_PTR(address, __svml_sinhf4_ha_e9);\n+      StubRoutines::_vector_sinh_float128   = CAST_FROM_FN_PTR(address, __svml_sinhf4_ha_e9);\n+      StubRoutines::_vector_sinh_float256   = CAST_FROM_FN_PTR(address, __svml_sinhf8_ha_e9);\n+      StubRoutines::_vector_sinh_double64   = CAST_FROM_FN_PTR(address, __svml_sinh1_ha_e9);\n+      StubRoutines::_vector_sinh_double128  = CAST_FROM_FN_PTR(address, __svml_sinh2_ha_e9);\n+      StubRoutines::_vector_sinh_double256  = CAST_FROM_FN_PTR(address, __svml_sinh4_ha_e9);\n+      StubRoutines::_vector_cosh_float64    = CAST_FROM_FN_PTR(address, __svml_coshf4_ha_e9);\n+      StubRoutines::_vector_cosh_float128   = CAST_FROM_FN_PTR(address, __svml_coshf4_ha_e9);\n+      StubRoutines::_vector_cosh_float256   = CAST_FROM_FN_PTR(address, __svml_coshf8_ha_e9);\n+      StubRoutines::_vector_cosh_double64   = CAST_FROM_FN_PTR(address, __svml_cosh1_ha_e9);\n+      StubRoutines::_vector_cosh_double128  = CAST_FROM_FN_PTR(address, __svml_cosh2_ha_e9);\n+      StubRoutines::_vector_cosh_double256  = CAST_FROM_FN_PTR(address, __svml_cosh4_ha_e9);\n+      StubRoutines::_vector_tanh_float64    = CAST_FROM_FN_PTR(address, __svml_tanhf4_ha_e9);\n+      StubRoutines::_vector_tanh_float128   = CAST_FROM_FN_PTR(address, __svml_tanhf4_ha_e9);\n+      StubRoutines::_vector_tanh_float256   = CAST_FROM_FN_PTR(address, __svml_tanhf8_ha_e9);\n+      StubRoutines::_vector_tanh_double64   = CAST_FROM_FN_PTR(address, __svml_tanh1_ha_e9);\n+      StubRoutines::_vector_tanh_double128  = CAST_FROM_FN_PTR(address, __svml_tanh2_ha_e9);\n+      StubRoutines::_vector_tanh_double256  = CAST_FROM_FN_PTR(address, __svml_tanh4_ha_e9);\n+      StubRoutines::_vector_acos_float64    = CAST_FROM_FN_PTR(address, __svml_acosf4_ha_e9);\n+      StubRoutines::_vector_acos_float128   = CAST_FROM_FN_PTR(address, __svml_acosf4_ha_e9);\n+      StubRoutines::_vector_acos_float256   = CAST_FROM_FN_PTR(address, __svml_acosf8_ha_e9);\n+      StubRoutines::_vector_acos_double64   = CAST_FROM_FN_PTR(address, __svml_acos1_ha_e9);\n+      StubRoutines::_vector_acos_double128  = CAST_FROM_FN_PTR(address, __svml_acos2_ha_e9);\n+      StubRoutines::_vector_acos_double256  = CAST_FROM_FN_PTR(address, __svml_acos4_ha_e9);\n+      StubRoutines::_vector_asin_float64    = CAST_FROM_FN_PTR(address, __svml_asinf4_ha_e9);\n+      StubRoutines::_vector_asin_float128   = CAST_FROM_FN_PTR(address, __svml_asinf4_ha_e9);\n+      StubRoutines::_vector_asin_float256   = CAST_FROM_FN_PTR(address, __svml_asinf8_ha_e9);\n+      StubRoutines::_vector_asin_double64   = CAST_FROM_FN_PTR(address, __svml_asin1_ha_e9);\n+      StubRoutines::_vector_asin_double128  = CAST_FROM_FN_PTR(address, __svml_asin2_ha_e9);\n+      StubRoutines::_vector_asin_double256  = CAST_FROM_FN_PTR(address, __svml_asin4_ha_e9);\n+      StubRoutines::_vector_atan_float64    = CAST_FROM_FN_PTR(address, __svml_atanf4_ha_e9);\n+      StubRoutines::_vector_atan_float128   = CAST_FROM_FN_PTR(address, __svml_atanf4_ha_e9);\n+      StubRoutines::_vector_atan_float256   = CAST_FROM_FN_PTR(address, __svml_atanf8_ha_e9);\n+      StubRoutines::_vector_atan_double64   = CAST_FROM_FN_PTR(address, __svml_atan1_ha_e9);\n+      StubRoutines::_vector_atan_double128  = CAST_FROM_FN_PTR(address, __svml_atan2_ha_e9);\n+      StubRoutines::_vector_atan_double256  = CAST_FROM_FN_PTR(address, __svml_atan4_ha_e9);\n+      StubRoutines::_vector_pow_float64     = CAST_FROM_FN_PTR(address, __svml_powf4_ha_e9);\n+      StubRoutines::_vector_pow_float128    = CAST_FROM_FN_PTR(address, __svml_powf4_ha_e9);\n+      StubRoutines::_vector_pow_float256    = CAST_FROM_FN_PTR(address, __svml_powf8_ha_e9);\n+      StubRoutines::_vector_pow_double64    = CAST_FROM_FN_PTR(address, __svml_pow1_ha_e9);\n+      StubRoutines::_vector_pow_double128   = CAST_FROM_FN_PTR(address, __svml_pow2_ha_e9);\n+      StubRoutines::_vector_pow_double256   = CAST_FROM_FN_PTR(address, __svml_pow4_ha_e9);\n+      StubRoutines::_vector_hypot_float64   = CAST_FROM_FN_PTR(address, __svml_hypotf4_ha_e9);\n+      StubRoutines::_vector_hypot_float128  = CAST_FROM_FN_PTR(address, __svml_hypotf4_ha_e9);\n+      StubRoutines::_vector_hypot_float256  = CAST_FROM_FN_PTR(address, __svml_hypotf8_ha_e9);\n+      StubRoutines::_vector_hypot_double64  = CAST_FROM_FN_PTR(address, __svml_hypot1_ha_e9);\n+      StubRoutines::_vector_hypot_double128 = CAST_FROM_FN_PTR(address, __svml_hypot2_ha_e9);\n+      StubRoutines::_vector_hypot_double256 = CAST_FROM_FN_PTR(address, __svml_hypot4_ha_e9);\n+      StubRoutines::_vector_cbrt_float64    = CAST_FROM_FN_PTR(address, __svml_cbrtf4_ha_e9);\n+      StubRoutines::_vector_cbrt_float128   = CAST_FROM_FN_PTR(address, __svml_cbrtf4_ha_e9);\n+      StubRoutines::_vector_cbrt_float256   = CAST_FROM_FN_PTR(address, __svml_cbrtf8_ha_e9);\n+      StubRoutines::_vector_cbrt_double64   = CAST_FROM_FN_PTR(address, __svml_cbrt1_ha_e9);\n+      StubRoutines::_vector_cbrt_double128  = CAST_FROM_FN_PTR(address, __svml_cbrt2_ha_e9);\n+      StubRoutines::_vector_cbrt_double256  = CAST_FROM_FN_PTR(address, __svml_cbrt4_ha_e9);\n+      StubRoutines::_vector_atan2_float64   = CAST_FROM_FN_PTR(address, __svml_atan2f4_ha_e9);\n+      StubRoutines::_vector_atan2_float128  = CAST_FROM_FN_PTR(address, __svml_atan2f4_ha_e9);\n+      StubRoutines::_vector_atan2_float256  = CAST_FROM_FN_PTR(address, __svml_atan2f8_ha_e9);\n+      StubRoutines::_vector_atan2_double64  = CAST_FROM_FN_PTR(address, __svml_atan21_ha_e9);\n+      StubRoutines::_vector_atan2_double128 = CAST_FROM_FN_PTR(address, __svml_atan22_ha_e9);\n+      StubRoutines::_vector_atan2_double256 = CAST_FROM_FN_PTR(address, __svml_atan24_ha_e9);\n+    } else {\n+      assert(UseAVX == 0 && UseSSE >= 2, \"\");\n+      StubRoutines::_vector_exp_float64     = CAST_FROM_FN_PTR(address, __svml_expf4_ha_ex);\n+      StubRoutines::_vector_exp_float128    = CAST_FROM_FN_PTR(address, __svml_expf4_ha_ex);\n+      StubRoutines::_vector_exp_double64    = CAST_FROM_FN_PTR(address, __svml_exp1_ha_ex);\n+      StubRoutines::_vector_exp_double128   = CAST_FROM_FN_PTR(address, __svml_exp2_ha_ex);\n+      StubRoutines::_vector_expm1_float64   = CAST_FROM_FN_PTR(address, __svml_expm1f4_ha_ex);\n+      StubRoutines::_vector_expm1_float128  = CAST_FROM_FN_PTR(address, __svml_expm1f4_ha_ex);\n+      StubRoutines::_vector_expm1_double64  = CAST_FROM_FN_PTR(address, __svml_expm11_ha_ex);\n+      StubRoutines::_vector_expm1_double128 = CAST_FROM_FN_PTR(address, __svml_expm12_ha_ex);\n+      StubRoutines::_vector_acos_float64    = CAST_FROM_FN_PTR(address, __svml_acosf4_ha_ex);\n+      StubRoutines::_vector_acos_float128   = CAST_FROM_FN_PTR(address, __svml_acosf4_ha_ex);\n+      StubRoutines::_vector_acos_double64   = CAST_FROM_FN_PTR(address, __svml_acos1_ha_ex);\n+      StubRoutines::_vector_acos_double128  = CAST_FROM_FN_PTR(address, __svml_acos2_ha_ex);\n+      StubRoutines::_vector_asin_float64    = CAST_FROM_FN_PTR(address, __svml_asinf4_ha_ex);\n+      StubRoutines::_vector_asin_float128   = CAST_FROM_FN_PTR(address, __svml_asinf4_ha_ex);\n+      StubRoutines::_vector_asin_double64   = CAST_FROM_FN_PTR(address, __svml_asin1_ha_ex);\n+      StubRoutines::_vector_asin_double128  = CAST_FROM_FN_PTR(address, __svml_asin2_ha_ex);\n+      StubRoutines::_vector_atan_float64    = CAST_FROM_FN_PTR(address, __svml_atanf4_ha_ex);\n+      StubRoutines::_vector_atan_float128   = CAST_FROM_FN_PTR(address, __svml_atanf4_ha_ex);\n+      StubRoutines::_vector_atan_double64   = CAST_FROM_FN_PTR(address, __svml_atan1_ha_ex);\n+      StubRoutines::_vector_atan_double128  = CAST_FROM_FN_PTR(address, __svml_atan2_ha_ex);\n+      StubRoutines::_vector_sin_float64     = CAST_FROM_FN_PTR(address, __svml_sinf4_ha_ex);\n+      StubRoutines::_vector_sin_float128    = CAST_FROM_FN_PTR(address, __svml_sinf4_ha_ex);\n+      StubRoutines::_vector_sin_double64    = CAST_FROM_FN_PTR(address, __svml_sin1_ha_ex);\n+      StubRoutines::_vector_sin_double128   = CAST_FROM_FN_PTR(address, __svml_sin2_ha_ex);\n+      StubRoutines::_vector_cos_float64     = CAST_FROM_FN_PTR(address, __svml_cosf4_ha_ex);\n+      StubRoutines::_vector_cos_float128    = CAST_FROM_FN_PTR(address, __svml_cosf4_ha_ex);\n+      StubRoutines::_vector_cos_double64    = CAST_FROM_FN_PTR(address, __svml_cos1_ha_ex);\n+      StubRoutines::_vector_cos_double128   = CAST_FROM_FN_PTR(address, __svml_cos2_ha_ex);\n+      StubRoutines::_vector_tan_float64     = CAST_FROM_FN_PTR(address, __svml_tanf4_ha_ex);\n+      StubRoutines::_vector_tan_float128    = CAST_FROM_FN_PTR(address, __svml_tanf4_ha_ex);\n+      StubRoutines::_vector_tan_double64    = CAST_FROM_FN_PTR(address, __svml_tan1_ha_ex);\n+      StubRoutines::_vector_tan_double128   = CAST_FROM_FN_PTR(address, __svml_tan2_ha_ex);\n+      StubRoutines::_vector_sinh_float64    = CAST_FROM_FN_PTR(address, __svml_sinhf4_ha_ex);\n+      StubRoutines::_vector_sinh_float128   = CAST_FROM_FN_PTR(address, __svml_sinhf4_ha_ex);\n+      StubRoutines::_vector_sinh_double64   = CAST_FROM_FN_PTR(address, __svml_sinh1_ha_ex);\n+      StubRoutines::_vector_sinh_double128  = CAST_FROM_FN_PTR(address, __svml_sinh2_ha_ex);\n+      StubRoutines::_vector_cosh_float64    = CAST_FROM_FN_PTR(address, __svml_coshf4_ha_ex);\n+      StubRoutines::_vector_cosh_float128   = CAST_FROM_FN_PTR(address, __svml_coshf4_ha_ex);\n+      StubRoutines::_vector_cosh_double64   = CAST_FROM_FN_PTR(address, __svml_cosh1_ha_ex);\n+      StubRoutines::_vector_cosh_double128  = CAST_FROM_FN_PTR(address, __svml_cosh2_ha_ex);\n+      StubRoutines::_vector_tanh_float64    = CAST_FROM_FN_PTR(address, __svml_tanhf4_ha_ex);\n+      StubRoutines::_vector_tanh_float128   = CAST_FROM_FN_PTR(address, __svml_tanhf4_ha_ex);\n+      StubRoutines::_vector_tanh_double64   = CAST_FROM_FN_PTR(address, __svml_tanh1_ha_ex);\n+      StubRoutines::_vector_tanh_double128  = CAST_FROM_FN_PTR(address, __svml_tanh2_ha_ex);\n+      StubRoutines::_vector_log_float64     = CAST_FROM_FN_PTR(address, __svml_logf4_ha_ex);\n+      StubRoutines::_vector_log_float128    = CAST_FROM_FN_PTR(address, __svml_logf4_ha_ex);\n+      StubRoutines::_vector_log_double64    = CAST_FROM_FN_PTR(address, __svml_log1_ha_ex);\n+      StubRoutines::_vector_log_double128   = CAST_FROM_FN_PTR(address, __svml_log2_ha_ex);\n+      StubRoutines::_vector_log10_float64   = CAST_FROM_FN_PTR(address, __svml_log10f4_ha_ex);\n+      StubRoutines::_vector_log10_float128  = CAST_FROM_FN_PTR(address, __svml_log10f4_ha_ex);\n+      StubRoutines::_vector_log10_double64  = CAST_FROM_FN_PTR(address, __svml_log101_ha_ex);\n+      StubRoutines::_vector_log10_double128 = CAST_FROM_FN_PTR(address, __svml_log102_ha_ex);\n+      StubRoutines::_vector_log1p_float64   = CAST_FROM_FN_PTR(address, __svml_log1pf4_ha_ex);\n+      StubRoutines::_vector_log1p_float128  = CAST_FROM_FN_PTR(address, __svml_log1pf4_ha_ex);\n+      StubRoutines::_vector_log1p_double64  = CAST_FROM_FN_PTR(address, __svml_log1p1_ha_ex);\n+      StubRoutines::_vector_log1p_double128 = CAST_FROM_FN_PTR(address, __svml_log1p2_ha_ex);\n+      StubRoutines::_vector_atan2_float64   = CAST_FROM_FN_PTR(address, __svml_atan2f4_ha_ex);\n+      StubRoutines::_vector_atan2_float128  = CAST_FROM_FN_PTR(address, __svml_atan2f4_ha_ex);\n+      StubRoutines::_vector_atan2_double64  = CAST_FROM_FN_PTR(address, __svml_atan21_ha_ex);\n+      StubRoutines::_vector_atan2_double128 = CAST_FROM_FN_PTR(address, __svml_atan22_ha_ex);\n+      StubRoutines::_vector_hypot_float64   = CAST_FROM_FN_PTR(address, __svml_hypotf4_ha_ex);\n+      StubRoutines::_vector_hypot_float128  = CAST_FROM_FN_PTR(address, __svml_hypotf4_ha_ex);\n+      StubRoutines::_vector_hypot_double64  = CAST_FROM_FN_PTR(address, __svml_hypot1_ha_ex);\n+      StubRoutines::_vector_hypot_double128 = CAST_FROM_FN_PTR(address, __svml_hypot2_ha_ex);\n+      StubRoutines::_vector_pow_float64     = CAST_FROM_FN_PTR(address, __svml_powf4_ha_ex);\n+      StubRoutines::_vector_pow_float128    = CAST_FROM_FN_PTR(address, __svml_powf4_ha_ex);\n+      StubRoutines::_vector_pow_double64    = CAST_FROM_FN_PTR(address, __svml_pow1_ha_ex);\n+      StubRoutines::_vector_pow_double128   = CAST_FROM_FN_PTR(address, __svml_pow2_ha_ex);\n+      StubRoutines::_vector_cbrt_float64    = CAST_FROM_FN_PTR(address, __svml_cbrtf4_ha_ex);\n+      StubRoutines::_vector_cbrt_float128   = CAST_FROM_FN_PTR(address, __svml_cbrtf4_ha_ex);\n+      StubRoutines::_vector_cbrt_double64   = CAST_FROM_FN_PTR(address, __svml_cbrt1_ha_ex);\n+      StubRoutines::_vector_cbrt_double128  = CAST_FROM_FN_PTR(address, __svml_cbrt2_ha_ex);\n+    }\n+#endif \/\/ __VECTOR_API_MATH_INTRINSICS_COMMON\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":612,"deletions":0,"binary":false,"changes":612,"status":"modified"},{"patch":"@@ -1634,0 +1634,5 @@\n+    case Op_CallLeafVector:\n+      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n+        return false;\n+      }\n+      break;\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1381,0 +1381,14 @@\n+\/\/ Vector calling convention not supported.\n+const bool Matcher::supports_vector_calling_convention() {\n+  return false;\n+}\n+\n+void Matcher::vector_calling_convention(VMRegPair *regs, uint num_bits, uint total_args_passed) {\n+  (void) SharedRuntime::vector_calling_convention(regs, num_bits, total_args_passed);\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  Unimplemented();\n+  return OptoRegPair(0, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1588,0 +1588,17 @@\n+const bool Matcher::supports_vector_calling_convention(void) {\n+  return true;\n+}\n+\n+void Matcher::vector_calling_convention(VMRegPair *regs, uint num_bits, uint total_args_passed) {\n+  (void) SharedRuntime::vector_calling_convention(regs, num_bits, total_args_passed);\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  int lo = XMM0_num;\n+  int hi = XMM0b_num;\n+  if (ideal_reg == Op_VecX) hi = XMM0d_num;\n+  else if (ideal_reg == Op_VecY) hi = XMM0h_num;\n+  else if (ideal_reg == Op_VecZ) hi = XMM0p_num;\n+  return OptoRegPair(hi, lo);\n+}\n+\n@@ -12408,0 +12425,12 @@\n+\/\/ Call runtime without safepoint and with vector arguments\n+instruct CallLeafDirectVector(method meth)\n+%{\n+  match(CallLeafVector);\n+  effect(USE meth);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf,vector \" %}\n+  ins_encode(Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -422,0 +422,2 @@\n+  if(_matrule->find_type(\"CallLeafVector\",idx))   return Form::JAVA_LEAF;\n+  idx = 0;\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -716,0 +716,3 @@\n+  product(bool, IncrementalInlineVirtual, true, DIAGNOSTIC,                 \\\n+          \"do post parse inlining of virtual calls\")                        \\\n+                                                                            \\\n@@ -756,0 +759,3 @@\n+  product(bool, UseVectorStubs, false, EXPERIMENTAL,                        \\\n+          \"Use stubs for vector transcendental operations\")                 \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -124,0 +124,3 @@\n+protected:\n+  void set_call_node(CallStaticJavaNode* call) { _call_node = call; }\n+\n@@ -132,1 +135,8 @@\n-  CallStaticJavaNode* call_node() const { return _call_node; }\n+  virtual bool is_direct() const { return true; }\n+\n+  virtual CallNode* call_node() const { return _call_node; }\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    DirectCallGenerator* dcg = new DirectCallGenerator(method(), _separate_io_proj);\n+    dcg->set_call_node(call->as_CallStaticJava());\n+    return dcg;\n+  }\n@@ -182,0 +192,6 @@\n+  bool _separate_io_proj;\n+  CallDynamicJavaNode* _call_node;\n+\n+protected:\n+  void set_call_node(CallDynamicJavaNode* call) { _call_node = call; }\n+\n@@ -183,2 +199,2 @@\n-  VirtualCallGenerator(ciMethod* method, int vtable_index)\n-    : CallGenerator(method), _vtable_index(vtable_index)\n+  VirtualCallGenerator(ciMethod* method, int vtable_index, bool separate_io_proj)\n+    : CallGenerator(method), _vtable_index(vtable_index), _separate_io_proj(separate_io_proj), _call_node(NULL)\n@@ -191,0 +207,9 @@\n+\n+  virtual CallNode* call_node() const { return _call_node; }\n+  int vtable_index() const { return _vtable_index; }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    VirtualCallGenerator* cg = new VirtualCallGenerator(method(), _vtable_index, _separate_io_proj);\n+    cg->set_call_node(call->as_CallDynamicJava());\n+    return cg;\n+  }\n@@ -253,0 +278,2 @@\n+  _call_node = call;  \/\/ Save the call node in case we need it later\n+\n@@ -254,2 +281,2 @@\n-  kit.set_edges_for_java_call(call);\n-  Node* ret = kit.set_results_for_java_call(call);\n+  kit.set_edges_for_java_call(call, false \/*must_throw*\/, _separate_io_proj);\n+  Node* ret = kit.set_results_for_java_call(call, _separate_io_proj);\n@@ -288,1 +315,1 @@\n-  return new VirtualCallGenerator(m, vtable_index);\n+  return new VirtualCallGenerator(m, vtable_index, false \/*separate_io_projs*\/);\n@@ -344,0 +371,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineCallGenerator* cg = new LateInlineCallGenerator(method(), _inline_cg, _is_pure_call);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -348,1 +381,1 @@\n-  CallStaticJavaNode* call = call_node();\n+  CallStaticJavaNode* call = call_node()->as_CallStaticJava();\n@@ -499,0 +532,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineMHCallGenerator* cg = new LateInlineMHCallGenerator(_caller, method(), _input_not_const);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -528,0 +567,230 @@\n+\/\/ Allow inlining decisions to be delayed\n+class LateInlineVirtualCallGenerator : public VirtualCallGenerator {\n+ private:\n+  jlong _unique_id;   \/\/ unique id for log compilation\n+  CallGenerator* _inline_cg;\n+  ciMethod* _callee;\n+  bool _is_pure_call;\n+  float _prof_factor;\n+ protected:\n+  virtual bool do_late_inline_check(Compile* C, JVMState* jvms);\n+\n+ public:\n+  LateInlineVirtualCallGenerator(ciMethod* method, int vtable_index, float prof_factor)\n+  : VirtualCallGenerator(method, vtable_index, true \/*separate_io_projs*\/), _unique_id(0), _inline_cg(NULL), _callee(NULL), _is_pure_call(false), _prof_factor(prof_factor) {}\n+\n+  virtual bool is_virtual_late_inline() const { return true; }\n+\n+  \/\/ Convert the CallDynamicJava into an inline\n+  virtual void do_late_inline();\n+\n+  virtual void set_callee_method(ciMethod* m) {\n+    assert(_callee == NULL, \"repeated inlining attempt\");\n+    _callee = m;\n+  }\n+\n+  virtual JVMState* generate(JVMState* jvms) {\n+    Compile* C = Compile::current();\n+\n+    \/\/ Emit the CallDynamicJava and request separate projections so\n+    \/\/ that the late inlining logic can distinguish between fall\n+    \/\/ through and exceptional uses of the memory and io projections\n+    \/\/ as is done for allocations and macro expansion.\n+    JVMState* new_jvms = VirtualCallGenerator::generate(jvms);\n+    if (call_node() != NULL) {\n+      call_node()->set_generator(this);\n+    }\n+    return new_jvms;\n+  }\n+\n+  virtual void print_inlining_late(const char* msg) {\n+    CallNode* call = call_node();\n+    Compile* C = Compile::current();\n+    C->print_inlining_assert_ready();\n+    C->print_inlining(method(), call->jvms()->depth()-1, call->jvms()->bci(), msg);\n+    C->print_inlining_move_to(this);\n+    C->print_inlining_update_delayed(this);\n+  }\n+\n+  virtual void set_unique_id(jlong id) {\n+    _unique_id = id;\n+  }\n+\n+  virtual jlong unique_id() const {\n+    return _unique_id;\n+  }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineVirtualCallGenerator* cg = new LateInlineVirtualCallGenerator(method(), vtable_index(), _prof_factor);\n+    cg->set_call_node(call->as_CallDynamicJava());\n+    return cg;\n+  }\n+};\n+\n+bool LateInlineVirtualCallGenerator::do_late_inline_check(Compile* C, JVMState* jvms) {\n+  \/\/ Method handle linker case is handled in CallDynamicJavaNode::Ideal().\n+  \/\/ Unless inlining is performed, _override_symbolic_info bit will be set in DirectCallGenerator::generate().\n+\n+  bool allow_inline = C->inlining_incrementally();\n+  CallGenerator* cg = C->call_generator(_callee,\n+                                        vtable_index(),\n+                                        false \/*call_does_dispatch*\/,\n+                                        jvms,\n+                                        allow_inline,\n+                                        _prof_factor,\n+                                        NULL \/*speculative_receiver_type*\/,\n+                                        true \/*allow_intrinsics*\/);\n+\n+  Compile::current()->print_inlining_update_delayed(this);\n+\n+  if (cg != NULL) {\n+    assert(!cg->is_late_inline() || cg->is_mh_late_inline() || AlwaysIncrementalInline, \"we're doing late inlining\");\n+    _inline_cg = cg;\n+    return true;\n+  }\n+  call_node()->set_generator(this);\n+  return false;\n+}\n+\n+void LateInlineVirtualCallGenerator::do_late_inline() {\n+  assert(_callee != NULL, \"required\");\n+\n+  \/\/ Can't inline it\n+  CallDynamicJavaNode* call = call_node()->as_CallDynamicJava();\n+  if (call == NULL || call->outcnt() == 0 ||\n+      call->in(0) == NULL || call->in(0)->is_top()) {\n+    return;\n+  }\n+\n+  const TypeTuple *r = call->tf()->domain();\n+  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n+    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+      assert(Compile::current()->inlining_incrementally(), \"shouldn't happen during parsing\");\n+      return;\n+    }\n+  }\n+\n+  if (call->in(TypeFunc::Memory)->is_top()) {\n+    assert(Compile::current()->inlining_incrementally(), \"shouldn't happen during parsing\");\n+    return;\n+  }\n+\n+  \/\/ check for unreachable loop\n+  CallProjections callprojs;\n+  call->extract_projections(&callprojs, true);\n+  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n+      (callprojs.catchall_catchproj    == call->in(0)) ||\n+      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs.resproj != NULL && call->find_edge(callprojs.resproj) != -1) ||\n+      (callprojs.exobj   != NULL && call->find_edge(callprojs.exobj) != -1)) {\n+    return;\n+  }\n+\n+  Compile* C = Compile::current();\n+  \/\/ Remove inlined methods from Compiler's lists.\n+  if (call->is_macro()) {\n+    C->remove_macro_node(call);\n+  }\n+\n+  bool result_not_used = (callprojs.resproj == NULL || callprojs.resproj->outcnt() == 0);\n+  if (_is_pure_call && result_not_used) {\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n+    GraphKit kit(call->jvms());\n+    kit.replace_call(call, C->top(), true);\n+  } else {\n+    \/\/ Make a clone of the JVMState that appropriate to use for driving a parse\n+    JVMState* old_jvms = call->jvms();\n+    JVMState* jvms = old_jvms->clone_shallow(C);\n+    uint size = call->req();\n+    SafePointNode* map = new SafePointNode(size, jvms);\n+    for (uint i1 = 0; i1 < size; i1++) {\n+      map->init_req(i1, call->in(i1));\n+    }\n+\n+    \/\/ Make sure the state is a MergeMem for parsing.\n+    if (!map->in(TypeFunc::Memory)->is_MergeMem()) {\n+      Node* mem = MergeMemNode::make(map->in(TypeFunc::Memory));\n+      C->initial_gvn()->set_type_bottom(mem);\n+      map->set_req(TypeFunc::Memory, mem);\n+    }\n+\n+    uint nargs = method()->arg_size();\n+    \/\/ blow away old call arguments\n+    Node* top = C->top();\n+    for (uint i1 = 0; i1 < nargs; i1++) {\n+      map->set_req(TypeFunc::Parms + i1, top);\n+    }\n+    jvms->set_map(map);\n+\n+    \/\/ Make enough space in the expression stack to transfer\n+    \/\/ the incoming arguments and return value.\n+    map->ensure_stack(jvms, jvms->method()->max_stack());\n+    for (uint i1 = 0; i1 < nargs; i1++) {\n+      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+    }\n+\n+    C->print_inlining_assert_ready();\n+\n+    C->print_inlining_move_to(this);\n+\n+    C->log_late_inline(this);\n+\n+    \/\/ This check is done here because for_method_handle_inline() method\n+    \/\/ needs jvms for inlined state.\n+    if (!do_late_inline_check(C, jvms)) {\n+      map->disconnect_inputs(C);\n+      return;\n+    }\n+\n+    \/\/ Setup default node notes to be picked up by the inlining\n+    Node_Notes* old_nn = C->node_notes_at(call->_idx);\n+    if (old_nn != NULL) {\n+      Node_Notes* entry_nn = old_nn->clone(C);\n+      entry_nn->set_jvms(jvms);\n+      C->set_default_node_notes(entry_nn);\n+    }\n+\n+    { \/\/ Virtual call sometimes involves implicit null check.\n+      GraphKit kit(jvms);\n+      kit.null_check_receiver();\n+      jvms = kit.transfer_exceptions_into_jvms();\n+    }\n+\n+    \/\/ Now perform the inlining using the synthesized JVMState\n+    JVMState* new_jvms = _inline_cg->generate(jvms);\n+    if (new_jvms == NULL)  return;  \/\/ no change\n+    if (C->failing())      return;\n+\n+    \/\/ Capture any exceptional control flow\n+    GraphKit kit(new_jvms);\n+\n+    \/\/ Find the result object\n+    Node* result = C->top();\n+    int   result_size = method()->return_type()->size();\n+    if (result_size != 0 && !kit.stopped()) {\n+      result = (result_size == 1) ? kit.pop() : kit.pop_pair();\n+    }\n+\n+    if (_inline_cg->is_inline()) {\n+      \/\/ TODO: is_inline() == true for intrinsics as well\n+      C->set_has_loops(C->has_loops() || _inline_cg->method()->has_loops());\n+      C->env()->notice_inlined_method(_inline_cg->method());\n+    } else {\n+      assert(_inline_cg->is_direct(), \"sanity\");\n+    }\n+    C->set_inlining_progress(true);\n+    C->set_do_cleanup(kit.stopped()); \/\/ path is dead; needs cleanup\n+    kit.replace_call(call, result, true);\n+  }\n+}\n+\n+CallGenerator* CallGenerator::for_late_inline_virtual(ciMethod* m, int vtable_index, float prof_factor) {\n+  assert(!m->is_static(), \"for_virtual_call mismatch\");\n+  assert(!m->is_method_handle_intrinsic(), \"should be a direct call\");\n+  return new LateInlineVirtualCallGenerator(m, vtable_index, prof_factor);\n+}\n+\n@@ -546,0 +815,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineStringCallGenerator* cg = new LateInlineStringCallGenerator(method(), _inline_cg);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -568,0 +843,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineBoxingCallGenerator* cg = new LateInlineBoxingCallGenerator(method(), _inline_cg);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -590,0 +871,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineVectorReboxingCallGenerator* cg = new LateInlineVectorReboxingCallGenerator(method(), _inline_cg);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -1057,1 +1344,1 @@\n-  virtual bool      is_inlined()   const    { return true; }\n+  virtual bool      is_inline()    const    { return true; }\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":295,"deletions":8,"binary":false,"changes":303,"status":"modified"},{"patch":"@@ -630,1 +630,1 @@\n-  for (JVMState* p = this; p->_caller != NULL; p = p->_caller) {\n+  for (JVMState* p = this; p != NULL; p = p->_caller) {\n@@ -733,3 +733,5 @@\n-    OptoRegPair regs = is_CallRuntime()\n-      ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-      : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n+    OptoRegPair regs = Opcode() == Op_CallLeafVector\n+      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+      : is_CallRuntime()\n+        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n+        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n@@ -737,0 +739,10 @@\n+\n+    \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+    if(ideal_reg >= Op_VecS && ideal_reg <= Op_VecZ) {\n+      if(OptoReg::is_valid(regs.second())) {\n+        for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+          rm.Insert(r);\n+        }\n+      }\n+    }\n+\n@@ -1114,0 +1126,42 @@\n+\n+Node* CallDynamicJavaNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  CallGenerator* cg = generator();\n+  if (can_reshape && cg != NULL) {\n+    assert(IncrementalInlineVirtual, \"required\");\n+    assert(cg->call_node() == this, \"mismatch\");\n+    assert(cg->is_virtual_late_inline(), \"not virtual\");\n+\n+    \/\/ Recover symbolic info for method resolution\n+    ciMethod* caller = jvms()->method();\n+    ciBytecodeStream iter(caller);\n+    iter.force_bci(jvms()->bci());\n+\n+    bool             not_used1;\n+    ciSignature*     not_used2;\n+    ciMethod*        orig_callee  = iter.get_method(not_used1, &not_used2);  \/\/ callee in the bytecode\n+    ciKlass*         holder       = iter.get_declared_method_holder();\n+    if (orig_callee->is_method_handle_intrinsic()) {\n+      assert(_override_symbolic_info, \"\");\n+      orig_callee = method();\n+      holder = method()->holder();\n+    }\n+\n+    ciInstanceKlass* klass = ciEnv::get_instance_klass_for_declared_method_holder(holder);\n+\n+    Node* receiver_node = in(TypeFunc::Parms);\n+    const TypeOopPtr* receiver_type = phase->type(receiver_node)->isa_oopptr();\n+\n+    int  not_used3;\n+    bool call_does_dispatch;\n+    ciMethod* callee = phase->C->optimize_virtual_call(caller, jvms()->bci(), klass, holder, orig_callee, receiver_type, true \/*is_virtual*\/,\n+                                                       call_does_dispatch, not_used3);  \/\/ out-parameters\n+    if (!call_does_dispatch) {\n+      \/\/ Register for late inlining\n+      cg->set_callee_method(callee);\n+      phase->C->prepend_late_inline(cg); \/\/ TODO prepend or append for virtual calls? MH late inlining prepends to the list.\n+      set_generator(NULL);\n+    }\n+  }\n+  return CallNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -1134,0 +1188,5 @@\n+uint CallLeafVectorNode::size_of() const { return sizeof(*this); }\n+bool CallLeafVectorNode::cmp( const Node &n ) const {\n+  CallLeafVectorNode &call = (CallLeafVectorNode&)n;\n+  return CallLeafNode::cmp(call) && _num_bits == call._num_bits;\n+}\n@@ -1205,0 +1264,15 @@\n+void CallLeafVectorNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n+#ifdef ASSERT\n+  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+         \"return vector size must match\");\n+  const TypeTuple* d = tf()->domain();\n+  for (uint i = TypeFunc::Parms; i < d->cnt(); i++) {\n+    Node* arg = in(i);\n+    assert(arg->bottom_type()->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+           \"vector argument size must match\");\n+  }\n+#endif\n+\n+  Matcher::vector_calling_convention(parm_regs, _num_bits, argcnt);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":78,"deletions":4,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+class         CallLeafVectorNode;\n@@ -619,1 +620,1 @@\n-    if (C->needs_clone_jvms() && jvms() != NULL) {\n+    if (jvms() != NULL) {\n@@ -767,0 +768,1 @@\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n@@ -775,0 +777,1 @@\n+protected:\n@@ -862,0 +865,18 @@\n+\/\/------------------------------CallLeafVectorNode-------------------------------\n+\/\/ CallLeafNode but calling with vector calling convention instead.\n+class CallLeafVectorNode : public CallLeafNode {\n+private:\n+  uint _num_bits;\n+protected:\n+  virtual bool cmp( const Node &n ) const;\n+  virtual uint size_of() const; \/\/ Size is bigger\n+public:\n+  CallLeafVectorNode(const TypeFunc* tf, address addr, const char* name,\n+                   const TypePtr* adr_type, uint num_bits)\n+    : CallLeafNode(tf, addr, name, adr_type), _num_bits(num_bits)\n+  {\n+  }\n+  virtual int   Opcode() const;\n+  virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":22,"deletions":1,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+macro(CallLeafVector)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -349,0 +349,17 @@\n+void Compile::remove_useless_late_inlines(GrowableArray<CallGenerator*>* inlines, Node* dead) {\n+  assert(dead != NULL && dead->is_Call(), \"sanity\");\n+  int shift = 0;\n+  for (int i = 0; i < inlines->length(); i++) {\n+    CallGenerator* cg = inlines->at(i);\n+    CallNode* call = cg->call_node();\n+    if (shift > 0) {\n+      inlines->at_put(i - shift, cg);\n+    }\n+    if (call == dead) {\n+      shift++;\n+    }\n+  }\n+  inlines->trunc_to(inlines->length() - shift);\n+  assert(shift <= 1, \"sanity\");\n+}\n+\n@@ -1859,0 +1876,1 @@\n+\n@@ -1861,2 +1879,2 @@\n-  int i = 0;\n-  for (; i <_late_inlines.length() && !inlining_progress(); i++) {\n+\n+  for (int i = 0; i < _late_inlines.length(); i++) {\n@@ -1865,0 +1883,1 @@\n+    assert(inlining_incrementally() || cg->is_virtual_late_inline(), \"no inlining allowed\");\n@@ -1866,5 +1885,8 @@\n-    if (failing())  return false;\n-  }\n-  int j = 0;\n-  for (; i < _late_inlines.length(); i++, j++) {\n-    _late_inlines.at_put(j, _late_inlines.at(i));\n+    assert(_late_inlines.at(i) == cg, \"no insertions before current position allowed\");\n+    if (failing()) {\n+      return false;\n+    } else if (inlining_progress()) {\n+      _late_inlines_pos = i+1; \/\/ restore the position in case new elements were inserted\n+      print_method(PHASE_INCREMENTAL_INLINE_STEP, cg->call_node(), 3);\n+      break; \/\/ process one call site at a time\n+    }\n@@ -1872,2 +1894,5 @@\n-  _late_inlines.trunc_to(j);\n-  assert(inlining_progress() || _late_inlines.length() == 0, \"\");\n+  \/\/ Remove processed elements.\n+  _late_inlines.truncate_to(_late_inlines_pos);\n+  _late_inlines_pos = 0;\n+\n+  assert(inlining_progress() || _late_inlines.length() == 0, \"no progress\");\n@@ -1879,0 +1904,1 @@\n+\n@@ -1893,0 +1919,1 @@\n+  print_method(PHASE_INCREMENTAL_INLINE_CLEANUP, 3);\n@@ -1934,0 +1961,4 @@\n+\n+    if (_late_inlines.length() == 0) {\n+      break; \/\/ no more progress\n+    }\n@@ -2080,1 +2111,2 @@\n-    for_igvn()->clear();\n+    Unique_Node_List* old_worklist = for_igvn();\n+    old_worklist->clear();\n@@ -2090,1 +2122,1 @@\n-    set_for_igvn(save_for_igvn);\n+    set_for_igvn(old_worklist); \/\/ new_worklist is dead beyond this point\n@@ -2232,0 +2264,25 @@\n+\n+  assert(_late_inlines.length() == 0 || IncrementalInlineVirtual, \"not empty\");\n+\n+  while (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual calls.\n+    \/\/ Though it's maybe too late for inlining, strength-reducing them to direct calls is still an option.\n+\n+    \/\/ \"inlining_incrementally() == false\" is used to signal that no inlining is allowed.\n+    \/\/ Tracking and verification of modified nodes is disabled by _modified_nodes == NULL as if inlining_incrementally() were set.\n+    assert(inlining_incrementally() == false, \"not allowed\");\n+\n+    for_igvn()->clear();\n+    initial_gvn()->replace_with(&igvn);\n+\n+    DEBUG_ONLY( int late_inlines_before = _late_inlines.length(); )\n+\n+    while (inline_incrementally_one()) {\n+      assert(!failing(), \"inconsistent\");\n+    }\n+    if (failing())  return;\n+\n+    inline_incrementally_cleanup(igvn);\n+\n+    assert(_late_inlines.length() < late_inlines_before, \"no progress\");\n+  }\n@@ -2906,0 +2963,1 @@\n+  case Op_CallLeafVector:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":69,"deletions":11,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -945,0 +945,1 @@\n+  void remove_useless_late_inlines(GrowableArray<CallGenerator*>* inlines, Node* dead);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2501,0 +2501,3 @@\n+  } else  if (flags & RC_VECTOR){\n+    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    call = new CallLeafVectorNode(call_type, call_addr, call_name, adr_type, num_bits);\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -814,0 +814,1 @@\n+    RC_VECTOR = 64,             \/\/ CallLeafVectorNode\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -857,0 +857,1 @@\n+    case Op_CallLeafVector:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -328,0 +328,1 @@\n+  Node* gen_call_to_svml(int vector_api_op_id, BasicType bt, int num_elem, Node* opd1, Node* opd2);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1365,2 +1365,4 @@\n-      if( !parm_regs[i].first()->is_valid() &&\n-          !parm_regs[i].second()->is_valid() ) {\n+      VMReg first = parm_regs[i].first();\n+      VMReg second = parm_regs[i].second();\n+      if( !first->is_valid() &&\n+          !second->is_valid() ) {\n@@ -1369,0 +1371,9 @@\n+      \/\/ Handle case where arguments are in vector registers.\n+      if(call->in(TypeFunc::Parms + i)->bottom_type()->isa_vect()) {\n+        OptoReg::Name reg_fst = OptoReg::as_OptoReg(first);\n+        OptoReg::Name reg_snd = OptoReg::as_OptoReg(second);\n+        assert (reg_fst <= reg_snd, \"fst=%d snd=%d\", reg_fst, reg_snd);\n+        for (OptoReg::Name r = reg_fst; r <= reg_snd; r++) {\n+          rm->Insert(r);\n+        }\n+      }\n@@ -1370,1 +1381,1 @@\n-      OptoReg::Name reg1 = warp_outgoing_stk_arg(parm_regs[i].first(), begin_out_arg_area, out_arg_limit_per_call );\n+      OptoReg::Name reg1 = warp_outgoing_stk_arg(first, begin_out_arg_area, out_arg_limit_per_call );\n@@ -1374,1 +1385,1 @@\n-      OptoReg::Name reg2 = warp_outgoing_stk_arg(parm_regs[i].second(), begin_out_arg_area, out_arg_limit_per_call );\n+      OptoReg::Name reg2 = warp_outgoing_stk_arg(second, begin_out_arg_area, out_arg_limit_per_call );\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/callGenerator.hpp\"\n@@ -557,1 +558,1 @@\n-  \/\/ cloning CallNode may need to clone JVMState\n+    \/\/ cloning CallNode may need to clone JVMState\n@@ -560,0 +561,6 @@\n+    \/\/ CallGenerator is linked to the original node.\n+    CallGenerator* cg = n->as_Call()->generator();\n+    if (cg != NULL) {\n+      CallGenerator* cloned_cg = cg->with_call_node(n->as_Call());\n+      n->as_Call()->set_generator(cloned_cg);\n+    }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1110,4 +1110,6 @@\n-  while (modified_list->size()) {\n-    Node* n = modified_list->pop();\n-    n->dump();\n-    assert(false, \"VerifyIterativeGVN: new modified node was added\");\n+  if (modified_list != NULL) {\n+    while (modified_list->size()) {\n+      Node* n = modified_list->pop();\n+      n->dump();\n+      assert(false, \"VerifyIterativeGVN: new modified node was added\");\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -235,0 +235,15 @@\n+  if (opc == Op_CallLeafVector) {\n+    if (!UseVectorStubs) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** vector stubs support is disabled\");\n+      }\n+      return false;\n+    }\n+    if (!Matcher::supports_vector_calling_convention()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** no vector calling conventions supported\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -284,10 +299,22 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n-  switch (n) {\n-    case 1:\n-    case 2: {\n-      operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n-      break;\n-    }\n-    case 3: {\n-      operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n-      break;\n+  if (sopc == Op_CallLeafVector) {\n+    assert(UseVectorStubs, \"sanity\");\n+    operation = gen_call_to_svml(opr->get_con(), elem_bt, num_elem, opd1, opd2);\n+    if (operation == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** svml call failed\");\n+      }\n+      return false;\n+     }\n+  } else {\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    switch (n) {\n+      case 1:\n+      case 2: {\n+        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n+        break;\n+      }\n+      case 3: {\n+        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n+        break;\n+      }\n+      default: fatal(\"unsupported arity: %d\", n);\n@@ -295,1 +322,0 @@\n-    default: fatal(\"unsupported arity: %d\", n);\n@@ -343,0 +369,3 @@\n+  if (!arch_supports_vector(Op_VectorLoadConst, num_elem, elem_bt, VecMaskNotUsed)) {\n+    return false;\n+  }\n@@ -374,1 +403,1 @@\n-    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(1));\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::ge));\n@@ -1178,0 +1207,417 @@\n+static void get_svml_address(int op, int bits, BasicType bt, const char** name_ptr, address* addr_ptr) {\n+  assert(UseVectorStubs, \"sanity\");\n+  assert(name_ptr != NULL, \"unexpected\");\n+  assert(addr_ptr != NULL, \"unexpected\");\n+\n+#ifdef __VECTOR_API_MATH_INTRINSICS_COMMON\n+  \/\/ Since the addresses are resolved at runtime, using switch instead of table - otherwise might get NULL addresses.\n+  if (bt == T_FLOAT) {\n+    switch(op) {\n+      case VectorSupport::VECTOR_OP_EXP: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_exp_float64\";  *addr_ptr = StubRoutines::vector_exp_float64();  break;\n+            case 128: *name_ptr = \"vector_exp_float128\"; *addr_ptr = StubRoutines::vector_exp_float128(); break;\n+            case 256: *name_ptr = \"vector_exp_float256\"; *addr_ptr = StubRoutines::vector_exp_float256(); break;\n+            case 512: *name_ptr = \"vector_exp_float512\"; *addr_ptr = StubRoutines::vector_exp_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_LOG1P: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_log1p_float64\";  *addr_ptr = StubRoutines::vector_log1p_float64();  break;\n+            case 128: *name_ptr = \"vector_log1p_float128\"; *addr_ptr = StubRoutines::vector_log1p_float128(); break;\n+            case 256: *name_ptr = \"vector_log1p_float256\"; *addr_ptr = StubRoutines::vector_log1p_float256(); break;\n+            case 512: *name_ptr = \"vector_log1p_float512\"; *addr_ptr = StubRoutines::vector_log1p_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_LOG: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_log_float64\";  *addr_ptr = StubRoutines::vector_log_float64();  break;\n+            case 128: *name_ptr = \"vector_log_float128\"; *addr_ptr = StubRoutines::vector_log_float128(); break;\n+            case 256: *name_ptr = \"vector_log_float256\"; *addr_ptr = StubRoutines::vector_log_float256(); break;\n+            case 512: *name_ptr = \"vector_log_float512\"; *addr_ptr = StubRoutines::vector_log_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_LOG10: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_log10_float64\";  *addr_ptr = StubRoutines::vector_log10_float64();  break;\n+            case 128: *name_ptr = \"vector_log10_float128\"; *addr_ptr = StubRoutines::vector_log10_float128(); break;\n+            case 256: *name_ptr = \"vector_log10_float256\"; *addr_ptr = StubRoutines::vector_log10_float256(); break;\n+            case 512: *name_ptr = \"vector_log10_float512\"; *addr_ptr = StubRoutines::vector_log10_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_EXPM1: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_expm1_float64\";  *addr_ptr = StubRoutines::vector_expm1_float64();  break;\n+            case 128: *name_ptr = \"vector_expm1_float128\"; *addr_ptr = StubRoutines::vector_expm1_float128(); break;\n+            case 256: *name_ptr = \"vector_expm1_float256\"; *addr_ptr = StubRoutines::vector_expm1_float256(); break;\n+            case 512: *name_ptr = \"vector_expm1_float512\"; *addr_ptr = StubRoutines::vector_expm1_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_SIN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_sin_float64\";  *addr_ptr = StubRoutines::vector_sin_float64();  break;\n+            case 128: *name_ptr = \"vector_sin_float128\"; *addr_ptr = StubRoutines::vector_sin_float128(); break;\n+            case 256: *name_ptr = \"vector_sin_float256\"; *addr_ptr = StubRoutines::vector_sin_float256(); break;\n+            case 512: *name_ptr = \"vector_sin_float512\"; *addr_ptr = StubRoutines::vector_sin_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_COS: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_cos_float64\";  *addr_ptr = StubRoutines::vector_cos_float64();  break;\n+            case 128: *name_ptr = \"vector_cos_float128\"; *addr_ptr = StubRoutines::vector_cos_float128(); break;\n+            case 256: *name_ptr = \"vector_cos_float256\"; *addr_ptr = StubRoutines::vector_cos_float256(); break;\n+            case 512: *name_ptr = \"vector_cos_float512\"; *addr_ptr = StubRoutines::vector_cos_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_TAN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_tan_float64\";  *addr_ptr = StubRoutines::vector_tan_float64();  break;\n+            case 128: *name_ptr = \"vector_tan_float128\"; *addr_ptr = StubRoutines::vector_tan_float128(); break;\n+            case 256: *name_ptr = \"vector_tan_float256\"; *addr_ptr = StubRoutines::vector_tan_float256(); break;\n+            case 512: *name_ptr = \"vector_tan_float512\"; *addr_ptr = StubRoutines::vector_tan_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_SINH: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_sinh_float64\";  *addr_ptr = StubRoutines::vector_sinh_float64();  break;\n+            case 128: *name_ptr = \"vector_sinh_float128\"; *addr_ptr = StubRoutines::vector_sinh_float128(); break;\n+            case 256: *name_ptr = \"vector_sinh_float256\"; *addr_ptr = StubRoutines::vector_sinh_float256(); break;\n+            case 512: *name_ptr = \"vector_sinh_float512\"; *addr_ptr = StubRoutines::vector_sinh_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_COSH: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_cosh_float64\";  *addr_ptr = StubRoutines::vector_cosh_float64();  break;\n+            case 128: *name_ptr = \"vector_cosh_float128\"; *addr_ptr = StubRoutines::vector_cosh_float128(); break;\n+            case 256: *name_ptr = \"vector_cosh_float256\"; *addr_ptr = StubRoutines::vector_cosh_float256(); break;\n+            case 512: *name_ptr = \"vector_cosh_float512\"; *addr_ptr = StubRoutines::vector_cosh_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_TANH: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_tanh_float64\";  *addr_ptr = StubRoutines::vector_tanh_float64();  break;\n+            case 128: *name_ptr = \"vector_tanh_float128\"; *addr_ptr = StubRoutines::vector_tanh_float128(); break;\n+            case 256: *name_ptr = \"vector_tanh_float256\"; *addr_ptr = StubRoutines::vector_tanh_float256(); break;\n+            case 512: *name_ptr = \"vector_tanh_float512\"; *addr_ptr = StubRoutines::vector_tanh_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ASIN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_asin_float64\";  *addr_ptr = StubRoutines::vector_asin_float64();  break;\n+            case 128: *name_ptr = \"vector_asin_float128\"; *addr_ptr = StubRoutines::vector_asin_float128(); break;\n+            case 256: *name_ptr = \"vector_asin_float256\"; *addr_ptr = StubRoutines::vector_asin_float256(); break;\n+            case 512: *name_ptr = \"vector_asin_float512\"; *addr_ptr = StubRoutines::vector_asin_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ACOS: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_acos_float64\";  *addr_ptr = StubRoutines::vector_acos_float64();  break;\n+            case 128: *name_ptr = \"vector_acos_float128\"; *addr_ptr = StubRoutines::vector_acos_float128(); break;\n+            case 256: *name_ptr = \"vector_acos_float256\"; *addr_ptr = StubRoutines::vector_acos_float256(); break;\n+            case 512: *name_ptr = \"vector_acos_float512\"; *addr_ptr = StubRoutines::vector_acos_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ATAN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_atan_float64\";  *addr_ptr = StubRoutines::vector_atan_float64();  break;\n+            case 128: *name_ptr = \"vector_atan_float128\"; *addr_ptr = StubRoutines::vector_atan_float128(); break;\n+            case 256: *name_ptr = \"vector_atan_float256\"; *addr_ptr = StubRoutines::vector_atan_float256(); break;\n+            case 512: *name_ptr = \"vector_atan_float512\"; *addr_ptr = StubRoutines::vector_atan_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_CBRT: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_cbrt_float64\";  *addr_ptr = StubRoutines::vector_cbrt_float64();  break;\n+            case 128: *name_ptr = \"vector_cbrt_float128\"; *addr_ptr = StubRoutines::vector_cbrt_float128(); break;\n+            case 256: *name_ptr = \"vector_cbrt_float256\"; *addr_ptr = StubRoutines::vector_cbrt_float256(); break;\n+            case 512: *name_ptr = \"vector_cbrt_float512\"; *addr_ptr = StubRoutines::vector_cbrt_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+       case VectorSupport::VECTOR_OP_HYPOT: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_hypot_float64\";  *addr_ptr = StubRoutines::vector_hypot_float64();  break;\n+            case 128: *name_ptr = \"vector_hypot_float128\"; *addr_ptr = StubRoutines::vector_hypot_float128(); break;\n+            case 256: *name_ptr = \"vector_hypot_float256\"; *addr_ptr = StubRoutines::vector_hypot_float256(); break;\n+            case 512: *name_ptr = \"vector_hypot_float512\"; *addr_ptr = StubRoutines::vector_hypot_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_POW: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_pow_float64\";  *addr_ptr = StubRoutines::vector_pow_float64();  break;\n+            case 128: *name_ptr = \"vector_pow_float128\"; *addr_ptr = StubRoutines::vector_pow_float128(); break;\n+            case 256: *name_ptr = \"vector_pow_float256\"; *addr_ptr = StubRoutines::vector_pow_float256(); break;\n+            case 512: *name_ptr = \"vector_pow_float512\"; *addr_ptr = StubRoutines::vector_pow_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ATAN2: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_atan2_float64\";  *addr_ptr = StubRoutines::vector_atan2_float64();  break;\n+            case 128: *name_ptr = \"vector_atan2_float128\"; *addr_ptr = StubRoutines::vector_atan2_float128(); break;\n+            case 256: *name_ptr = \"vector_atan2_float256\"; *addr_ptr = StubRoutines::vector_atan2_float256(); break;\n+            case 512: *name_ptr = \"vector_atan2_float512\"; *addr_ptr = StubRoutines::vector_atan2_float512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      default:\n+        *name_ptr = \"invalid\";\n+        *addr_ptr = NULL;\n+        break;\n+    }\n+  } else {\n+    assert(bt == T_DOUBLE, \"must be FP type only\");\n+    switch(op) {\n+      case VectorSupport::VECTOR_OP_EXP: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_exp_double64\";  *addr_ptr = StubRoutines::vector_exp_double64();  break;\n+            case 128: *name_ptr = \"vector_exp_double128\"; *addr_ptr = StubRoutines::vector_exp_double128(); break;\n+            case 256: *name_ptr = \"vector_exp_double256\"; *addr_ptr = StubRoutines::vector_exp_double256(); break;\n+            case 512: *name_ptr = \"vector_exp_double512\"; *addr_ptr = StubRoutines::vector_exp_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_LOG1P: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_log1p_double64\";  *addr_ptr = StubRoutines::vector_log1p_double64();  break;\n+            case 128: *name_ptr = \"vector_log1p_double128\"; *addr_ptr = StubRoutines::vector_log1p_double128(); break;\n+            case 256: *name_ptr = \"vector_log1p_double256\"; *addr_ptr = StubRoutines::vector_log1p_double256(); break;\n+            case 512: *name_ptr = \"vector_log1p_double512\"; *addr_ptr = StubRoutines::vector_log1p_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_LOG: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_log_double64\";  *addr_ptr = StubRoutines::vector_log_double64();  break;\n+            case 128: *name_ptr = \"vector_log_double128\"; *addr_ptr = StubRoutines::vector_log_double128(); break;\n+            case 256: *name_ptr = \"vector_log_double256\"; *addr_ptr = StubRoutines::vector_log_double256(); break;\n+            case 512: *name_ptr = \"vector_log_double512\"; *addr_ptr = StubRoutines::vector_log_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_LOG10: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_log10_double64\";  *addr_ptr = StubRoutines::vector_log10_double64();  break;\n+            case 128: *name_ptr = \"vector_log10_double128\"; *addr_ptr = StubRoutines::vector_log10_double128(); break;\n+            case 256: *name_ptr = \"vector_log10_double256\"; *addr_ptr = StubRoutines::vector_log10_double256(); break;\n+            case 512: *name_ptr = \"vector_log10_double512\"; *addr_ptr = StubRoutines::vector_log10_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_EXPM1: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_expm1_double64\";  *addr_ptr = StubRoutines::vector_expm1_double64();  break;\n+            case 128: *name_ptr = \"vector_expm1_double128\"; *addr_ptr = StubRoutines::vector_expm1_double128(); break;\n+            case 256: *name_ptr = \"vector_expm1_double256\"; *addr_ptr = StubRoutines::vector_expm1_double256(); break;\n+            case 512: *name_ptr = \"vector_expm1_double512\"; *addr_ptr = StubRoutines::vector_expm1_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_SIN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_sin_double64\";  *addr_ptr = StubRoutines::vector_sin_double64();  break;\n+            case 128: *name_ptr = \"vector_sin_double128\"; *addr_ptr = StubRoutines::vector_sin_double128(); break;\n+            case 256: *name_ptr = \"vector_sin_double256\"; *addr_ptr = StubRoutines::vector_sin_double256(); break;\n+            case 512: *name_ptr = \"vector_sin_double512\"; *addr_ptr = StubRoutines::vector_sin_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_COS: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_cos_double64\";  *addr_ptr = StubRoutines::vector_cos_double64();  break;\n+            case 128: *name_ptr = \"vector_cos_double128\"; *addr_ptr = StubRoutines::vector_cos_double128(); break;\n+            case 256: *name_ptr = \"vector_cos_double256\"; *addr_ptr = StubRoutines::vector_cos_double256(); break;\n+            case 512: *name_ptr = \"vector_cos_double512\"; *addr_ptr = StubRoutines::vector_cos_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_TAN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_tan_double64\";  *addr_ptr = StubRoutines::vector_tan_double64();  break;\n+            case 128: *name_ptr = \"vector_tan_double128\"; *addr_ptr = StubRoutines::vector_tan_double128(); break;\n+            case 256: *name_ptr = \"vector_tan_double256\"; *addr_ptr = StubRoutines::vector_tan_double256(); break;\n+            case 512: *name_ptr = \"vector_tan_double512\"; *addr_ptr = StubRoutines::vector_tan_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_SINH: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_sinh_double64\";  *addr_ptr = StubRoutines::vector_sinh_double64();  break;\n+            case 128: *name_ptr = \"vector_sinh_double128\"; *addr_ptr = StubRoutines::vector_sinh_double128(); break;\n+            case 256: *name_ptr = \"vector_sinh_double256\"; *addr_ptr = StubRoutines::vector_sinh_double256(); break;\n+            case 512: *name_ptr = \"vector_sinh_double512\"; *addr_ptr = StubRoutines::vector_sinh_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_COSH: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_cosh_double64\";  *addr_ptr = StubRoutines::vector_cosh_double64();  break;\n+            case 128: *name_ptr = \"vector_cosh_double128\"; *addr_ptr = StubRoutines::vector_cosh_double128(); break;\n+            case 256: *name_ptr = \"vector_cosh_double256\"; *addr_ptr = StubRoutines::vector_cosh_double256(); break;\n+            case 512: *name_ptr = \"vector_cosh_double512\"; *addr_ptr = StubRoutines::vector_cosh_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_TANH: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_tanh_double64\";  *addr_ptr = StubRoutines::vector_tanh_double64();  break;\n+            case 128: *name_ptr = \"vector_tanh_double128\"; *addr_ptr = StubRoutines::vector_tanh_double128(); break;\n+            case 256: *name_ptr = \"vector_tanh_double256\"; *addr_ptr = StubRoutines::vector_tanh_double256(); break;\n+            case 512: *name_ptr = \"vector_tanh_double512\"; *addr_ptr = StubRoutines::vector_tanh_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ASIN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_asin_double64\";  *addr_ptr = StubRoutines::vector_asin_double64();  break;\n+            case 128: *name_ptr = \"vector_asin_double128\"; *addr_ptr = StubRoutines::vector_asin_double128(); break;\n+            case 256: *name_ptr = \"vector_asin_double256\"; *addr_ptr = StubRoutines::vector_asin_double256(); break;\n+            case 512: *name_ptr = \"vector_asin_double512\"; *addr_ptr = StubRoutines::vector_asin_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ACOS: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_acos_double64\";  *addr_ptr = StubRoutines::vector_acos_double64();  break;\n+            case 128: *name_ptr = \"vector_acos_double128\"; *addr_ptr = StubRoutines::vector_acos_double128(); break;\n+            case 256: *name_ptr = \"vector_acos_double256\"; *addr_ptr = StubRoutines::vector_acos_double256(); break;\n+            case 512: *name_ptr = \"vector_acos_double512\"; *addr_ptr = StubRoutines::vector_acos_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ATAN: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_atan_double64\";  *addr_ptr = StubRoutines::vector_atan_double64();  break;\n+            case 128: *name_ptr = \"vector_atan_double128\"; *addr_ptr = StubRoutines::vector_atan_double128(); break;\n+            case 256: *name_ptr = \"vector_atan_double256\"; *addr_ptr = StubRoutines::vector_atan_double256(); break;\n+            case 512: *name_ptr = \"vector_atan_double512\"; *addr_ptr = StubRoutines::vector_atan_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_CBRT: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_cbrt_double64\";  *addr_ptr = StubRoutines::vector_cbrt_double64();  break;\n+            case 128: *name_ptr = \"vector_cbrt_double128\"; *addr_ptr = StubRoutines::vector_cbrt_double128(); break;\n+            case 256: *name_ptr = \"vector_cbrt_double256\"; *addr_ptr = StubRoutines::vector_cbrt_double256(); break;\n+            case 512: *name_ptr = \"vector_cbrt_double512\"; *addr_ptr = StubRoutines::vector_cbrt_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_HYPOT: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_hypot_double64\";  *addr_ptr = StubRoutines::vector_hypot_double64();  break;\n+            case 128: *name_ptr = \"vector_hypot_double128\"; *addr_ptr = StubRoutines::vector_hypot_double128(); break;\n+            case 256: *name_ptr = \"vector_hypot_double256\"; *addr_ptr = StubRoutines::vector_hypot_double256(); break;\n+            case 512: *name_ptr = \"vector_hypot_double512\"; *addr_ptr = StubRoutines::vector_hypot_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_POW: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_pow_double64\";  *addr_ptr = StubRoutines::vector_pow_double64();  break;\n+            case 128: *name_ptr = \"vector_pow_double128\"; *addr_ptr = StubRoutines::vector_pow_double128(); break;\n+            case 256: *name_ptr = \"vector_pow_double256\"; *addr_ptr = StubRoutines::vector_pow_double256(); break;\n+            case 512: *name_ptr = \"vector_pow_double512\"; *addr_ptr = StubRoutines::vector_pow_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+      case VectorSupport::VECTOR_OP_ATAN2: {\n+          switch(bits) {\n+            case 64:  *name_ptr = \"vector_atan2_double64\";  *addr_ptr = StubRoutines::vector_atan2_double64();  break;\n+            case 128: *name_ptr = \"vector_atan2_double128\"; *addr_ptr = StubRoutines::vector_atan2_double128(); break;\n+            case 256: *name_ptr = \"vector_atan2_double256\"; *addr_ptr = StubRoutines::vector_atan2_double256(); break;\n+            case 512: *name_ptr = \"vector_atan2_double512\"; *addr_ptr = StubRoutines::vector_atan2_double512(); break;\n+            default: Unimplemented(); break;\n+          }\n+        }\n+        break;\n+\n+      default:\n+        *name_ptr = \"invalid\";\n+        *addr_ptr = NULL;\n+        break;\n+    }\n+  }\n+#else\n+  *name_ptr = \"invalid\";\n+  *addr_ptr = NULL;\n+#endif \/\/ __VECTOR_API_MATH_INTRINSICS_COMMON\n+}\n+\n+Node* LibraryCallKit::gen_call_to_svml(int vector_api_op_id, BasicType bt, int num_elem, Node* opd1, Node* opd2) {\n+  assert(UseVectorStubs, \"sanity\");\n+  assert(vector_api_op_id >= VectorSupport::VECTOR_OP_SVML_START && vector_api_op_id <= VectorSupport::VECTOR_OP_SVML_END, \"need valid op id\");\n+  assert(opd1 != NULL, \"must not be null\");\n+  const TypeVect* vt = TypeVect::make(bt, num_elem);\n+  const TypeFunc* call_type = OptoRuntime::Math_Vector_Vector_Type(opd2 != NULL ? 2 : 1, vt, vt);\n+  const char* name = NULL;\n+  address addr = NULL;\n+\n+  \/\/ Get address for svml method.\n+  get_svml_address(vector_api_op_id, vt->length_in_bytes() * BitsPerByte, bt, &name, &addr);\n+\n+  if (addr == NULL) {\n+    return NULL;\n+  }\n+\n+  assert(name != NULL, \"name must not be null\");\n+  Node* operation = make_runtime_call(RC_VECTOR,\n+                                      call_type,\n+                                      addr,\n+                                      name,\n+                                      TypePtr::BOTTOM,\n+                                      opd1,\n+                                      opd2);\n+  return _gvn.transform(new ProjNode(_gvn.transform(operation), TypeFunc::Parms));\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":458,"deletions":12,"binary":false,"changes":470,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -1112,0 +1112,1 @@\n+    assert((BoolTest::mask)predicate_node->get_int() == predicate, \"Unmatched predicates\");\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -351,0 +351,19 @@\n+    case VECTOR_OP_TAN:\n+    case VECTOR_OP_TANH:\n+    case VECTOR_OP_SIN:\n+    case VECTOR_OP_SINH:\n+    case VECTOR_OP_COS:\n+    case VECTOR_OP_COSH:\n+    case VECTOR_OP_ASIN:\n+    case VECTOR_OP_ACOS:\n+    case VECTOR_OP_ATAN:\n+    case VECTOR_OP_ATAN2:\n+    case VECTOR_OP_CBRT:\n+    case VECTOR_OP_LOG:\n+    case VECTOR_OP_LOG10:\n+    case VECTOR_OP_LOG1P:\n+    case VECTOR_OP_POW:\n+    case VECTOR_OP_EXP:\n+    case VECTOR_OP_EXPM1:\n+    case VECTOR_OP_HYPOT:\n+      return Op_CallLeafVector;\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -80,1 +80,24 @@\n-    VECTOR_OP_REINTERPRET = 18\n+    VECTOR_OP_REINTERPRET = 18,\n+\n+    \/\/ Vector Math Library\n+    VECTOR_OP_TAN   = 101,\n+    VECTOR_OP_TANH  = 102,\n+    VECTOR_OP_SIN   = 103,\n+    VECTOR_OP_SINH  = 104,\n+    VECTOR_OP_COS   = 105,\n+    VECTOR_OP_COSH  = 106,\n+    VECTOR_OP_ASIN  = 107,\n+    VECTOR_OP_ACOS  = 108,\n+    VECTOR_OP_ATAN  = 109,\n+    VECTOR_OP_ATAN2 = 110,\n+    VECTOR_OP_CBRT  = 111,\n+    VECTOR_OP_LOG   = 112,\n+    VECTOR_OP_LOG10 = 113,\n+    VECTOR_OP_LOG1P = 114,\n+    VECTOR_OP_POW   = 115,\n+    VECTOR_OP_EXP   = 116,\n+    VECTOR_OP_EXPM1 = 117,\n+    VECTOR_OP_HYPOT = 118,\n+\n+    VECTOR_OP_SVML_START = VECTOR_OP_TAN,\n+    VECTOR_OP_SVML_END   = VECTOR_OP_HYPOT\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.hpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -4237,0 +4237,5 @@\n+\n+    if (!FLAG_IS_DEFAULT(UseVectorStubs) && UseVectorStubs) {\n+      warning(\"Disabling UseVectorStubs since EnableVectorSupport is turned off.\");\n+    }\n+    FLAG_SET_DEFAULT(UseVectorStubs, false);\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -390,0 +390,4 @@\n+  static int vector_calling_convention(VMRegPair *regs,\n+                                       uint num_bits,\n+                                       uint total_args_passed);\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1527,0 +1527,1 @@\n+  declare_c2_type(CallLeafVectorNode, CallLeafNode)                       \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -315,0 +315,11 @@\n+  void truncate_to(int idx) {\n+    for (int i = 0, j = idx; j < length(); i++, j++) {\n+      at_put(i, at(j));\n+    }\n+    trunc_to(length() - idx);\n+  }\n+\n+  void truncate_from(int idx) {\n+    trunc_to(idx);\n+  }\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -552,31 +552,0 @@\n-            if (op == SIN) {\n-                return uOp((i, a) -> (double) Math.sin(a));\n-            } else if (op == COS) {\n-                return uOp((i, a) -> (double) Math.cos(a));\n-            } else if (op == TAN) {\n-                return uOp((i, a) -> (double) Math.tan(a));\n-            } else if (op == ASIN) {\n-                return uOp((i, a) -> (double) Math.asin(a));\n-            } else if (op == ACOS) {\n-                return uOp((i, a) -> (double) Math.acos(a));\n-            } else if (op == ATAN) {\n-                return uOp((i, a) -> (double) Math.atan(a));\n-            } else if (op == EXP) {\n-                return uOp((i, a) -> (double) Math.exp(a));\n-            } else if (op == LOG) {\n-                return uOp((i, a) -> (double) Math.log(a));\n-            } else if (op == LOG10) {\n-                return uOp((i, a) -> (double) Math.log10(a));\n-            } else if (op == CBRT) {\n-                return uOp((i, a) -> (double) Math.cbrt(a));\n-            } else if (op == SINH) {\n-                return uOp((i, a) -> (double) Math.sinh(a));\n-            } else if (op == COSH) {\n-                return uOp((i, a) -> (double) Math.cosh(a));\n-            } else if (op == TANH) {\n-                return uOp((i, a) -> (double) Math.tanh(a));\n-            } else if (op == EXPM1) {\n-                return uOp((i, a) -> (double) Math.expm1(a));\n-            } else if (op == LOG1P) {\n-                return uOp((i, a) -> (double) Math.log1p(a));\n-            }\n@@ -594,0 +563,18 @@\n+                case VECTOR_OP_SIN: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.sin(a));\n+                case VECTOR_OP_COS: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.cos(a));\n+                case VECTOR_OP_TAN: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.tan(a));\n+                case VECTOR_OP_ASIN: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.asin(a));\n+                case VECTOR_OP_ACOS: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.acos(a));\n+                case VECTOR_OP_ATAN: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.atan(a));\n+                case VECTOR_OP_EXP: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.exp(a));\n+                case VECTOR_OP_LOG: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.log(a));\n+                case VECTOR_OP_LOG10: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.log10(a));\n@@ -596,0 +583,12 @@\n+                case VECTOR_OP_CBRT: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.cbrt(a));\n+                case VECTOR_OP_SINH: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.sinh(a));\n+                case VECTOR_OP_COSH: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.cosh(a));\n+                case VECTOR_OP_TANH: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.tanh(a));\n+                case VECTOR_OP_EXPM1: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.expm1(a));\n+                case VECTOR_OP_LOG1P: return v0 ->\n+                        v0.uOp((i, a) -> (double) Math.log1p(a));\n@@ -642,7 +641,0 @@\n-            if (op == ATAN2) {\n-                return bOp(that, (i, a, b) -> (double) Math.atan2(a, b));\n-            } else if (op == POW) {\n-                return bOp(that, (i, a, b) -> (double) Math.pow(a, b));\n-            } else if (op == HYPOT) {\n-                return bOp(that, (i, a, b) -> (double) Math.hypot(a, b));\n-            }\n@@ -670,0 +662,6 @@\n+                case VECTOR_OP_ATAN2: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n@@ -2740,0 +2738,1 @@\n+\n@@ -2872,1 +2871,1 @@\n-     * Stores this vector into an array of {@code double}\n+     * Stores this vector into an array of type {@code double[]}\n@@ -3029,0 +3028,1 @@\n+\n@@ -3135,0 +3135,1 @@\n+\n@@ -3270,0 +3271,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":41,"deletions":39,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -552,31 +552,0 @@\n-            if (op == SIN) {\n-                return uOp((i, a) -> (float) Math.sin(a));\n-            } else if (op == COS) {\n-                return uOp((i, a) -> (float) Math.cos(a));\n-            } else if (op == TAN) {\n-                return uOp((i, a) -> (float) Math.tan(a));\n-            } else if (op == ASIN) {\n-                return uOp((i, a) -> (float) Math.asin(a));\n-            } else if (op == ACOS) {\n-                return uOp((i, a) -> (float) Math.acos(a));\n-            } else if (op == ATAN) {\n-                return uOp((i, a) -> (float) Math.atan(a));\n-            } else if (op == EXP) {\n-                return uOp((i, a) -> (float) Math.exp(a));\n-            } else if (op == LOG) {\n-                return uOp((i, a) -> (float) Math.log(a));\n-            } else if (op == LOG10) {\n-                return uOp((i, a) -> (float) Math.log10(a));\n-            } else if (op == CBRT) {\n-                return uOp((i, a) -> (float) Math.cbrt(a));\n-            } else if (op == SINH) {\n-                return uOp((i, a) -> (float) Math.sinh(a));\n-            } else if (op == COSH) {\n-                return uOp((i, a) -> (float) Math.cosh(a));\n-            } else if (op == TANH) {\n-                return uOp((i, a) -> (float) Math.tanh(a));\n-            } else if (op == EXPM1) {\n-                return uOp((i, a) -> (float) Math.expm1(a));\n-            } else if (op == LOG1P) {\n-                return uOp((i, a) -> (float) Math.log1p(a));\n-            }\n@@ -594,0 +563,18 @@\n+                case VECTOR_OP_SIN: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.sin(a));\n+                case VECTOR_OP_COS: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.cos(a));\n+                case VECTOR_OP_TAN: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.tan(a));\n+                case VECTOR_OP_ASIN: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.asin(a));\n+                case VECTOR_OP_ACOS: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.acos(a));\n+                case VECTOR_OP_ATAN: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.atan(a));\n+                case VECTOR_OP_EXP: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.exp(a));\n+                case VECTOR_OP_LOG: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.log(a));\n+                case VECTOR_OP_LOG10: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.log10(a));\n@@ -596,0 +583,12 @@\n+                case VECTOR_OP_CBRT: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.cbrt(a));\n+                case VECTOR_OP_SINH: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.sinh(a));\n+                case VECTOR_OP_COSH: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.cosh(a));\n+                case VECTOR_OP_TANH: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.tanh(a));\n+                case VECTOR_OP_EXPM1: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.expm1(a));\n+                case VECTOR_OP_LOG1P: return v0 ->\n+                        v0.uOp((i, a) -> (float) Math.log1p(a));\n@@ -642,7 +641,0 @@\n-            if (op == ATAN2) {\n-                return bOp(that, (i, a, b) -> (float) Math.atan2(a, b));\n-            } else if (op == POW) {\n-                return bOp(that, (i, a, b) -> (float) Math.pow(a, b));\n-            } else if (op == HYPOT) {\n-                return bOp(that, (i, a, b) -> (float) Math.hypot(a, b));\n-            }\n@@ -670,0 +662,6 @@\n+                case VECTOR_OP_ATAN2: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n@@ -2746,0 +2744,1 @@\n+\n@@ -2878,1 +2877,1 @@\n-     * Stores this vector into an array of {@code float}\n+     * Stores this vector into an array of type {@code float[]}\n@@ -3016,0 +3015,1 @@\n+\n@@ -3122,0 +3122,1 @@\n+\n@@ -3257,0 +3258,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":41,"deletions":39,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -455,1 +455,1 @@\n-    public static final \/*float*\/ Unary SIN = unary(\"SIN\", \"sin\", -1 \/*VectorSupport.VECTOR_OP_SIN*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary SIN = unary(\"SIN\", \"sin\", VectorSupport.VECTOR_OP_SIN, VO_ONLYFP);\n@@ -457,1 +457,1 @@\n-    public static final \/*float*\/ Unary COS = unary(\"COS\", \"cos\", -1 \/*VectorSupport.VECTOR_OP_COS*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary COS = unary(\"COS\", \"cos\", VectorSupport.VECTOR_OP_COS, VO_ONLYFP);\n@@ -459,1 +459,1 @@\n-    public static final \/*float*\/ Unary TAN = unary(\"TAN\", \"tan\", -1 \/*VectorSupport.VECTOR_OP_TAN*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary TAN = unary(\"TAN\", \"tan\", VectorSupport.VECTOR_OP_TAN, VO_ONLYFP);\n@@ -461,1 +461,1 @@\n-    public static final \/*float*\/ Unary ASIN = unary(\"ASIN\", \"asin\", -1 \/*VectorSupport.VECTOR_OP_ASIN*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary ASIN = unary(\"ASIN\", \"asin\", VectorSupport.VECTOR_OP_ASIN, VO_ONLYFP);\n@@ -463,1 +463,1 @@\n-    public static final \/*float*\/ Unary ACOS = unary(\"ACOS\", \"acos\", -1 \/*VectorSupport.VECTOR_OP_ACOS*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary ACOS = unary(\"ACOS\", \"acos\", VectorSupport.VECTOR_OP_ACOS, VO_ONLYFP);\n@@ -465,1 +465,1 @@\n-    public static final \/*float*\/ Unary ATAN = unary(\"ATAN\", \"atan\", -1 \/*VectorSupport.VECTOR_OP_ATAN*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary ATAN = unary(\"ATAN\", \"atan\", VectorSupport.VECTOR_OP_ATAN, VO_ONLYFP);\n@@ -468,1 +468,1 @@\n-    public static final \/*float*\/ Unary EXP = unary(\"EXP\", \"exp\", -1 \/*VectorSupport.VECTOR_OP_EXP*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary EXP = unary(\"EXP\", \"exp\", VectorSupport.VECTOR_OP_EXP, VO_ONLYFP);\n@@ -470,1 +470,1 @@\n-    public static final \/*float*\/ Unary LOG = unary(\"LOG\", \"log\", -1 \/*VectorSupport.VECTOR_OP_LOG*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary LOG = unary(\"LOG\", \"log\", VectorSupport.VECTOR_OP_LOG, VO_ONLYFP);\n@@ -472,1 +472,1 @@\n-    public static final \/*float*\/ Unary LOG10 = unary(\"LOG10\", \"log10\", -1 \/*VectorSupport.VECTOR_OP_LOG10*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary LOG10 = unary(\"LOG10\", \"log10\", VectorSupport.VECTOR_OP_LOG10, VO_ONLYFP);\n@@ -476,1 +476,1 @@\n-    public static final \/*float*\/ Unary CBRT = unary(\"CBRT\", \"cbrt\", -1 \/*VectorSupport.VECTOR_OP_CBRT*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary CBRT = unary(\"CBRT\", \"cbrt\", VectorSupport.VECTOR_OP_CBRT, VO_ONLYFP);\n@@ -479,1 +479,1 @@\n-    public static final \/*float*\/ Unary SINH = unary(\"SINH\", \"sinh\", -1 \/*VectorSupport.VECTOR_OP_SINH*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary SINH = unary(\"SINH\", \"sinh\", VectorSupport.VECTOR_OP_SINH, VO_ONLYFP);\n@@ -481,1 +481,1 @@\n-    public static final \/*float*\/ Unary COSH = unary(\"COSH\", \"cosh\", -1 \/*VectorSupport.VECTOR_OP_COSH*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary COSH = unary(\"COSH\", \"cosh\", VectorSupport.VECTOR_OP_COSH, VO_ONLYFP);\n@@ -483,1 +483,1 @@\n-    public static final \/*float*\/ Unary TANH = unary(\"TANH\", \"tanh\", -1 \/*VectorSupport.VECTOR_OP_TANH*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary TANH = unary(\"TANH\", \"tanh\", VectorSupport.VECTOR_OP_TANH, VO_ONLYFP);\n@@ -485,1 +485,1 @@\n-    public static final \/*float*\/ Unary EXPM1 = unary(\"EXPM1\", \"expm1\", -1 \/*VectorSupport.VECTOR_OP_EXPM1*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary EXPM1 = unary(\"EXPM1\", \"expm1\", VectorSupport.VECTOR_OP_EXPM1, VO_ONLYFP);\n@@ -487,1 +487,1 @@\n-    public static final \/*float*\/ Unary LOG1P = unary(\"LOG1P\", \"log1p\", -1 \/*VectorSupport.VECTOR_OP_LOG1P*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Unary LOG1P = unary(\"LOG1P\", \"log1p\", VectorSupport.VECTOR_OP_LOG1P, VO_ONLYFP);\n@@ -529,1 +529,1 @@\n-    public static final \/*float*\/ Binary ATAN2 = binary(\"ATAN2\", \"atan2\", -1 \/*VectorSupport.VECTOR_OP_ATAN2*\/ , VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Binary ATAN2 = binary(\"ATAN2\", \"atan2\", VectorSupport.VECTOR_OP_ATAN2, VO_ONLYFP);\n@@ -531,1 +531,1 @@\n-    public static final \/*float*\/ Binary POW = binary(\"POW\", \"pow\", -1 \/*VectorSupport.VECTOR_OP_POW*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Binary POW = binary(\"POW\", \"pow\", VectorSupport.VECTOR_OP_POW, VO_ONLYFP);\n@@ -533,1 +533,1 @@\n-    public static final \/*float*\/ Binary HYPOT = binary(\"HYPOT\", \"hypot\", -1 \/*VectorSupport.VECTOR_OP_HYPOT*\/, VO_ONLYFP | VO_SPECIAL);\n+    public static final \/*float*\/ Binary HYPOT = binary(\"HYPOT\", \"hypot\", VectorSupport.VECTOR_OP_HYPOT, VO_ONLYFP);\n@@ -1235,5 +1235,1 @@\n-                       op == BITWISE_BLEND ||\n-                       op == SIN   || op == COS   || op == TAN   || op == ASIN || op == ACOS || op == ATAN || op == EXP  ||\n-                       op == LOG   || op == LOG10 || op == SQRT  || op == CBRT || op == SINH || op == COSH || op == TANH ||\n-                       op == EXPM1 || op == LOG1P || op == ATAN2 || op == POW || op == HYPOT\n-                ) : op;\n+                       op == BITWISE_BLEND) : op;\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorOperators.java","additions":19,"deletions":23,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -572,33 +572,0 @@\n-#if[FP]\n-            if (op == SIN) {\n-                return uOp((i, a) -> ($type$) Math.sin(a));\n-            } else if (op == COS) {\n-                return uOp((i, a) -> ($type$) Math.cos(a));\n-            } else if (op == TAN) {\n-                return uOp((i, a) -> ($type$) Math.tan(a));\n-            } else if (op == ASIN) {\n-                return uOp((i, a) -> ($type$) Math.asin(a));\n-            } else if (op == ACOS) {\n-                return uOp((i, a) -> ($type$) Math.acos(a));\n-            } else if (op == ATAN) {\n-                return uOp((i, a) -> ($type$) Math.atan(a));\n-            } else if (op == EXP) {\n-                return uOp((i, a) -> ($type$) Math.exp(a));\n-            } else if (op == LOG) {\n-                return uOp((i, a) -> ($type$) Math.log(a));\n-            } else if (op == LOG10) {\n-                return uOp((i, a) -> ($type$) Math.log10(a));\n-            } else if (op == CBRT) {\n-                return uOp((i, a) -> ($type$) Math.cbrt(a));\n-            } else if (op == SINH) {\n-                return uOp((i, a) -> ($type$) Math.sinh(a));\n-            } else if (op == COSH) {\n-                return uOp((i, a) -> ($type$) Math.cosh(a));\n-            } else if (op == TANH) {\n-                return uOp((i, a) -> ($type$) Math.tanh(a));\n-            } else if (op == EXPM1) {\n-                return uOp((i, a) -> ($type$) Math.expm1(a));\n-            } else if (op == LOG1P) {\n-                return uOp((i, a) -> ($type$) Math.log1p(a));\n-            }\n-#end[FP]\n@@ -617,0 +584,18 @@\n+                case VECTOR_OP_SIN: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.sin(a));\n+                case VECTOR_OP_COS: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.cos(a));\n+                case VECTOR_OP_TAN: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.tan(a));\n+                case VECTOR_OP_ASIN: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.asin(a));\n+                case VECTOR_OP_ACOS: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.acos(a));\n+                case VECTOR_OP_ATAN: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.atan(a));\n+                case VECTOR_OP_EXP: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.exp(a));\n+                case VECTOR_OP_LOG: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.log(a));\n+                case VECTOR_OP_LOG10: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.log10(a));\n@@ -619,0 +604,12 @@\n+                case VECTOR_OP_CBRT: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.cbrt(a));\n+                case VECTOR_OP_SINH: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.sinh(a));\n+                case VECTOR_OP_COSH: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.cosh(a));\n+                case VECTOR_OP_TANH: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.tanh(a));\n+                case VECTOR_OP_EXPM1: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.expm1(a));\n+                case VECTOR_OP_LOG1P: return v0 ->\n+                        v0.uOp((i, a) -> ($type$) Math.log1p(a));\n@@ -692,9 +689,0 @@\n-#if[FP]\n-            if (op == ATAN2) {\n-                return bOp(that, (i, a, b) -> ($type$) Math.atan2(a, b));\n-            } else if (op == POW) {\n-                return bOp(that, (i, a, b) -> ($type$) Math.pow(a, b));\n-            } else if (op == HYPOT) {\n-                return bOp(that, (i, a, b) -> ($type$) Math.hypot(a, b));\n-            }\n-#end[FP]\n@@ -737,0 +725,6 @@\n+                case VECTOR_OP_ATAN2: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n+                case VECTOR_OP_POW: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n+                case VECTOR_OP_HYPOT: return (v0, v1) ->\n+                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n@@ -3369,0 +3363,152 @@\n+#if[short]\n+    \/**\n+     * Loads a vector from an array of type {@code char[]}\n+     * starting at an offset.\n+     * For each vector lane, where {@code N} is the vector lane index, the\n+     * array element at index {@code offset + N}\n+     * is first cast to a {@code short} value and then\n+     * placed into the resulting vector at lane index {@code N}.\n+     *\n+     * @param species species of desired vector\n+     * @param a the array\n+     * @param offset the offset into the array\n+     * @return the vector loaded from an array\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code offset+N < 0} or {@code offset+N >= a.length}\n+     *         for any lane {@code N} in the vector\n+     *\/\n+    @ForceInline\n+    public static\n+    $abstractvectortype$ fromCharArray(VectorSpecies<$Boxtype$> species,\n+                                       char[] a, int offset) {\n+        offset = checkFromIndexSize(offset, species.length(), a.length);\n+        $Type$Species vsp = ($Type$Species) species;\n+        return vsp.dummyVector().fromCharArray0(a, offset);\n+    }\n+\n+    \/**\n+     * Loads a vector from an array of type {@code char[]}\n+     * starting at an offset and using a mask.\n+     * Lanes where the mask is unset are filled with the default\n+     * value of {@code $type$} ({#if[FP]?positive }zero).\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * if the mask lane at index {@code N} is set then the array element at\n+     * index {@code offset + N}\n+     * is first cast to a {@code short} value and then\n+     * placed into the resulting vector at lane index\n+     * {@code N}, otherwise the default element value is placed into the\n+     * resulting vector at lane index {@code N}.\n+     *\n+     * @param species species of desired vector\n+     * @param a the array\n+     * @param offset the offset into the array\n+     * @param m the mask controlling lane selection\n+     * @return the vector loaded from an array\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code offset+N < 0} or {@code offset+N >= a.length}\n+     *         for any lane {@code N} in the vector\n+     *         where the mask is set\n+     *\/\n+    @ForceInline\n+    public static\n+    $abstractvectortype$ fromCharArray(VectorSpecies<$Boxtype$> species,\n+                                       char[] a, int offset,\n+                                       VectorMask<$Boxtype$> m) {\n+        $Type$Species vsp = ($Type$Species) species;\n+        if (offset >= 0 && offset <= (a.length - species.length())) {\n+            $abstractvectortype$ zero = vsp.zero();\n+            return zero.blend(zero.fromCharArray0(a, offset), m);\n+        }\n+\n+        \/\/ FIXME: optimize\n+        checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+        return vsp.vOp(m, i -> (short) a[offset + i]);\n+    }\n+\n+    \/**\n+     * Gathers a new vector composed of elements from an array of type\n+     * {@code char[]},\n+     * using indexes obtained by adding a fixed {@code offset} to a\n+     * series of secondary offsets from an <em>index map<\/em>.\n+     * The index map is a contiguous sequence of {@code VLENGTH}\n+     * elements in a second array of {@code int}s, starting at a given\n+     * {@code mapOffset}.\n+     * <p>\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * the lane is loaded from the expression\n+     * {@code (short) a[f(N)]}, where {@code f(N)} is the\n+     * index mapping expression\n+     * {@code offset + indexMap[mapOffset + N]]}.\n+     *\n+     * @param species species of desired vector\n+     * @param a the array\n+     * @param offset the offset into the array, may be negative if relative\n+     * indexes in the index map compensate to produce a value within the\n+     * array bounds\n+     * @param indexMap the index map\n+     * @param mapOffset the offset into the index map\n+     * @return the vector loaded from the indexed elements of the array\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code mapOffset+N < 0}\n+     *         or if {@code mapOffset+N >= indexMap.length},\n+     *         or if {@code f(N)=offset+indexMap[mapOffset+N]}\n+     *         is an invalid index into {@code a},\n+     *         for any lane {@code N} in the vector\n+     * @see $abstractvectortype$#toIntArray()\n+     *\/\n+    @ForceInline\n+    public static\n+    $abstractvectortype$ fromCharArray(VectorSpecies<$Boxtype$> species,\n+                                       char[] a, int offset,\n+                                       int[] indexMap, int mapOffset) {\n+        $Type$Species vsp = ($Type$Species) species;\n+        return vsp.vOp(n -> (short) a[offset + indexMap[mapOffset + n]]);\n+    }\n+\n+    \/**\n+     * Gathers a new vector composed of elements from an array of type\n+     * {@code char[]},\n+     * under the control of a mask, and\n+     * using indexes obtained by adding a fixed {@code offset} to a\n+     * series of secondary offsets from an <em>index map<\/em>.\n+     * The index map is a contiguous sequence of {@code VLENGTH}\n+     * elements in a second array of {@code int}s, starting at a given\n+     * {@code mapOffset}.\n+     * <p>\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * if the lane is set in the mask,\n+     * the lane is loaded from the expression\n+     * {@code (short) a[f(N)]}, where {@code f(N)} is the\n+     * index mapping expression\n+     * {@code offset + indexMap[mapOffset + N]]}.\n+     * Unset lanes in the resulting vector are set to zero.\n+     *\n+     * @param species species of desired vector\n+     * @param a the array\n+     * @param offset the offset into the array, may be negative if relative\n+     * indexes in the index map compensate to produce a value within the\n+     * array bounds\n+     * @param indexMap the index map\n+     * @param mapOffset the offset into the index map\n+     * @param m the mask controlling lane selection\n+     * @return the vector loaded from the indexed elements of the array\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code mapOffset+N < 0}\n+     *         or if {@code mapOffset+N >= indexMap.length},\n+     *         or if {@code f(N)=offset+indexMap[mapOffset+N]}\n+     *         is an invalid index into {@code a},\n+     *         for any lane {@code N} in the vector\n+     *         where the mask is set\n+     * @see $abstractvectortype$#toIntArray()\n+     *\/\n+    @ForceInline\n+    public static\n+    $abstractvectortype$ fromCharArray(VectorSpecies<$Boxtype$> species,\n+                                       char[] a, int offset,\n+                                       int[] indexMap, int mapOffset,\n+                                       VectorMask<$Boxtype$> m) {\n+        $Type$Species vsp = ($Type$Species) species;\n+        return vsp.vOp(m, n -> (short) a[offset + indexMap[mapOffset + n]]);\n+    }\n+#end[short]\n+\n@@ -3507,1 +3653,1 @@\n-     * Stores this vector into an array of {@code $type$}\n+     * Stores this vector into an array of type {@code $type$[]}\n@@ -3698,0 +3844,154 @@\n+#if[short]\n+    \/**\n+     * Stores this vector into an array of type {@code char[]}\n+     * starting at an offset.\n+     * <p>\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * the lane element at index {@code N}\n+     * is first cast to a {@code char} value and then\n+     * stored into the array element {@code a[offset+N]}.\n+     *\n+     * @param a the array, of type {@code char[]}\n+     * @param offset the offset into the array\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code offset+N < 0} or {@code offset+N >= a.length}\n+     *         for any lane {@code N} in the vector\n+     *\/\n+    @ForceInline\n+    public final\n+    void intoCharArray(char[] a, int offset) {\n+        offset = checkFromIndexSize(offset, length(), a.length);\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.store(\n+            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this,\n+            a, offset,\n+            (arr, off, v)\n+            -> v.stOp(arr, off,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+\n+    \/**\n+     * Stores this vector into an array of type {@code char[]}\n+     * starting at offset and using a mask.\n+     * <p>\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * the lane element at index {@code N}\n+     * is first cast to a {@code char} value and then\n+     * stored into the array element {@code a[offset+N]}.\n+     * If the mask lane at {@code N} is unset then the corresponding\n+     * array element {@code a[offset+N]} is left unchanged.\n+     * <p>\n+     * Array range checking is done for lanes where the mask is set.\n+     * Lanes where the mask is unset are not stored and do not need\n+     * to correspond to legitimate elements of {@code a}.\n+     * That is, unset lanes may correspond to array indexes less than\n+     * zero or beyond the end of the array.\n+     *\n+     * @param a the array, of type {@code char[]}\n+     * @param offset the offset into the array\n+     * @param m the mask controlling lane storage\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code offset+N < 0} or {@code offset+N >= a.length}\n+     *         for any lane {@code N} in the vector\n+     *         where the mask is set\n+     *\/\n+    @ForceInline\n+    public final\n+    void intoCharArray(char[] a, int offset,\n+                       VectorMask<$Boxtype$> m) {\n+        if (m.allTrue()) {\n+            intoCharArray(a, offset);\n+        } else {\n+            \/\/ FIXME: optimize\n+            $Type$Species vsp = vspecies();\n+            checkMaskFromIndexSize(offset, vsp, m, 1, a.length);\n+            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+        }\n+    }\n+\n+    \/**\n+     * Scatters this vector into an array of type {@code char[]}\n+     * using indexes obtained by adding a fixed {@code offset} to a\n+     * series of secondary offsets from an <em>index map<\/em>.\n+     * The index map is a contiguous sequence of {@code VLENGTH}\n+     * elements in a second array of {@code int}s, starting at a given\n+     * {@code mapOffset}.\n+     * <p>\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * the lane element at index {@code N}\n+     * is first cast to a {@code char} value and then\n+     * stored into the array\n+     * element {@code a[f(N)]}, where {@code f(N)} is the\n+     * index mapping expression\n+     * {@code offset + indexMap[mapOffset + N]]}.\n+     *\n+     * @param a the array\n+     * @param offset an offset to combine with the index map offsets\n+     * @param indexMap the index map\n+     * @param mapOffset the offset into the index map\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code mapOffset+N < 0}\n+     *         or if {@code mapOffset+N >= indexMap.length},\n+     *         or if {@code f(N)=offset+indexMap[mapOffset+N]}\n+     *         is an invalid index into {@code a},\n+     *         for any lane {@code N} in the vector\n+     * @see $abstractvectortype$#toIntArray()\n+     *\/\n+    @ForceInline\n+    public final\n+    void intoCharArray(char[] a, int offset,\n+                       int[] indexMap, int mapOffset) {\n+        stOp(a, offset,\n+             (arr, off, i, e) -> {\n+                 int j = indexMap[mapOffset + i];\n+                 arr[off + j] = (char) e;\n+             });\n+    }\n+\n+    \/**\n+     * Scatters this vector into an array of type {@code char[]},\n+     * under the control of a mask, and\n+     * using indexes obtained by adding a fixed {@code offset} to a\n+     * series of secondary offsets from an <em>index map<\/em>.\n+     * The index map is a contiguous sequence of {@code VLENGTH}\n+     * elements in a second array of {@code int}s, starting at a given\n+     * {@code mapOffset}.\n+     * <p>\n+     * For each vector lane, where {@code N} is the vector lane index,\n+     * if the mask lane at index {@code N} is set then\n+     * the lane element at index {@code N}\n+     * is first cast to a {@code char} value and then\n+     * stored into the array\n+     * element {@code a[f(N)]}, where {@code f(N)} is the\n+     * index mapping expression\n+     * {@code offset + indexMap[mapOffset + N]]}.\n+     *\n+     * @param a the array\n+     * @param offset an offset to combine with the index map offsets\n+     * @param indexMap the index map\n+     * @param mapOffset the offset into the index map\n+     * @param m the mask\n+     * @throws IndexOutOfBoundsException\n+     *         if {@code mapOffset+N < 0}\n+     *         or if {@code mapOffset+N >= indexMap.length},\n+     *         or if {@code f(N)=offset+indexMap[mapOffset+N]}\n+     *         is an invalid index into {@code a},\n+     *         for any lane {@code N} in the vector\n+     *         where the mask is set\n+     * @see $abstractvectortype$#toIntArray()\n+     *\/\n+    @ForceInline\n+    public final\n+    void intoCharArray(char[] a, int offset,\n+                       int[] indexMap, int mapOffset,\n+                       VectorMask<$Boxtype$> m) {\n+        stOp(a, offset, m,\n+             (arr, off, i, e) -> {\n+                 int j = indexMap[mapOffset + i];\n+                 arr[off + j] = (char) e;\n+             });\n+    }\n+#end[short]\n+\n@@ -3804,0 +4104,17 @@\n+#if[short]\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ fromCharArray0Template(char[] a, int offset) {\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.load(\n+            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            a, offset, vsp,\n+            (arr, off, s) -> s.ldOp(arr, off,\n+                                    (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n+#end[short]\n+\n@@ -3941,0 +4258,12 @@\n+#if[short]\n+    static final int ARRAY_CHAR_SHIFT =\n+            31 - Integer.numberOfLeadingZeros(Unsafe.ARRAY_CHAR_INDEX_SCALE);\n+    static final long ARRAY_CHAR_BASE =\n+            Unsafe.ARRAY_CHAR_BASE_OFFSET;\n+\n+    @ForceInline\n+    static long charArrayAddress(char[] a, int index) {\n+        return ARRAY_CHAR_BASE + (((long)index) << ARRAY_CHAR_SHIFT);\n+    }\n+#end[short]\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":372,"deletions":43,"binary":false,"changes":415,"status":"modified"},{"patch":"@@ -1483,15 +1483,15 @@\n-generate(SpecialCases, [[\"ccmn\",   \"__ ccmn(zr, zr, 3u, Assembler::LE);\",                \"ccmn\\txzr, xzr, #3, LE\"],\n-                        [\"ccmnw\",  \"__ ccmnw(zr, zr, 5u, Assembler::EQ);\",               \"ccmn\\twzr, wzr, #5, EQ\"],\n-                        [\"ccmp\",   \"__ ccmp(zr, 1, 4u, Assembler::NE);\",                 \"ccmp\\txzr, 1, #4, NE\"],\n-                        [\"ccmpw\",  \"__ ccmpw(zr, 2, 2, Assembler::GT);\",                 \"ccmp\\twzr, 2, #2, GT\"],\n-                        [\"extr\",   \"__ extr(zr, zr, zr, 0);\",                            \"extr\\txzr, xzr, xzr, 0\"],\n-                        [\"stlxp\",  \"__ stlxp(r0, zr, zr, sp);\",                          \"stlxp\\tw0, xzr, xzr, [sp]\"],\n-                        [\"stlxpw\", \"__ stlxpw(r2, zr, zr, r3);\",                         \"stlxp\\tw2, wzr, wzr, [x3]\"],\n-                        [\"stxp\",   \"__ stxp(r4, zr, zr, r5);\",                           \"stxp\\tw4, xzr, xzr, [x5]\"],\n-                        [\"stxpw\",  \"__ stxpw(r6, zr, zr, sp);\",                          \"stxp\\tw6, wzr, wzr, [sp]\"],\n-                        [\"dup\",    \"__ dup(v0, __ T16B, zr);\",                           \"dup\\tv0.16b, wzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T1D, 0, zr);\",                         \"mov\\tv1.d[0], xzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T2S, 1, zr);\",                         \"mov\\tv1.s[1], wzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T4H, 2, zr);\",                         \"mov\\tv1.h[2], wzr\"],\n-                        [\"mov\",    \"__ mov(v1, __ T8B, 3, zr);\",                         \"mov\\tv1.b[3], wzr\"],\n-                        [\"ld1\",    \"__ ld1(v31, v0, __ T2D, Address(__ post(r1, r0)));\", \"ld1\\t{v31.2d, v0.2d}, [x1], x0\"],\n+generate(SpecialCases, [[\"ccmn\",    \"__ ccmn(zr, zr, 3u, Assembler::LE);\",                \"ccmn\\txzr, xzr, #3, LE\"],\n+                        [\"ccmnw\",   \"__ ccmnw(zr, zr, 5u, Assembler::EQ);\",               \"ccmn\\twzr, wzr, #5, EQ\"],\n+                        [\"ccmp\",    \"__ ccmp(zr, 1, 4u, Assembler::NE);\",                 \"ccmp\\txzr, 1, #4, NE\"],\n+                        [\"ccmpw\",   \"__ ccmpw(zr, 2, 2, Assembler::GT);\",                 \"ccmp\\twzr, 2, #2, GT\"],\n+                        [\"extr\",    \"__ extr(zr, zr, zr, 0);\",                            \"extr\\txzr, xzr, xzr, 0\"],\n+                        [\"stlxp\",   \"__ stlxp(r0, zr, zr, sp);\",                          \"stlxp\\tw0, xzr, xzr, [sp]\"],\n+                        [\"stlxpw\",  \"__ stlxpw(r2, zr, zr, r3);\",                         \"stlxp\\tw2, wzr, wzr, [x3]\"],\n+                        [\"stxp\",    \"__ stxp(r4, zr, zr, r5);\",                           \"stxp\\tw4, xzr, xzr, [x5]\"],\n+                        [\"stxpw\",   \"__ stxpw(r6, zr, zr, sp);\",                          \"stxp\\tw6, wzr, wzr, [sp]\"],\n+                        [\"dup\",     \"__ dup(v0, __ T16B, zr);\",                           \"dup\\tv0.16b, wzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T1D, 0, zr);\",                         \"mov\\tv1.d[0], xzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T2S, 1, zr);\",                         \"mov\\tv1.s[1], wzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T4H, 2, zr);\",                         \"mov\\tv1.h[2], wzr\"],\n+                        [\"mov\",     \"__ mov(v1, __ T8B, 3, zr);\",                         \"mov\\tv1.b[3], wzr\"],\n+                        [\"ld1\",     \"__ ld1(v31, v0, __ T2D, Address(__ post(r1, r0)));\", \"ld1\\t{v31.2d, v0.2d}, [x1], x0\"],\n@@ -1499,33 +1499,79 @@\n-                        [\"cpy\",    \"__ sve_cpy(z0, __ S, p0, v1);\",                      \"mov\\tz0.s, p0\/m, s1\"],\n-                        [\"inc\",    \"__ sve_inc(r0, __ S);\",                              \"incw\\tx0\"],\n-                        [\"dec\",    \"__ sve_dec(r1, __ H);\",                              \"dech\\tx1\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ B, z1, 7);\",                       \"lsl\\tz0.b, z1.b, #7\"],\n-                        [\"lsl\",    \"__ sve_lsl(z21, __ H, z1, 15);\",                     \"lsl\\tz21.h, z1.h, #15\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ S, z1, 31);\",                      \"lsl\\tz0.s, z1.s, #31\"],\n-                        [\"lsl\",    \"__ sve_lsl(z0, __ D, z1, 63);\",                      \"lsl\\tz0.d, z1.d, #63\"],\n-                        [\"lsr\",    \"__ sve_lsr(z0, __ B, z1, 7);\",                       \"lsr\\tz0.b, z1.b, #7\"],\n-                        [\"asr\",    \"__ sve_asr(z0, __ H, z11, 15);\",                     \"asr\\tz0.h, z11.h, #15\"],\n-                        [\"lsr\",    \"__ sve_lsr(z30, __ S, z1, 31);\",                     \"lsr\\tz30.s, z1.s, #31\"],\n-                        [\"asr\",    \"__ sve_asr(z0, __ D, z1, 63);\",                      \"asr\\tz0.d, z1.d, #63\"],\n-                        [\"addvl\",  \"__ sve_addvl(sp, r0, 31);\",                          \"addvl\\tsp, x0, #31\"],\n-                        [\"addpl\",  \"__ sve_addpl(r1, sp, -32);\",                         \"addpl\\tx1, sp, -32\"],\n-                        [\"cntp\",   \"__ sve_cntp(r8, __ B, p0, p1);\",                     \"cntp\\tx8, p0, p1.b\"],\n-                        [\"dup\",    \"__ sve_dup(z0, __ B, 127);\",                         \"dup\\tz0.b, 127\"],\n-                        [\"dup\",    \"__ sve_dup(z1, __ H, -128);\",                        \"dup\\tz1.h, -128\"],\n-                        [\"dup\",    \"__ sve_dup(z2, __ S, 32512);\",                       \"dup\\tz2.s, 32512\"],\n-                        [\"dup\",    \"__ sve_dup(z7, __ D, -32768);\",                      \"dup\\tz7.d, -32768\"],\n-                        [\"ld1b\",   \"__ sve_ld1b(z0, __ B, p0, Address(sp));\",            \"ld1b\\t{z0.b}, p0\/z, [sp]\"],\n-                        [\"ld1h\",   \"__ sve_ld1h(z10, __ H, p1, Address(sp, -8));\",       \"ld1h\\t{z10.h}, p1\/z, [sp, #-8, MUL VL]\"],\n-                        [\"ld1w\",   \"__ sve_ld1w(z20, __ S, p2, Address(r0, 7));\",        \"ld1w\\t{z20.s}, p2\/z, [x0, #7, MUL VL]\"],\n-                        [\"ld1b\",   \"__ sve_ld1b(z30, __ B, p3, Address(sp, r8));\",       \"ld1b\\t{z30.b}, p3\/z, [sp, x8]\"],\n-                        [\"ld1w\",   \"__ sve_ld1w(z0, __ S, p4, Address(sp, r28));\",       \"ld1w\\t{z0.s}, p4\/z, [sp, x28, LSL #2]\"],\n-                        [\"ld1d\",   \"__ sve_ld1d(z11, __ D, p5, Address(r0, r1));\",       \"ld1d\\t{z11.d}, p5\/z, [x0, x1, LSL #3]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z22, __ B, p6, Address(sp));\",           \"st1b\\t{z22.b}, p6, [sp]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z31, __ B, p7, Address(sp, -8));\",       \"st1b\\t{z31.b}, p7, [sp, #-8, MUL VL]\"],\n-                        [\"st1w\",   \"__ sve_st1w(z0, __ S, p1, Address(r0, 7));\",         \"st1w\\t{z0.s}, p1, [x0, #7, MUL VL]\"],\n-                        [\"st1b\",   \"__ sve_st1b(z0, __ B, p2, Address(sp, r1));\",        \"st1b\\t{z0.b}, p2, [sp, x1]\"],\n-                        [\"st1h\",   \"__ sve_st1h(z0, __ H, p3, Address(sp, r8));\",        \"st1h\\t{z0.h}, p3, [sp, x8, LSL #1]\"],\n-                        [\"st1d\",   \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n-                        [\"ldr\",    \"__ sve_ldr(z0, Address(sp));\",                       \"ldr\\tz0, [sp]\"],\n-                        [\"ldr\",    \"__ sve_ldr(z31, Address(sp, -256));\",                \"ldr\\tz31, [sp, #-256, MUL VL]\"],\n-                        [\"str\",    \"__ sve_str(z8, Address(r8, 255));\",                  \"str\\tz8, [x8, #255, MUL VL]\"],\n+                        [\"cpy\",     \"__ sve_cpy(z0, __ S, p0, v1);\",                      \"mov\\tz0.s, p0\/m, s1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z0, __ B, p0, 127, true);\",               \"mov\\tz0.b, p0\/m, 127\"],\n+                        [\"cpy\",     \"__ sve_cpy(z1, __ H, p0, -128, true);\",              \"mov\\tz1.h, p0\/m, -128\"],\n+                        [\"cpy\",     \"__ sve_cpy(z2, __ S, p0, 32512, true);\",             \"mov\\tz2.s, p0\/m, 32512\"],\n+                        [\"cpy\",     \"__ sve_cpy(z5, __ D, p0, -32768, false);\",           \"mov\\tz5.d, p0\/z, -32768\"],\n+                        [\"cpy\",     \"__ sve_cpy(z10, __ B, p0, -1, false);\",              \"mov\\tz10.b, p0\/z, -1\"],\n+                        [\"cpy\",     \"__ sve_cpy(z11, __ S, p0, -1, false);\",              \"mov\\tz11.s, p0\/z, -1\"],\n+                        [\"inc\",     \"__ sve_inc(r0, __ S);\",                              \"incw\\tx0\"],\n+                        [\"dec\",     \"__ sve_dec(r1, __ H);\",                              \"dech\\tx1\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, z1, 7);\",                       \"lsl\\tz0.b, z1.b, #7\"],\n+                        [\"lsl\",     \"__ sve_lsl(z21, __ H, z1, 15);\",                     \"lsl\\tz21.h, z1.h, #15\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ S, z1, 31);\",                      \"lsl\\tz0.s, z1.s, #31\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ D, z1, 63);\",                      \"lsl\\tz0.d, z1.d, #63\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, z1, 7);\",                       \"lsr\\tz0.b, z1.b, #7\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ H, z11, 15);\",                     \"asr\\tz0.h, z11.h, #15\"],\n+                        [\"lsr\",     \"__ sve_lsr(z30, __ S, z1, 31);\",                     \"lsr\\tz30.s, z1.s, #31\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ D, z1, 63);\",                      \"asr\\tz0.d, z1.d, #63\"],\n+                        [\"addvl\",   \"__ sve_addvl(sp, r0, 31);\",                          \"addvl\\tsp, x0, #31\"],\n+                        [\"addpl\",   \"__ sve_addpl(r1, sp, -32);\",                         \"addpl\\tx1, sp, -32\"],\n+                        [\"cntp\",    \"__ sve_cntp(r8, __ B, p0, p1);\",                     \"cntp\\tx8, p0, p1.b\"],\n+                        [\"dup\",     \"__ sve_dup(z0, __ B, 127);\",                         \"dup\\tz0.b, 127\"],\n+                        [\"dup\",     \"__ sve_dup(z1, __ H, -128);\",                        \"dup\\tz1.h, -128\"],\n+                        [\"dup\",     \"__ sve_dup(z2, __ S, 32512);\",                       \"dup\\tz2.s, 32512\"],\n+                        [\"dup\",     \"__ sve_dup(z7, __ D, -32768);\",                      \"dup\\tz7.d, -32768\"],\n+                        [\"dup\",     \"__ sve_dup(z10, __ B, -1);\",                         \"dup\\tz10.b, -1\"],\n+                        [\"dup\",     \"__ sve_dup(z11, __ S, -1);\",                         \"dup\\tz11.s, -1\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ B, p0, Address(sp));\",            \"ld1b\\t{z0.b}, p0\/z, [sp]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ H, p1, Address(sp));\",            \"ld1b\\t{z0.h}, p1\/z, [sp]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ S, p2, Address(sp, r8));\",        \"ld1b\\t{z0.s}, p2\/z, [sp, x8]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z0, __ D, p3, Address(sp, 7));\",         \"ld1b\\t{z0.d}, p3\/z, [sp, #7, MUL VL]\"],\n+                        [\"ld1h\",    \"__ sve_ld1h(z10, __ H, p1, Address(sp, -8));\",       \"ld1h\\t{z10.h}, p1\/z, [sp, #-8, MUL VL]\"],\n+                        [\"ld1w\",    \"__ sve_ld1w(z20, __ S, p2, Address(r0, 7));\",        \"ld1w\\t{z20.s}, p2\/z, [x0, #7, MUL VL]\"],\n+                        [\"ld1b\",    \"__ sve_ld1b(z30, __ B, p3, Address(sp, r8));\",       \"ld1b\\t{z30.b}, p3\/z, [sp, x8]\"],\n+                        [\"ld1w\",    \"__ sve_ld1w(z0, __ S, p4, Address(sp, r28));\",       \"ld1w\\t{z0.s}, p4\/z, [sp, x28, LSL #2]\"],\n+                        [\"ld1d\",    \"__ sve_ld1d(z11, __ D, p5, Address(r0, r1));\",       \"ld1d\\t{z11.d}, p5\/z, [x0, x1, LSL #3]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z22, __ B, p6, Address(sp));\",           \"st1b\\t{z22.b}, p6, [sp]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z31, __ B, p7, Address(sp, -8));\",       \"st1b\\t{z31.b}, p7, [sp, #-8, MUL VL]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ H, p1, Address(sp));\",            \"st1b\\t{z0.h}, p1, [sp]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ S, p2, Address(sp, r8));\",        \"st1b\\t{z0.s}, p2, [sp, x8]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ D, p3, Address(sp));\",            \"st1b\\t{z0.d}, p3, [sp]\"],\n+                        [\"st1w\",    \"__ sve_st1w(z0, __ S, p1, Address(r0, 7));\",         \"st1w\\t{z0.s}, p1, [x0, #7, MUL VL]\"],\n+                        [\"st1b\",    \"__ sve_st1b(z0, __ B, p2, Address(sp, r1));\",        \"st1b\\t{z0.b}, p2, [sp, x1]\"],\n+                        [\"st1h\",    \"__ sve_st1h(z0, __ H, p3, Address(sp, r8));\",        \"st1h\\t{z0.h}, p3, [sp, x8, LSL #1]\"],\n+                        [\"st1d\",    \"__ sve_st1d(z0, __ D, p4, Address(r0, r17));\",       \"st1d\\t{z0.d}, p4, [x0, x17, LSL #3]\"],\n+                        [\"ldr\",     \"__ sve_ldr(z0, Address(sp));\",                       \"ldr\\tz0, [sp]\"],\n+                        [\"ldr\",     \"__ sve_ldr(z31, Address(sp, -256));\",                \"ldr\\tz31, [sp, #-256, MUL VL]\"],\n+                        [\"str\",     \"__ sve_str(z8, Address(r8, 255));\",                  \"str\\tz8, [x8, #255, MUL VL]\"],\n+                        [\"sel\",     \"__ sve_sel(z0, __ B, p0, z1, z2);\",                  \"sel\\tz0.b, p0, z1.b, z2.b\"],\n+                        [\"sel\",     \"__ sve_sel(z4, __ D, p0, z5, z6);\",                  \"sel\\tz4.d, p0, z5.d, z6.d\"],\n+                        [\"cmpeq\",   \"__ sve_cmpeq(p1, __ B, p0, z0, z1);\",                \"cmpeq\\tp1.b, p0\/z, z0.b, z1.b\"],\n+                        [\"cmpne\",   \"__ sve_cmpne(p1, __ H, p0, z2, z3);\",                \"cmpne\\tp1.h, p0\/z, z2.h, z3.h\"],\n+                        [\"cmpge\",   \"__ sve_cmpge(p1, __ S, p2, z4, z5);\",                \"cmpge\\tp1.s, p2\/z, z4.s, z5.s\"],\n+                        [\"cmpgt\",   \"__ sve_cmpgt(p1, __ D, p3, z6, z7);\",                \"cmpgt\\tp1.d, p3\/z, z6.d, z7.d\"],\n+                        [\"cmple\",   \"__ sve_cmpge(p2, __ B, p0, z10, z11);\",              \"cmple\\tp2.b, p0\/z, z11.b, z10.b\"],\n+                        [\"cmplt\",   \"__ sve_cmpgt(p3, __ S, p0, z16, z17);\",              \"cmplt\\tp3.s, p0\/z, z17.s, z16.s\"],\n+                        [\"cmpeq\",   \"__ sve_cmpeq(p1, __ B, p4, z0, 15);\",                \"cmpeq\\tp1.b, p4\/z, z0.b, #15\"],\n+                        [\"cmpne\",   \"__ sve_cmpne(p1, __ H, p0, z2, -16);\",               \"cmpne\\tp1.h, p0\/z, z2.h, #-16\"],\n+                        [\"cmple\",   \"__ sve_cmple(p1, __ S, p1, z4, 0);\",                 \"cmple\\tp1.s, p1\/z, z4.s, #0\"],\n+                        [\"cmplt\",   \"__ sve_cmplt(p1, __ D, p2, z6, -1);\",                \"cmplt\\tp1.d, p2\/z, z6.d, #-1\"],\n+                        [\"cmpge\",   \"__ sve_cmpge(p1, __ S, p3, z4, 5);\",                 \"cmpge\\tp1.s, p3\/z, z4.s, #5\"],\n+                        [\"cmpgt\",   \"__ sve_cmpgt(p1, __ B, p4, z6, -2);\",                \"cmpgt\\tp1.b, p4\/z, z6.b, #-2\"],\n+                        [\"fcmeq\",   \"__ sve_fcmeq(p1, __ S, p0, z0, z1);\",                \"fcmeq\\tp1.s, p0\/z, z0.s, z1.s\"],\n+                        [\"fcmne\",   \"__ sve_fcmne(p1, __ D, p0, z2, z3);\",                \"fcmne\\tp1.d, p0\/z, z2.d, z3.d\"],\n+                        [\"fcmgt\",   \"__ sve_fcmgt(p1, __ S, p2, z4, z5);\",                \"fcmgt\\tp1.s, p2\/z, z4.s, z5.s\"],\n+                        [\"fcmge\",   \"__ sve_fcmge(p1, __ D, p3, z6, z7);\",                \"fcmge\\tp1.d, p3\/z, z6.d, z7.d\"],\n+                        [\"fcmlt\",   \"__ sve_fcmgt(p2, __ S, p0, z10, z11);\",              \"fcmlt\\tp2.s, p0\/z, z11.s, z10.s\"],\n+                        [\"fcmle\",   \"__ sve_fcmge(p3, __ D, p0, z16, z17);\",              \"fcmle\\tp3.d, p0\/z, z17.d, z16.d\"],\n+                        [\"uunpkhi\", \"__ sve_uunpkhi(z0, __ H, z1);\",                      \"uunpkhi\\tz0.h, z1.b\"],\n+                        [\"uunpklo\", \"__ sve_uunpklo(z4, __ S, z5);\",                      \"uunpklo\\tz4.s, z5.h\"],\n+                        [\"sunpkhi\", \"__ sve_sunpkhi(z6, __ D, z7);\",                      \"sunpkhi\\tz6.d, z7.s\"],\n+                        [\"sunpklo\", \"__ sve_sunpklo(z10, __ H, z11);\",                    \"sunpklo\\tz10.h, z11.b\"],\n+                        [\"whilelt\", \"__ sve_whilelt(p0, __ B, r1, r2);\",                  \"whilelt\\tp0.b, x1, x2\"],\n+                        [\"whilelt\", \"__ sve_whileltw(p1, __ H, r3, r4);\",                 \"whilelt\\tp1.h, w3, w4\"],\n+                        [\"whilele\", \"__ sve_whilele(p2, __ S, r5, r6);\",                  \"whilele\\tp2.s, x5, x6\"],\n+                        [\"whilele\", \"__ sve_whilelew(p3, __ D, r10, r11);\",               \"whilele\\tp3.d, w10, w11\"],\n+                        [\"whilelo\", \"__ sve_whilelo(p4, __ B, r1, r2);\",                  \"whilelo\\tp4.b, x1, x2\"],\n+                        [\"whilelo\", \"__ sve_whilelow(p0, __ H, r3, r4);\",                 \"whilelo\\tp0.h, w3, w4\"],\n+                        [\"whilels\", \"__ sve_whilels(p1, __ S, r5, r6);\",                  \"whilels\\tp1.s, x5, x6\"],\n+                        [\"whilels\", \"__ sve_whilelsw(p2, __ D, r10, r11);\",               \"whilels\\tp2.d, w10, w11\"],\n@@ -1600,0 +1646,2 @@\n+                       [\"uzp1\", \"ZZZ\"],\n+                       [\"uzp2\", \"ZZZ\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":96,"deletions":48,"binary":false,"changes":144,"status":"modified"}]}