{"files":[{"patch":"@@ -75,2 +75,2 @@\n-      \"directory\": \"$(strip $2)\"$(COMMA) \\\n-      \"file\": \"$(strip $3)\"$(COMMA) \\\n+      \"directory\": \"$(strip $(call FixPathArgs, $2))\"$(COMMA) \\\n+      \"file\": \"$(strip $(call FixPathArgs, $3))\"$(COMMA) \\\n@@ -78,1 +78,1 @@\n-        $(subst $(FIXPATH),,$4))))\" \\\n+        $(subst $(FIXPATH),,$(call FixPathArgs, $4)))))\" \\\n@@ -240,2 +240,4 @@\n-  # When compiling with relative paths, the deps file comes out with relative\n-  # paths.\n+  # When compiling with relative paths, the deps file may come out with relative\n+  # paths, and that path may start with '.\/'. First remove any leading .\/, then\n+  # add WORKSPACE_ROOT to any line not starting with \/, while allowing for\n+  # leading spaces.\n@@ -243,1 +245,4 @@\n-\t$(SED) -e 's|^\\([ ]*\\)|\\1$(WORKSPACE_ROOT)|' $1.tmp > $1\n+\t$(SED) \\\n+\t    -e 's|^\\([ ]*\\)\\.\/|\\1|' \\\n+\t    -e '\/^[ ]*[^\/ ]\/s|^\\([ ]*\\)|\\1$(WORKSPACE_ROOT)\/|' \\\n+\t    $1.tmp > $1\n@@ -815,1 +820,2 @@\n-            $$($1_OPT_CFLAGS) -x c++-header -c $(C_FLAG_DEPS) $$($1_PCH_DEPS_FILE)\n+            $$($1_OPT_CFLAGS) -x c++-header -c $(C_FLAG_DEPS) \\\n+            $$(addsuffix .tmp, $$($1_PCH_DEPS_FILE))\n@@ -822,0 +828,1 @@\n+\t\t$$(call fix-deps-file, $$($1_PCH_DEPS_FILE))\n","filename":"make\/common\/NativeCompilation.gmk","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1773,0 +1773,5 @@\n+int MachCallNativeNode::ret_addr_offset() {\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -575,1 +575,1 @@\n-\/\/ vector max\n+\/\/ vector min\/max\n@@ -577,29 +577,2 @@\n-instruct vmaxF(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_fmax $dst_src1, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n-  ins_encode %{\n-    __ sve_fmax(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vmaxD(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_fmax $dst_src1, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n-  ins_encode %{\n-    __ sve_fmax(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vminF(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 4 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+instruct vmin(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n@@ -608,1 +581,1 @@\n-  format %{ \"sve_fmin $dst_src1, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -610,2 +583,10 @@\n-    __ sve_fmin(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"Unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    }\n@@ -616,4 +597,3 @@\n-instruct vminD(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= 2 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+instruct vmax(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MaxV dst_src1 src2));\n@@ -621,1 +601,1 @@\n-  format %{ \"sve_fmin $dst_src1, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -623,2 +603,10 @@\n-    __ sve_fmin(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"Unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":28,"deletions":40,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -431,8 +431,5 @@\n-dnl\n-dnl BINARY_OP_TRUE_PREDICATE_ETYPE($1,        $2,      $3,           $4,   $5,          $6  )\n-dnl BINARY_OP_TRUE_PREDICATE_ETYPE(insn_name, op_name, element_type, size, min_vec_len, insn)\n-define(`BINARY_OP_TRUE_PREDICATE_ETYPE', `\n-instruct $1(vReg dst_src1, vReg src2) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length() >= $5 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == $3);\n-  match(Set dst_src1 ($2 dst_src1 src2));\n+\/\/ vector min\/max\n+\n+instruct vmin(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MinV dst_src1 src2));\n@@ -440,1 +437,1 @@\n-  format %{ \"$6 $dst_src1, $dst_src1, $src2\\t # vector (sve) ($4)\" %}\n+  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -442,2 +439,10 @@\n-    __ $6(as_FloatRegister($dst_src1$$reg), __ $4,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"Unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    }\n@@ -446,7 +451,21 @@\n-%}')dnl\n-dnl\n-\/\/ vector max\n-BINARY_OP_TRUE_PREDICATE_ETYPE(vmaxF, MaxV, T_FLOAT,  S, 4,  sve_fmax)\n-BINARY_OP_TRUE_PREDICATE_ETYPE(vmaxD, MaxV, T_DOUBLE, D, 2,  sve_fmax)\n-BINARY_OP_TRUE_PREDICATE_ETYPE(vminF, MinV, T_FLOAT,  S, 4,  sve_fmin)\n-BINARY_OP_TRUE_PREDICATE_ETYPE(vminD, MinV, T_DOUBLE, D, 2,  sve_fmin)\n+%}\n+\n+instruct vmax(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() >= 16);\n+  match(Set dst_src1 (MaxV dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"Unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":37,"deletions":18,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -207,1 +207,1 @@\n-    uint32_t mask = (1U << nbits) - 1;\n+    uint32_t mask = checked_cast<uint32_t>(right_n_bits(nbits));\n@@ -222,1 +222,1 @@\n-    unsigned mask = (1U << nbits) - 1;\n+    unsigned mask = checked_cast<unsigned>(right_n_bits(nbits));\n@@ -236,1 +236,1 @@\n-    unsigned mask = (1U << nbits) - 1;\n+    unsigned mask = checked_cast<unsigned>(right_n_bits(nbits));\n@@ -248,1 +248,1 @@\n-    guarantee(val < (1U << nbits), \"Field too big for insn\");\n+    guarantee(val < (1ULL << nbits), \"Field too big for insn\");\n@@ -250,1 +250,1 @@\n-    unsigned mask = (1U << nbits) - 1;\n+    unsigned mask = checked_cast<unsigned>(right_n_bits(nbits));\n@@ -269,1 +269,1 @@\n-    unsigned mask = (1U << nbits) - 1;\n+    unsigned mask = checked_cast<unsigned>(right_n_bits(nbits));\n@@ -302,1 +302,1 @@\n-    unsigned mask = ((1U << nbits) - 1) << lsb;\n+    unsigned mask = checked_cast<unsigned>(right_n_bits(nbits)) << lsb;\n@@ -507,2 +507,1 @@\n-          if (_ext.shift() > 0)\n-            assert(_ext.shift() == (int)size, \"bad shift\");\n+          assert(_ext.shift() <= 0 || _ext.shift() == (int)size, \"bad shift\");\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1206,1 +1206,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic) {\n+  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n@@ -3080,0 +3080,7 @@\n+\n+BufferBlob* SharedRuntime::make_native_invoker(address call_target,\n+                                           int shadow_space_bytes,\n+                                           const GrowableArray<VMReg>& input_registers,\n+                                           const GrowableArray<VMReg>& output_registers) {\n+  return NULL;\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1908,0 +1908,8 @@\n+\n+BufferBlob* SharedRuntime::make_native_invoker(address call_target,\n+                                               int shadow_space_bytes,\n+                                               const GrowableArray<VMReg>& input_registers,\n+                                               const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1118,0 +1118,2 @@\n+  if (rule() == CallRuntimeDirect_rule) {\n+    \/\/ CallRuntimeDirectNode uses call_c.\n@@ -1119,1 +1121,1 @@\n-  return 28;\n+    return 28;\n@@ -1121,1 +1123,1 @@\n-  return 40;\n+    return 40;\n@@ -1123,0 +1125,9 @@\n+  }\n+  assert(rule() == CallLeafDirect_rule, \"unexpected node with rule %u\", rule());\n+  \/\/ CallLeafDirectNode uses bl.\n+  return 4;\n+}\n+\n+int MachCallNativeNode::ret_addr_offset() {\n+  Unimplemented();\n+  return -1;\n@@ -1518,1 +1529,1 @@\n-    __ std(return_pc, _abi(lr), callers_sp);\n+    __ std(return_pc, _abi0(lr), callers_sp);\n@@ -1564,1 +1575,1 @@\n-    __ ld(return_pc, ((int)framesize) + _abi(lr), R1_SP);\n+    __ ld(return_pc, ((int)framesize) + _abi0(lr), R1_SP);\n@@ -3590,0 +3601,1 @@\n+    call->_guaranteed_safepoint = true;\n@@ -3790,0 +3802,1 @@\n+    call->_guaranteed_safepoint = false;\n@@ -3797,1 +3810,0 @@\n-\n@@ -14512,1 +14524,1 @@\n-    __ ld(R4_ARG2\/* issuing pc *\/, _abi(lr), R1_SP);\n+    __ ld(R4_ARG2\/* issuing pc *\/, _abi0(lr), R1_SP);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":18,"deletions":6,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2012, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 2012, 2020 SAP SE. All rights reserved.\n@@ -288,1 +288,1 @@\n-  __ std(R30, frame_size_in_bytes + _abi(cr), R1_SP);\n+  __ std(R30, frame_size_in_bytes + _abi0(cr), R1_SP);\n@@ -299,1 +299,1 @@\n-    __ std(R31, frame_size_in_bytes + _abi(lr), R1_SP);\n+    __ std(R31, frame_size_in_bytes + _abi0(lr), R1_SP);\n@@ -429,1 +429,1 @@\n-  __ ld(R31, frame_size_in_bytes + _abi(lr), R1_SP);\n+  __ ld(R31, frame_size_in_bytes + _abi0(lr), R1_SP);\n@@ -432,1 +432,1 @@\n-  __ ld(R31, frame_size_in_bytes + _abi(cr), R1_SP);\n+  __ ld(R31, frame_size_in_bytes + _abi0(cr), R1_SP);\n@@ -974,1 +974,1 @@\n-  __ std(return_pc, _abi(lr), R1_SP);\n+  __ std(return_pc, _abi0(lr), R1_SP);\n@@ -980,1 +980,1 @@\n-  __ ld(return_pc, _abi(lr), R1_SP);\n+  __ ld(return_pc, _abi0(lr), R1_SP);\n@@ -1629,1 +1629,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic) {\n+  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n@@ -2548,1 +2548,1 @@\n-  __ std(pc_reg, _abi(lr), R1_SP);\n+  __ std(pc_reg, _abi0(lr), R1_SP);\n@@ -2619,1 +2619,1 @@\n-  __ std(R12_scratch2, _abi(lr), R1_SP);\n+  __ std(R12_scratch2, _abi0(lr), R1_SP);\n@@ -2647,1 +2647,1 @@\n-  __ std(R0, _abi(lr), R1_SP);\n+  __ std(R0, _abi0(lr), R1_SP);\n@@ -2725,1 +2725,1 @@\n-  __ std(R4_ARG2, _abi(lr), R1_SP);\n+  __ std(R4_ARG2, _abi0(lr), R1_SP);\n@@ -3086,1 +3086,1 @@\n-    __ ld(R0, frame_size_in_bytes + _abi(lr), R1_SP);\n+    __ ld(R0, frame_size_in_bytes + _abi0(lr), R1_SP);\n@@ -3092,1 +3092,1 @@\n-    __ std(R31, frame_size_in_bytes + _abi(lr), R1_SP);\n+    __ std(R31, frame_size_in_bytes + _abi0(lr), R1_SP);\n@@ -3441,0 +3441,8 @@\n+\n+BufferBlob* SharedRuntime::make_native_invoker(address call_target,\n+                                               int shadow_space_bytes,\n+                                               const GrowableArray<VMReg>& input_registers,\n+                                               const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":22,"deletions":14,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -642,0 +642,5 @@\n+int MachCallNativeNode::ret_addr_offset() {\n+  Unimplemented();\n+  return -1;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -930,1 +930,2 @@\n-    guarantee(special_dispatch == vmIntrinsics::_invokeBasic, \"special_dispatch=%d\", special_dispatch);\n+    guarantee(special_dispatch == vmIntrinsics::_invokeBasic || special_dispatch == vmIntrinsics::_linkToNative,\n+              \"special_dispatch=%d\", special_dispatch);\n@@ -3474,0 +3475,8 @@\n+\n+BufferBlob* SharedRuntime::make_native_invoker(address call_target,\n+                                               int shadow_space_bytes,\n+                                               const GrowableArray<VMReg>& input_registers,\n+                                               const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2996,0 +2996,8 @@\n+\n+BufferBlob* SharedRuntime::make_native_invoker(address call_target,\n+                                                int shadow_space_bytes,\n+                                                const GrowableArray<VMReg>& input_registers,\n+                                                const GrowableArray<VMReg>& output_registers) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -92,3 +92,4 @@\n-#define DEF_XMM_OFFS(regnum) xmm ## regnum ## _off = xmm_off + (regnum)*16\/BytesPerInt, xmm ## regnum ## H_off\n-#define DEF_YMM_OFFS(regnum) ymm ## regnum ## _off = ymm_off + (regnum)*16\/BytesPerInt, ymm ## regnum ## H_off\n-#define DEF_ZMM_OFFS(regnum) zmm ## regnum ## _off = zmm_off + (regnum-16)*64\/BytesPerInt, zmm ## regnum ## H_off\n+#define DEF_XMM_OFFS(regnum)       xmm ## regnum ## _off = xmm_off + (regnum)*16\/BytesPerInt, xmm ## regnum ## H_off\n+#define DEF_YMM_OFFS(regnum)       ymm ## regnum ## _off = ymm_off + (regnum)*16\/BytesPerInt, ymm ## regnum ## H_off\n+#define DEF_ZMM_OFFS(regnum)       zmm ## regnum ## _off = zmm_off + (regnum)*32\/BytesPerInt, zmm ## regnum ## H_off\n+#define DEF_ZMM_UPPER_OFFS(regnum) zmm ## regnum ## _off = zmm_upper_off + (regnum-16)*64\/BytesPerInt, zmm ## regnum ## H_off\n@@ -105,4 +106,6 @@\n-    zmm_high = xmm_off + (XSAVE_AREA_ZMM_BEGIN - XSAVE_AREA_BEGIN)\/BytesPerInt,\n-    zmm_off = xmm_off + (XSAVE_AREA_UPPERBANK - XSAVE_AREA_BEGIN)\/BytesPerInt,\n-    DEF_ZMM_OFFS(16),\n-    DEF_ZMM_OFFS(17),\n+    zmm_off = xmm_off + (XSAVE_AREA_ZMM_BEGIN - XSAVE_AREA_BEGIN)\/BytesPerInt,\n+    DEF_ZMM_OFFS(0),\n+    DEF_ZMM_OFFS(1),\n+    zmm_upper_off = xmm_off + (XSAVE_AREA_UPPERBANK - XSAVE_AREA_BEGIN)\/BytesPerInt,\n+    DEF_ZMM_UPPER_OFFS(16),\n+    DEF_ZMM_UPPER_OFFS(17),\n@@ -140,1 +143,1 @@\n-  static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_vectors = false);\n+  static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_vectors);\n@@ -165,3 +168,2 @@\n-  if (save_vectors) {\n-    assert(UseAVX > 0, \"Vectors larger than 16 byte long are supported only with AVX\");\n-    assert(MaxVectorSize <= 64, \"Only up to 64 byte long vectors are supported\");\n+  if (save_vectors && UseAVX == 0) {\n+    save_vectors = false; \/\/ vectors larger than 16 byte long are supported only with AVX\n@@ -169,0 +171,1 @@\n+  assert(!save_vectors || MaxVectorSize <= 64, \"Only up to 64 byte long vectors are supported\");\n@@ -170,1 +173,1 @@\n-  assert(!save_vectors, \"vectors are generated only by C2 and JVMCI\");\n+  save_vectors = false; \/\/ vectors are generated only by C2 and JVMCI\n@@ -262,1 +265,1 @@\n-  if(UseAVX > 2) {\n+  if (UseAVX > 2) {\n@@ -275,0 +278,1 @@\n+    \/\/ Save upper half of YMM registers(0..15)\n@@ -276,1 +280,1 @@\n-    int delta = ymm1_off - off;\n+    delta = ymm1_off - ymm0_off;\n@@ -282,0 +286,10 @@\n+    if (VM_Version::supports_evex()) {\n+      \/\/ Save upper half of ZMM registers(0..15)\n+      off = zmm0_off;\n+      delta = zmm1_off - zmm0_off;\n+      for (int n = 0; n < 16; n++) {\n+        XMMRegister zmm_name = as_XMMRegister(n);\n+        map->set_callee_saved(STACK_OFFSET(off), zmm_name->as_VMReg()->next(8));\n+        off += delta;\n+      }\n+    }\n@@ -1657,1 +1671,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic) {\n+  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n@@ -2634,0 +2648,3 @@\n+  if (UseAVX > 2) {\n+    pad += 1024;\n+  }\n@@ -2639,1 +2656,1 @@\n-  CodeBuffer buffer(\"deopt_blob\", 2048+pad, 1024);\n+  CodeBuffer buffer(\"deopt_blob\", 2560+pad, 1024);\n@@ -2681,1 +2698,1 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n@@ -2699,1 +2716,1 @@\n-  (void) RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+  (void) RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n@@ -2718,1 +2735,1 @@\n-    RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+    RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n@@ -2765,1 +2782,1 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n@@ -3367,1 +3384,2 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+  \/\/ No need to save vector registers since they are caller-saved anyway.\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n@@ -3427,0 +3445,210 @@\n+static const int native_invoker_code_size = MethodHandles::adapter_code_size;\n+\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  address _call_target;\n+  int _shadow_space_bytes;\n+\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         address call_target,\n+                         int shadow_space_bytes,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _call_target(call_target),\n+     _shadow_space_bytes(shadow_space_bytes),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers) {}\n+  void generate();\n+\n+  void spill_register(VMReg reg) {\n+    assert(reg->is_reg(), \"must be a register\");\n+    MacroAssembler* masm = _masm;\n+    if (reg->is_Register()) {\n+      __ push(reg->as_Register());\n+    } else if (reg->is_XMMRegister()) {\n+      if (UseAVX >= 3) {\n+        __ subptr(rsp, 64); \/\/ bytes\n+        __ evmovdqul(Address(rsp, 0), reg->as_XMMRegister(), Assembler::AVX_512bit);\n+      } else if (UseAVX >= 1) {\n+        __ subptr(rsp, 32);\n+        __ vmovdqu(Address(rsp, 0), reg->as_XMMRegister());\n+      } else {\n+        __ subptr(rsp, 16);\n+        __ movdqu(Address(rsp, 0), reg->as_XMMRegister());\n+      }\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+  void fill_register(VMReg reg) {\n+    assert(reg->is_reg(), \"must be a register\");\n+    MacroAssembler* masm = _masm;\n+    if (reg->is_Register()) {\n+      __ pop(reg->as_Register());\n+    } else if (reg->is_XMMRegister()) {\n+      if (UseAVX >= 3) {\n+        __ evmovdqul(reg->as_XMMRegister(), Address(rsp, 0), Assembler::AVX_512bit);\n+        __ addptr(rsp, 64); \/\/ bytes\n+      } else if (UseAVX >= 1) {\n+        __ vmovdqu(reg->as_XMMRegister(), Address(rsp, 0));\n+        __ addptr(rsp, 32);\n+      } else {\n+        __ movdqu(reg->as_XMMRegister(), Address(rsp, 0));\n+        __ addptr(rsp, 16);\n+      }\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+private:\n+#ifdef ASSERT\n+bool target_uses_register(VMReg reg) {\n+  return _input_registers.contains(reg) || _output_registers.contains(reg);\n+}\n+#endif\n+};\n+\n+BufferBlob* SharedRuntime::make_native_invoker(address call_target,\n+                                               int shadow_space_bytes,\n+                                               const GrowableArray<VMReg>& input_registers,\n+                                               const GrowableArray<VMReg>& output_registers) {\n+  BufferBlob* _invoke_native_blob = BufferBlob::create(\"nep_invoker_blob\", native_invoker_code_size);\n+  if (_invoke_native_blob == NULL)\n+    return NULL; \/\/ allocation failure\n+\n+  CodeBuffer code(_invoke_native_blob);\n+  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  return _invoke_native_blob;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  assert(!(target_uses_register(r15_thread->as_VMReg()) || target_uses_register(rscratch1->as_VMReg())), \"Register conflict\");\n+\n+  MacroAssembler* masm = _masm;\n+  __ enter();\n+\n+  Address java_pc(r15_thread, JavaThread::last_Java_pc_offset());\n+  __ movptr(rscratch1, Address(rsp, 8)); \/\/ read return address from stack\n+  __ movptr(java_pc, rscratch1);\n+\n+  __ movptr(rscratch1, rsp);\n+  __ addptr(rscratch1, 16); \/\/ skip return and frame\n+  __ movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), rscratch1);\n+\n+  __ movptr(Address(r15_thread, JavaThread::saved_rbp_address_offset()), rsp); \/\/ rsp points at saved RBP\n+\n+    \/\/ State transition\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+\n+  if (_shadow_space_bytes != 0) {\n+    \/\/ needed here for correct stack args offset on Windows\n+    __ subptr(rsp, _shadow_space_bytes);\n+  }\n+\n+  __ call(RuntimeAddress(_call_target));\n+\n+  if (_shadow_space_bytes != 0) {\n+    \/\/ needed here for correct stack args offset on Windows\n+    __ addptr(rsp, _shadow_space_bytes);\n+  }\n+\n+  assert(_output_registers.length() <= 1\n+    || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n+  bool need_spills = _output_registers.length() != 0;\n+  VMReg ret_reg = need_spills ? _output_registers.at(0) : VMRegImpl::Bad();\n+\n+  __ restore_cpu_control_state_after_jni();\n+\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::Membar_mask_bits(\n+          Assembler::LoadLoad | Assembler::LoadStore |\n+          Assembler::StoreLoad | Assembler::StoreStore));\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n+  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ jcc(Assembler::equal, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(r15_thread, true);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+  __ vzeroupper();\n+\n+  if (need_spills) {\n+    spill_register(ret_reg);\n+  }\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  if (need_spills) {\n+    fill_register(ret_reg);\n+  }\n+\n+  __ jmp(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+  __ vzeroupper();\n+\n+  if (need_spills) {\n+    spill_register(ret_reg);\n+  }\n+\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  if (need_spills) {\n+    fill_register(ret_reg);\n+  }\n+\n+  __ jmp(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":249,"deletions":21,"binary":false,"changes":270,"status":"modified"},{"patch":"@@ -846,20 +846,0 @@\n-  \/\/ Support for intptr_t get_previous_fp()\n-  \/\/\n-  \/\/ This routine is used to find the previous frame pointer for the\n-  \/\/ caller (current_frame_guess). This is used as part of debugging\n-  \/\/ ps() is seemingly lost trying to find frames.\n-  \/\/ This code assumes that caller current_frame_guess) has a frame.\n-  address generate_get_previous_fp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"get_previous_fp\");\n-    const Address old_fp(rbp, 0);\n-    const Address older_fp(rax, 0);\n-    address start = __ pc();\n-\n-    __ enter();\n-    __ movptr(rax, old_fp); \/\/ callers fp\n-    __ movptr(rax, older_fp); \/\/ the frame for ps()\n-    __ pop(rbp);\n-    __ ret(0);\n-\n-    return start;\n-  }\n@@ -6997,1 +6977,0 @@\n-    StubRoutines::x86::_get_previous_fp_entry = generate_get_previous_fp();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":21,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1524,0 +1524,7 @@\n+    case Op_VectorMaskGen:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (UseAVX < 3) {\n+        return false;\n+      }\n+      break;\n@@ -1597,0 +1604,10 @@\n+    case Op_VectorMaskGen:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -3717,0 +3734,4 @@\n+    } else if (VM_Version::supports_avx2()) {\n+      int vlen_enc = vector_length_encoding(this);\n+      __ movdl($dst$$XMMRegister, $src$$Register);\n+      __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -3796,0 +3817,4 @@\n+    } else if (VM_Version::supports_avx2()) {\n+      int vlen_enc = vector_length_encoding(this);\n+      __ movdl($dst$$XMMRegister, $src$$Register);\n+      __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -3871,0 +3896,4 @@\n+    } else if (VM_Version::supports_avx2()) {\n+      int vlen_enc = vector_length_encoding(this);\n+      __ movdl($dst$$XMMRegister, $src$$Register);\n+      __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -3966,0 +3995,5 @@\n+    } else if (VM_Version::supports_avx2()) {\n+      assert(vlen == 4, \"sanity\");\n+      int vlen_enc = vector_length_encoding(this);\n+      __ movdq($dst$$XMMRegister, $src$$Register);\n+      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n@@ -7902,0 +7936,47 @@\n+#ifdef _LP64\n+\/\/ ---------------------------------- Masked Block Copy ------------------------------------\n+\n+instruct vmasked_load64(vec dst, memory mem, rRegL mask) %{\n+  match(Set dst (LoadVectorMasked mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(this);\n+    __ kmovql(k2, $mask$$Register);\n+    __ evmovdqu(elmType, k2, $dst$$XMMRegister, $mem$$Address, vector_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_gen(rRegL dst, rRegL len, rRegL tempLen) %{\n+  match(Set dst (VectorMaskGen len));\n+  effect(TEMP_DEF dst, TEMP tempLen);\n+  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  ins_encode %{\n+    __ genmask($dst$$Register, $len$$Register, $tempLen$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_gen_imm(rRegL dst, immL len) %{\n+  match(Set dst (VectorMaskGen len));\n+  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, (0xFFFFFFFFFFFFFFFFUL >> (64 -$len$$constant)));\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmasked_store64(memory mem, vec src, rRegL mask) %{\n+  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(src_node);\n+    __ kmovql(k2, $mask$$Register);\n+    __ evmovdqu(elmType, k2, $mem$$Address, $src$$XMMRegister, vector_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -311,1 +311,6 @@\n-  return sizeof_FFree_Float_Stack_All + 5 + pre_call_resets_size();\n+  return 5 + pre_call_resets_size() + (_leaf_no_fp ? 0 : sizeof_FFree_Float_Stack_All);\n+}\n+\n+int MachCallNativeNode::ret_addr_offset() {\n+  ShouldNotCallThis();\n+  return -1;\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -460,0 +460,5 @@\n+int MachCallNativeNode::ret_addr_offset() {\n+  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n+  offset += clear_avx_size();\n+  return offset;\n+}\n@@ -12432,0 +12437,12 @@\n+\/\/\n+instruct CallNativeDirect(method meth)\n+%{\n+  match(CallNative);\n+  effect(USE meth);\n+\n+  ins_cost(300);\n+  format %{ \"call_native \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -424,0 +424,2 @@\n+  if(_matrule->find_type(\"CallNative\",idx))       return Form::JAVA_NATIVE;\n+  idx = 0;\n@@ -784,0 +786,1 @@\n+       !strcmp(_matrule->_rChild->_opType,\"VectorMaskGen\")||\n@@ -1136,0 +1139,3 @@\n+  else if( is_ideal_call() == Form::JAVA_NATIVE ) {\n+    return \"MachCallNativeNode\";\n+  }\n@@ -3489,1 +3495,1 @@\n-    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\",\n+    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n@@ -4181,1 +4187,1 @@\n-    \"VectorMaskWrapper\", \"VectorMaskCmp\", \"VectorReinterpret\",\n+    \"VectorMaskWrapper\", \"VectorMaskCmp\", \"VectorReinterpret\",\"LoadVectorMasked\",\"StoreVectorMasked\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -83,0 +83,4 @@\n+  product(intx, ArrayCopyPartialInlineSize, -1, DIAGNOSTIC,                 \\\n+          \"Partial inline size used for array copy acceleration.\")          \\\n+          range(-1, 64)                                                     \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,0 +43,2 @@\n+#include \"ci\/ciNativeEntryPoint.hpp\"\n+#include \"utilities\/debug.hpp\"\n@@ -1157,0 +1159,25 @@\n+class NativeCallGenerator : public CallGenerator {\n+private:\n+  ciNativeEntryPoint* _nep;\n+public:\n+  NativeCallGenerator(ciMethod* m, ciNativeEntryPoint* nep)\n+   : CallGenerator(m), _nep(nep) {}\n+\n+  virtual JVMState* generate(JVMState* jvms);\n+};\n+\n+JVMState* NativeCallGenerator::generate(JVMState* jvms) {\n+  GraphKit kit(jvms);\n+\n+  Node* call = kit.make_native_call(tf(), method()->arg_size(), _nep); \/\/ -fallback, - nep\n+  if (call == NULL) return NULL;\n+\n+  kit.C->print_inlining_update(this);\n+  address addr = _nep->entry_point();\n+  if (kit.C->log() != NULL) {\n+    kit.C->log()->elem(\"l2n_intrinsification_success bci='%d' entry_point='\" INTPTR_FORMAT \"'\", jvms->bci(), p2i(addr));\n+  }\n+\n+  return kit.transfer_exceptions_into_jvms();\n+}\n+\n@@ -1279,0 +1306,14 @@\n+    case vmIntrinsics::_linkToNative:\n+    {\n+      Node* nep = kit.argument(callee->arg_size() - 1);\n+      if (nep->Opcode() == Op_ConP) {\n+        const TypeOopPtr* oop_ptr = nep->bottom_type()->is_oopptr();\n+        ciNativeEntryPoint* nep = oop_ptr->const_oop()->as_native_entry_point();\n+        return new NativeCallGenerator(callee, nep);\n+      } else {\n+        print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                               \"NativeEntryPoint not constant\");\n+      }\n+    }\n+    break;\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -409,0 +410,12 @@\n+\/\/---------------------print_method_with_lineno--------------------------------\n+void JVMState::print_method_with_lineno(outputStream* st, bool show_name) const {\n+  if (show_name) _method->print_short_name(st);\n+\n+  int lineno = _method->line_number_from_bci(_bci);\n+  if (lineno != -1) {\n+    st->print(\" @ bci:%d (line %d)\", _bci, lineno);\n+  } else {\n+    st->print(\" @ bci:%d\", _bci);\n+  }\n+}\n+\n@@ -413,2 +426,1 @@\n-    _method->print_short_name(st);\n-    st->print(\" @ bci:%d \",_bci);\n+    print_method_with_lineno(st, true);\n@@ -540,3 +552,1 @@\n-    if (!printed)\n-      _method->print_short_name(st);\n-    st->print(\" @ bci:%d\",_bci);\n+    print_method_with_lineno(st, !printed);\n@@ -1184,0 +1194,65 @@\n+\/\/=============================================================================\n+uint CallNativeNode::size_of() const { return sizeof(*this); }\n+bool CallNativeNode::cmp( const Node &n ) const {\n+  CallNativeNode &call = (CallNativeNode&)n;\n+  return CallNode::cmp(call) && !strcmp(_name,call._name)\n+    && _arg_regs == call._arg_regs && _ret_regs == call._ret_regs;\n+}\n+Node* CallNativeNode::match(const ProjNode *proj, const Matcher *matcher) {\n+  switch (proj->_con) {\n+    case TypeFunc::Control:\n+    case TypeFunc::I_O:\n+    case TypeFunc::Memory:\n+      return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n+    case TypeFunc::ReturnAdr:\n+    case TypeFunc::FramePtr:\n+      ShouldNotReachHere();\n+    case TypeFunc::Parms: {\n+      const Type* field_at_con = tf()->range()->field_at(proj->_con);\n+      const BasicType bt = field_at_con->basic_type();\n+      OptoReg::Name optoreg = OptoReg::as_OptoReg(_ret_regs.at(proj->_con - TypeFunc::Parms));\n+      OptoRegPair regs;\n+      if (bt == T_DOUBLE || bt == T_LONG) {\n+        regs.set2(optoreg);\n+      } else {\n+        regs.set1(optoreg);\n+      }\n+      RegMask rm = RegMask(regs.first());\n+      if(OptoReg::is_valid(regs.second()))\n+        rm.Insert(regs.second());\n+      return new MachProjNode(this, proj->_con, rm, field_at_con->ideal_reg());\n+    }\n+    case TypeFunc::Parms + 1: {\n+      assert(tf()->range()->field_at(proj->_con) == Type::HALF, \"Expected HALF\");\n+      assert(_ret_regs.at(proj->_con - TypeFunc::Parms) == VMRegImpl::Bad(), \"Unexpected register for Type::HALF\");\n+      \/\/ 2nd half of doubles and longs\n+      return new MachProjNode(this, proj->_con, RegMask::Empty, (uint) OptoReg::Bad);\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+#ifndef PRODUCT\n+void CallNativeNode::print_regs(const GrowableArray<VMReg>& regs, outputStream* st) {\n+  st->print(\"{ \");\n+  for (int i = 0; i < regs.length(); i++) {\n+    regs.at(i)->print_on(st);\n+    if (i < regs.length() - 1) {\n+      st->print(\", \");\n+    }\n+  }\n+  st->print(\" } \");\n+}\n+\n+void CallNativeNode::dump_spec(outputStream *st) const {\n+  st->print(\"# \");\n+  st->print(\"%s \", _name);\n+  st->print(\"_arg_regs: \");\n+  print_regs(_arg_regs, st);\n+  st->print(\"_ret_regs: \");\n+  print_regs(_ret_regs, st);\n+  CallNode::dump_spec(st);\n+}\n+#endif\n+\n@@ -1204,0 +1279,34 @@\n+void CallNativeNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n+  assert((tf()->domain()->cnt() - TypeFunc::Parms) == argcnt, \"arg counts must match!\");\n+#ifdef ASSERT\n+  for (uint i = 0; i < argcnt; i++) {\n+    assert(tf()->domain()->field_at(TypeFunc::Parms + i)->basic_type() == sig_bt[i], \"types must match!\");\n+  }\n+#endif\n+  for (uint i = 0; i < argcnt; i++) {\n+    switch (sig_bt[i]) {\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT:\n+      case T_FLOAT:\n+        parm_regs[i].set1(_arg_regs.at(i));\n+        break;\n+      case T_LONG:\n+      case T_DOUBLE:\n+        assert((i + 1) < argcnt && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        parm_regs[i].set2(_arg_regs.at(i));\n+        break;\n+      case T_VOID: \/\/ Halves of longs and doubles\n+        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+        assert(_arg_regs.at(i) == VMRegImpl::Bad(), \"expecting bad reg\");\n+        parm_regs[i].set_bad();\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":114,"deletions":5,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -52,0 +53,1 @@\n+class     CallNativeNode;\n@@ -310,0 +312,1 @@\n+  void      print_method_with_lineno(outputStream* st, bool show_name) const;\n@@ -812,0 +815,36 @@\n+\/\/------------------------------CallNativeNode-----------------------------------\n+\/\/ Make a direct call into a foreign function with an arbitrary ABI\n+\/\/ safepoints\n+class CallNativeNode : public CallNode {\n+  friend class MachCallNativeNode;\n+  virtual bool cmp( const Node &n ) const;\n+  virtual uint size_of() const;\n+  static void print_regs(const GrowableArray<VMReg>& regs, outputStream* st);\n+public:\n+  GrowableArray<VMReg> _arg_regs;\n+  GrowableArray<VMReg> _ret_regs;\n+  const int _shadow_space_bytes;\n+  const bool _need_transition;\n+\n+  CallNativeNode(const TypeFunc* tf, address addr, const char* name,\n+                 const TypePtr* adr_type,\n+                 const GrowableArray<VMReg>& arg_regs,\n+                 const GrowableArray<VMReg>& ret_regs,\n+                 int shadow_space_bytes,\n+                 bool need_transition)\n+    : CallNode(tf, addr, adr_type), _arg_regs(arg_regs),\n+      _ret_regs(ret_regs), _shadow_space_bytes(shadow_space_bytes),\n+      _need_transition(need_transition)\n+  {\n+    init_class_id(Class_CallNative);\n+    _name = name;\n+  }\n+  virtual int   Opcode() const;\n+  virtual bool  guaranteed_safepoint()  { return _need_transition; }\n+  virtual Node* match(const ProjNode *proj, const Matcher *m);\n+  virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;\n+#ifndef PRODUCT\n+  virtual void  dump_spec(outputStream *st) const;\n+#endif\n+};\n+\n@@ -821,0 +860,1 @@\n+    init_class_id(Class_CallLeafNoFP);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,0 +63,1 @@\n+macro(CallNative)\n@@ -410,0 +411,3 @@\n+macro(LoadVectorMasked)\n+macro(StoreVectorMasked)\n+macro(VectorMaskGen)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -556,0 +556,1 @@\n+                  _native_invokers(comp_arena(), 1, 0, NULL),\n@@ -852,0 +853,1 @@\n+    _native_invokers(),\n@@ -971,1 +973,1 @@\n-    if (method_has_option(\"NoRTMLockEliding\") || ((rtm_state & NoRTM) != 0)) {\n+    if (method_has_option(CompileCommand::NoRTMLockEliding) || ((rtm_state & NoRTM) != 0)) {\n@@ -974,1 +976,1 @@\n-    } else if (method_has_option(\"UseRTMLockEliding\") || ((rtm_state & UseRTM) != 0) || !UseRTMDeopt) {\n+    } else if (method_has_option(CompileCommand::UseRTMLockEliding) || ((rtm_state & UseRTM) != 0) || !UseRTMDeopt) {\n@@ -2962,0 +2964,1 @@\n+  case Op_CallNative:\n@@ -3313,4 +3316,3 @@\n-    if (OptimizeStringConcat || IncrementalInline || IncrementalInlineVirtual) {\n-      ProjNode* proj = n->as_Proj();\n-      if (proj->_is_io_use) {\n-        assert(proj->_con == TypeFunc::I_O || proj->_con == TypeFunc::Memory, \"\");\n+    if (OptimizeStringConcat) {\n+      ProjNode* p = n->as_Proj();\n+      if (p->_is_io_use) {\n@@ -3320,4 +3322,13 @@\n-        \/\/ the original one. Merge them.\n-        Node* non_io_proj = proj->in(0)->as_Multi()->proj_out_or_null(proj->_con, false \/*is_io_use*\/);\n-        if (non_io_proj  != NULL) {\n-          proj->subsume_by(non_io_proj , this);\n+        \/\/ the original one.\n+        Node* proj = NULL;\n+        \/\/ Replace with just one\n+        for (SimpleDUIterator i(p->in(0)); i.has_next(); i.next()) {\n+          Node *use = i.get();\n+          if (use->is_Proj() && p != use && use->as_Proj()->_con == p->_con) {\n+            proj = use;\n+            break;\n+          }\n+        }\n+        assert(proj != NULL || p->_con == TypeFunc::I_O, \"io may be dropped at an infinite loop\");\n+        if (proj != NULL) {\n+          p->subsume_by(proj, this);\n@@ -3405,0 +3416,3 @@\n+  case Op_VectorMaskGen:\n+  case Op_LoadVectorMasked:\n+  case Op_StoreVectorMasked:\n@@ -4179,1 +4193,1 @@\n-    if (!cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+    if (!cg->is_late_inline()) {\n@@ -4233,3 +4247,1 @@\n-        bool is_virtual = cg->is_virtual_late_inline();\n-        const char* msg = (is_virtual ? \"virtual call\"\n-                                      : \"live nodes > LiveNodeCountInliningCutoff\");\n+        const char* msg = \"live nodes > LiveNodeCountInliningCutoff\";\n@@ -4751,0 +4763,3 @@\n+void Compile::add_native_invoker(BufferBlob* stub) {\n+  _native_invokers.append(stub);\n+}\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":29,"deletions":14,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -388,0 +388,1 @@\n+  GrowableArray<BufferBlob*>    _native_invokers;\n@@ -596,1 +597,1 @@\n-  bool          method_has_option(const char * option) {\n+  bool          method_has_option(enum CompileCommand option) {\n@@ -939,0 +940,4 @@\n+  void add_native_invoker(BufferBlob* stub);\n+\n+  const GrowableArray<BufferBlob*>& native_invokers() const { return _native_invokers; }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,0 +28,3 @@\n+#include \"ci\/ciNativeEntryPoint.hpp\"\n+#include \"ci\/ciObjArray.hpp\"\n+#include \"asm\/register.hpp\"\n@@ -50,0 +53,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -2566,0 +2570,122 @@\n+\/\/ i2b\n+Node* GraphKit::sign_extend_byte(Node* in) {\n+  Node* tmp = _gvn.transform(new LShiftINode(in, _gvn.intcon(24)));\n+  return _gvn.transform(new RShiftINode(tmp, _gvn.intcon(24)));\n+}\n+\n+\/\/ i2s\n+Node* GraphKit::sign_extend_short(Node* in) {\n+  Node* tmp = _gvn.transform(new LShiftINode(in, _gvn.intcon(16)));\n+  return _gvn.transform(new RShiftINode(tmp, _gvn.intcon(16)));\n+}\n+\n+\/\/-----------------------------make_native_call-------------------------------\n+Node* GraphKit::make_native_call(const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep) {\n+  uint n_filtered_args = nargs - 2; \/\/ -fallback, -nep;\n+  ResourceMark rm;\n+  Node** argument_nodes = NEW_RESOURCE_ARRAY(Node*, n_filtered_args);\n+  const Type** arg_types = TypeTuple::fields(n_filtered_args);\n+  GrowableArray<VMReg> arg_regs(C->comp_arena(), n_filtered_args, n_filtered_args, VMRegImpl::Bad());\n+\n+  VMReg* argRegs = nep->argMoves();\n+  {\n+    for (uint vm_arg_pos = 0, java_arg_read_pos = 0;\n+        vm_arg_pos < n_filtered_args; vm_arg_pos++) {\n+      uint vm_unfiltered_arg_pos = vm_arg_pos + 1; \/\/ +1 to skip fallback handle argument\n+      Node* node = argument(vm_unfiltered_arg_pos);\n+      const Type* type = call_type->domain()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n+      VMReg reg = type == Type::HALF\n+        ? VMRegImpl::Bad()\n+        : argRegs[java_arg_read_pos++];\n+\n+      argument_nodes[vm_arg_pos] = node;\n+      arg_types[TypeFunc::Parms + vm_arg_pos] = type;\n+      arg_regs.at_put(vm_arg_pos, reg);\n+    }\n+  }\n+\n+  uint n_returns = call_type->range()->cnt() - TypeFunc::Parms;\n+  GrowableArray<VMReg> ret_regs(C->comp_arena(), n_returns, n_returns, VMRegImpl::Bad());\n+  const Type** ret_types = TypeTuple::fields(n_returns);\n+\n+  VMReg* retRegs = nep->returnMoves();\n+  {\n+    for (uint vm_ret_pos = 0, java_ret_read_pos = 0;\n+        vm_ret_pos < n_returns; vm_ret_pos++) { \/\/ 0 or 1\n+      const Type* type = call_type->range()->field_at(TypeFunc::Parms + vm_ret_pos);\n+      VMReg reg = type == Type::HALF\n+        ? VMRegImpl::Bad()\n+        : retRegs[java_ret_read_pos++];\n+\n+      ret_regs.at_put(vm_ret_pos, reg);\n+      ret_types[TypeFunc::Parms + vm_ret_pos] = type;\n+    }\n+  }\n+\n+  const TypeFunc* new_call_type = TypeFunc::make(\n+    TypeTuple::make(TypeFunc::Parms + n_filtered_args, arg_types),\n+    TypeTuple::make(TypeFunc::Parms + n_returns, ret_types)\n+  );\n+\n+  address call_addr = nep->entry_point();\n+  if (nep->need_transition()) {\n+    BufferBlob* invoker = SharedRuntime::make_native_invoker(call_addr,\n+                                                             nep->shadow_space(),\n+                                                             arg_regs, ret_regs);\n+    if (invoker == NULL) {\n+      C->record_failure(\"native invoker not implemented on this platform\");\n+      return NULL;\n+    }\n+    C->add_native_invoker(invoker);\n+    call_addr = invoker->code_begin();\n+  }\n+  assert(call_addr != NULL, \"sanity\");\n+\n+  CallNativeNode* call = new CallNativeNode(new_call_type, call_addr, nep->name(), TypePtr::BOTTOM,\n+                                            arg_regs,\n+                                            ret_regs,\n+                                            nep->shadow_space(),\n+                                            nep->need_transition());\n+\n+  if (call->_need_transition) {\n+    add_safepoint_edges(call);\n+  }\n+\n+  set_predefined_input_for_runtime_call(call);\n+\n+  for (uint i = 0; i < n_filtered_args; i++) {\n+    call->init_req(i + TypeFunc::Parms, argument_nodes[i]);\n+  }\n+\n+  Node* c = gvn().transform(call);\n+  assert(c == call, \"cannot disappear\");\n+\n+  set_predefined_output_for_runtime_call(call);\n+\n+  Node* ret;\n+  if (method() == NULL || method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else {\n+    ret =  gvn().transform(new ProjNode(call, TypeFunc::Parms));\n+    \/\/ Unpack native results if needed\n+    \/\/ Need this method type since it's unerased\n+    switch (nep->method_type()->rtype()->basic_type()) {\n+      case T_CHAR:\n+        ret = _gvn.transform(new AndINode(ret, _gvn.intcon(0xFFFF)));\n+        break;\n+      case T_BYTE:\n+        ret = sign_extend_byte(ret);\n+        break;\n+      case T_SHORT:\n+        ret = sign_extend_short(ret);\n+        break;\n+      default: \/\/ do nothing\n+        break;\n+    }\n+  }\n+\n+  push_node(method()->return_type()->basic_type(), ret);\n+\n+  return call;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":126,"deletions":0,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -801,0 +801,6 @@\n+\n+  Node* sign_extend_byte(Node* in);\n+  Node* sign_extend_short(Node* in);\n+\n+  Node* make_native_call(const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep);\n+\n@@ -893,0 +899,5 @@\n+\n+  \/\/ Vector API support (implemented in vectorIntrinsics.cpp)\n+  Node* box_vector(Node* in, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool deoptimize_on_exception = false);\n+  Node* unbox_vector(Node* in, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool shuffle_to_vector = false);\n+  Node* vector_shift_count(Node* cnt, int shift_op, BasicType bt, int num_elem);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -690,0 +690,1 @@\n+        case Op_StoreVectorMasked:\n@@ -866,0 +867,6 @@\n+    case Op_CallNative:\n+      \/\/ We use the c reg save policy here since Panama\n+      \/\/ only supports the C ABI currently.\n+      \/\/ TODO compute actual save policy based on nep->abi\n+      save_policy = _matcher._c_reg_save_policy;\n+      break;\n@@ -879,1 +886,8 @@\n-  bool exclude_soe = op == Op_CallRuntime;\n+  \/\/\n+  \/\/ Also, native callees can not save oops, so we kill the SOE registers\n+  \/\/ here in case a native call has a safepoint. This doesn't work for\n+  \/\/ RBP though, which seems to be special-cased elsewhere to always be\n+  \/\/ treated as alive, so we instead manually save the location of RBP\n+  \/\/ before doing the native call (see NativeInvokerGenerator::generate).\n+  bool exclude_soe = op == Op_CallRuntime\n+    || (op == Op_CallNative && mcall->guaranteed_safepoint());\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -184,2 +184,2 @@\n-  Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);\n-  Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);\n+  Node* load_field_from_object(Node* fromObj, const char* fieldName, const char* fieldTypeString, DecoratorSet decorators, bool is_static, ciInstanceKlass* fromKls);\n+  Node* field_address_from_object(Node* fromObj, const char* fieldName, const char* fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass* fromKls);\n@@ -267,0 +267,1 @@\n+  bool inline_reference_refersTo0(bool is_phantom);\n@@ -327,3 +328,0 @@\n-  Node* box_vector(Node* in, const TypeInstPtr* vbox_type, BasicType bt, int num_elem);\n-  Node* unbox_vector(Node* in, const TypeInstPtr* vbox_type, BasicType bt, int num_elem, bool shuffle_to_vector = false);\n-  Node* shift_count(Node* cnt, int shift_op, BasicType bt, int num_elem);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1239,3 +1239,4 @@\n-    mcall->set_tf(         call->tf());\n-    mcall->set_entry_point(call->entry_point());\n-    mcall->set_cnt(        call->cnt());\n+    mcall->set_tf(                  call->tf());\n+    mcall->set_entry_point(         call->entry_point());\n+    mcall->set_cnt(                 call->cnt());\n+    mcall->set_guaranteed_safepoint(call->guaranteed_safepoint());\n@@ -1266,1 +1267,10 @@\n-      mcall->as_MachCallRuntime()->_name = call->as_CallRuntime()->_name;\n+      MachCallRuntimeNode* mach_call_rt = mcall->as_MachCallRuntime();\n+      mach_call_rt->_name = call->as_CallRuntime()->_name;\n+      mach_call_rt->_leaf_no_fp = call->is_CallLeafNoFP();\n+    }\n+    else if( mcall->is_MachCallNative() ) {\n+      MachCallNativeNode* mach_call_native = mcall->as_MachCallNative();\n+      CallNativeNode* call_native = call->as_CallNative();\n+      mach_call_native->_name = call_native->_name;\n+      mach_call_native->_arg_regs = call_native->_arg_regs;\n+      mach_call_native->_ret_regs = call_native->_ret_regs;\n@@ -1302,0 +1312,2 @@\n+  if( call != NULL && call->is_CallNative() )\n+    out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call, call->as_CallNative()->_shadow_space_bytes);\n@@ -2217,0 +2229,1 @@\n+    case Op_LoadVectorMasked:\n@@ -2319,0 +2332,6 @@\n+    case Op_StoreVectorMasked: {\n+      Node* pair = new BinaryNode(n->in(3), n->in(4));\n+      n->set_req(3, pair);\n+      n->del_req(4);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":23,"deletions":4,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2291,5 +2291,0 @@\n-\/\/ Clear all entries in _nodes to NULL but keep storage\n-void Node_Array::clear() {\n-  Copy::zero_to_bytes( _nodes, _max*sizeof(Node*) );\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -693,1 +693,2 @@\n-PhaseValues::PhaseValues( Arena *arena, uint est_max_size ) : PhaseTransform(arena, GVN), _table(arena, est_max_size) {\n+PhaseValues::PhaseValues( Arena *arena, uint est_max_size )\n+  : PhaseTransform(arena, GVN), _table(arena, est_max_size), _iterGVN(false) {\n@@ -699,2 +700,2 @@\n-PhaseValues::PhaseValues( PhaseValues *ptv ) : PhaseTransform( ptv, GVN ),\n-  _table(&ptv->_table) {\n+PhaseValues::PhaseValues(PhaseValues* ptv)\n+  : PhaseTransform(ptv, GVN), _table(&ptv->_table), _iterGVN(false) {\n@@ -935,4 +936,4 @@\n-PhaseIterGVN::PhaseIterGVN( PhaseIterGVN *igvn ) : PhaseGVN(igvn),\n-                                                   _delay_transform(igvn->_delay_transform),\n-                                                   _stack( igvn->_stack ),\n-                                                   _worklist( igvn->_worklist )\n+PhaseIterGVN::PhaseIterGVN(PhaseIterGVN* igvn) : PhaseGVN(igvn),\n+                                                 _delay_transform(igvn->_delay_transform),\n+                                                 _stack(igvn->_stack ),\n+                                                 _worklist(igvn->_worklist)\n@@ -940,0 +941,1 @@\n+  _iterGVN = true;\n@@ -944,2 +946,2 @@\n-PhaseIterGVN::PhaseIterGVN( PhaseGVN *gvn ) : PhaseGVN(gvn),\n-                                              _delay_transform(false),\n+PhaseIterGVN::PhaseIterGVN(PhaseGVN* gvn) : PhaseGVN(gvn),\n+                                            _delay_transform(false),\n@@ -950,2 +952,2 @@\n-                                              _stack(C->comp_arena(), 32),\n-                                              _worklist(*C->for_igvn())\n+                                            _stack(C->comp_arena(), 32),\n+                                            _worklist(*C->for_igvn())\n@@ -953,0 +955,1 @@\n+  _iterGVN = true;\n@@ -963,1 +966,3 @@\n-      assert( false, \"Parse::remove_useless_nodes missed this node\");\n+      \/\/ If remove_useless_nodes() has run, we expect no such nodes left.\n+      assert(!UseLoopSafepoints || !OptoRemoveUseless,\n+             \"remove_useless_nodes missed this node\");\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":17,"deletions":12,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -32,0 +32,70 @@\n+#ifdef ASSERT\n+static bool is_vector(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorPayload_klass());\n+}\n+\n+static bool check_vbox(const TypeInstPtr* vbox_type) {\n+  assert(vbox_type->klass_is_exact(), \"\");\n+\n+  ciInstanceKlass* ik = vbox_type->klass()->as_instance_klass();\n+  assert(is_vector(ik), \"not a vector\");\n+\n+  ciField* fd1 = ik->get_field_by_name(ciSymbol::ETYPE_name(), ciSymbol::class_signature(), \/* is_static *\/ true);\n+  assert(fd1 != NULL, \"element type info is missing\");\n+\n+  ciConstant val1 = fd1->constant_value();\n+  BasicType elem_bt = val1.as_object()->as_instance()->java_mirror_type()->basic_type();\n+  assert(is_java_primitive(elem_bt), \"element type info is missing\");\n+\n+  ciField* fd2 = ik->get_field_by_name(ciSymbol::VLENGTH_name(), ciSymbol::int_signature(), \/* is_static *\/ true);\n+  assert(fd2 != NULL, \"vector length info is missing\");\n+\n+  ciConstant val2 = fd2->constant_value();\n+  assert(val2.as_int() > 0, \"vector length info is missing\");\n+\n+  return true;\n+}\n+#endif\n+\n+Node* GraphKit::box_vector(Node* vector, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool deoptimize_on_exception) {\n+  assert(EnableVectorSupport, \"\");\n+\n+  PreserveReexecuteState preexecs(this);\n+  jvms()->set_should_reexecute(true);\n+\n+  VectorBoxAllocateNode* alloc = new VectorBoxAllocateNode(C, vbox_type);\n+  set_edges_for_java_call(alloc, \/*must_throw=*\/false, \/*separate_io_proj=*\/true);\n+  make_slow_call_ex(alloc, env()->Throwable_klass(), \/*separate_io_proj=*\/true, deoptimize_on_exception);\n+  set_i_o(gvn().transform( new ProjNode(alloc, TypeFunc::I_O) ));\n+  set_all_memory(gvn().transform( new ProjNode(alloc, TypeFunc::Memory) ));\n+  Node* ret = gvn().transform(new ProjNode(alloc, TypeFunc::Parms));\n+\n+  assert(check_vbox(vbox_type), \"\");\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  VectorBoxNode* vbox = new VectorBoxNode(C, ret, vector, vbox_type, vt);\n+  return gvn().transform(vbox);\n+}\n+\n+Node* GraphKit::unbox_vector(Node* v, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool shuffle_to_vector) {\n+  assert(EnableVectorSupport, \"\");\n+  const TypeInstPtr* vbox_type_v = gvn().type(v)->is_instptr();\n+  if (vbox_type->klass() != vbox_type_v->klass()) {\n+    return NULL; \/\/ arguments don't agree on vector shapes\n+  }\n+  if (vbox_type_v->maybe_null()) {\n+    return NULL; \/\/ no nulls are allowed\n+  }\n+  assert(check_vbox(vbox_type), \"\");\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  Node* unbox = gvn().transform(new VectorUnboxNode(C, vt, v, merged_memory(), shuffle_to_vector));\n+  return unbox;\n+}\n+\n+Node* GraphKit::vector_shift_count(Node* cnt, int shift_op, BasicType bt, int num_elem) {\n+  assert(bt == T_INT || bt == T_LONG || bt == T_SHORT || bt == T_BYTE, \"byte, short, long and int are supported\");\n+  juint mask = (type2aelembytes(bt) * BitsPerByte - 1);\n+  Node* nmask = gvn().transform(ConNode::make(TypeInt::make(mask)));\n+  Node* mcnt = gvn().transform(new AndINode(cnt, nmask));\n+  return gvn().transform(VectorNode::shift_count(shift_op, mcnt, num_elem, bt));\n+}\n+\n@@ -111,60 +181,0 @@\n-#ifdef ASSERT\n-static bool is_vector(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorPayload_klass());\n-}\n-\n-static bool check_vbox(const TypeInstPtr* vbox_type) {\n-  assert(vbox_type->klass_is_exact(), \"\");\n-\n-  ciInstanceKlass* ik = vbox_type->klass()->as_instance_klass();\n-  assert(is_vector(ik), \"not a vector\");\n-\n-  ciField* fd1 = ik->get_field_by_name(ciSymbol::ETYPE_name(), ciSymbol::class_signature(), \/* is_static *\/ true);\n-  assert(fd1 != NULL, \"element type info is missing\");\n-\n-  ciConstant val1 = fd1->constant_value();\n-  BasicType elem_bt = val1.as_object()->as_instance()->java_mirror_type()->basic_type();\n-  assert(is_java_primitive(elem_bt), \"element type info is missing\");\n-\n-  ciField* fd2 = ik->get_field_by_name(ciSymbol::VLENGTH_name(), ciSymbol::int_signature(), \/* is_static *\/ true);\n-  assert(fd2 != NULL, \"vector length info is missing\");\n-\n-  ciConstant val2 = fd2->constant_value();\n-  assert(val2.as_int() > 0, \"vector length info is missing\");\n-\n-  return true;\n-}\n-#endif\n-\n-Node* LibraryCallKit::box_vector(Node* vector, const TypeInstPtr* vbox_type,\n-                                 BasicType elem_bt, int num_elem) {\n-  assert(EnableVectorSupport, \"\");\n-  const TypeVect* vec_type = TypeVect::make(elem_bt, num_elem);\n-\n-  VectorBoxAllocateNode* alloc = new VectorBoxAllocateNode(C, vbox_type);\n-  set_edges_for_java_call(alloc, \/*must_throw=*\/false, \/*separate_io_proj=*\/true);\n-  make_slow_call_ex(alloc, env()->Throwable_klass(), \/*separate_io_proj=*\/true);\n-  set_i_o(gvn().transform( new ProjNode(alloc, TypeFunc::I_O) ));\n-  set_all_memory(gvn().transform( new ProjNode(alloc, TypeFunc::Memory) ));\n-  Node* ret = gvn().transform(new ProjNode(alloc, TypeFunc::Parms));\n-\n-  assert(check_vbox(vbox_type), \"\");\n-  VectorBoxNode* vbox = new VectorBoxNode(C, ret, vector, vbox_type, vec_type);\n-  return gvn().transform(vbox);\n-}\n-\n-Node* LibraryCallKit::unbox_vector(Node* v, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool shuffle_to_vector) {\n-  assert(EnableVectorSupport, \"\");\n-  const TypeInstPtr* vbox_type_v = gvn().type(v)->is_instptr();\n-  if (vbox_type->klass() != vbox_type_v->klass()) {\n-    return NULL; \/\/ arguments don't agree on vector shapes\n-  }\n-  if (vbox_type_v->maybe_null()) {\n-    return NULL; \/\/ no nulls are allowed\n-  }\n-  assert(check_vbox(vbox_type), \"\");\n-  const TypeVect* vec_type = TypeVect::make(elem_bt, num_elem);\n-  Node* unbox = gvn().transform(new VectorUnboxNode(C, vec_type, v, merged_memory(), shuffle_to_vector));\n-  return unbox;\n-}\n-\n@@ -1197,8 +1207,0 @@\n-Node* LibraryCallKit::shift_count(Node* cnt, int shift_op, BasicType bt, int num_elem) {\n-  assert(bt == T_INT || bt == T_LONG || bt == T_SHORT || bt == T_BYTE, \"byte, short, long and int are supported\");\n-  juint mask = (type2aelembytes(bt) * BitsPerByte - 1);\n-  Node* nmask = gvn().transform(ConNode::make(TypeInt::make(mask)));\n-  Node* mcnt = gvn().transform(new AndINode(cnt, nmask));\n-  return gvn().transform(VectorNode::shift_count(shift_op, mcnt, num_elem, bt));\n-}\n-\n@@ -1672,1 +1674,1 @@\n-  Node* opd2 = shift_count(argument(5), opc, elem_bt, num_elem);\n+  Node* opd2 = vector_shift_count(argument(5), opc, elem_bt, num_elem);\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":71,"deletions":69,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -781,0 +781,50 @@\n+class StoreVectorMaskedNode : public StoreVectorNode {\n+ public:\n+  StoreVectorMaskedNode(Node* c, Node* mem, Node* dst, Node* src, const TypePtr* at, Node* mask)\n+   : StoreVectorNode(c, mem, dst, at, src) {\n+    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    init_class_id(Class_StoreVector);\n+    set_mismatched_access();\n+    add_req(mask);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  virtual uint match_edge(uint idx) const {\n+    return idx > 1;\n+  }\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n+\n+class LoadVectorMaskedNode : public LoadVectorNode {\n+ public:\n+  LoadVectorMaskedNode(Node* c, Node* mem, Node* src, const TypePtr* at, const TypeVect* vt, Node* mask)\n+   : LoadVectorNode(c, mem, src, at, vt) {\n+    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    init_class_id(Class_LoadVector);\n+    set_mismatched_access();\n+    add_req(mask);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  virtual uint match_edge(uint idx) const {\n+    return idx > 1;\n+  }\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n+\n+class VectorMaskGenNode : public TypeNode {\n+ public:\n+  VectorMaskGenNode(Node* length, const Type* ty, const Type* ety): TypeNode(ty, 2), _elemType(ety) {\n+    init_req(1, length);\n+  }\n+\n+  virtual int Opcode() const;\n+  const Type* get_elem_type()  { return _elemType;}\n+  virtual  uint  size_of() const { return sizeof(VectorMaskGenNode); }\n+\n+  private:\n+   const Type* _elemType;\n+};\n+\n@@ -1098,7 +1148,2 @@\n-  VectorTestNode( Node *in1, Node *in2, BoolTest::mask predicate) : Node(NULL, in1, in2), _predicate(predicate) {\n-    assert(in1->is_Vector() || in1->is_LoadVector(), \"must be vector\");\n-    assert(in2->is_Vector() || in2->is_LoadVector(), \"must be vector\");\n-    assert(in1->bottom_type()->is_vect()->element_basic_type() == in2->bottom_type()->is_vect()->element_basic_type(),\n-           \"same type elements are needed\");\n-    assert(in1->bottom_type()->is_vect()->length() == in2->bottom_type()->is_vect()->length(),\n-           \"same number of elements is needed\");\n+  VectorTestNode(Node* in1, Node* in2, BoolTest::mask predicate) : Node(NULL, in1, in2), _predicate(predicate) {\n+    assert(in2->bottom_type()->is_vect() == in2->bottom_type()->is_vect(), \"same vector type\");\n@@ -1142,2 +1187,1 @@\n-\n-class VectorLoadMaskNode : public VectorNode {\n+class VectorLoadShuffleNode : public VectorNode {\n@@ -1145,1 +1189,1 @@\n-  VectorLoadMaskNode(Node* in, const TypeVect* vt)\n+  VectorLoadShuffleNode(Node* in, const TypeVect* vt)\n@@ -1147,2 +1191,1 @@\n-    assert(in->is_LoadVector(), \"expected load vector\");\n-    assert(in->as_LoadVector()->vect_type()->element_basic_type() == T_BOOLEAN, \"must be boolean\");\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_BYTE, \"must be BYTE\");\n@@ -1151,0 +1194,1 @@\n+  int GetOutShuffleSize() const { return type2aelembytes(vect_type()->element_basic_type()); }\n@@ -1154,1 +1198,1 @@\n-class VectorLoadShuffleNode : public VectorNode {\n+class VectorLoadMaskNode : public VectorNode {\n@@ -1156,4 +1200,2 @@\n-  VectorLoadShuffleNode(Node* in, const TypeVect* vt)\n-    : VectorNode(in, vt) {\n-    assert(in->is_LoadVector(), \"expected load vector\");\n-    assert(in->as_LoadVector()->vect_type()->element_basic_type() == T_BYTE, \"must be BYTE\");\n+  VectorLoadMaskNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN, \"must be boolean\");\n@@ -1162,1 +1204,1 @@\n-  int GetOutShuffleSize() const { return type2aelembytes(vect_type()->element_basic_type()); }\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1168,2 +1210,1 @@\n-  VectorStoreMaskNode(Node* in1, ConINode* in2, const TypeVect* vt)\n-    : VectorNode(in1, in2, vt) { }\n+  VectorStoreMaskNode(Node* in1, ConINode* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n@@ -1173,0 +1214,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1191,1 +1233,1 @@\n-  virtual Node *Identity(PhaseGVN *phase);\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1204,0 +1246,2 @@\n+\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -1324,1 +1368,2 @@\n-  virtual Node *Identity(PhaseGVN *phase);\n+  virtual Node* Identity(PhaseGVN* phase);\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":67,"deletions":22,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -86,3 +86,9 @@\n-void VectorSupport::init_vector_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr) {\n-  int elem_size = type2aelembytes(elem_bt);\n-  for (int i = 0; i < num_elem; i++) {\n+void VectorSupport::init_payload_element(typeArrayOop arr, bool is_mask, BasicType elem_bt, int index, address addr) {\n+  if (is_mask) {\n+    \/\/ Masks require special handling: when boxed they are packed and stored in boolean\n+    \/\/ arrays, but in scalarized form they have the same size as corresponding vectors.\n+    \/\/ For example, Int512Mask is represented in memory as boolean[16], but\n+    \/\/ occupies the whole 512-bit vector register when scalarized.\n+    \/\/ (In generated code, the conversion is performed by VectorStoreMask.)\n+    \/\/\n+    \/\/ TODO: revisit when predicate registers are fully supported.\n@@ -90,32 +96,8 @@\n-      case T_BYTE: {\n-        jbyte elem_value = *(jbyte*) (value_addr + i * elem_size);\n-        arr->byte_at_put(i, elem_value);\n-        break;\n-      }\n-      case T_SHORT: {\n-        jshort elem_value = *(jshort*) (value_addr + i * elem_size);\n-        arr->short_at_put(i, elem_value);\n-        break;\n-      }\n-      case T_INT: {\n-        jint elem_value = *(jint*) (value_addr + i * elem_size);\n-        arr->int_at_put(i, elem_value);\n-        break;\n-      }\n-      case T_LONG: {\n-        jlong elem_value = *(jlong*) (value_addr + i * elem_size);\n-        arr->long_at_put(i, elem_value);\n-        break;\n-      }\n-      case T_FLOAT: {\n-        jfloat elem_value = *(jfloat*) (value_addr + i * elem_size);\n-        arr->float_at_put(i, elem_value);\n-        break;\n-      }\n-      case T_DOUBLE: {\n-        jdouble elem_value = *(jdouble*) (value_addr + i * elem_size);\n-        arr->double_at_put(i, elem_value);\n-        break;\n-      }\n-      default:\n-        fatal(\"unsupported: %s\", type2name(elem_bt));\n+      case T_BYTE:   arr->bool_at_put(index,  (*(jbyte*)addr) != 0); break;\n+      case T_SHORT:  arr->bool_at_put(index, (*(jshort*)addr) != 0); break;\n+      case T_INT:    \/\/ fall-through\n+      case T_FLOAT:  arr->bool_at_put(index,   (*(jint*)addr) != 0); break;\n+      case T_LONG:   \/\/ fall-through\n+      case T_DOUBLE: arr->bool_at_put(index,  (*(jlong*)addr) != 0); break;\n+\n+      default: fatal(\"unsupported: %s\", type2name(elem_bt));\n@@ -123,7 +105,1 @@\n-  }\n-}\n-\n-void VectorSupport::init_mask_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr) {\n-  int elem_size = type2aelembytes(elem_bt);\n-\n-  for (int i = 0; i < num_elem; i++) {\n+  } else {\n@@ -131,24 +107,8 @@\n-      case T_BYTE: {\n-        jbyte elem_value = *(jbyte*) (value_addr + i * elem_size);\n-        arr->bool_at_put(i, elem_value != 0);\n-        break;\n-      }\n-      case T_SHORT: {\n-        jshort elem_value = *(jshort*) (value_addr + i * elem_size);\n-        arr->bool_at_put(i, elem_value != 0);\n-        break;\n-      }\n-      case T_INT:   \/\/ fall-through\n-      case T_FLOAT: {\n-        jint elem_value = *(jint*) (value_addr + i * elem_size);\n-        arr->bool_at_put(i, elem_value != 0);\n-        break;\n-      }\n-      case T_LONG: \/\/ fall-through\n-      case T_DOUBLE: {\n-        jlong elem_value = *(jlong*) (value_addr + i * elem_size);\n-        arr->bool_at_put(i, elem_value != 0);\n-        break;\n-      }\n-      default:\n-        fatal(\"unsupported: %s\", type2name(elem_bt));\n+      case T_BYTE:   arr->  byte_at_put(index,   *(jbyte*)addr); break;\n+      case T_SHORT:  arr-> short_at_put(index,  *(jshort*)addr); break;\n+      case T_INT:    arr->   int_at_put(index,    *(jint*)addr); break;\n+      case T_FLOAT:  arr-> float_at_put(index,  *(jfloat*)addr); break;\n+      case T_LONG:   arr->  long_at_put(index,   *(jlong*)addr); break;\n+      case T_DOUBLE: arr->double_at_put(index, *(jdouble*)addr); break;\n+\n+      default: fatal(\"unsupported: %s\", type2name(elem_bt));\n@@ -159,2 +119,1 @@\n-oop VectorSupport::allocate_vector_payload_helper(InstanceKlass* ik, BasicType elem_bt, int num_elem, address value_addr, TRAPS) {\n-\n+Handle VectorSupport::allocate_vector_payload_helper(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, Location location, TRAPS) {\n@@ -163,0 +122,4 @@\n+  int num_elem = klass2length(ik);\n+  BasicType elem_bt = klass2bt(ik);\n+  int elem_size = type2aelembytes(elem_bt);\n+\n@@ -166,1 +129,1 @@\n-  typeArrayOop arr = tak->allocate(num_elem, CHECK_NULL); \/\/ safepoint\n+  typeArrayOop arr = tak->allocate(num_elem, CHECK_NH); \/\/ safepoint\n@@ -168,2 +131,27 @@\n-  if (is_mask) {\n-    init_mask_array(arr, elem_bt, num_elem, value_addr);\n+  if (location.is_register()) {\n+    \/\/ Value was in a callee-saved register.\n+    VMReg vreg = VMRegImpl::as_VMReg(location.register_number());\n+\n+    for (int i = 0; i < num_elem; i++) {\n+      int vslot = (i * elem_size) \/ VMRegImpl::stack_slot_size;\n+      int off   = (i * elem_size) % VMRegImpl::stack_slot_size;\n+\n+      address elem_addr = reg_map->location(vreg->next(vslot)) + off;\n+      init_payload_element(arr, is_mask, elem_bt, i, elem_addr);\n+    }\n+  } else {\n+    \/\/ Value was directly saved on the stack.\n+    address base_addr = ((address)fr->unextended_sp()) + location.stack_offset();\n+    for (int i = 0; i < num_elem; i++) {\n+      init_payload_element(arr, is_mask, elem_bt, i, base_addr + i * elem_size);\n+    }\n+  }\n+  return Handle(THREAD, arr);\n+}\n+\n+Handle VectorSupport::allocate_vector_payload(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, ScopeValue* payload, TRAPS) {\n+  if (payload->is_location() &&\n+      payload->as_LocationValue()->location().type() == Location::vector) {\n+    \/\/ Vector value in an aligned adjacent tuple (1, 2, 4, 8, or 16 slots).\n+    Location location = payload->as_LocationValue()->location();\n+    return allocate_vector_payload_helper(ik, fr, reg_map, location, THREAD); \/\/ safepoint\n@@ -171,1 +159,3 @@\n-    init_vector_array(arr, elem_bt, num_elem, value_addr);\n+    \/\/ Scalar-replaced boxed vector representation.\n+    StackValue* value = StackValue::create_stack_value(fr, reg_map, payload);\n+    return value->get_obj();\n@@ -173,1 +163,0 @@\n-  return arr;\n@@ -176,1 +165,1 @@\n-oop VectorSupport::allocate_vector(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, ObjectValue* ov, TRAPS) {\n+instanceOop VectorSupport::allocate_vector(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, ObjectValue* ov, TRAPS) {\n@@ -180,26 +169,5 @@\n-  \/\/ Vector value in an aligned adjacent tuple (1, 2, 4, 8, or 16 slots).\n-  LocationValue* loc_value = ov->field_at(0)->as_LocationValue();\n-\n-  BasicType elem_bt = klass2bt(ik);\n-  int num_elem = klass2length(ik);\n-\n-  Handle vbox = ik->allocate_instance_handle(CHECK_NULL);\n-\n-  Location loc = loc_value->location();\n-\n-  oop payload = NULL;\n-  if (loc.type() == Location::vector) {\n-    address value_addr = loc.is_register()\n-        \/\/ Value was in a callee-save register\n-        ? reg_map->location(VMRegImpl::as_VMReg(loc.register_number()))\n-        \/\/ Else value was directly saved on the stack. The frame's original stack pointer,\n-        \/\/ before any extension by its callee (due to Compiler1 linkage on SPARC), must be used.\n-        : ((address)fr->unextended_sp()) + loc.stack_offset();\n-    payload = allocate_vector_payload_helper(ik, elem_bt, num_elem, value_addr, CHECK_NULL); \/\/ safepoint\n-  } else {\n-    \/\/ assert(false, \"interesting\");\n-    StackValue* value = StackValue::create_stack_value(fr, reg_map, loc_value);\n-    payload = value->get_obj()();\n-  }\n-  vector_VectorPayload::set_payload(vbox(), payload);\n-  return vbox();\n+  ScopeValue* payload_value = ov->field_at(0);\n+  Handle payload_instance = VectorSupport::allocate_vector_payload(ik, fr, reg_map, payload_value, CHECK_NULL);\n+  instanceOop vbox = ik->allocate_instance(CHECK_NULL);\n+  vector_VectorPayload::set_payload(vbox, payload_instance());\n+  return vbox;\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.cpp","additions":68,"deletions":100,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -42,3 +42,4 @@\n-  static void init_mask_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr);\n-  static void init_vector_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr);\n-  static oop  allocate_vector_payload_helper(InstanceKlass* ik, BasicType elem_bt, int num_elem, address value_addr, TRAPS);\n+  static Handle allocate_vector_payload(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, ScopeValue* payload, TRAPS);\n+  static Handle allocate_vector_payload_helper(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, Location location, TRAPS);\n+\n+  static void init_payload_element(typeArrayOop arr, bool is_mask, BasicType elem_bt, int index, address addr);\n@@ -107,1 +108,1 @@\n-  static oop  allocate_vector(InstanceKlass* holder, frame* fr, RegisterMap* reg_map, ObjectValue* sv, TRAPS);\n+  static instanceOop allocate_vector(InstanceKlass* holder, frame* fr, RegisterMap* reg_map, ObjectValue* sv, TRAPS);\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -572,10 +572,0 @@\n-#ifndef COMPILER2\n-  \/\/ These flags were generally available, but are C2 only, now.\n-  { \"MaxInlineLevel\",               JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },\n-  { \"MaxRecursiveInlineLevel\",      JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },\n-  { \"InlineSmallCode\",              JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },\n-  { \"MaxInlineSize\",                JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },\n-  { \"FreqInlineSize\",               JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },\n-  { \"MaxTrivialSize\",               JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },\n-#endif\n-\n@@ -1681,1 +1671,0 @@\n-#ifndef ZERO\n@@ -1702,1 +1691,0 @@\n-#endif \/\/ ZERO\n@@ -1709,1 +1697,0 @@\n-#ifndef ZERO\n@@ -1737,1 +1724,0 @@\n-#endif \/\/ !ZERO\n@@ -1756,1 +1742,0 @@\n-#ifndef ZERO\n@@ -1767,1 +1752,0 @@\n-#endif \/\/ !ZERO\n@@ -4197,2 +4181,0 @@\n-  LP64_ONLY(FLAG_SET_DEFAULT(UseCompressedOops, false));\n-  LP64_ONLY(FLAG_SET_DEFAULT(UseCompressedClassPointers, false));\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -523,0 +523,5 @@\n+  static BufferBlob* make_native_invoker(address call_target,\n+                                         int shadow_space_bytes,\n+                                         const GrowableArray<VMReg>& input_registers,\n+                                         const GrowableArray<VMReg>& output_registers);\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -868,3 +868,2 @@\n-  nonstatic_field(ciObjectFactory,             _ci_metadata,                                  GrowableArray<ciMetadata*>*)           \\\n-  nonstatic_field(ciObjectFactory,             _symbols,                                      GrowableArray<ciSymbol*>*)             \\\n-  nonstatic_field(ciObjectFactory,             _unloaded_methods,                             GrowableArray<ciMethod*>*)             \\\n+  nonstatic_field(ciObjectFactory,             _ci_metadata,                                  GrowableArray<ciMetadata*>)            \\\n+  nonstatic_field(ciObjectFactory,             _symbols,                                      GrowableArray<ciSymbol*>)              \\\n@@ -1526,0 +1525,1 @@\n+  declare_c2_type(CallNativeNode, CallNode)                               \\\n@@ -1643,0 +1643,1 @@\n+  declare_c2_type(MachCallNativeNode, MachCallNode)                       \\\n@@ -2558,0 +2559,1 @@\n+  declare_constant(vmIntrinsics::_linkToNative)                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -127,0 +127,17 @@\n+  const static GrowableArrayView EMPTY;\n+\n+  bool operator==(const GrowableArrayView<E>& rhs) const {\n+    if (_len != rhs._len)\n+      return false;\n+    for (int i = 0; i < _len; i++) {\n+      if (at(i) != rhs.at(i)) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  bool operator!=(const GrowableArrayView<E>& rhs) const {\n+    return !(*this == rhs);\n+  }\n+\n@@ -309,1 +326,5 @@\n-  void print() {\n+  size_t data_size_in_bytes() const {\n+    return _len * sizeof(E);\n+  }\n+\n+  void print() const {\n@@ -319,0 +340,3 @@\n+template<typename E>\n+const GrowableArrayView<E> GrowableArrayView<E>::EMPTY(nullptr, 0, 0);\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -403,1 +403,1 @@\n-        return  Double.doubleToLongBits(e);\n+        return  Double.doubleToRawLongBits(e);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -403,1 +403,1 @@\n-        return  Float.floatToIntBits(e);\n+        return  Float.floatToRawIntBits(e);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -398,36 +398,0 @@\n-        \/**\n-         * An in-place version of a narrowing\n-         * conversion between two types.\n-         * The output of the conversion must be no larger\n-         * than the type {@code E}.\n-         * Any unused lane bits are ignored and\n-         * overwritten by zero bits (not a copied sign bit).\n-         * @param <E> the domain and range type (boxed version of a lane type)\n-         * @param conv the narrowing conversion to treat in-place\n-         * @return a Java narrowing conversion,\n-         *          stored back to the original lane of type {@code E}\n-         *\/\n-        @ForceInline\n-        static <E> Conversion<E,E> ofNarrowing(Conversion<E,?> conv) {\n-            Class<E> lt = conv.domainType();\n-            return ConversionImpl.ofInplace((ConversionImpl<?,?>) conv, false).check(lt, lt);\n-        }\n-\n-        \/**\n-         * An in-place version of a widening\n-         * conversion between two types.\n-         * The input of the conversion must be no larger\n-         * than the type {@code E}.\n-         * Any unused lane bits are ignored and\n-         * overwritten by the result.\n-         * @param <E> the domain and range type (boxed version of a lane type)\n-         * @param conv the widening conversion to treat in-place\n-         * @return a Java widening conversion,\n-         *          loading its input from same lane of type {@code E}\n-         *\/\n-        @ForceInline\n-        static <E> Conversion<E,E> ofWidening(Conversion<?,E> conv) {\n-            Class<E> lt = conv.rangeType();\n-            return ConversionImpl.ofInplace((ConversionImpl<?,?>) conv, true).check(lt, lt);\n-        }\n-\n@@ -470,2 +434,1 @@\n-        VO_KIND_BITWISE            = 0x100,\n-        VO_KIND_INPLACE            = 0x200;\n+        VO_KIND_BITWISE            = 0x100;\n@@ -669,52 +632,0 @@\n-    \/** In-place narrow {@code doubleVal} to {@code (byte)doubleVal} inside double. *\/\n-    public static final Conversion<Double,Double> INPLACE_D2B = convert(\"INPLACE_D2B\", 'N', double.class, double.class, VO_KIND_INPLACE + 0x78, VO_ALL);\n-    \/** In-place narrow {@code doubleVal} to {@code (float)doubleVal} inside double. *\/\n-    public static final Conversion<Double,Double> INPLACE_D2F = convert(\"INPLACE_D2F\", 'N', double.class, double.class, VO_KIND_INPLACE + 0x76, VO_ALL);\n-    \/** In-place narrow {@code doubleVal} to {@code (int)doubleVal} inside double. *\/\n-    public static final Conversion<Double,Double> INPLACE_D2I = convert(\"INPLACE_D2I\", 'N', double.class, double.class, VO_KIND_INPLACE + 0x7a, VO_ALL);\n-    \/** In-place narrow {@code doubleVal} to {@code (short)doubleVal} inside double. *\/\n-    public static final Conversion<Double,Double> INPLACE_D2S = convert(\"INPLACE_D2S\", 'N', double.class, double.class, VO_KIND_INPLACE + 0x79, VO_ALL);\n-    \/** In-place narrow {@code floatVal} to {@code (byte)floatVal} inside float. *\/\n-    public static final Conversion<Float,Float> INPLACE_F2B = convert(\"INPLACE_F2B\", 'N', float.class, float.class, VO_KIND_INPLACE + 0x68, VO_ALL);\n-    \/** In-place narrow {@code floatVal} to {@code (short)floatVal} inside float. *\/\n-    public static final Conversion<Float,Float> INPLACE_F2S = convert(\"INPLACE_F2S\", 'N', float.class, float.class, VO_KIND_INPLACE + 0x69, VO_ALL);\n-    \/** In-place narrow {@code intVal} to {@code (byte)intVal} inside int. *\/\n-    public static final Conversion<Integer,Integer> INPLACE_I2B = convert(\"INPLACE_I2B\", 'N', int.class, int.class, VO_KIND_INPLACE + 0xa8, VO_ALL);\n-    \/** In-place narrow {@code intVal} to {@code (short)intVal} inside int. *\/\n-    public static final Conversion<Integer,Integer> INPLACE_I2S = convert(\"INPLACE_I2S\", 'N', int.class, int.class, VO_KIND_INPLACE + 0xa9, VO_ALL);\n-    \/** In-place narrow {@code longVal} to {@code (byte)longVal} inside long. *\/\n-    public static final Conversion<Long,Long> INPLACE_L2B = convert(\"INPLACE_L2B\", 'N', long.class, long.class, VO_KIND_INPLACE + 0xb8, VO_ALL);\n-    \/** In-place narrow {@code longVal} to {@code (float)longVal} inside long. *\/\n-    public static final Conversion<Long,Long> INPLACE_L2F = convert(\"INPLACE_L2F\", 'N', long.class, long.class, VO_KIND_INPLACE + 0xb6, VO_ALL);\n-    \/** In-place narrow {@code longVal} to {@code (int)longVal} inside long. *\/\n-    public static final Conversion<Long,Long> INPLACE_L2I = convert(\"INPLACE_L2I\", 'N', long.class, long.class, VO_KIND_INPLACE + 0xba, VO_ALL);\n-    \/** In-place narrow {@code longVal} to {@code (short)longVal} inside long. *\/\n-    public static final Conversion<Long,Long> INPLACE_L2S = convert(\"INPLACE_L2S\", 'N', long.class, long.class, VO_KIND_INPLACE + 0xb9, VO_ALL);\n-    \/** In-place narrow {@code shortVal} to {@code (byte)shortVal} inside short. *\/\n-    public static final Conversion<Short,Short> INPLACE_S2B = convert(\"INPLACE_S2B\", 'N', short.class, short.class, VO_KIND_INPLACE + 0x98, VO_ALL);\n-    \/** In-place widen {@code byteVal} inside double to {@code (double)byteVal}. *\/\n-    public static final Conversion<Double,Double> INPLACE_B2D = convert(\"INPLACE_B2D\", 'W', double.class, double.class, VO_KIND_INPLACE + 0x87, VO_ALL);\n-    \/** In-place widen {@code byteVal} inside float to {@code (float)byteVal}. *\/\n-    public static final Conversion<Float,Float> INPLACE_B2F = convert(\"INPLACE_B2F\", 'W', float.class, float.class, VO_KIND_INPLACE + 0x86, VO_ALL);\n-    \/** In-place widen {@code byteVal} inside int to {@code (int)byteVal}. *\/\n-    public static final Conversion<Integer,Integer> INPLACE_B2I = convert(\"INPLACE_B2I\", 'W', int.class, int.class, VO_KIND_INPLACE + 0x8a, VO_ALL);\n-    \/** In-place widen {@code byteVal} inside long to {@code (long)byteVal}. *\/\n-    public static final Conversion<Long,Long> INPLACE_B2L = convert(\"INPLACE_B2L\", 'W', long.class, long.class, VO_KIND_INPLACE + 0x8b, VO_ALL);\n-    \/** In-place widen {@code byteVal} inside short to {@code (short)byteVal}. *\/\n-    public static final Conversion<Short,Short> INPLACE_B2S = convert(\"INPLACE_B2S\", 'W', short.class, short.class, VO_KIND_INPLACE + 0x89, VO_ALL);\n-    \/** In-place widen {@code floatVal} inside double to {@code (double)floatVal}. *\/\n-    public static final Conversion<Double,Double> INPLACE_F2D = convert(\"INPLACE_F2D\", 'W', double.class, double.class, VO_KIND_INPLACE + 0x67, VO_ALL);\n-    \/** In-place widen {@code floatVal} inside long to {@code (long)floatVal}. *\/\n-    public static final Conversion<Long,Long> INPLACE_F2L = convert(\"INPLACE_F2L\", 'W', long.class, long.class, VO_KIND_INPLACE + 0x6b, VO_ALL);\n-    \/** In-place widen {@code intVal} inside double to {@code (double)intVal}. *\/\n-    public static final Conversion<Double,Double> INPLACE_I2D = convert(\"INPLACE_I2D\", 'W', double.class, double.class, VO_KIND_INPLACE + 0xa7, VO_ALL);\n-    \/** In-place widen {@code intVal} inside long to {@code (long)intVal}. *\/\n-    public static final Conversion<Long,Long> INPLACE_I2L = convert(\"INPLACE_I2L\", 'W', long.class, long.class, VO_KIND_INPLACE + 0xab, VO_ALL);\n-    \/** In-place widen {@code shortVal} inside double to {@code (double)shortVal}. *\/\n-    public static final Conversion<Double,Double> INPLACE_S2D = convert(\"INPLACE_S2D\", 'W', double.class, double.class, VO_KIND_INPLACE + 0x97, VO_ALL);\n-    \/** In-place widen {@code shortVal} inside float to {@code (float)shortVal}. *\/\n-    public static final Conversion<Float,Float> INPLACE_S2F = convert(\"INPLACE_S2F\", 'W', float.class, float.class, VO_KIND_INPLACE + 0x96, VO_ALL);\n-    \/** In-place widen {@code shortVal} inside int to {@code (int)shortVal}. *\/\n-    public static final Conversion<Integer,Integer> INPLACE_S2I = convert(\"INPLACE_S2I\", 'W', int.class, int.class, VO_KIND_INPLACE + 0x9a, VO_ALL);\n-    \/** In-place widen {@code shortVal} inside long to {@code (long)shortVal}. *\/\n-    public static final Conversion<Long,Long> INPLACE_S2L = convert(\"INPLACE_S2L\", 'W', long.class, long.class, VO_KIND_INPLACE + 0x9b, VO_ALL);\n@@ -799,3 +710,0 @@\n-            } else {\n-                \/\/ only the widening or narrowing guys specify their own opcode dom\/ran\n-                assert(dom == ran && \"WN\".indexOf(kind) >= 0);\n@@ -1035,13 +943,0 @@\n-        static ConversionImpl<?,?> ofInplace(ConversionImpl<?,?> conv,\n-                                             boolean widening) {\n-            int domSize = conv.dom.elementSize;\n-            int ranSize = conv.ran.elementSize;\n-            if (domSize >= ranSize && widening)\n-                throw new IllegalArgumentException(conv + \": must be a widening conversion\");\n-            if (domSize <= ranSize && !widening)\n-                throw new IllegalArgumentException(conv + \": must be a narrowing conversion\");\n-            if (conv.kind != 'C')\n-                throw new IllegalArgumentException(conv + \": must be a standard Java conversion\");\n-            char kind = (widening ? 'W' : 'N');\n-            return findConv(kind, conv.dom, conv.ran);\n-        }\n@@ -1091,10 +986,0 @@\n-            case 'W':\n-                name = \"INPLACE_\"+a2b(dom, ran);\n-                opCode += VO_KIND_INPLACE;\n-                domType = ranType;  \/\/ slice narrow domain from range\n-                break;\n-            case 'N':\n-                name = \"INPLACE_\"+a2b(dom, ran);\n-                opCode += VO_KIND_INPLACE;\n-                ranType = domType;  \/\/ zero-fill narrow range\n-                break;\n@@ -1187,1 +1072,1 @@\n-                    for (int i = 0; i <= 2; i++) {\n+                    for (int i = 0; i <= 1; i++) {\n@@ -1190,3 +1075,1 @@\n-                            c = ((i == 0) ? ofCast(l1, l2) :\n-                                 (i == 1) ? ofReinterpret(l1, l2) :\n-                                 ofInplace(ofCast(l1, l2), (l1.elementSize < l2.elementSize)));\n+                            c = ((i == 0) ? ofCast(l1, l2) : ofReinterpret(l1, l2));\n@@ -1209,1 +1092,0 @@\n-                            case VO_KIND_INPLACE: opcs = \"VO_KIND_INPLACE\"; break;\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorOperators.java","additions":3,"deletions":121,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -407,1 +407,1 @@\n-        return {#if[FP]? $Type$.$type$To$Bitstype$Bits(e): e};\n+        return {#if[FP]? $Type$.$type$ToRaw$Bitstype$Bits(e): e};\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/hotspot\/cpu\/aarch64\/aarch64-asmtest.py","status":"renamed"}]}