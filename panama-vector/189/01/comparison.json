{"files":[{"patch":"@@ -7954,0 +7954,26 @@\n+void Assembler::evplzcntd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512cd() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n+void Assembler::evplzcntq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512cd() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n@@ -8484,0 +8510,14 @@\n+void Assembler::vpunpckhwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x69, (0xC0 | encode));\n+}\n+\n+void Assembler::vpunpcklwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x61, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -1937,0 +1937,6 @@\n+  \/\/ Interleave High Word\n+  void vpunpckhwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n+  \/\/ Interleave Low Word\n+  void vpunpcklwd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n@@ -2428,0 +2434,2 @@\n+  void evplzcntd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evplzcntq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4726,0 +4726,171 @@\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                        XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                        KRegister ktmp, Register rtmp, bool merge, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  assert(VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit, \"\");\n+  assert(VM_Version::supports_avx512cd(), \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      evplzcntq(dst, ktmp, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evplzcntd(dst, ktmp, src, merge, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vpternlogd(xtmp1, 0xff, xtmp1, xtmp1, vec_enc);\n+      vpunpcklwd(xtmp2, xtmp1, src, vec_enc);\n+      evplzcntd(xtmp2, k0, xtmp2, merge, vec_enc);\n+      vpunpckhwd(dst, xtmp1, src, vec_enc);\n+      evplzcntd(dst, k0, dst, merge, vec_enc);\n+      vpackusdw(dst, xtmp2, dst, vec_enc);\n+      break;\n+    case T_BYTE:\n+      \/\/ T1 = Compute leading zero counts of 4 LSB bits of each byte by\n+      \/\/ accessing the lookup table.\n+      \/\/ T2 = Compute leading zero counts of 4 MSB bits of each byte by\n+      \/\/ accessing the lookup table.\n+      \/\/ Add T1 to T2 if 4 MSB bits of byte are all zeros.\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      evmovdquq(xtmp1, ExternalAddress(StubRoutines::x86::vector_count_leading_zeros_lut()), vec_enc, rtmp);\n+      movl(rtmp, 0x0F0F0F0F);\n+      evpbroadcastd(xtmp2, rtmp, vec_enc);\n+      vpand(dst, xtmp2, src, vec_enc);\n+      vpshufb(dst, xtmp1, dst, vec_enc);\n+      vpsrlw(xtmp3, src, 4, vec_enc);\n+      vpand(xtmp3, xtmp2, xtmp3, vec_enc);\n+      vpshufb(xtmp2, xtmp1, xtmp3, vec_enc);\n+      vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n+      evpcmpeqb(ktmp, xtmp1, xtmp3, vec_enc);\n+      vpaddb(dst, dst, xtmp2, vec_enc);\n+      evpblendmb(dst, ktmp, xtmp2, dst, true, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+void C2_MacroAssembler::vector_count_leading_zeros_byte_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_count_leading_zeros_lut()));\n+  movl(rtmp, 0x0F0F0F0F);\n+  movdl(xtmp2, rtmp);\n+  vpbroadcastd(xtmp2, xtmp2, vec_enc);\n+  \/\/ T1 = Compute leading zero counts of 4 LSB bits of each byte by\n+  \/\/ accessing the lookup table.\n+  vpand(dst, xtmp2, src, vec_enc);\n+  vpshufb(dst, xtmp1, dst, vec_enc);\n+  \/\/ T2 = Compute leading zero counts of 4 MSB bits of each byte by\n+  \/\/ accessing the lookup table.\n+  vpsrlw(xtmp3, src, 4, vec_enc);\n+  vpand(xtmp3, xtmp2, xtmp3, vec_enc);\n+  vpshufb(xtmp2, xtmp1, xtmp3, vec_enc);\n+  \/\/ Add T1 to T2 if 4 MSB bits of byte are all zeros.\n+  vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n+  vpcmpeqb(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpaddb(dst, dst, xtmp2, vec_enc);\n+  vpblendvb(dst, xtmp2, dst, xtmp3, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_short_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                             XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+   vector_count_leading_zeros_byte_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+   \/\/ Add zero counts of lower byte and upper byte of a word if\n+   \/\/ upper byte holds a zero value.\n+   vpsrlw(xtmp3, src, 8, vec_enc);\n+   vpcmpeqw(xtmp3, xtmp1, xtmp3, vec_enc);\n+   vpsllw(xtmp2, dst, 8, vec_enc);\n+   vpaddw(xtmp2, xtmp2, dst, vec_enc);\n+   vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+   vpsrlw(dst, dst, 8, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_int_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                           XMMRegister xtmp2, XMMRegister xtmp3, int vec_enc) {\n+   \/\/ Since IEEE 754 floating point format represents mantissa in 1.0 format\n+   \/\/ hence biased exponent can be used to compute leading zero count as per\n+   \/\/ following formula:-\n+   \/\/ LZCNT = 32 - (biased_exp - 127)\n+   \/\/ Special handling has been introduced for Zero, Max_Int and -ve source values.\n+\n+   \/\/ Broadcast 0xFF\n+   vpcmpeqd(xtmp1, xtmp1, xtmp1, vec_enc);\n+   vpsrld(xtmp1, xtmp1, 24, vec_enc);\n+\n+   \/\/ Extract biased exponent.\n+   vcvtdq2ps(dst, src, vec_enc);\n+   vpsrld(dst, dst, 23, vec_enc);\n+   vpand(dst, dst, xtmp1, vec_enc);\n+\n+   \/\/ Broadcast 127.\n+   vpsrld(xtmp1, xtmp1, 1, vec_enc);\n+   \/\/ Exponent = biased_exp - 127\n+   vpsubd(dst, dst, xtmp1, vec_enc);\n+\n+   \/\/ Exponent = Exponent  + 1\n+   vpsrld(xtmp3, xtmp1, 6, vec_enc);\n+   vpaddd(dst, dst, xtmp3, vec_enc);\n+\n+   \/\/ Replace -ve exponent with zero, exponent is -ve when src\n+   \/\/ lane contains a zero value.\n+   vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+   vblendvps(dst, dst, xtmp2, dst, vec_enc);\n+\n+   \/\/ Rematerialize broadcast 32.\n+   vpslld(xtmp1, xtmp3, 5, vec_enc);\n+   \/\/ Exponent is 32 if corresponding source lane contains max_int value.\n+   vpcmpeqd(xtmp2, dst, xtmp1, vec_enc);\n+   \/\/ LZCNT = 32 - exponent\n+   vpsubd(dst, xtmp1, dst, vec_enc);\n+\n+   \/\/ Replace LZCNT with a value 1 if corresponding source lane\n+   \/\/ contains max_int value.\n+   vpblendvb(dst, dst, xtmp3, xtmp2, vec_enc);\n+\n+   \/\/ Replace biased_exp with 0 if source lane value is less than zero.\n+   vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+   vblendvps(dst, dst, xtmp2, src, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_long_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+  \/\/ Add zero counts of lower word and upper word of a double word if\n+  \/\/ upper word holds a zero value.\n+  vpsrld(xtmp3, src, 16, vec_enc);\n+  vpcmpeqd(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpslld(xtmp2, dst, 16, vec_enc);\n+  vpaddd(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrld(dst, dst, 16, vec_enc);\n+  \/\/ Add zero counts of lower doubleword and upper doubleword of a\n+  \/\/ quadword if upper doubleword holds a zero value.\n+  vpsrlq(xtmp3, src, 32, vec_enc);\n+  vpcmpeqq(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpsllq(xtmp2, dst, 32, vec_enc);\n+  vpaddq(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrlq(dst, dst, 32, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                       Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"unexpected type\");\n+  assert(vec_enc < Assembler::AVX_512bit, \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      vector_count_leading_zeros_long_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vector_count_leading_zeros_int_avx(dst, src, xtmp1, xtmp2, xtmp3, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    case T_BYTE:\n+      vector_count_leading_zeros_byte_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":171,"deletions":0,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -356,0 +356,19 @@\n+  void vector_count_leading_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                       KRegister ktmp, Register rtmp, bool merge, int vec_enc);\n+\n+  void vector_count_leading_zeros_byte_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_short_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_int_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                          XMMRegister xtmp2, XMMRegister xtmp3, int vec_enc);\n+\n+  void vector_count_leading_zeros_long_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                      XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -190,0 +190,2 @@\n+      case Op_CountLeadingZerosV:\n+         return VM_Version::supports_avx512cd() && (ety == T_INT || ety == T_LONG) ? 0 : 40;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -591,0 +591,24 @@\n+  address generate_count_leading_zeros_lut(const char *stub_name) {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x02020304, relocInfo::none, 0);\n+    __ emit_data(0x01010101, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    __ emit_data(0x00000000, relocInfo::none, 0);\n+    return start;\n+  }\n+\n+\n@@ -4122,0 +4146,1 @@\n+    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -798,0 +798,15 @@\n+  address generate_count_leading_zeros_lut(const char *stub_name) {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -7790,0 +7805,1 @@\n+    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+address StubRoutines::x86::_vector_count_leading_zeros_lut = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+  static address _vector_count_leading_zeros_lut;\n@@ -348,0 +349,4 @@\n+  static address vector_count_leading_zeros_lut() {\n+    return _vector_count_leading_zeros_lut;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1248,0 +1248,5 @@\n+static inline bool is_clz_evex_predicate(BasicType bt, int vlen_bytes) {\n+  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n+           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+}\n+\n@@ -1256,0 +1261,1 @@\n+\n@@ -1914,0 +1920,6 @@\n+      break;\n+    case Op_CountLeadingZerosV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n@@ -2068,0 +2080,4 @@\n+    case Op_CountLeadingZerosV:\n+      if ((bt == T_INT || bt == T_LONG) && VM_Version::supports_avx512cd()) {\n+        return true;\n+      }\n@@ -9075,0 +9091,105 @@\n+\/\/ ---------------------------------- Vector Count Leading Zeros -----------------------------------\n+\n+instruct vcount_leading_zeros_IL_reg_evex(vec dst, vec src) %{\n+  predicate(is_clz_evex_predicate(Matcher::vector_element_basic_type(n->in(1)), Matcher::vector_length_in_bytes(n)));\n+  match(Set dst (CountLeadingZerosV src));\n+  format %{ \"vector_count_leading_zeros $dst, $src\" %}\n+  ins_encode %{\n+     int vlen_enc = vector_length_encoding(this, $src);\n+     BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+     BasicType rbt = Matcher::vector_element_basic_type(this);\n+     __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,\n+                                        xnoreg, xnoreg, k0, noreg, true, vlen_enc);\n+     \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n+     \/\/ should be succeeded by its corresponding vector IR and following\n+     \/\/ special handling should be removed.\n+     if (rbt == T_INT && bt == T_LONG) {\n+       __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+     }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_IL_reg_evex_masked(vec dst, vec src, kReg mask) %{\n+  predicate(is_clz_evex_predicate(Matcher::vector_element_basic_type(n->in(1)), Matcher::vector_length_in_bytes(n)));\n+  match(Set dst (CountLeadingZerosV src mask));\n+  format %{ \"vector_count_leading_zeros $dst, $src, $mask\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg,\n+                                       xnoreg, $mask$$KRegister, noreg, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+             VM_Version::supports_avx512cd() &&\n+             (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vector_count_leading_zeros $dst, $src!\\t using $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, xnoreg, knoreg, noreg, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, kReg ktmp, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE &&\n+             (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP ktmp, TEMP rtmp);\n+  format %{ \"vector_count_leading_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3, $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $ktmp$$KRegister,\n+                                       $rtmp$$Register, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_int_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_INT &&\n+            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3);\n+  format %{ \"vector_count_leading_zeros $dst, $src\\t! using $xtmp1, $xtmp2 and $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, noreg, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) != T_INT &&\n+            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  format %{ \"vector_count_leading_zeros $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (rbt == T_INT && bt == T_LONG) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":121,"deletions":0,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -978,0 +978,1 @@\n+      case Op_CountLeadingZerosV:\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2566,1 +2566,2 @@\n-                 opc == Op_PopCountI || opc == Op_PopCountL) {\n+                 opc == Op_PopCountI || opc == Op_PopCountL ||\n+                 opc == Op_CountLeadingZerosI || opc == Op_CountLeadingZerosL) {\n@@ -2958,3 +2959,3 @@\n-  if (VectorNode::is_vpopcnt_long(use)) {\n-    \/\/ VPOPCNT_LONG takes long and produces int - hence the special checks\n-    \/\/ on alignment and size.\n+  if (VectorNode::is_type_transition_long_to_int(use)) {\n+    \/\/ PopCountL\/CountLeadingZerosL takes long and produces\n+    \/\/ int - hence the special checks on alignment and size.\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -313,3 +313,7 @@\n-bool VectorNode::is_vpopcnt_long(Node* n) {\n-  if (n->Opcode() == Op_PopCountL) {\n-    return true;\n+bool VectorNode::is_type_transition_long_to_int(Node* n) {\n+  switch(n->Opcode()) {\n+    case Op_PopCountL:\n+    case Op_CountLeadingZerosL:\n+       return true;\n+    default:\n+       return false;\n@@ -317,1 +321,0 @@\n-  return false;\n@@ -320,3 +323,0 @@\n-\n-\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-  static bool is_vpopcnt_long(Node* n);\n+  static bool is_type_transition_long_to_int(Node* n);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}