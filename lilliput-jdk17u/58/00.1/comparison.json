{"files":[{"patch":"@@ -1989,1 +1989,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -3831,32 +3833,40 @@\n-    \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-    __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-    \/\/ Initialize the box. (Must happen before we update the object mark!)\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Compare object markWord with an unlocked value (tmp) and if\n-    \/\/ equal exchange the stack address of our box with object markWord.\n-    \/\/ On failure disp_hdr contains the possibly locked markWord.\n-    __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-               \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-    __ br(Assembler::EQ, cont);\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-    \/\/ object, will have now locked it will continue at label cont\n-\n-    __ bind(cas_failed);\n-    \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-    \/\/ Check if the owner is self by comparing the value in the\n-    \/\/ markWord of object (disp_hdr) with the stack pointer.\n-    __ mov(rscratch1, sp);\n-    __ sub(disp_hdr, disp_hdr, rscratch1);\n-    __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-    \/\/ If condition is true we are cont and hence we can store 0 as the\n-    \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-    __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-    __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    __ b(cont);\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+      \/\/ Initialize the box. (Must happen before we update the object mark!)\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Compare object markWord with an unlocked value (tmp) and if\n+      \/\/ equal exchange the stack address of our box with object markWord.\n+      \/\/ On failure disp_hdr contains the possibly locked markWord.\n+      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+      __ br(Assembler::EQ, cont);\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+      \/\/ object, will have now locked it will continue at label cont\n+\n+      __ bind(cas_failed);\n+      \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+      \/\/ Check if the owner is self by comparing the value in the\n+      \/\/ markWord of object (disp_hdr) with the stack pointer.\n+      __ mov(rscratch1, sp);\n+      __ sub(disp_hdr, disp_hdr, rscratch1);\n+      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+      \/\/ If condition is true we are cont and hence we can store 0 as the\n+      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      __ b(cont);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_lock(oop, disp_hdr, tmp, rscratch1, cont);\n+      __ b(cont);\n+    }\n@@ -3875,7 +3885,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3911,2 +3922,3 @@\n-    \/\/ Find the lock address and load the displaced header from the stack.\n-    __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Find the lock address and load the displaced header from the stack.\n+      __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n@@ -3914,3 +3926,4 @@\n-    \/\/ If the displaced header is 0, we have a recursive unlock.\n-    __ cmp(disp_hdr, zr);\n-    __ br(Assembler::EQ, cont);\n+      \/\/ If the displaced header is 0, we have a recursive unlock.\n+      __ cmp(disp_hdr, zr);\n+      __ br(Assembler::EQ, cont);\n+    }\n@@ -3922,7 +3935,16 @@\n-    \/\/ Check if it is still a light weight lock, this is is true if we\n-    \/\/ see the stack address of the basicLock in the markWord of the\n-    \/\/ object.\n-\n-    __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-               \/*release*\/ true, \/*weak*\/ false, tmp);\n-    __ b(cont);\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Check if it is still a light weight lock, this is is true if we\n+      \/\/ see the stack address of the basicLock in the markWord of the\n+      \/\/ object.\n+\n+      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+      __ b(cont);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_unlock(oop, tmp, box, disp_hdr, cont);\n+      __ b(cont);\n+    }\n@@ -3936,0 +3958,14 @@\n+\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -7442,1 +7478,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7452,0 +7488,26 @@\n+instruct loadNKlassLilliput(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":115,"deletions":53,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -3808,2 +3809,7 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n@@ -3811,0 +3817,32 @@\n+    return;\n+  }\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n+void MacroAssembler::load_klass(Register dst, Register src, bool null_check_src) {\n+  if (null_check_src) {\n+    if (UseCompactObjectHeaders) {\n+      null_check(src, oopDesc::mark_offset_in_bytes());\n+    } else {\n+      null_check(src, oopDesc::klass_offset_in_bytes());\n+    }\n+  }\n+\n+  if (UseCompressedClassPointers) {\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(dst, src);\n+    } else {\n+      ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -3849,0 +3887,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -3850,1 +3889,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -3867,5 +3910,0 @@\n-void MacroAssembler::load_prototype_header(Register dst, Register src) {\n-  load_klass(dst, src);\n-  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n-}\n-\n@@ -3890,0 +3928,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5382,0 +5425,94 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with ZF set.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n+  br(Assembler::GT, slow);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with ZF set.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - t1, t2: temporary registers\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GT, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(t1, t1, oopSize);\n+    ldr(t1, Address(rthread, t1));\n+    cmpoop(t1, obj);\n+    br(Assembler::EQ, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    tst(hdr, markWord::lock_mask_in_place);\n+    br(Assembler::EQ, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(t1, t1, oopSize);\n+#ifdef ASSERT\n+  str(zr, Address(rthread, t1));\n+#endif\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":145,"deletions":8,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -826,1 +826,2 @@\n-  void load_klass(Register dst, Register src);\n+  void load_nklass(Register dst, Register src);\n+  void load_klass(Register dst, Register src, bool null_check = false);\n@@ -853,2 +854,0 @@\n-  void load_prototype_header(Register dst, Register src);\n-\n@@ -857,0 +856,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -1426,0 +1427,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1775,7 +1775,6 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n-    }\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %r0\n-    __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ orr(swap_reg, rscratch1, 1);\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n+      }\n@@ -1783,2 +1782,3 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+      \/\/ Load (object->mark() | 1) into swap_reg %r0\n+      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ orr(swap_reg, rscratch1, 1);\n@@ -1786,4 +1786,2 @@\n-    \/\/ src -> dest iff dest == r0 else r0 <- dest\n-    { Label here;\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n-    }\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n@@ -1791,1 +1789,4 @@\n-    \/\/ Hmm should this move to the slow path code area???\n+      \/\/ src -> dest iff dest == r0 else r0 <- dest\n+      { Label here;\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n+      }\n@@ -1793,8 +1794,1 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ Hmm should this move to the slow path code area???\n@@ -1802,3 +1796,8 @@\n-    __ sub(swap_reg, sp, swap_reg);\n-    __ neg(swap_reg, swap_reg);\n-    __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n@@ -1806,3 +1805,3 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-    __ br(Assembler::NE, slow_path_lock);\n+      __ sub(swap_reg, sp, swap_reg);\n+      __ neg(swap_reg, swap_reg);\n+      __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n@@ -1810,0 +1809,8 @@\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+      __ br(Assembler::NE, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+    }\n@@ -1931,4 +1938,5 @@\n-    \/\/ Simple recursive lock?\n-\n-    __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    __ cbz(rscratch1, done);\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Simple recursive lock?\n+      __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      __ cbz(rscratch1, done);\n+    }\n@@ -1942,4 +1950,7 @@\n-    \/\/ get address of the stack lock\n-    __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    \/\/  get old displaced header\n-    __ ldr(old_hdr, Address(r0, 0));\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ get address of the stack lock\n+      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ ldr(old_hdr, Address(r0, 0));\n@@ -1947,4 +1958,10 @@\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    Label succeed;\n-    __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n-    __ bind(succeed);\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      Label succeed;\n+      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n+      __ bind(succeed);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":57,"deletions":40,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -3226,2 +3226,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(r0, recv);\n+  __ load_klass(r0, recv, true);\n@@ -3316,2 +3315,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(r3, r2);\n+  __ load_klass(r3, r2, true);\n@@ -3333,2 +3331,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(r3, r2);\n+  __ load_klass(r3, r2, true);\n@@ -3529,1 +3526,1 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n@@ -3534,1 +3531,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      if (!is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong)) {\n+        __ strw(zr, Address(__ post(r2, BytesPerInt)));\n+        __ sub(r3, r3, BytesPerInt);\n+        __ cbz(r3, initialize_header);\n+      }\n@@ -3544,1 +3546,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || UseCompactObjectHeaders) {\n@@ -3550,3 +3552,4 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+    if (!UseCompactObjectHeaders) {\n+      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1646,1 +1646,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3071,0 +3071,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3262,7 +3263,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3428,4 +3423,1 @@\n-\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3434,2 +3426,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3438,2 +3429,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3501,1 +3491,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -3505,1 +3495,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || LockingMode == LM_LIGHTWEIGHT) {\n@@ -3534,1 +3524,14 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(*op->stub()->continuation());\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -3645,4 +3648,0 @@\n-#ifndef ASSERT\n-      __ jmpb(next);\n-    }\n-#else\n@@ -3651,0 +3650,1 @@\n+#ifdef ASSERT\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -57,0 +57,6 @@\n+#ifdef COMPILER2\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/output.hpp\"\n+#endif\n+\n@@ -3774,1 +3780,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -3780,0 +3786,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -3792,1 +3811,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -4736,1 +4754,23 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+ Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n@@ -4739,0 +4779,7 @@\n+  if (null_check_src) {\n+    if (UseCompactObjectHeaders) {\n+      null_check(src, oopDesc::mark_offset_in_bytes());\n+    } else {\n+      null_check(src, oopDesc::klass_offset_in_bytes());\n+    }\n+  }\n@@ -4741,1 +4788,1 @@\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    load_nklass(dst, src);\n@@ -4754,0 +4801,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4762,1 +4810,40 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+   movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    load_nklass(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass(tmp1, src);\n+    load_nklass(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n@@ -8689,0 +8776,67 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n+  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n+  \/\/ avoids one branch.\n+  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n+  jcc(Assembler::greater, slow);\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lock_mask bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set unlocked_value bit.\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), 0);\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":159,"deletions":5,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -342,1 +342,4 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#endif\n@@ -345,0 +348,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n@@ -1915,0 +1926,3 @@\n+\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1825,4 +1825,7 @@\n-    if (UseBiasedLocking) {\n-      \/\/ Note that oop_handle_reg is trashed during this call\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n-    }\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        \/\/ Note that oop_handle_reg is trashed during this call\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n+      }\n@@ -1830,2 +1833,2 @@\n-    \/\/ Load immediate 1 into swap_reg %rax,\n-    __ movptr(swap_reg, 1);\n+      \/\/ Load immediate 1 into swap_reg %rax,\n+      __ movptr(swap_reg, 1);\n@@ -1833,2 +1836,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-    __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1836,2 +1839,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -1839,5 +1842,5 @@\n-    \/\/ src -> dest iff dest == rax, else rax, <- dest\n-    \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-    __ lock();\n-    __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::equal, lock_done);\n+      \/\/ src -> dest iff dest == rax, else rax, <- dest\n+      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+      __ lock();\n+      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::equal, lock_done);\n@@ -1845,8 +1848,8 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n@@ -1854,2 +1857,2 @@\n-    __ subptr(swap_reg, rsp);\n-    __ andptr(swap_reg, 3 - os::vm_page_size());\n+      __ subptr(swap_reg, rsp);\n+      __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -1857,3 +1860,9 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-    __ jcc(Assembler::notEqual, slow_path_lock);\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      __ jcc(Assembler::notEqual, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+     \/\/ Load object header\n+     __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+     __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+    }\n@@ -1999,1 +2008,2 @@\n-    \/\/ Simple recursive lock?\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Simple recursive lock?\n@@ -2001,2 +2011,3 @@\n-    __ cmpptr(Address(rbp, lock_slot_rbp_offset), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, done);\n+      __ cmpptr(Address(rbp, lock_slot_rbp_offset), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, done);\n+    }\n@@ -2009,12 +2020,21 @@\n-    \/\/  get old displaced header\n-    __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ get address of the stack lock\n-    __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    \/\/ src -> dest iff dest == rax, else rax, <- dest\n-    \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-    __ lock();\n-    __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::notEqual, slow_path_unlock);\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/  get old displaced header\n+      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+      \/\/ get address of the stack lock\n+      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      \/\/ src -> dest iff dest == rax, else rax, <- dest\n+      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+      __ lock();\n+      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::notEqual, slow_path_unlock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":63,"deletions":43,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -2074,3 +2074,6 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n-    }\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n+      }\n@@ -2078,2 +2081,2 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    __ movl(swap_reg, 1);\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      __ movl(swap_reg, 1);\n@@ -2081,2 +2084,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2084,2 +2087,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2087,4 +2090,4 @@\n-    \/\/ src -> dest iff dest == rax else rax <- dest\n-    __ lock();\n-    __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::equal, lock_done);\n+      \/\/ src -> dest iff dest == rax else rax <- dest\n+      __ lock();\n+      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::equal, lock_done);\n@@ -2092,1 +2095,1 @@\n-    \/\/ Hmm should this move to the slow path code area???\n+      \/\/ Hmm should this move to the slow path code area???\n@@ -2094,8 +2097,8 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2103,2 +2106,2 @@\n-    __ subptr(swap_reg, rsp);\n-    __ andptr(swap_reg, 3 - os::vm_page_size());\n+      __ subptr(swap_reg, rsp);\n+      __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -2106,3 +2109,9 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-    __ jcc(Assembler::notEqual, slow_path_lock);\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      __ jcc(Assembler::notEqual, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+    }\n@@ -2233,1 +2242,2 @@\n-    \/\/ Simple recursive lock?\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Simple recursive lock?\n@@ -2235,2 +2245,3 @@\n-    __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, done);\n+      __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, done);\n+    }\n@@ -2243,10 +2254,18 @@\n-\n-    \/\/ get address of the stack lock\n-    __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    \/\/  get old displaced header\n-    __ movptr(old_hdr, Address(rax, 0));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    __ lock();\n-    __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::notEqual, slow_path_unlock);\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ get address of the stack lock\n+      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ movptr(old_hdr, Address(rax, 0));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      __ lock();\n+      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::notEqual, slow_path_unlock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":59,"deletions":40,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -931,0 +931,1 @@\n+      if (opLoadKlass->_stub) do_stub(opLoadKlass->_stub);\n@@ -1113,0 +1114,3 @@\n+  if (stub()) {\n+    masm->append_code_stub(stub());\n+  }\n@@ -2097,0 +2101,3 @@\n+  if (stub()) {\n+    out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1921,0 +1921,1 @@\n+  CodeStub* _stub;\n@@ -1922,1 +1923,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1925,1 +1926,1 @@\n-    {}\n+    , _stub(stub) {}\n@@ -1928,0 +1929,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2400,1 +2402,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -639,1 +639,1 @@\n-  CodeStub* slow_path = new MonitorExitStub(lock, UseFastLocking, monitor_no);\n+  CodeStub* slow_path = new MonitorExitStub(lock, LockingMode != LM_MONITOR, monitor_no);\n@@ -1260,1 +1260,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = UseCompactObjectHeaders ? new LoadKlassStub(klass) : NULL;\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -1777,0 +1778,2 @@\n+  SlidingForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -204,0 +205,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -209,0 +212,2 @@\n+\n+  SlidingForwarding::end();\n@@ -315,0 +320,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -689,1 +689,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -96,0 +97,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -114,0 +117,2 @@\n+  SlidingForwarding::end();\n+\n@@ -277,8 +282,24 @@\n-  {\n-    StrongRootsScope srs(0);\n-\n-    gch->full_process_roots(true,  \/\/ this is the adjust phase\n-                            GenCollectedHeap::SO_AllCodeCache,\n-                            false, \/\/ all roots\n-                            &adjust_pointer_closure,\n-                            &adjust_cld_closure);\n+  if (UseAltGCForwarding) {\n+    AdjustPointerClosure<true> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n+    {\n+      StrongRootsScope srs(0);\n+      gch->full_process_roots(true,  \/\/ this is the adjust phase\n+                              GenCollectedHeap::SO_AllCodeCache,\n+                              false, \/\/ all roots\n+                              &adjust_pointer_closure,\n+                              &adjust_cld_closure);\n+    }\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+  } else {\n+    AdjustPointerClosure<false> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n+    {\n+      StrongRootsScope srs(0);\n+      gch->full_process_roots(true,  \/\/ this is the adjust phase\n+                              GenCollectedHeap::SO_AllCodeCache,\n+                              false, \/\/ all roots\n+                              &adjust_pointer_closure,\n+                              &adjust_cld_closure);\n+    }\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n@@ -286,3 +307,0 @@\n-\n-  gch->gen_process_weak_roots(&adjust_pointer_closure);\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":29,"deletions":11,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -223,4 +223,0 @@\n-  if (is_in(object->klass_or_null())) {\n-    return false;\n-  }\n-\n@@ -245,2 +241,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -395,1 +393,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -401,5 +401,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -419,2 +416,3 @@\n-    Copy::fill_to_words(start + filler_array_hdr_size(),\n-                        words - filler_array_hdr_size(), 0XDEAFBABE);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, 0XDEAFBABE);\n@@ -431,2 +429,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -157,1 +157,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -140,0 +141,2 @@\n+  SlidingForwarding::initialize(_reserved, SpaceAlignment \/ HeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -360,1 +360,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -368,0 +370,2 @@\n+  } else if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n@@ -375,1 +379,3 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -404,0 +405,2 @@\n+  SlidingForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n@@ -955,1 +958,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1303,0 +1306,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -286,1 +286,1 @@\n-  size_t size = p->size();\n+  size_t size = p->forward_safe_size();\n@@ -503,1 +503,1 @@\n-    int size = obj->size();\n+    size_t size = obj->forward_safe_size();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -196,0 +196,10 @@\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -201,1 +211,1 @@\n-                           _prototype_header(markWord::prototype()),\n+                           _prototype_header(make_prototype(this)),\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -39,0 +39,2 @@\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -55,0 +57,3 @@\n+markWord oopDesc::mark_acquire() const {\n+  return Atomic::load_acquire(&_mark);\n+}\n@@ -71,0 +76,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -79,0 +88,17 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n+  markWord hdr = mark();\n+  if (hdr.has_displaced_mark_helper()) {\n+    hdr = hdr.displaced_mark_helper();\n+  }\n+  return hdr;\n+}\n+\n+markWord oopDesc::prototype_mark() const {\n+  if (UseCompactObjectHeaders) {\n+    return klass()->prototype_header();\n+  } else {\n+    return markWord::prototype();\n+  }\n+}\n+\n@@ -84,1 +110,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -86,3 +117,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -92,1 +123,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -94,3 +130,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -100,6 +136,14 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n-  }\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = mark_acquire();\n+    if (header.has_monitor()) {\n+      header = header.monitor()->header();\n+    }\n+    return header.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  return Atomic::load_acquire(&_metadata._klass);\n@@ -110,0 +154,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -119,0 +164,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -129,0 +175,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't get Klass* gap with compact headers\");\n@@ -133,0 +180,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -139,0 +187,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -206,0 +255,47 @@\n+#ifdef _LP64\n+Klass* oopDesc::forward_safe_klass_impl(markWord m) const {\n+  assert(UseCompactObjectHeaders, \"Only get here with compact headers\");\n+  if (m.is_marked()) {\n+    oop fwd = forwardee(m);\n+    markWord m2 = fwd->mark();\n+    assert(!m2.is_marked() || m2.self_forwarded(), \"no double forwarding: this: \" PTR_FORMAT \" (\" INTPTR_FORMAT \"), fwd: \" PTR_FORMAT \" (\" INTPTR_FORMAT \")\", p2i(this), m.value(), p2i(fwd), m2.value());\n+    m = m2;\n+  }\n+  return m.actual_mark().klass();\n+}\n+#endif\n+\n+Klass* oopDesc::forward_safe_klass(markWord m) const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(m);\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+Klass* oopDesc::forward_safe_klass() const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(mark());\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+size_t oopDesc::forward_safe_size() {\n+  return size_given_klass(forward_safe_klass());\n+}\n+\n+void oopDesc::forward_safe_init_mark() {\n+  if (UseCompactObjectHeaders) {\n+    set_mark(forward_safe_klass()->prototype_header());\n+  } else {\n+    init_mark();\n+  }\n+}\n+\n@@ -277,0 +373,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -279,1 +376,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -283,6 +380,18 @@\n-\/\/ Used by parallel scavengers\n-bool oopDesc::cas_forward_to(oop p, markWord compare, atomic_memory_order order) {\n-  verify_forwardee(p);\n-  markWord m = markWord::encode_pointer_as_mark(p);\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n-  return cas_set_mark(m, compare, order) == compare;\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    markWord m = mark();\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    set_mark(m);\n+  } else\n+#endif\n+  {\n+    forward_to(oop(this));\n+  }\n@@ -292,0 +401,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -294,1 +404,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -299,1 +409,39 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+   markWord m = compare;\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    markWord old_mark = cas_set_mark(m, compare, order);\n+    if (old_mark == compare) {\n+      return nullptr;\n+    } else {\n+      assert(old_mark.is_marked(), \"must be marked here\");\n+      return forwardee(old_mark);\n+    }\n+  } else\n+#endif\n+  {\n+    return forward_to_atomic(cast_to_oop(this), compare, order);\n+  }\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"only decode when actually forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    return cast_to_oop(header.decode_pointer());\n@@ -307,1 +455,1 @@\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n@@ -362,1 +510,1 @@\n-  assert(k == klass(), \"wrong klass\");\n+  assert(UseCompactObjectHeaders || k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":173,"deletions":25,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -1680,1 +1680,1 @@\n-  if (UseBiasedLocking && Opcode() == Op_Allocate) {\n+  if ((UseBiasedLocking && Opcode() == Op_Allocate) || UseCompactObjectHeaders) {\n@@ -1685,0 +1685,1 @@\n+    \/\/ For now only enable fast locking for non-array types\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1650,0 +1650,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3859,2 +3859,2 @@\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+  Node *hash_mask      = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_mask_compact  : markWord::hash_mask);\n+  Node *hash_shift     = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_shift_compact : markWord::hash_shift);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1680,1 +1680,4 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1850,0 +1850,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2020,0 +2027,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2104,1 +2118,1 @@\n-  if (alloc != nullptr && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n+  if (alloc != nullptr && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking) && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -228,67 +228,0 @@\n-volatile int C2SafepointPollStubTable::_stub_size = 0;\n-\n-Label& C2SafepointPollStubTable::add_safepoint(uintptr_t safepoint_offset) {\n-  C2SafepointPollStub* entry = new (Compile::current()->comp_arena()) C2SafepointPollStub(safepoint_offset);\n-  _safepoints.append(entry);\n-  return entry->_stub_label;\n-}\n-\n-void C2SafepointPollStubTable::emit(CodeBuffer& cb) {\n-  MacroAssembler masm(&cb);\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    \/\/ Make sure there is enough space in the code buffer\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    }\n-\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-  }\n-}\n-\n-int C2SafepointPollStubTable::stub_size_lazy() const {\n-  int size = Atomic::load(&_stub_size);\n-\n-  if (size != 0) {\n-    return size;\n-  }\n-\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-  MacroAssembler masm(&cb);\n-  C2SafepointPollStub* entry = _safepoints.at(0);\n-  emit_stub(masm, entry);\n-  size += cb.insts_size();\n-\n-  Atomic::store(&_stub_size, size);\n-\n-  return size;\n-}\n-\n-int C2SafepointPollStubTable::estimate_stub_size() const {\n-  if (_safepoints.length() == 0) {\n-    return 0;\n-  }\n-\n-  int result = stub_size_lazy() * _safepoints.length();\n-\n-#ifdef ASSERT\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  int size = 0;\n-\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-    MacroAssembler masm(&cb);\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-    size += cb.insts_size();\n-  }\n-  assert(size == result, \"stubs should not have variable size\");\n-#endif\n-\n-  return result;\n-}\n-\n@@ -301,0 +234,1 @@\n+    _stub_list(),\n@@ -1316,1 +1250,0 @@\n-  stub_req += safepoint_poll_table()->estimate_stub_size();\n@@ -1823,2 +1756,2 @@\n-  \/\/ Fill in stubs for calling the runtime from safepoint polls.\n-  safepoint_poll_table()->emit(*cb);\n+  \/\/ Fill in stubs.\n+  _stub_list.emit(*cb);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":3,"deletions":70,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/c2_CodeStubs.hpp\"\n@@ -75,41 +76,0 @@\n-class C2SafepointPollStubTable {\n-private:\n-  struct C2SafepointPollStub: public ResourceObj {\n-    uintptr_t _safepoint_offset;\n-    Label     _stub_label;\n-    Label     _trampoline_label;\n-    C2SafepointPollStub(uintptr_t safepoint_offset) :\n-      _safepoint_offset(safepoint_offset),\n-      _stub_label(),\n-      _trampoline_label() {}\n-  };\n-\n-  GrowableArray<C2SafepointPollStub*> _safepoints;\n-\n-  static volatile int _stub_size;\n-\n-  void emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const;\n-\n-  \/\/ The selection logic below relieves the need to add dummy files to unsupported platforms.\n-  template <bool enabled>\n-  typename EnableIf<enabled>::type\n-  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-    emit_stub_impl(masm, entry);\n-  }\n-\n-  template <bool enabled>\n-  typename EnableIf<!enabled>::type\n-  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {}\n-\n-  void emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-    select_emit_stub<VM_Version::supports_stack_watermark_barrier()>(masm, entry);\n-  }\n-\n-  int stub_size_lazy() const;\n-\n-public:\n-  Label& add_safepoint(uintptr_t safepoint_offset);\n-  int estimate_stub_size() const;\n-  void emit(CodeBuffer& cb);\n-};\n-\n@@ -124,1 +84,1 @@\n-  C2SafepointPollStubTable _safepoint_poll_table;\/\/ Table for safepoint polls\n+  C2CodeStubList         _stub_list;             \/\/ List of code stubs\n@@ -172,2 +132,2 @@\n-  \/\/ Safepoint poll table\n-  C2SafepointPollStubTable* safepoint_poll_table() { return &_safepoint_poll_table; }\n+  \/\/ Code stubs list\n+  void add_stub(C2CodeStub* stub) { _stub_list.add_stub(stub); }\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":4,"deletions":44,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -307,3 +307,2 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n-    \/\/ Align to next 8 bytes to avoid trashing arrays's length.\n-    const size_t aligned_hs = align_object_offset(hs);\n+    size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n+    assert(is_aligned(hs_bytes, BytesPerInt), \"must be 4 byte aligned\");\n@@ -311,2 +310,3 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (!is_aligned(hs_bytes, BytesPerLong)) {\n+      *reinterpret_cast<jint*>(reinterpret_cast<char*>(obj) + hs_bytes) = 0;\n+      hs_bytes += BytesPerInt;\n@@ -314,0 +314,1 @@\n+\n@@ -315,0 +316,2 @@\n+    assert(is_aligned(hs_bytes, BytesPerLong), \"must be 8-byte aligned\");\n+    const size_t aligned_hs = hs_bytes \/ BytesPerLong;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -4564,1 +4564,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -133,0 +133,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use 64-bit object headers instead of 96-bit headers\")            \\\n+                                                                            \\\n@@ -150,0 +153,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -2094,0 +2098,13 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(int, LockingMode, LM_LEGACY, EXPERIMENTAL,                        \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only (LM_MONITOR), \"                                 \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n+          range(0, 2)                                                       \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n@@ -1138,1 +1154,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1358,1 +1374,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1407,0 +1423,1 @@\n+  assert(cur != anon_owner_ptr(), \"no anon owner here\");\n@@ -1410,1 +1427,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -278,4 +280,12 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -351,11 +361,13 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-NULL so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ Biased Locking in the object's header, the second check is for\n-    \/\/ stack-locking in the object's header, the third check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-NULL so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ Biased Locking in the object's header, the second check is for\n+      \/\/ stack-locking in the object's header, the third check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -428,0 +440,8 @@\n+static bool useHeavyMonitors() {\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64)\n+  return LockingMode == LM_MONITOR;\n+#else\n+  return false;\n+#endif\n+}\n+\n@@ -439,3 +459,23 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-  }\n+  if (!useHeavyMonitors()) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+        }\n+      }\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        BiasedLocking::revoke(current, obj);\n+      }\n@@ -443,2 +483,16 @@\n-  markWord mark = obj->mark();\n-  assert(!mark.has_bias_pattern(), \"should not see bias pattern here\");\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(NULL));\n+        return;\n+      }\n@@ -446,6 +500,5 @@\n-  if (mark.is_neutral()) {\n-    \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-    \/\/ be visible <= the ST performed by the CAS.\n-    lock->set_displaced_header(mark);\n-    if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n-      return;\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n@@ -453,7 +506,0 @@\n-    \/\/ Fall through to inflate() ...\n-  } else if (mark.has_locker() &&\n-             current->is_lock_owned((address)mark.locker())) {\n-    assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-    assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-    lock->set_displaced_header(markWord::from_pointer(NULL));\n-    return;\n@@ -462,5 +508,0 @@\n-  \/\/ The object header will never be displaced to this lock,\n-  \/\/ so it does not matter what the value is, except that it\n-  \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-  \/\/ and must not look locked either.\n-  lock->set_displaced_header(markWord::unused_mark());\n@@ -479,29 +520,21 @@\n-  markWord mark = object->mark();\n-  \/\/ We cannot check for Biased Locking if we are racing an inflation.\n-  assert(mark == markWord::INFLATING() ||\n-         !mark.has_bias_pattern(), \"should not see bias pattern here\");\n-\n-  markWord dhw = lock->displaced_header();\n-  if (dhw.value() == 0) {\n-    \/\/ If the displaced header is NULL, then this exit matches up with\n-    \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-    if (mark != markWord::INFLATING()) {\n-      \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-      \/\/ exiting a recursive enter of a Java Monitor that is being\n-      \/\/ inflated is safe; see the has_monitor() comment below.\n-      assert(!mark.is_neutral(), \"invariant\");\n-      assert(!mark.has_locker() ||\n-             current->is_lock_owned((address)mark.locker()), \"invariant\");\n-      if (mark.has_monitor()) {\n-        \/\/ The BasicLock's displaced_header is marked as a recursive\n-        \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-        \/\/ This is a special case where the Java Monitor was inflated\n-        \/\/ after this thread entered the stack-lock recursively. When a\n-        \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-        \/\/ Monitor owner's stack and update the BasicLocks because a\n-        \/\/ Java Monitor can be asynchronously inflated by a thread that\n-        \/\/ does not own the Java Monitor.\n-        ObjectMonitor* m = mark.monitor();\n-        assert(m->object()->mark() == mark, \"invariant\");\n-        assert(m->is_entered(current), \"invariant\");\n+  if (!useHeavyMonitors()) {\n+    markWord mark = object->mark();\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n+        }\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -509,1 +542,27 @@\n-    }\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is NULL, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -511,2 +570,2 @@\n-    return;\n-  }\n+        return;\n+      }\n@@ -514,6 +573,8 @@\n-  if (mark == markWord::from_pointer(lock)) {\n-    \/\/ If the object is stack-locked by the current thread, try to\n-    \/\/ swing the displaced header from the BasicLock back to the mark.\n-    assert(dhw.is_neutral(), \"invariant\");\n-    if (object->cas_set_mark(dhw, mark) == mark) {\n-      return;\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -527,0 +588,7 @@\n+  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -691,3 +759,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -710,3 +785,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -738,1 +820,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -847,1 +930,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -853,0 +936,7 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -926,1 +1016,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -1003,2 +1100,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -1007,0 +1104,6 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -1031,2 +1134,0 @@\n-  address owner = NULL;\n-\n@@ -1035,3 +1136,10 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n-    owner = (address) mark.locker();\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1041,1 +1149,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -1046,6 +1154,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != NULL) {\n-    \/\/ owning_thread_from_monitor_owner() may also return NULL here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1255,2 +1359,8 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n@@ -1266,0 +1376,5 @@\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1275,3 +1390,65 @@\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(NULL, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be Java thread\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1290,3 +1467,2 @@\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n-    if (mark.has_locker()) {\n+    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1485,0 +1661,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1537,0 +1723,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1539,0 +1728,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":298,"deletions":107,"binary":false,"changes":405,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -95,1 +96,1 @@\n-#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -152,0 +153,3 @@\n+#if INCLUDE_VM_STRUCTS\n+#include \"runtime\/vmStructs.hpp\"\n+#endif\n@@ -708,0 +712,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n@@ -1097,2 +1102,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack(this) {\n@@ -1576,1 +1582,2 @@\n-  if (Thread::is_lock_owned(adr)) return true;\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n+ if (Thread::is_lock_owned(adr)) return true;\n@@ -2026,0 +2033,4 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    lock_stack().oops_do(f);\n+  }\n@@ -2854,0 +2865,7 @@\n+  \/\/ Should happen before any agent attaches and pokes into vmStructs\n+#if INCLUDE_VM_STRUCTS\n+  if (UseCompactObjectHeaders) {\n+    VMStructs::compact_headers_overrides();\n+  }\n+#endif\n+\n@@ -3743,0 +3761,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Not with new lightweight locking\");\n@@ -3772,0 +3791,25 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"Only with new lightweight locking\");\n+  DO_JAVA_THREADS(t_list, q) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return NULL;\n+}\n+\n+JavaThread* Threads::owning_thread_from_monitor(ThreadsList* t_list, ObjectMonitor* monitor) {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == NULL || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":48,"deletions":4,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1642,0 +1643,13 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_offset()      { return byte_offset_of(JavaThread, _lock_stack); }\n+  \/\/ Those offsets are used in code generators to access the LockStack that is embedded in this\n+  \/\/ JavaThread structure. Those accesses are relative to the current thread, which\n+  \/\/ is typically in a dedicated register.\n+  static ByteSize lock_stack_top_offset()  { return lock_stack_offset() + LockStack::top_offset(); }\n+  static ByteSize lock_stack_base_offset() { return lock_stack_offset() + LockStack::base_offset(); }\n+\n@@ -1774,0 +1788,3 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n+  static JavaThread* owning_thread_from_monitor(ThreadsList* t_list, ObjectMonitor* owner);\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -963,0 +963,9 @@\n+enum LockingMode {\n+  \/\/ Use only heavy monitors for locking\n+  LM_MONITOR     = 0,\n+  \/\/ Legacy stack-locking, with monitors as 2nd tier\n+  LM_LEGACY      = 1,\n+  \/\/ New lightweight locking, with monitors as 2nd tier\n+  LM_LIGHTWEIGHT = 2\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -214,0 +214,1 @@\n+        assert(VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != LockingMode.getLightweight());\n@@ -231,1 +232,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getLightweight()) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -150,0 +150,1 @@\n+  private Boolean compactObjectHeadersEnabled;\n@@ -963,0 +964,9 @@\n+  public boolean isCompactObjectHeadersEnabled() {\n+    if (compactObjectHeadersEnabled == null) {\n+      Flag flag = getCommandLineFlag(\"UseCompactObjectHeaders\");\n+      compactObjectHeadersEnabled = (flag == null) ? Boolean.FALSE:\n+                     (flag.getBool()? Boolean.TRUE: Boolean.FALSE);\n+    }\n+    return compactObjectHeadersEnabled.booleanValue();\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/VM.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -75,1 +76,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = Platform.is64bit() ? 3266 : 3258;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -316,0 +316,3 @@\n+    static final boolean COMPACT_HEADERS = WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+    static final int HEADER_SIZE = COMPACT_HEADERS ? 8 : (Platform.is64bit() ? 16 : 8);\n+\n@@ -375,1 +378,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(HEADER_SIZE, OBJ_ALIGN);\n@@ -382,1 +385,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(HEADER_SIZE, OBJ_ALIGN);\n@@ -392,1 +395,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(HEADER_SIZE, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"}]}