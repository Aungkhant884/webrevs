{"files":[{"patch":"@@ -2594,0 +2594,1 @@\n+  assert(type == T_BYTE || type == T_SHORT || type == T_CHAR || type == T_INT || type == T_LONG, \"\");\n@@ -2595,2 +2596,2 @@\n-  bool wide = type == T_SHORT || type == T_LONG || type == T_CHAR;\n-  bool bwinstr = type == T_BYTE ||  type == T_SHORT || type == T_CHAR;\n+  bool wide = type == T_SHORT || type == T_CHAR || type == T_LONG;\n+  int prefix = (type == T_BYTE ||  type == T_SHORT || type == T_CHAR) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n@@ -2601,1 +2602,0 @@\n-  int prefix = bwinstr ? VEX_SIMD_F2 : VEX_SIMD_F3;\n@@ -2610,0 +2610,1 @@\n+  assert(type == T_BYTE || type == T_SHORT || type == T_CHAR || type == T_INT || type == T_LONG, \"\");\n@@ -2611,2 +2612,2 @@\n-  bool wide = type == T_SHORT || type == T_LONG || type == T_CHAR;\n-  bool bwinstr = type == T_BYTE ||  type == T_SHORT || type == T_CHAR;\n+  bool wide = type == T_SHORT || type == T_CHAR || type == T_LONG;\n+  int prefix = (type == T_BYTE ||  type == T_SHORT || type == T_CHAR) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n@@ -2618,1 +2619,0 @@\n-  int prefix = bwinstr ? VEX_SIMD_F2 : VEX_SIMD_F3;\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -7967,67 +7967,0 @@\n-void MacroAssembler::copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                                       KRegister mask, Register length, Register index,\n-                                       Register temp, int shift, int offset,\n-                                       bool use64byteVector) {\n-  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n-  use64byteVector |= MaxVectorSize > 32 && AVX3Threshold == 0;\n-  if (!use64byteVector) {\n-    copy32_avx(dst, src, index, xmm, shift, offset);\n-    subptr(length, 32 >> shift);\n-    copy32_masked_avx(dst, src, xmm, mask, length, index, temp, shift, offset+32);\n-  } else {\n-    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-    assert(MaxVectorSize == 64, \"vector length != 64\");\n-    negptr(length);\n-    addq(length, 64);\n-    mov64(temp, -1);\n-    shrxq(temp, temp, length);\n-    kmovql(mask, temp);\n-    evmovdqu(xmm, mask, Address(src, index, scale, offset), Assembler::AVX_512bit, type[shift]);\n-    evmovdqu(Address(dst, index, scale, offset), mask, xmm, Assembler::AVX_512bit, type[shift]);\n-  }\n-}\n-\n-void MacroAssembler::copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                                       KRegister mask, Register length, Register index,\n-                                       Register temp, int shift, int offset) {\n-  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n-  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-  mov64(temp, 1);\n-  shlxq(temp, temp, length);\n-  decq(temp);\n-  kmovql(mask, temp);\n-  evmovdqu(xmm, mask, Address(src, index, scale, offset), Assembler::AVX_256bit, type[shift]);\n-  evmovdqu(Address(dst, index, scale, offset), mask, xmm, Assembler::AVX_256bit, type[shift]);\n-}\n-\n-\n-void MacroAssembler::copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                                int shift, int offset) {\n-  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n-  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-  vmovdqu(xmm, Address(src, index, scale, offset));\n-  vmovdqu(Address(dst, index, scale, offset), xmm);\n-}\n-\n-\n-void MacroAssembler::copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                                bool conjoint, int shift, int offset, bool use64byteVector) {\n-  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n-  use64byteVector |= MaxVectorSize > 32 && AVX3Threshold == 0;\n-  if (!use64byteVector) {\n-    if (conjoint) {\n-      copy32_avx(dst, src, index, xmm, shift, offset+32);\n-      copy32_avx(dst, src, index, xmm, shift, offset);\n-    } else {\n-      copy32_avx(dst, src, index, xmm, shift, offset);\n-      copy32_avx(dst, src, index, xmm, shift, offset+32);\n-    }\n-  } else {\n-    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-    evmovdquq(xmm, Address(src, index, scale, offset), Assembler::AVX_512bit);\n-    evmovdquq(Address(dst, index, scale, offset), xmm, Assembler::AVX_512bit);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -1042,0 +1042,12 @@\n+#ifdef _LP64\n+  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                    Register to, Register count, int shift,\n+                                    Register index, Register temp,\n+                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                             Register to, Register start_index, Register end_index,\n+                                             Register count, int shift, Register temp,\n+                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,249 @@\n+\/*\n+* Copyright (c) 2020, Intel Corporation.\n+*\n+* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+*\n+* This code is free software; you can redistribute it and\/or modify it\n+* under the terms of the GNU General Public License version 2 only, as\n+* published by the Free Software Foundation.\n+*\n+* This code is distributed in the hope that it will be useful, but WITHOUT\n+* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+* version 2 for more details (a copy is included in the LICENSE file that\n+* accompanied this code).\n+*\n+* You should have received a copy of the GNU General Public License version\n+* 2 along with this work; if not, write to the Free Software Foundation,\n+* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+*\n+* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+* or visit www.oracle.com if you need additional information or have any\n+* questions.\n+*\n+*\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#ifdef _LP64\n+\n+void MacroAssembler::arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                                  Register to, Register count, int shift,\n+                                                  Register index, Register temp,\n+                                                  bool use64byteVector, Label& L_entry, Label& L_exit) {\n+  Label L_entry_64, L_entry_96, L_entry_128;\n+  Label L_entry_160, L_entry_192;\n+\n+  int size_mat[][6] = {\n+  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+  };\n+\n+  \/\/ Case A) Special case for length less than equal to 32 bytes.\n+  cmpq(count, size_mat[shift][0]);\n+  jccb(Assembler::greater, L_entry_64);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift);\n+  jmp(L_exit);\n+\n+  \/\/ Case B) Special case for length less than equal to 64 bytes.\n+  BIND(L_entry_64);\n+  cmpq(count, size_mat[shift][1]);\n+  jccb(Assembler::greater, L_entry_96);\n+  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, use64byteVector);\n+  jmp(L_exit);\n+\n+  \/\/ Case C) Special case for length less than equal to 96 bytes.\n+  BIND(L_entry_96);\n+  cmpq(count, size_mat[shift][2]);\n+  jccb(Assembler::greater, L_entry_128);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  subq(count, 64 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64);\n+  jmp(L_exit);\n+\n+  \/\/ Case D) Special case for length less than equal to 128 bytes.\n+  BIND(L_entry_128);\n+  cmpq(count, size_mat[shift][3]);\n+  jccb(Assembler::greater, L_entry_160);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy32_avx(to, from, index, xmm, shift, 64);\n+  subq(count, 96 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 96);\n+  jmp(L_exit);\n+\n+  \/\/ Case E) Special case for length less than equal to 160 bytes.\n+  BIND(L_entry_160);\n+  cmpq(count, size_mat[shift][4]);\n+  jccb(Assembler::greater, L_entry_192);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+  subq(count, 128 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128);\n+  jmp(L_exit);\n+\n+  \/\/ Case F) Special case for length less than equal to 192 bytes.\n+  BIND(L_entry_192);\n+  cmpq(count, size_mat[shift][5]);\n+  jcc(Assembler::greater, L_entry);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+  copy32_avx(to, from, index, xmm, shift, 128);\n+  subq(count, 160 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 160);\n+  jmp(L_exit);\n+}\n+\n+void MacroAssembler::arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                                           Register to, Register start_index, Register end_index,\n+                                                           Register count, int shift, Register temp,\n+                                                           bool use64byteVector, Label& L_entry, Label& L_exit) {\n+  Label L_entry_64, L_entry_96, L_entry_128;\n+  Label L_entry_160, L_entry_192;\n+  bool avx3 = MaxVectorSize > 32 && AVX3Threshold == 0;\n+\n+  int size_mat[][6] = {\n+  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+  };\n+\n+  \/\/ Case A) Special case for length less than equal to 32 bytes.\n+  cmpq(count, size_mat[shift][0]);\n+  jccb(Assembler::greater, L_entry_64);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  jmp(L_exit);\n+\n+  \/\/ Case B) Special case for length less than equal to 64 bytes.\n+  BIND(L_entry_64);\n+  cmpq(count, size_mat[shift][1]);\n+  jccb(Assembler::greater, L_entry_96);\n+  if (avx3) {\n+     copy64_masked_avx(to, from, xmm, mask, count, start_index, temp, shift, 0, true);\n+  } else {\n+     copy32_avx(to, from, end_index, xmm, shift, -32);\n+     subq(count, 32 >> shift);\n+     copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  }\n+  jmp(L_exit);\n+\n+  \/\/ Case C) Special case for length less than equal to 96 bytes.\n+  BIND(L_entry_96);\n+  cmpq(count, size_mat[shift][2]);\n+  jccb(Assembler::greater, L_entry_128);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  subq(count, 64 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  jmp(L_exit);\n+\n+  \/\/ Case D) Special case for length less than equal to 128 bytes.\n+  BIND(L_entry_128);\n+  cmpq(count, size_mat[shift][3]);\n+  jccb(Assembler::greater, L_entry_160);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy32_avx(to, from, end_index, xmm, shift, -96);\n+  subq(count, 96 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  jmp(L_exit);\n+\n+  \/\/ Case E) Special case for length less than equal to 160 bytes.\n+  BIND(L_entry_160);\n+  cmpq(count, size_mat[shift][4]);\n+  jccb(Assembler::greater, L_entry_192);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+  subq(count, 128 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  jmp(L_exit);\n+\n+  \/\/ Case F) Special case for length less than equal to 192 bytes.\n+  BIND(L_entry_192);\n+  cmpq(count, size_mat[shift][5]);\n+  jcc(Assembler::greater, L_entry);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+  copy32_avx(to, from, end_index, xmm, shift, -160);\n+  subq(count, 160 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  jmp(L_exit);\n+}\n+\n+void MacroAssembler::copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset,\n+                                       bool use64byteVector) {\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  if (!use64byteVector) {\n+    copy32_avx(dst, src, index, xmm, shift, offset);\n+    subptr(length, 32 >> shift);\n+    copy32_masked_avx(dst, src, xmm, mask, length, index, temp, shift, offset+32);\n+  } else {\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    assert(MaxVectorSize == 64, \"vector length != 64\");\n+    negptr(length);\n+    addq(length, 64);\n+    mov64(temp, -1);\n+    shrxq(temp, temp, length);\n+    kmovql(mask, temp);\n+    evmovdqu(xmm, mask, Address(src, index, scale, offset), Assembler::AVX_512bit, type[shift]);\n+    evmovdqu(Address(dst, index, scale, offset), mask, xmm, Assembler::AVX_512bit, type[shift]);\n+  }\n+}\n+\n+\n+void MacroAssembler::copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  mov64(temp, 1);\n+  shlxq(temp, temp, length);\n+  decq(temp);\n+  kmovql(mask, temp);\n+  evmovdqu(xmm, mask, Address(src, index, scale, offset), Assembler::AVX_256bit, type[shift]);\n+  evmovdqu(Address(dst, index, scale, offset), mask, xmm, Assembler::AVX_256bit, type[shift]);\n+}\n+\n+\n+void MacroAssembler::copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                int shift, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  vmovdqu(xmm, Address(src, index, scale, offset));\n+  vmovdqu(Address(dst, index, scale, offset), xmm);\n+}\n+\n+\n+void MacroAssembler::copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                bool conjoint, int shift, int offset, bool use64byteVector) {\n+  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n+  if (!use64byteVector) {\n+    if (conjoint) {\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+    } else {\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+    }\n+  } else {\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    evmovdquq(xmm, Address(src, index, scale, offset), Assembler::AVX_512bit);\n+    evmovdquq(Address(dst, index, scale, offset), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_arrayCopy_avx3.cpp","additions":249,"deletions":0,"binary":false,"changes":249,"status":"added"},{"patch":"@@ -1295,67 +1295,11 @@\n-  void generate_arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n-                                             Register to, Register count, int shift,\n-                                             Register index, Register temp,\n-                                             bool use64byteVector, Label& L_entry, Label& L_exit) {\n-    Label L_entry_64, L_entry_96, L_entry_128;\n-    Label L_entry_160, L_entry_192;\n-\n-    int size_mat[][6] = {\n-    \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n-    \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n-    \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n-    \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n-    };\n-\n-    \/\/ Case A) Special case for length less than equal to 32 bytes.\n-    __ cmpq(count, size_mat[shift][0]);\n-    __ jccb(Assembler::greater, L_entry_64);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case B) Special case for length less than equal to 64 bytes.\n-    __ BIND(L_entry_64);\n-    __ cmpq(count, size_mat[shift][1]);\n-    __ jccb(Assembler::greater, L_entry_96);\n-    __ copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, use64byteVector);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case C) Special case for length less than equal to 96 bytes.\n-    __ BIND(L_entry_96);\n-    __ cmpq(count, size_mat[shift][2]);\n-    __ jccb(Assembler::greater, L_entry_128);\n-    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-    __ subq(count, 64 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case D) Special case for length less than equal to 128 bytes.\n-    __ BIND(L_entry_128);\n-    __ cmpq(count, size_mat[shift][3]);\n-    __ jccb(Assembler::greater, L_entry_160);\n-    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-    __ copy32_avx(to, from, index, xmm, shift, 64);\n-    __ subq(count, 96 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 96);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case E) Special case for length less than equal to 160 bytes.\n-    __ BIND(L_entry_160);\n-    __ cmpq(count, size_mat[shift][4]);\n-    __ jccb(Assembler::greater, L_entry_192);\n-    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-    __ copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n-    __ subq(count, 128 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case F) Special case for length less than equal to 192 bytes.\n-    __ BIND(L_entry_192);\n-    __ cmpq(count, size_mat[shift][5]);\n-    __ jcc(Assembler::greater, L_entry);\n-    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-    __ copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n-    __ copy32_avx(to, from, index, xmm, shift, 128);\n-    __ subq(count, 160 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 160);\n-    __ jmp(L_exit);\n-  }\n+  \/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n+  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+  \/\/   default configuration.\n+  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+  \/\/   copy performs better.\n+  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+  \/\/   64 byte vector registers (ZMMs).\n@@ -1373,0 +1317,1 @@\n+\n@@ -1379,1 +1324,1 @@\n-    bool use64byteVector = false;\n+    bool use64byteVector = MaxVectorSize > 32 && AVX3Threshold == 0;\n@@ -1436,2 +1381,2 @@\n-      generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                            temp4, temp3, use64byteVector, L_entry, L_exit);\n+      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n@@ -1560,2 +1505,2 @@\n-        generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                              temp4, temp3, use64byteVector, L_entry, L_exit);\n+        __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                        temp4, temp3, use64byteVector, L_entry, L_exit);\n@@ -1582,75 +1527,0 @@\n-  void generate_arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                                      Register to, Register start_index, Register end_index,\n-                                                      Register count, int shift, Register temp,\n-                                                      bool use64byteVector, Label& L_entry, Label& L_exit) {\n-    Label L_entry_64, L_entry_96, L_entry_128;\n-    Label L_entry_160, L_entry_192;\n-    bool avx3 = MaxVectorSize > 32 && AVX3Threshold == 0;\n-\n-    int size_mat[][6] = {\n-    \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n-    \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n-    \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n-    \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n-    };\n-\n-    \/\/ Case A) Special case for length less than equal to 32 bytes.\n-    __ cmpq(count, size_mat[shift][0]);\n-    __ jccb(Assembler::greater, L_entry_64);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case B) Special case for length less than equal to 64 bytes.\n-    __ BIND(L_entry_64);\n-    __ cmpq(count, size_mat[shift][1]);\n-    __ jccb(Assembler::greater, L_entry_96);\n-    if (avx3) {\n-       __ copy64_masked_avx(to, from, xmm, mask, count, start_index, temp, shift, 0, true);\n-    } else {\n-       __ copy32_avx(to, from, end_index, xmm, shift, -32);\n-       __ subq(count, 32 >> shift);\n-       __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-    }\n-    __ jmp(L_exit);\n-\n-    \/\/ Case C) Special case for length less than equal to 96 bytes.\n-    __ BIND(L_entry_96);\n-    __ cmpq(count, size_mat[shift][2]);\n-    __ jccb(Assembler::greater, L_entry_128);\n-    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-    __ subq(count, 64 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case D) Special case for length less than equal to 128 bytes.\n-    __ BIND(L_entry_128);\n-    __ cmpq(count, size_mat[shift][3]);\n-    __ jccb(Assembler::greater, L_entry_160);\n-    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-    __ copy32_avx(to, from, end_index, xmm, shift, -96);\n-    __ subq(count, 96 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case E) Special case for length less than equal to 160 bytes.\n-    __ BIND(L_entry_160);\n-    __ cmpq(count, size_mat[shift][4]);\n-    __ jccb(Assembler::greater, L_entry_192);\n-    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-    __ copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n-    __ subq(count, 128 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case F) Special case for length less than equal to 192 bytes.\n-    __ BIND(L_entry_192);\n-    __ cmpq(count, size_mat[shift][5]);\n-    __ jcc(Assembler::greater, L_entry);\n-    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-    __ copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n-    __ copy32_avx(to, from, end_index, xmm, shift, -160);\n-    __ subq(count, 160 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-    __ jmp(L_exit);\n-  }\n-\n@@ -1670,1 +1540,1 @@\n-    bool use64byteVector = false;\n+    bool use64byteVector = MaxVectorSize > 32 && AVX3Threshold == 0;\n@@ -1729,2 +1599,2 @@\n-      generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                                     temp4, use64byteVector, L_entry, L_exit);\n+      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                               temp4, use64byteVector, L_entry, L_exit);\n@@ -1808,2 +1678,2 @@\n-        generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                                       temp4, use64byteVector, L_entry, L_exit);\n+        __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                                 temp4, use64byteVector, L_entry, L_exit);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":22,"deletions":152,"binary":false,"changes":174,"status":"modified"},{"patch":"@@ -1167,1 +1167,1 @@\n-    if (AVX3Threshold != 0 && !is_power_of_2(AVX3Threshold)) {\n+    if (!is_power_of_2(AVX3Threshold)) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64ArrayCompareToOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64StringLatin1InflateOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64StringUTF16CompressOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}