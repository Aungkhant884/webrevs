{"files":[{"patch":"@@ -7968,2 +7968,4 @@\n-                                       KRegister mask, Register length, Register temp,\n-                                       BasicType type, int offset, bool use64byteVector) {\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset,\n+                                       bool use64byteVector) {\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n@@ -7973,2 +7975,1 @@\n-    int shift = exact_log2_long(type2aelembytes(type));\n-    copy32_avx(dst,src, xmm, offset);\n+    copy32_avx(dst, src, index, xmm, shift, offset);\n@@ -7976,1 +7977,1 @@\n-    copy32_masked_avx(dst, src, xmm, mask, length, temp, type, offset+32);\n+    copy32_masked_avx(dst, src, xmm, mask, length, index, temp, shift, offset+32);\n@@ -7978,0 +7979,1 @@\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n@@ -7984,2 +7986,2 @@\n-    evmovdqu(xmm, mask, Address(src, offset), Assembler::AVX_512bit, type);\n-    evmovdqu(Address(dst, offset), mask, xmm, Assembler::AVX_512bit, type);\n+    evmovdqu(xmm, mask, Address(src, index, scale, offset), Assembler::AVX_512bit, type[shift]);\n+    evmovdqu(Address(dst, index, scale, offset), mask, xmm, Assembler::AVX_512bit, type[shift]);\n@@ -7990,2 +7992,2 @@\n-                                       KRegister mask, Register length, Register temp,\n-                                       BasicType type, int offset) {\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset) {\n@@ -7993,0 +7995,2 @@\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n@@ -7997,2 +8001,2 @@\n-  evmovdqu(xmm, mask, Address(src, offset), Assembler::AVX_256bit, type);\n-  evmovdqu(Address(dst, offset), mask, xmm, Assembler::AVX_256bit, type);\n+  evmovdqu(xmm, mask, Address(src, index, scale, offset), Assembler::AVX_256bit, type[shift]);\n+  evmovdqu(Address(dst, index, scale, offset), mask, xmm, Assembler::AVX_256bit, type[shift]);\n@@ -8002,1 +8006,2 @@\n-void MacroAssembler::copy32_avx(Register dst, Register src, XMMRegister xmm, int offset) {\n+void MacroAssembler::copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                int shift, int offset) {\n@@ -8004,2 +8009,3 @@\n-  vmovdqu(xmm, Address(src, offset));\n-  vmovdqu(Address(dst, offset), xmm);\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  vmovdqu(xmm, Address(src, index, scale, offset));\n+  vmovdqu(Address(dst, index, scale, offset), xmm);\n@@ -8009,1 +8015,2 @@\n-void MacroAssembler::copy64_avx(Register dst, Register src, XMMRegister xmm, int offset, bool use64byteVector) {\n+void MacroAssembler::copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                bool conjoint, int shift, int offset, bool use64byteVector) {\n@@ -8013,18 +8020,7 @@\n-     vmovdqu(xmm, Address(src, offset));\n-     vmovdqu(Address(dst, offset), xmm);\n-     vmovdqu(xmm, Address(src, offset+32));\n-     vmovdqu(Address(dst, offset+32), xmm);\n-  } else {\n-     evmovdquq(xmm, Address(src, offset), Assembler::AVX_512bit);\n-     evmovdquq(Address(dst, offset), xmm, Assembler::AVX_512bit);\n-  }\n-}\n-\n-void MacroAssembler::copy64_conjoint_avx(Register dst, Register src, XMMRegister xmm, int offset, bool use64byteVector) {\n-  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n-  use64byteVector |= MaxVectorSize > 32 && AVX3Threshold == 0;\n-  if (!use64byteVector) {\n-     vmovdqu(xmm, Address(src, offset+32));\n-     vmovdqu(Address(dst, offset+32), xmm);\n-     vmovdqu(xmm, Address(src, offset));\n-     vmovdqu(Address(dst, offset), xmm);\n+    if (conjoint) {\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+    } else {\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+    }\n@@ -8032,2 +8028,3 @@\n-     evmovdquq(xmm, Address(src, offset), Assembler::AVX_512bit);\n-     evmovdquq(Address(dst, offset), xmm, Assembler::AVX_512bit);\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    evmovdquq(xmm, Address(src, index, scale, offset), Assembler::AVX_512bit);\n+    evmovdquq(Address(dst, index, scale, offset), xmm, Assembler::AVX_512bit);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":32,"deletions":35,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -1731,4 +1731,0 @@\n-  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register temp,\n-                         BasicType type, int offset=0);\n-\n@@ -1736,2 +1732,3 @@\n-                         KRegister mask, Register length, Register temp,\n-                         BasicType type, int offset=0, bool use64byteVector=false);\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0,\n+                         bool use64byteVector = false);\n@@ -1739,2 +1736,3 @@\n-  void copy64_avx(Register dst, Register src, XMMRegister xmm,\n-                  int offset=0, bool use64byteVector=false);\n+  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0);\n@@ -1742,2 +1740,2 @@\n-  void copy64_conjoint_avx(Register dst, Register src, XMMRegister xmm,\n-                           int offset=0, bool use64byteVector=false);\n+  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  int shift = Address::times_1, int offset = 0);\n@@ -1745,1 +1743,3 @@\n-  void copy32_avx(Register dst, Register src, XMMRegister xmm, int offset=0);\n+  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  bool conjoint, int shift = Address::times_1, int offset = 0,\n+                  bool use64byteVector = false);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1277,0 +1277,18 @@\n+  void setup_argument_regs(BasicType type) {\n+    if (type == T_BYTE || type == T_SHORT) {\n+      setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                        \/\/ r9 and r10 may be used to save non-volatile registers\n+    } else {\n+      setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                     \/\/ r9 is used to save r15_thread\n+    }\n+  }\n+\n+  void restore_argument_regs(BasicType type) {\n+    if (type == T_BYTE || type == T_SHORT) {\n+      restore_arg_regs();\n+    } else {\n+      restore_arg_regs_using_thread();\n+    }\n+  }\n+\n@@ -1279,1 +1297,1 @@\n-                                             BasicType type_shift, Register byte_size, Register temp,\n+                                             Register index, Register temp,\n@@ -1284,62 +1302,6 @@\n-    \/\/ Case A) Special case for length less than equal to 32 bytes.\n-    __ cmpq(byte_size, 32);\n-    __ jccb(Assembler::greater, L_entry_64);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case B) Special case for length less than equal to 64 bytes.\n-    __ BIND(L_entry_64);\n-    __ cmpq(byte_size, 64);\n-    __ jccb(Assembler::greater, L_entry_96);\n-    __ copy64_masked_avx(to, from, xmm, mask, count, temp, type_shift, 0, use64byteVector);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case C) Special case for length less than equal to 96 bytes.\n-    __ BIND(L_entry_96);\n-    __ cmpq(byte_size, 96);\n-    __ jccb(Assembler::greater, L_entry_128);\n-    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n-    __ subq(count, 64 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 64);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case D) Special case for length less than equal to 128 bytes.\n-    __ BIND(L_entry_128);\n-    __ cmpq(byte_size, 128);\n-    __ jccb(Assembler::greater, L_entry_160);\n-    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n-    __ copy32_avx(to, from, xmm, 64);\n-    __ subq(count, 96 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 96);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case E) Special case for length less than equal to 160 bytes.\n-    __ BIND(L_entry_160);\n-    __ cmpq(byte_size, 160);\n-    __ jccb(Assembler::greater, L_entry_192);\n-    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n-    __ copy64_avx(to, from, xmm, 64, use64byteVector);\n-    __ subq(count, 128 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 128);\n-    __ jmp(L_exit);\n-\n-    \/\/ Case F) Special case for length less than equal to 192 bytes.\n-    __ BIND(L_entry_192);\n-    __ cmpq(byte_size, 192);\n-    __ jcc(Assembler::greater, L_entry);\n-    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n-    __ copy64_avx(to, from, xmm, 64, use64byteVector);\n-    __ copy32_avx(to, from, xmm, 128);\n-    __ subq(count, 160 >> shift);\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 160);\n-    __ jmp(L_exit);\n-  }\n-\n-  void generate_arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                                      Register to, Register from_end, Register to_end,\n-                                                      Register count, int shift, BasicType type_shift,\n-                                                      Register byte_size, Register temp,\n-                                                      bool use64byteVector, Label& L_entry, Label& L_exit) {\n-    Label L_entry_64, L_entry_96, L_entry_128;\n-    Label L_entry_160, L_entry_192;\n-    bool avx3 = MaxVectorSize > 32 && AVX3Threshold == 0;\n+    int size_mat[][6] = {\n+    \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+    \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+    \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+    \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+    };\n@@ -1348,1 +1310,1 @@\n-    __ cmpq(byte_size, 32);\n+    __ cmpq(count, size_mat[shift][0]);\n@@ -1350,1 +1312,1 @@\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift);\n@@ -1355,1 +1317,1 @@\n-    __ cmpq(byte_size, 64);\n+    __ cmpq(count, size_mat[shift][1]);\n@@ -1357,7 +1319,1 @@\n-    if (avx3) {\n-       __ copy64_masked_avx(to, from, xmm, mask, count, temp, type_shift, 0, true);\n-    } else {\n-       __ copy32_avx(to_end, from_end, xmm, -32);\n-       __ subq(count, 32 >> shift);\n-       __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n-    }\n+    __ copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, use64byteVector);\n@@ -1368,1 +1324,1 @@\n-    __ cmpq(byte_size, 96);\n+    __ cmpq(count, size_mat[shift][2]);\n@@ -1370,1 +1326,1 @@\n-    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n+    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n@@ -1372,1 +1328,1 @@\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64);\n@@ -1377,1 +1333,1 @@\n-    __ cmpq(byte_size, 128);\n+    __ cmpq(count, size_mat[shift][3]);\n@@ -1379,2 +1335,2 @@\n-    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n-    __ copy32_avx(to_end, from_end, xmm, -96);\n+    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+    __ copy32_avx(to, from, index, xmm, shift, 64);\n@@ -1382,1 +1338,1 @@\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 96);\n@@ -1387,1 +1343,1 @@\n-    __ cmpq(byte_size, 160);\n+    __ cmpq(count, size_mat[shift][4]);\n@@ -1389,2 +1345,2 @@\n-    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n-    __ copy64_conjoint_avx(to_end, from_end, xmm, -128, use64byteVector);\n+    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+    __ copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n@@ -1392,1 +1348,1 @@\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128);\n@@ -1397,1 +1353,1 @@\n-    __ cmpq(byte_size, 192);\n+    __ cmpq(count, size_mat[shift][5]);\n@@ -1399,3 +1355,3 @@\n-    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n-    __ copy64_conjoint_avx(to_end, from_end, xmm, -128, use64byteVector);\n-    __ copy32_avx(to_end, from_end, xmm, -160);\n+    __ copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+    __ copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+    __ copy32_avx(to, from, index, xmm, shift, 128);\n@@ -1403,1 +1359,1 @@\n-    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 160);\n@@ -1407,3 +1363,0 @@\n-  \/\/ Arguments:\n-  \/\/   name    - stub name string\n-  \/\/\n@@ -1416,2 +1369,6 @@\n-  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             address nooverlap_target) {\n+  \/\/ Side Effects:\n+  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n+  \/\/\n+  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             bool aligned, bool is_oop, bool dest_uninitialized) {\n@@ -1423,2 +1380,0 @@\n-\n-    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n@@ -1426,0 +1381,1 @@\n+    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n@@ -1429,2 +1385,1 @@\n-    const Register from_end    = r8;\n-    const Register to_end      = rcx;\n+    const Register temp1       = r8;\n@@ -1433,0 +1388,1 @@\n+    const Register temp4       = rcx;\n@@ -1445,1 +1401,14 @@\n-    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+    setup_argument_regs(type);\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+    if (aligned) {\n+      decorators |= ARRAYCOPY_ALIGNED;\n+    }\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n@@ -1447,2 +1416,0 @@\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -1452,1 +1419,1 @@\n-      BasicType type_const[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+      int threshold[]        = { 4096,    2048,     1024,    512};\n@@ -1455,1 +1422,1 @@\n-      UnsafeCopyMemoryMark ucmm(this, true, true);\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -1458,0 +1425,5 @@\n+      \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n+      \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n+      __ mov64(temp4, 0);\n+      __ movq(temp1, count);\n+\n@@ -1460,1 +1432,1 @@\n-      __ cmpq(count, 0);\n+      __ cmpq(temp1, 0);\n@@ -1463,5 +1435,0 @@\n-      __ movq(temp2, count);\n-      if(shift) {\n-        __ shlptr(temp2, shift);\n-      }\n-\n@@ -1469,6 +1436,2 @@\n-      __ movq(from_end, from);\n-      __ movq(to_end, to);\n-      __ addptr(from_end, temp2);\n-      __ addptr(to_end, temp2);\n-      generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, from_end, to_end, count, shift, type_const[shift],\n-                                                     temp2, temp3, use64byteVector, L_entry, L_exit);\n+      generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                            temp4, temp3, use64byteVector, L_entry, L_exit);\n@@ -1479,3 +1442,10 @@\n-      if (MaxVectorSize > 32 && AVX3Threshold != 0) {\n-        __ cmpq(temp2, AVX3Threshold);\n-        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      if (AVX3Threshold != 0) {\n+        __ cmpq(count, threshold[shift]);\n+        if (MaxVectorSize == 64) {\n+          \/\/ Copy using 64 byte vectors.\n+          __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+        } else {\n+          assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n+          \/\/ REP MOVS offer a faster copy path.\n+          __ jcc(Assembler::greaterEqual, L_repmovs);\n+        }\n@@ -1486,2 +1456,2 @@\n-        __ movq(temp3, to_end);\n-        __ andq(temp3, 31);\n+        __ movq(temp2, to);\n+        __ andq(temp2, 31);\n@@ -1490,11 +1460,4 @@\n-        if(shift) {\n-          __ subptr(to_end, temp3);\n-          __ subptr(from_end, temp3);\n-          __ shrq(temp3, shift);\n-          __ copy32_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift]);\n-          __ subq(count, temp3);\n-        } else {\n-          __ subptr(to_end, temp3);\n-          __ subptr(from_end, temp3);\n-          __ copy32_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift]);\n-          __ subq(count, temp3);\n+        __ negptr(temp2);\n+        __ addq(temp2, 32);\n+        if (shift) {\n+          __ shrq(temp2, shift);\n@@ -1502,1 +1465,7 @@\n-        __ cmpq(count, loop_size[shift]);\n+        __ movq(temp3, temp2);\n+        __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n@@ -1506,1 +1475,1 @@\n-        __ subq(count, loop_size[shift]);\n+        __ subq(temp1, loop_size[shift]);\n@@ -1510,6 +1479,5 @@\n-           __ copy64_conjoint_avx(to_end, from_end, xmm1, -64);\n-           __ copy64_conjoint_avx(to_end, from_end, xmm1, -128);\n-           __ copy64_conjoint_avx(to_end, from_end, xmm1, -192);\n-           __ subptr(to_end, 192);\n-           __ subptr(from_end, 192);\n-           __ subq(count, loop_size[shift]);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n+           __ addptr(temp4, loop_size[shift]);\n+           __ subq(temp1, loop_size[shift]);\n@@ -1518,1 +1486,1 @@\n-        __ addq(count, loop_size[shift]);\n+        __ addq(temp1, loop_size[shift]);\n@@ -1522,0 +1490,26 @@\n+\n+        __ BIND(L_repmovs);\n+          __ movq(temp2, temp1);\n+          \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n+          __ movq(temp3, to);\n+          __ movq(to,  from);\n+          __ movq(from, temp3);\n+          \/\/ Save to\/from for restoration post rep_mov.\n+          __ movq(temp1, to);\n+          __ movq(temp3, from);\n+          if(shift < 3) {\n+            __ shrq(temp2, 3-shift);     \/\/ quad word count\n+          }\n+          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n+          __ rep_mov();\n+          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n+          if(shift) {\n+            __ shrq(temp2, shift);       \/\/ type specific count.\n+          }\n+          \/\/ Restore original addresses in to\/from.\n+          __ movq(to, temp3);\n+          __ movq(from, temp1);\n+          __ movq(temp4, temp2);\n+          __ movq(temp1, count);\n+          __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n+          __ jmp(L_tail);\n@@ -1527,2 +1521,2 @@\n-        __ movq(temp3, to_end);\n-        __ andq(temp3, 63);\n+        __ movq(temp2, to);\n+        __ andq(temp2, 63);\n@@ -1530,11 +1524,5 @@\n-        if(shift) {\n-          __ subptr(to_end, temp3);\n-          __ subptr(from_end, temp3);\n-          __ shrq(temp3, shift);\n-          __ subq(count, temp3);\n-          __ copy64_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift], 0, true);\n-        } else {\n-          __ subptr(to_end, temp3);\n-          __ subptr(from_end, temp3);\n-          __ subq(count, temp3);\n-          __ copy64_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift], 0, true);\n+\n+        __ negptr(temp2);\n+        __ addq(temp2, 64);\n+        if (shift) {\n+          __ shrq(temp2, shift);\n@@ -1542,1 +1530,7 @@\n-        __ cmpq(count, loop_size[shift]);\n+        __ movq(temp3, temp2);\n+        __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n@@ -1546,1 +1540,1 @@\n-        __ subq(count, loop_size[shift]);\n+        __ subq(temp1, loop_size[shift]);\n@@ -1551,6 +1545,5 @@\n-           __ copy64_conjoint_avx(to_end, from_end, xmm1, -64 , true);\n-           __ copy64_conjoint_avx(to_end, from_end, xmm1, -128, true);\n-           __ copy64_conjoint_avx(to_end, from_end, xmm1, -192, true);\n-           __ subptr(to_end, 192);\n-           __ subptr(from_end, 192);\n-           __ subq(count, loop_size[shift]);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n+           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n+           __ addptr(temp4, loop_size[shift]);\n+           __ subq(temp1, loop_size[shift]);\n@@ -1559,1 +1552,1 @@\n-        __ addq(count, loop_size[shift]);\n+        __ addq(temp1, loop_size[shift]);\n@@ -1564,4 +1557,0 @@\n-        __ movq(temp2, count);\n-        if(shift) {\n-          __ shlptr(temp2, shift);\n-        }\n@@ -1571,3 +1560,2 @@\n-        generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, from_end, to_end, count,\n-                                                       shift, type_const[shift], temp2, temp3,\n-                                                       use64byteVector, L_entry, L_exit);\n+        generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                              temp4, temp3, use64byteVector, L_entry, L_exit);\n@@ -1579,1 +1567,7 @@\n-    restore_arg_regs();\n+    \/\/ When called from generic_arraycopy r11 contains specific values\n+    \/\/ used during arraycopy epilogue, re-initializing r11.\n+    if (is_oop) {\n+      __ movq(r11, shift == 3 ? count : to);\n+    }\n+    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+    restore_argument_regs(type);\n@@ -1588,0 +1582,74 @@\n+  void generate_arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                                      Register to, Register start_index, Register end_index,\n+                                                      Register count, int shift, Register temp,\n+                                                      bool use64byteVector, Label& L_entry, Label& L_exit) {\n+    Label L_entry_64, L_entry_96, L_entry_128;\n+    Label L_entry_160, L_entry_192;\n+    bool avx3 = MaxVectorSize > 32 && AVX3Threshold == 0;\n+\n+    int size_mat[][6] = {\n+    \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+    \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+    \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+    \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+    };\n+\n+    \/\/ Case A) Special case for length less than equal to 32 bytes.\n+    __ cmpq(count, size_mat[shift][0]);\n+    __ jccb(Assembler::greater, L_entry_64);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case B) Special case for length less than equal to 64 bytes.\n+    __ BIND(L_entry_64);\n+    __ cmpq(count, size_mat[shift][1]);\n+    __ jccb(Assembler::greater, L_entry_96);\n+    if (avx3) {\n+       __ copy64_masked_avx(to, from, xmm, mask, count, start_index, temp, shift, 0, true);\n+    } else {\n+       __ copy32_avx(to, from, end_index, xmm, shift, -32);\n+       __ subq(count, 32 >> shift);\n+       __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+    }\n+    __ jmp(L_exit);\n+\n+    \/\/ Case C) Special case for length less than equal to 96 bytes.\n+    __ BIND(L_entry_96);\n+    __ cmpq(count, size_mat[shift][2]);\n+    __ jccb(Assembler::greater, L_entry_128);\n+    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+    __ subq(count, 64 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case D) Special case for length less than equal to 128 bytes.\n+    __ BIND(L_entry_128);\n+    __ cmpq(count, size_mat[shift][3]);\n+    __ jccb(Assembler::greater, L_entry_160);\n+    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+    __ copy32_avx(to, from, end_index, xmm, shift, -96);\n+    __ subq(count, 96 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case E) Special case for length less than equal to 160 bytes.\n+    __ BIND(L_entry_160);\n+    __ cmpq(count, size_mat[shift][4]);\n+    __ jccb(Assembler::greater, L_entry_192);\n+    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+    __ copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+    __ subq(count, 128 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case F) Special case for length less than equal to 192 bytes.\n+    __ BIND(L_entry_192);\n+    __ cmpq(count, size_mat[shift][5]);\n+    __ jcc(Assembler::greater, L_entry);\n+    __ copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+    __ copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+    __ copy32_avx(to, from, end_index, xmm, shift, -160);\n+    __ subq(count, 160 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+    __ jmp(L_exit);\n+  }\n@@ -1589,3 +1657,0 @@\n-  \/\/ Arguments:\n-  \/\/   name    - stub name string\n-  \/\/\n@@ -1598,5 +1663,3 @@\n-  \/\/ Side Effects:\n-  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n-  \/\/\n-  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift) {\n+  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             address nooverlap_target, bool aligned, bool is_oop,\n+                                             bool dest_uninitialized) {\n@@ -1608,0 +1671,2 @@\n+\n+    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n@@ -1609,1 +1674,0 @@\n-    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n@@ -1614,3 +1678,3 @@\n-    const Register temp2       = r11;\n-    const Register temp3       = rax;\n-    const Register temp4       = rcx;\n+    const Register temp2       = rcx;\n+    const Register temp3       = r11;\n+    const Register temp4       = rax;\n@@ -1629,2 +1693,16 @@\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n+    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+\n+    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+    setup_argument_regs(type);\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (dest_uninitialized) {\n+      decorators |= IS_DEST_UNINITIALIZED;\n+    }\n+    if (aligned) {\n+      decorators |= ARRAYCOPY_ALIGNED;\n+    }\n+    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n@@ -1632,4 +1710,3 @@\n-      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n-      int loop_size[]        = { 192,     96,       48,      24};\n-      int rep_movesize[]     = { 4096,    2048,     1024,    512};\n-      BasicType type_const[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+      \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n+      int loop_size[]   = { 192,     96,       48,      24};\n+      int threshold[]   = { 4096,    2048,     1024,    512};\n@@ -1638,1 +1715,1 @@\n-      UnsafeCopyMemoryMark ucmm(this, true, true);\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -1641,0 +1718,3 @@\n+      \/\/ temp1 holds remaining count.\n+      __ movq(temp1, count);\n+\n@@ -1643,1 +1723,1 @@\n-      __ cmpq(count, 0);\n+      __ cmpq(temp1, 0);\n@@ -1646,5 +1726,2 @@\n-      __ movq(temp2, count);\n-      if(shift) {\n-        __ shlptr(temp2, shift);\n-      }\n-\n+      __ mov64(temp2, 0);\n+      __ movq(temp3, temp1);\n@@ -1652,2 +1729,2 @@\n-      generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, count, shift, type_const[shift],\n-                                            temp2, temp3, use64byteVector, L_entry, L_exit);\n+      generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                                     temp4, use64byteVector, L_entry, L_exit);\n@@ -1657,1 +1734,0 @@\n-      __ movptr(temp1, to);\n@@ -1660,1 +1736,1 @@\n-        __ cmpq(temp2, AVX3Threshold);\n+        __ cmpq(temp1, threshold[shift]);\n@@ -1665,5 +1741,0 @@\n-        \/\/ If MaxVectorSize == 32 and copy size is more than 4096 bytes,\n-        \/\/ REP MOVS offer a faster copy path.\n-        __ cmpq(count, rep_movesize[shift]);\n-        __ jcc(Assembler::greaterEqual, L_repmovs);\n-\n@@ -1671,1 +1742,2 @@\n-        __ andq(temp1, 31);\n+        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+        __ andq(temp2, 31);\n@@ -1674,19 +1746,2 @@\n-        if(shift) {\n-          __ negptr(temp1);\n-          __ addq(temp1, 32);\n-          __ movq(temp4, temp1);\n-          __ shrq(temp1, shift);\n-          __ movq(temp2, temp1);\n-          __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp3, type_const[shift]);\n-          __ addptr(to, temp4);\n-          __ addptr(from, temp4);\n-          __ subq(count, temp1);\n-        } else {\n-          __ movq(temp4, temp1);\n-          __ negptr(temp4);\n-          __ addq(temp4, 32);\n-          __ movq(temp1, temp4);\n-          __ copy32_masked_avx(to, from, xmm1, k2, temp1, temp2, type_const[shift]);\n-          __ addptr(to, temp4);\n-          __ addptr(from, temp4);\n-          __ subq(count, temp4);\n+        if (shift) {\n+          __ shrq(temp2, shift);\n@@ -1694,1 +1749,4 @@\n-        __ cmpq(count, loop_size[shift]);\n+        __ subq(temp1, temp2);\n+        __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n+\n+        __ cmpq(temp1, loop_size[shift]);\n@@ -1698,1 +1756,0 @@\n-        __ subq(count, loop_size[shift]);\n@@ -1702,6 +1759,5 @@\n-           __ copy64_avx(to, from, xmm1);\n-           __ copy64_avx(to, from, xmm1, 64);\n-           __ copy64_avx(to, from, xmm1, 128);\n-           __ addptr(to, 192);\n-           __ addptr(from, 192);\n-           __ subq(count, loop_size[shift]);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n+           __ subptr(temp1, loop_size[shift]);\n+           __ cmpq(temp1, loop_size[shift]);\n@@ -1710,2 +1766,0 @@\n-        __ addq(count, loop_size[shift]);\n-\n@@ -1714,21 +1768,0 @@\n-\n-        __ BIND(L_repmovs);\n-          __ movq(temp2, count);\n-          \/\/ Swap to(RSI) and from(RDI) addresses.\n-          __ movq(temp1, to);\n-          __ movq(to,  from);\n-          __ movq(from, temp1);\n-          if(shift < 3) {\n-            __ shrq(temp2, 3-shift);     \/\/ quad word count\n-          }\n-          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n-          __ rep_mov();\n-          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n-          if(shift) {\n-            __ shrq(temp2, shift);       \/\/ type specific count.\n-          }\n-          __ subq(count, temp2);         \/\/ tailing part (less than a quad ward size).\n-          __ movq(temp1, to);\n-          __ movq(to,  from);\n-          __ movq(from, temp1);\n-          __ jmp(L_tail);\n@@ -1740,1 +1773,2 @@\n-        __ andq(temp1, 63);\n+        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+        __ andq(temp2, 63);\n@@ -1743,19 +1777,2 @@\n-        if(shift) {\n-          __ negptr(temp1);\n-          __ addq(temp1, 64);\n-          __ movq(temp4, temp1);\n-          __ shrq(temp1, shift);\n-          __ movq(temp2, temp1);\n-          __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp3, type_const[shift], 0, true);\n-          __ addptr(to, temp4);\n-          __ addptr(from, temp4);\n-          __ subq(count, temp1);\n-        } else {\n-          __ movq(temp4, temp1);\n-          __ negptr(temp4);\n-          __ addq(temp4, 64);\n-          __ movq(temp1, temp4);\n-          __ copy64_masked_avx(to, from, xmm1, k2, temp1, temp2, type_const[shift], 0, true);\n-          __ addptr(to, temp4);\n-          __ addptr(from, temp4);\n-          __ subq(count, temp4);\n+        if (shift) {\n+          __ shrq(temp2, shift);\n@@ -1763,0 +1780,2 @@\n+        __ subq(temp1, temp2);\n+        __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n@@ -1764,1 +1783,1 @@\n-        __ cmpq(count, loop_size[shift]);\n+        __ cmpq(temp1, loop_size[shift]);\n@@ -1768,1 +1787,0 @@\n-        __ subq(count, loop_size[shift]);\n@@ -1773,6 +1791,5 @@\n-           __ copy64_avx(to, from, xmm1, 0 , true);\n-           __ copy64_avx(to, from, xmm1, 64, true);\n-           __ copy64_avx(to, from, xmm1, 128, true);\n-           __ addptr(to, 192);\n-           __ addptr(from, 192);\n-           __ subq(count, loop_size[shift]);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n+           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n+           __ subq(temp1, loop_size[shift]);\n+           __ cmpq(temp1, loop_size[shift]);\n@@ -1781,1 +1798,0 @@\n-        __ addq(count, loop_size[shift]);\n@@ -1783,0 +1799,1 @@\n+        __ cmpq(temp1, 0);\n@@ -1786,4 +1803,0 @@\n-        __ movq(temp2, count);\n-        if(shift) {\n-          __ shlptr(temp2, shift);\n-        }\n@@ -1793,2 +1806,4 @@\n-        generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, count, shift, type_const[shift],\n-                                              temp2, temp3, use64byteVector, L_entry, L_exit);\n+        __ mov64(temp2, 0);\n+        __ movq(temp3, temp1);\n+        generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                                       temp4, use64byteVector, L_entry, L_exit);\n@@ -1798,1 +1813,0 @@\n-\n@@ -1800,1 +1814,7 @@\n-    restore_arg_regs();\n+    \/\/ When called from generic_arraycopy r11 contains specific values\n+    \/\/ used during arraycopy epilogue, re-initializing r11.\n+    if(is_oop) {\n+      __ movq(r11, count);\n+    }\n+    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+    restore_argument_regs(type);\n@@ -1831,1 +1851,2 @@\n-       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0);\n+       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n+                                                 aligned, false, false);\n@@ -1945,1 +1966,1 @@\n-                                                 nooverlap_target);\n+                                                 nooverlap_target, aligned, false, false);\n@@ -2053,1 +2074,2 @@\n-       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1);\n+       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n+                                                 aligned, false, false);\n@@ -2182,1 +2204,1 @@\n-                                                 nooverlap_target);\n+                                                 nooverlap_target, aligned, false, false);\n@@ -2282,2 +2304,3 @@\n-    if (VM_Version::supports_avx512vlbw() && !is_oop && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2);\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n+                                                 aligned, is_oop, dest_uninitialized);\n@@ -2390,1 +2413,1 @@\n-    if (VM_Version::supports_avx512vlbw() && !is_oop && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n@@ -2392,1 +2415,1 @@\n-                                                 nooverlap_target);\n+                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n@@ -2501,2 +2524,3 @@\n-    if (VM_Version::supports_avx512vlbw() && !is_oop && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3);\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n+                                                 aligned, is_oop, dest_uninitialized);\n@@ -2608,1 +2632,1 @@\n-    if (VM_Version::supports_avx512vlbw() && !is_oop && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n@@ -2610,1 +2634,1 @@\n-                                                 nooverlap_target);\n+                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":328,"deletions":304,"binary":false,"changes":632,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  code_size2 = 35300 LP64_ONLY(+16400)          \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 35300 LP64_ONLY(+21400)          \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,121 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package org.openjdk.bench.vm.compiler;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.results.Result;\n+import org.openjdk.jmh.results.RunResult;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.TimeValue;\n+\n+\n+\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.Arrays;\n+\n+class MyClass {\n+ public int field1;\n+ public int field2;\n+ public int field3;\n+\n+ public MyClass(int val) {\n+   field1 = val;\n+   field2 = val;\n+   field3 = val;\n+ }\n+}\n+\n+@State(Scope.Benchmark)\n+@BenchmarkMode(Mode.Throughput)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class ArrayCopyObject {\n+    @Param({\"31\", \"63\", \"127\" , \"2047\" , \"4095\", \"8191\"}) private int size;\n+\n+    private MyClass [] src;\n+    private MyClass [] dst;\n+\n+    @Setup\n+    public void setup() {\n+      src = new MyClass[size];\n+      dst = new MyClass[size];\n+      for (int i = 0; i < src.length ; i++) {\n+        src[i] = new MyClass(i);\n+        dst[i] = new MyClass(0);\n+      }\n+    }\n+\n+    @Benchmark\n+    public void disjoint_micro() {\n+      System.arraycopy(src, 0 , dst, 0 , size);\n+    }\n+\n+    @Benchmark\n+    public void conjoint_micro() {\n+      System.arraycopy(src, 0 , src, 10 , size - 10 );\n+    }\n+\n+    public static void main(String[] args) throws RunnerException {\n+       String [] base_opts =\n+          { \"-XX:+UnlockDiagnosticVMOptions \",\n+            \"-XX:+IgnoreUnrecognizedVMOptions \",\n+          \"-XX:UseAVX=3\" };\n+       String [] opts_str1 = {\"-XX:-UseCompressedOops \"};\n+       String [] opts_str2 = {\"-XX:+UseCompressedOops \"};\n+\n+       Options baseOpts = new OptionsBuilder()\n+          .include(ArrayCopyObject.class.getName())\n+          .warmupTime(TimeValue.seconds(30))\n+          .measurementTime(TimeValue.seconds(10))\n+          .warmupIterations(1)\n+          .measurementIterations(2)\n+          .jvmArgs(base_opts)\n+          .forks(1)\n+          .build();\n+\n+       RunResult r1 = new Runner(new OptionsBuilder()\n+         .parent(baseOpts)\n+         .jvmArgs(opts_str1)\n+         .build()).runSingle();\n+\n+       RunResult r2 = new Runner(new OptionsBuilder()\n+         .parent(baseOpts)\n+         .jvmArgs(opts_str2)\n+         .build()).runSingle();\n+\n+        System.out.println(r1.getPrimaryResult().getScore() + r2.getPrimaryResult().getScore());\n+    }\n+}\n+\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/lang\/ArrayCopyObject.java","additions":121,"deletions":0,"binary":false,"changes":121,"status":"added"}]}