{"files":[{"patch":"@@ -2592,0 +2592,32 @@\n+void Assembler::evmovdqu(XMMRegister dst, KRegister mask, Address src, int vector_len, int type) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionMark im(this);\n+  bool wide = type == T_SHORT || type == T_LONG || type == T_CHAR;\n+  bool bwinstr = type == T_BYTE ||  type == T_SHORT || type == T_CHAR;\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ wide, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  int prefix = bwinstr ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  vex_prefix(src, 0, dst->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmovdqu(Address dst, KRegister mask, XMMRegister src, int vector_len, int type) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  assert(src != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  bool wide = type == T_SHORT || type == T_LONG || type == T_CHAR;\n+  bool bwinstr = type == T_BYTE ||  type == T_SHORT || type == T_CHAR;\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ wide, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  int prefix = bwinstr ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  vex_prefix(dst, 0, src->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x7F);\n+  emit_operand(src, dst);\n+}\n+\n@@ -7806,0 +7838,7 @@\n+void Assembler::shrxq(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF7, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -829,1 +829,0 @@\n-  void decq(Register dst);\n@@ -914,0 +913,1 @@\n+  void decq(Register dst);\n@@ -1522,0 +1522,4 @@\n+  \/\/ Generic move instructions.\n+  void evmovdqu(Address dst, KRegister mask, XMMRegister src, int vector_len, int type);\n+  void evmovdqu(XMMRegister dst, KRegister mask, Address src, int vector_len, int type);\n+\n@@ -2024,0 +2028,2 @@\n+  void shrxq(Register dst, Register src1, Register src2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -7966,0 +7966,71 @@\n+\n+void MacroAssembler::copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register temp,\n+                                       BasicType type, int offset, bool use64byteVector) {\n+  assert(MaxVectorSize >= 32, \"vector length < 32\");\n+  use64byteVector |= MaxVectorSize > 32 && AVX3Threshold == 0;\n+  if (use64byteVector == false) {\n+    int shift = exact_log2_long(type2aelembytes(type));\n+    copy32_avx(dst,src, xmm, offset);\n+    subptr(length, 32 >> shift);\n+    copy32_masked_avx(dst, src, xmm, mask, length, temp, type, offset+32);\n+  } else {\n+    assert(MaxVectorSize == 64, \"vector length != 64\");\n+    negptr(length);\n+    addq(length, 64);\n+    mov64(temp, -1);\n+    shrxq(temp, temp, length);\n+    kmovql(mask, temp);\n+    evmovdqu(xmm, mask, Address(src, offset), Assembler::AVX_512bit, type);\n+    evmovdqu(Address(dst, offset), mask, xmm, Assembler::AVX_512bit, type);\n+  }\n+}\n+\n+void MacroAssembler::copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register temp,\n+                                       BasicType type, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length < 32\");\n+  mov64(temp, 1);\n+  shlxq(temp, temp, length);\n+  decq(temp);\n+  kmovql(mask, temp);\n+  evmovdqu(xmm, mask, Address(src, offset), Assembler::AVX_256bit, type);\n+  evmovdqu(Address(dst, offset), mask, xmm, Assembler::AVX_256bit, type);\n+}\n+\n+\n+void MacroAssembler::copy32_avx(Register dst, Register src, XMMRegister xmm, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length < 32\");\n+  vmovdqu(xmm, Address(src, offset));\n+  vmovdqu(Address(dst, offset), xmm);\n+}\n+\n+\n+void MacroAssembler::copy64_avx(Register dst, Register src, XMMRegister xmm, int offset, bool use64byteVector) {\n+  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n+  use64byteVector |= MaxVectorSize > 32 && AVX3Threshold == 0;\n+  if (use64byteVector == false) {\n+     vmovdqu(xmm, Address(src, offset));\n+     vmovdqu(Address(dst, offset), xmm);\n+     vmovdqu(xmm, Address(src, offset+32));\n+     vmovdqu(Address(dst, offset+32), xmm);\n+  } else {\n+     evmovdquq(xmm, Address(src, offset), Assembler::AVX_512bit);\n+     evmovdquq(Address(dst, offset), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n+void MacroAssembler::copy64_conjoint_avx(Register dst, Register src, XMMRegister xmm, int offset, bool use64byteVector) {\n+  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n+  use64byteVector |= MaxVectorSize > 32 && AVX3Threshold == 0;\n+  if (use64byteVector == false) {\n+     vmovdqu(xmm, Address(src, offset+32));\n+     vmovdqu(Address(dst, offset+32), xmm);\n+     vmovdqu(xmm, Address(src, offset));\n+     vmovdqu(Address(dst, offset), xmm);\n+  } else {\n+     evmovdquq(xmm, Address(src, offset), Assembler::AVX_512bit);\n+     evmovdquq(Address(dst, offset), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -1730,0 +1730,17 @@\n+\n+  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register temp,\n+                         BasicType type, int offset=0);\n+\n+  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register temp,\n+                         BasicType type, int offset=0, bool use64byteVector=false);\n+\n+  void copy64_avx(Register dst, Register src, XMMRegister xmm,\n+                  int offset=0, bool use64byteVector=false);\n+\n+  void copy64_conjoint_avx(Register dst, Register src, XMMRegister xmm,\n+                           int offset=0, bool use64byteVector=false);\n+\n+  void copy32_avx(Register dst, Register src, XMMRegister xmm, int offset=0);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1127,18 +1127,2 @@\n-      \/\/ Copy 64-bytes per iteration\n-      if (UseAVX > 2) {\n-        Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;\n-\n-        __ BIND(L_copy_bytes);\n-        __ cmpptr(qword_count, (-1 * AVX3Threshold \/ 8));\n-        __ jccb(Assembler::less, L_above_threshold);\n-        __ jmpb(L_below_threshold);\n-\n-        __ bind(L_loop_avx512);\n-        __ evmovdqul(xmm0, Address(end_from, qword_count, Address::times_8, -56), Assembler::AVX_512bit);\n-        __ evmovdqul(Address(end_to, qword_count, Address::times_8, -56), xmm0, Assembler::AVX_512bit);\n-        __ bind(L_above_threshold);\n-        __ addptr(qword_count, 8);\n-        __ jcc(Assembler::lessEqual, L_loop_avx512);\n-        __ jmpb(L_32_byte_head);\n-\n-        __ bind(L_loop_avx2);\n+      __ BIND(L_loop);\n+      if (UseAVX >= 2) {\n@@ -1149,7 +1133,0 @@\n-        __ bind(L_below_threshold);\n-        __ addptr(qword_count, 8);\n-        __ jcc(Assembler::lessEqual, L_loop_avx2);\n-\n-        __ bind(L_32_byte_head);\n-        __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-        __ jccb(Assembler::greater, L_end);\n@@ -1157,22 +1134,8 @@\n-        __ BIND(L_loop);\n-        if (UseAVX == 2) {\n-          __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-          __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-          __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-          __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-        } else {\n-          __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-          __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-          __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-          __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-          __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-        }\n-\n-        __ BIND(L_copy_bytes);\n-        __ addptr(qword_count, 8);\n-        __ jcc(Assembler::lessEqual, L_loop);\n-        __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-        __ jccb(Assembler::greater, L_end);\n+        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n+        __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n+        __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n+        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n@@ -1180,0 +1143,6 @@\n+\n+      __ BIND(L_copy_bytes);\n+      __ addptr(qword_count, 8);\n+      __ jcc(Assembler::lessEqual, L_loop);\n+      __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n+      __ jccb(Assembler::greater, L_end);\n@@ -1235,18 +1204,2 @@\n-      \/\/ Copy 64-bytes per iteration\n-      if (UseAVX > 2) {\n-        Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;\n-\n-        __ BIND(L_copy_bytes);\n-        __ cmpptr(qword_count, (AVX3Threshold \/ 8));\n-        __ jccb(Assembler::greater, L_above_threshold);\n-        __ jmpb(L_below_threshold);\n-\n-        __ BIND(L_loop_avx512);\n-        __ evmovdqul(xmm0, Address(from, qword_count, Address::times_8, 0), Assembler::AVX_512bit);\n-        __ evmovdqul(Address(dest, qword_count, Address::times_8, 0), xmm0, Assembler::AVX_512bit);\n-        __ bind(L_above_threshold);\n-        __ subptr(qword_count, 8);\n-        __ jcc(Assembler::greaterEqual, L_loop_avx512);\n-        __ jmpb(L_32_byte_head);\n-\n-        __ bind(L_loop_avx2);\n+      __ BIND(L_loop);\n+      if (UseAVX >= 2) {\n@@ -1255,9 +1208,2 @@\n-        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8, 0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm1);\n-        __ bind(L_below_threshold);\n-        __ subptr(qword_count, 8);\n-        __ jcc(Assembler::greaterEqual, L_loop_avx2);\n-\n-        __ bind(L_32_byte_head);\n-        __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-        __ jccb(Assembler::less, L_end);\n+        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+        __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n@@ -1265,16 +1211,9 @@\n-        __ BIND(L_loop);\n-        if (UseAVX == 2) {\n-          __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-          __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-          __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-          __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-        } else {\n-          __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-          __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-          __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-          __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-          __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-          __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-          __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-          __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-        }\n+        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n+        __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n+        __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n+        __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n+        __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n+        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n+        __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n+        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n+      }\n@@ -1282,3 +1221,3 @@\n-        __ BIND(L_copy_bytes);\n-        __ subptr(qword_count, 8);\n-        __ jcc(Assembler::greaterEqual, L_loop);\n+      __ BIND(L_copy_bytes);\n+      __ subptr(qword_count, 8);\n+      __ jcc(Assembler::greaterEqual, L_loop);\n@@ -1286,3 +1225,2 @@\n-        __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-        __ jccb(Assembler::less, L_end);\n-      }\n+      __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n+      __ jccb(Assembler::less, L_end);\n@@ -1326,0 +1264,546 @@\n+#ifndef PRODUCT\n+    int& get_profile_ctr(int shift) {\n+      if ( 0 == shift)\n+        return SharedRuntime::_jbyte_array_copy_ctr;\n+      else if(1 == shift)\n+        return SharedRuntime::_jshort_array_copy_ctr;\n+      else if(2 == shift)\n+        return SharedRuntime::_jint_array_copy_ctr;\n+      else\n+        return SharedRuntime::_jlong_array_copy_ctr;\n+    }\n+#endif\n+\n+  void generate_arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                             Register to, Register count, int shift,\n+                                             BasicType type_shift, Register byte_size, Register temp,\n+                                             bool use64byteVector, Label& L_entry, Label& L_exit) {\n+    Label L_entry_64, L_entry_96, L_entry_128;\n+    Label L_entry_160, L_entry_192;\n+\n+    \/\/ Case A) Special case for length less than equal to 32 bytes.\n+    __ cmpq(byte_size, 32);\n+    __ jccb(Assembler::greater, L_entry_64);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case B) Special case for length less than equal to 64 bytes.\n+    __ BIND(L_entry_64);\n+    __ cmpq(byte_size, 64);\n+    __ jccb(Assembler::greater, L_entry_96);\n+    __ copy64_masked_avx(to, from, xmm, mask, count, temp, type_shift, 0, use64byteVector);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case C) Special case for length less than equal to 96 bytes.\n+    __ BIND(L_entry_96);\n+    __ cmpq(byte_size, 96);\n+    __ jccb(Assembler::greater, L_entry_128);\n+    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n+    __ subq(count, 64 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 64);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case D) Special case for length less than equal to 128 bytes.\n+    __ BIND(L_entry_128);\n+    __ cmpq(byte_size, 128);\n+    __ jccb(Assembler::greater, L_entry_160);\n+    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n+    __ copy32_avx(to, from, xmm, 64);\n+    __ subq(count, 96 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 96);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case E) Special case for length less than equal to 160 bytes.\n+    __ BIND(L_entry_160);\n+    __ cmpq(byte_size, 160);\n+    __ jccb(Assembler::greater, L_entry_192);\n+    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n+    __ copy64_avx(to, from, xmm, 64, use64byteVector);\n+    __ subq(count, 128 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 128);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case F) Special case for length less than equal to 192 bytes.\n+    __ BIND(L_entry_192);\n+    __ cmpq(byte_size, 192);\n+    __ jcc(Assembler::greater, L_entry);\n+    __ copy64_avx(to, from, xmm, 0, use64byteVector);\n+    __ copy64_avx(to, from, xmm, 64, use64byteVector);\n+    __ copy32_avx(to, from, xmm, 128);\n+    __ subq(count, 160 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift, 160);\n+    __ jmp(L_exit);\n+  }\n+\n+  void generate_arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                                      Register to, Register from_end, Register to_end,\n+                                                      Register count, int shift, BasicType type_shift,\n+                                                      Register byte_size, Register temp,\n+                                                      bool use64byteVector, Label& L_entry, Label& L_exit) {\n+    Label L_entry_64, L_entry_96, L_entry_128;\n+    Label L_entry_160, L_entry_192;\n+    bool avx3 = MaxVectorSize > 32 && AVX3Threshold == 0;\n+\n+    \/\/ Case A) Special case for length less than equal to 32 bytes.\n+    __ cmpq(byte_size, 32);\n+    __ jccb(Assembler::greater, L_entry_64);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case B) Special case for length less than equal to 64 bytes.\n+    __ BIND(L_entry_64);\n+    __ cmpq(byte_size, 64);\n+    __ jccb(Assembler::greater, L_entry_96);\n+    if (avx3) {\n+       __ copy64_masked_avx(to, from, xmm, mask, count, temp, type_shift, 0, true);\n+    } else {\n+       __ copy32_avx(to_end, from_end, xmm, -32);\n+       __ subq(count, 32 >> shift);\n+       __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    }\n+    __ jmp(L_exit);\n+\n+    \/\/ Case C) Special case for length less than equal to 96 bytes.\n+    __ BIND(L_entry_96);\n+    __ cmpq(byte_size, 96);\n+    __ jccb(Assembler::greater, L_entry_128);\n+    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n+    __ subq(count, 64 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case D) Special case for length less than equal to 128 bytes.\n+    __ BIND(L_entry_128);\n+    __ cmpq(byte_size, 128);\n+    __ jccb(Assembler::greater, L_entry_160);\n+    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n+    __ copy32_avx(to_end, from_end, xmm, -96);\n+    __ subq(count, 96 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case E) Special case for length less than equal to 160 bytes.\n+    __ BIND(L_entry_160);\n+    __ cmpq(byte_size, 160);\n+    __ jccb(Assembler::greater, L_entry_192);\n+    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n+    __ copy64_conjoint_avx(to_end, from_end, xmm, -128, use64byteVector);\n+    __ subq(count, 128 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case F) Special case for length less than equal to 192 bytes.\n+    __ BIND(L_entry_192);\n+    __ cmpq(byte_size, 192);\n+    __ jcc(Assembler::greater, L_entry);\n+    __ copy64_conjoint_avx(to_end, from_end, xmm, -64, use64byteVector);\n+    __ copy64_conjoint_avx(to_end, from_end, xmm, -128, use64byteVector);\n+    __ copy32_avx(to_end, from_end, xmm, -160);\n+    __ subq(count, 160 >> shift);\n+    __ copy32_masked_avx(to, from, xmm, mask, count, temp, type_shift);\n+    __ jmp(L_exit);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/\n+  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             address nooverlap_target) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    bool use64byteVector = false;\n+\n+    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+    const Register from        = rdi;  \/\/ source array address\n+    const Register to          = rsi;  \/\/ destination array address\n+    const Register count       = rdx;  \/\/ elements count\n+    const Register from_end    = r8;\n+    const Register to_end      = rcx;\n+    const Register temp2       = r11;\n+    const Register temp3       = rax;\n+    \/\/ End pointers are inclusive, and if count is not zero they point\n+    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+\n+    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                      \/\/ r9 and r10 may be used to save non-volatile registers\n+    {\n+      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n+      int loop_size[]        = { 192,     96,       48,      24};\n+      BasicType type_const[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, true, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+\n+      \/\/ Zero length check.\n+      __ BIND(L_tail);\n+      __ cmpq(count, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ movq(temp2, count);\n+      if(shift) {\n+        __ shlptr(temp2, shift);\n+      }\n+\n+      \/\/ Special cases using 32 byte [masked] vector copy operations.\n+      __ movq(from_end, from);\n+      __ movq(to_end, to);\n+      __ addptr(from_end, temp2);\n+      __ addptr(to_end, temp2);\n+      generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, from_end, to_end, count, shift, type_const[shift],\n+                                                     temp2, temp3, use64byteVector, L_entry, L_exit);\n+\n+      \/\/ PRE-MAIN-POST loop for aligned copy.\n+      __ BIND(L_entry);\n+\n+      if (MaxVectorSize > 32 && AVX3Threshold != 0) {\n+        __ cmpq(temp2, AVX3Threshold);\n+        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      }\n+\n+      if (MaxVectorSize < 64  || AVX3Threshold != 0) {\n+        \/\/ Partial copy to make dst address 32 byte aligned.\n+        __ movq(temp3, to_end);\n+        __ andq(temp3, 31);\n+        __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+        if(shift) {\n+          __ subptr(to_end, temp3);\n+          __ subptr(from_end, temp3);\n+          __ shrq(temp3, shift);\n+          __ copy32_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift]);\n+          __ subq(count, temp3);\n+        } else {\n+          __ subptr(to_end, temp3);\n+          __ subptr(from_end, temp3);\n+          __ copy32_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift]);\n+          __ subq(count, temp3);\n+        }\n+        __ cmpq(count, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail);\n+\n+        __ BIND(L_main_pre_loop);\n+        __ subq(count, loop_size[shift]);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+        __ BIND(L_main_loop);\n+           __ copy64_conjoint_avx(to_end, from_end, xmm1, -64);\n+           __ copy64_conjoint_avx(to_end, from_end, xmm1, -128);\n+           __ copy64_conjoint_avx(to_end, from_end, xmm1, -192);\n+           __ subptr(to_end, 192);\n+           __ subptr(from_end, 192);\n+           __ subq(count, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop);\n+\n+        __ addq(count, loop_size[shift]);\n+\n+        \/\/ Tail loop.\n+        __ jmp(L_tail);\n+      }\n+\n+      if (MaxVectorSize > 32) {\n+        __ BIND(L_pre_main_post_64);\n+        \/\/ Partial copy to make dst address 64 byte aligned.\n+        __ movq(temp3, to_end);\n+        __ andq(temp3, 63);\n+        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+        if(shift) {\n+          __ subptr(to_end, temp3);\n+          __ subptr(from_end, temp3);\n+          __ shrq(temp3, shift);\n+          __ subq(count, temp3);\n+          __ copy64_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift], 0, true);\n+        } else {\n+          __ subptr(to_end, temp3);\n+          __ subptr(from_end, temp3);\n+          __ subq(count, temp3);\n+          __ copy64_masked_avx(to_end, from_end, xmm1, k2, temp3, temp2, type_const[shift], 0, true);\n+        }\n+        __ cmpq(count, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail64);\n+\n+        __ BIND(L_main_pre_loop_64bytes);\n+        __ subq(count, loop_size[shift]);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at\n+        \/\/ 64 byte copy granularity.\n+        __ BIND(L_main_loop_64bytes);\n+           __ copy64_conjoint_avx(to_end, from_end, xmm1, -64 , true);\n+           __ copy64_conjoint_avx(to_end, from_end, xmm1, -128, true);\n+           __ copy64_conjoint_avx(to_end, from_end, xmm1, -192, true);\n+           __ subptr(to_end, 192);\n+           __ subptr(from_end, 192);\n+           __ subq(count, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+        __ addq(count, loop_size[shift]);\n+        \/\/ Zero length check.\n+        __ jcc(Assembler::lessEqual, L_exit);\n+\n+        __ BIND(L_tail64);\n+        __ movq(temp2, count);\n+        if(shift) {\n+          __ shlptr(temp2, shift);\n+        }\n+\n+        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+        use64byteVector = true;\n+        generate_arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, from_end, to_end, count,\n+                                                       shift, type_const[shift], temp2, temp3,\n+                                                       use64byteVector, L_entry, L_exit);\n+      }\n+      __ BIND(L_exit);\n+    }\n+\n+    address ucme_exit_pc = __ pc();\n+    restore_arg_regs();\n+    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+    return start;\n+  }\n+\n+\n+  \/\/ Arguments:\n+  \/\/   name    - stub name string\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source array address\n+  \/\/   c_rarg1   - destination array address\n+  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+  \/\/\n+  \/\/\n+  \/\/ Side Effects:\n+  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n+  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n+  \/\/\n+  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    bool use64byteVector = false;\n+    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+    const Register from        = rdi;  \/\/ source array address\n+    const Register to          = rsi;  \/\/ destination array address\n+    const Register count       = rdx;  \/\/ elements count\n+    const Register temp1       = r8;\n+    const Register temp2       = r11;\n+    const Register temp3       = rax;\n+    const Register temp4       = rcx;\n+    \/\/ End pointers are inclusive, and if count is not zero they point\n+    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+    if (entry != NULL) {\n+      *entry = __ pc();\n+       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+      BLOCK_COMMENT(\"Entry:\");\n+    }\n+\n+    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                      \/\/ r9 and r10 may be used to save non-volatile registers\n+    {\n+      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n+      int loop_size[]        = { 192,     96,       48,      24};\n+      int rep_movesize[]     = { 4096,    2048,     1024,    512};\n+      BasicType type_const[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, true, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+\n+      \/\/ Zero length check.\n+      __ BIND(L_tail);\n+      __ cmpq(count, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ movq(temp2, count);\n+      if(shift) {\n+        __ shlptr(temp2, shift);\n+      }\n+\n+      \/\/ Special cases using 32 byte [masked] vector copy operations.\n+      generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, count, shift, type_const[shift],\n+                                            temp2, temp3, use64byteVector, L_entry, L_exit);\n+\n+      \/\/ PRE-MAIN-POST loop for aligned copy.\n+      __ BIND(L_entry);\n+      __ movptr(temp1, to);\n+\n+      if (MaxVectorSize > 32 && AVX3Threshold != 0) {\n+        __ cmpq(temp2, AVX3Threshold);\n+        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      }\n+\n+      if (MaxVectorSize < 64  || AVX3Threshold != 0) {\n+        \/\/ If MaxVectorSize == 32 and copy size is more than 4096 bytes,\n+        \/\/ REP MOVS offer a faster copy path.\n+        __ cmpq(count, rep_movesize[shift]);\n+        __ jcc(Assembler::greaterEqual, L_repmovs);\n+\n+        \/\/ Partial copy to make dst address 32 byte aligned.\n+        __ andq(temp1, 31);\n+        __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+        if(shift) {\n+          __ negptr(temp1);\n+          __ addq(temp1, 32);\n+          __ movq(temp4, temp1);\n+          __ shrq(temp1, shift);\n+          __ movq(temp2, temp1);\n+          __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp3, type_const[shift]);\n+          __ addptr(to, temp4);\n+          __ addptr(from, temp4);\n+          __ subq(count, temp1);\n+        } else {\n+          __ movq(temp4, temp1);\n+          __ negptr(temp4);\n+          __ addq(temp4, 32);\n+          __ movq(temp1, temp4);\n+          __ copy32_masked_avx(to, from, xmm1, k2, temp1, temp2, type_const[shift]);\n+          __ addptr(to, temp4);\n+          __ addptr(from, temp4);\n+          __ subq(count, temp4);\n+        }\n+        __ cmpq(count, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail);\n+\n+        __ BIND(L_main_pre_loop);\n+        __ subq(count, loop_size[shift]);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+        __ BIND(L_main_loop);\n+           __ copy64_avx(to, from, xmm1);\n+           __ copy64_avx(to, from, xmm1, 64);\n+           __ copy64_avx(to, from, xmm1, 128);\n+           __ addptr(to, 192);\n+           __ addptr(from, 192);\n+           __ subq(count, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop);\n+\n+        __ addq(count, loop_size[shift]);\n+\n+        \/\/ Tail loop.\n+        __ jmp(L_tail);\n+\n+        __ BIND(L_repmovs);\n+          __ movq(temp2, count);\n+          \/\/ Swap to(RSI) and from(RDI) addresses.\n+          __ movq(temp1, to);\n+          __ movq(to,  from);\n+          __ movq(from, temp1);\n+          if(shift < 3) {\n+            __ shrq(temp2, 3-shift);     \/\/ quad word count\n+          }\n+          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n+          __ rep_mov();\n+          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n+          if(shift) {\n+            __ shrq(temp2, shift);       \/\/ type specific count.\n+          }\n+          __ subq(count, temp2);         \/\/ tailing part (less than a quad ward size).\n+          __ movq(temp1, to);\n+          __ movq(to,  from);\n+          __ movq(from, temp1);\n+          __ jmp(L_tail);\n+      }\n+\n+      if (MaxVectorSize > 32) {\n+        __ BIND(L_pre_main_post_64);\n+        \/\/ Partial copy to make dst address 64 byte aligned.\n+        __ andq(temp1, 63);\n+        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+        if(shift) {\n+          __ negptr(temp1);\n+          __ addq(temp1, 64);\n+          __ movq(temp4, temp1);\n+          __ shrq(temp1, shift);\n+          __ movq(temp2, temp1);\n+          __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp3, type_const[shift], 0, true);\n+          __ addptr(to, temp4);\n+          __ addptr(from, temp4);\n+          __ subq(count, temp1);\n+        } else {\n+          __ movq(temp4, temp1);\n+          __ negptr(temp4);\n+          __ addq(temp4, 64);\n+          __ movq(temp1, temp4);\n+          __ copy64_masked_avx(to, from, xmm1, k2, temp1, temp2, type_const[shift], 0, true);\n+          __ addptr(to, temp4);\n+          __ addptr(from, temp4);\n+          __ subq(count, temp4);\n+        }\n+\n+        __ cmpq(count, loop_size[shift]);\n+        __ jcc(Assembler::less, L_tail64);\n+\n+        __ BIND(L_main_pre_loop_64bytes);\n+        __ subq(count, loop_size[shift]);\n+\n+        \/\/ Main loop with aligned copy block size of 192 bytes at\n+        \/\/ 64 byte copy granularity.\n+        __ BIND(L_main_loop_64bytes);\n+           __ copy64_avx(to, from, xmm1, 0 , true);\n+           __ copy64_avx(to, from, xmm1, 64, true);\n+           __ copy64_avx(to, from, xmm1, 128, true);\n+           __ addptr(to, 192);\n+           __ addptr(from, 192);\n+           __ subq(count, loop_size[shift]);\n+           __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+        __ addq(count, loop_size[shift]);\n+        \/\/ Zero length check.\n+        __ jcc(Assembler::lessEqual, L_exit);\n+\n+        __ BIND(L_tail64);\n+        __ movq(temp2, count);\n+        if(shift) {\n+          __ shlptr(temp2, shift);\n+        }\n+\n+        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+        use64byteVector = true;\n+        generate_arraycopy_avx3_special_cases(xmm1, k2, from, to, count, shift, type_const[shift],\n+                                              temp2, temp3, use64byteVector, L_entry, L_exit);\n+      }\n+      __ BIND(L_exit);\n+    }\n+\n+    address ucme_exit_pc = __ pc();\n+    restore_arg_regs();\n+    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+    return start;\n+  }\n+\n+\n@@ -1346,0 +1830,3 @@\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0);\n+    }\n@@ -1456,0 +1943,4 @@\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n+                                                 nooverlap_target);\n+    }\n@@ -1561,0 +2052,4 @@\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1);\n+    }\n+\n@@ -1685,0 +2180,4 @@\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n+                                                 nooverlap_target);\n+    }\n@@ -1783,0 +2282,4 @@\n+    if (VM_Version::supports_avx512vlbw() && false == is_oop && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2);\n+    }\n+\n@@ -1887,0 +2390,4 @@\n+    if (VM_Version::supports_avx512vlbw() && false == is_oop && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n+                                                 nooverlap_target);\n+    }\n@@ -1994,0 +2501,3 @@\n+    if (VM_Version::supports_avx512vlbw() && false == is_oop && MaxVectorSize  >= 32) {\n+       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3);\n+    }\n@@ -2098,0 +2608,4 @@\n+    if (VM_Version::supports_avx512vlbw() && false == is_oop && MaxVectorSize  >= 32) {\n+       return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n+                                                 nooverlap_target);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":610,"deletions":96,"binary":false,"changes":706,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  code_size2 = 35300 LP64_ONLY(+11400)          \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 35300 LP64_ONLY(+16400)          \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -764,0 +764,2 @@\n+      _features &= ~CPU_AVX512BW;\n+      _features &= ~CPU_AVX512VL;\n@@ -1165,1 +1167,1 @@\n-    if (!is_power_of_2(AVX3Threshold)) {\n+    if (AVX3Threshold !=0 && !is_power_of_2(AVX3Threshold)) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64ArrayCompareToOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64StringLatin1InflateOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64StringUTF16CompressOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,269 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.arraycopy;\n+import java.util.Random;\n+\n+\/**\n+ * @test\n+ * @bug 8251871\n+ * @summary Optimize arrayCopy using AVX-512 masked instructions.\n+ *\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=0 -XX:MaxVectorSize=32 -XX:+UnlockDiagnosticVMOptions\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=0 -XX:MaxVectorSize=64\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=32 -XX:+UnlockDiagnosticVMOptions -XX:MaxVectorSize=32 -XX:+UnlockDiagnosticVMOption\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=32 -XX:+UnlockDiagnosticVMOptions -XX:MaxVectorSize=64\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=64 -XX:MaxVectorSize=64\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=32 -XX:+UnlockDiagnosticVMOptions -XX:MaxVectorSize=32 -XX:+UnlockDiagnosticVMOption -XX:ArrayCopyLoadStoreMaxElem=16\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=64 -XX:MaxVectorSize=64 -XX:ArrayCopyLoadStoreMaxElem=16\n+ *      compiler.arraycopy.TestArrayCopyConjoint\n+ *\n+ *\/\n+\n+public class TestArrayCopyConjoint {\n+\n+   public static final int SIZE = 4096;\n+   public static byte[] fromByteArr, toByteArr, valByteArr;\n+   public static char[] fromCharArr, toCharArr, valCharArr;\n+   public static int[] fromIntArr, toIntArr, valIntArr;\n+   public static long[] fromLongArr, toLongArr, valLongArr;\n+\n+   static public  void reinit(Class<?> c) {\n+     if (c == byte.class) {\n+       for (int i = 0 ; i < SIZE ; i++) {\n+         fromByteArr[i] = (byte)i;\n+       }\n+     } else if (c == char.class) {\n+       for (int i = 0 ; i < SIZE ; i++) {\n+         fromCharArr[i] = (char)i;\n+       }\n+     } else if (c == int.class) {\n+       for (int i = 0 ; i < SIZE ; i++) {\n+         fromIntArr[i]  = i;\n+       }\n+     } else {\n+       assert c == long.class;\n+       for (int i = 0 ; i < SIZE ; i++) {\n+         fromLongArr[i] = i;\n+       }\n+     }\n+   }\n+\n+   static public void setup() {\n+     \/\/ Both positions aligned\n+     fromByteArr = new byte[SIZE];\n+     valByteArr  = new byte[SIZE];\n+     toByteArr = fromByteArr;\n+     fromCharArr = new char[SIZE];\n+     valCharArr  = new char[SIZE];\n+     toCharArr = fromCharArr;\n+     fromIntArr = new int[SIZE];\n+     valIntArr  = new int[SIZE];\n+     toIntArr = fromIntArr;\n+     fromLongArr = new long[SIZE];\n+     valLongArr  = new long[SIZE];\n+     toLongArr = fromLongArr;\n+\n+     for (int i = 0 ; i < SIZE ; i++) {\n+        fromByteArr[i] = (byte)i;\n+        valByteArr[i] = (byte)i;\n+        fromCharArr[i] = (char)i;\n+        valCharArr[i] = (char)i;\n+        fromIntArr[i]  = i;\n+        valIntArr[i]  = i;\n+        fromLongArr[i] = i;\n+        valLongArr[i] = i;\n+     }\n+    }\n+\n+    public static int validate_ctr = 0;\n+    public static <E> void validate(String msg, E arr, int length, int fromPos, int toPos) {\n+      validate_ctr++;\n+      if (arr instanceof byte [])  {\n+        byte [] barr = (byte [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (valByteArr[i+fromPos] != barr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + valByteArr[i+fromPos]\n+                                + \" actual   = \" + barr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \"  toPos = \" + toPos);\n+             throw new Error(\"Fail\");\n+\n+          }\n+      }\n+      else if (arr instanceof char [])  {\n+        char [] carr = (char [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (valCharArr[i+fromPos] != carr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + valCharArr[i+fromPos]\n+                                + \" actual   = \" + carr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+             throw new Error(\"Fail\");\n+          }\n+      }\n+      else if (arr instanceof int [])  {\n+        int [] iarr = (int [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (valIntArr[i+fromPos] != iarr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + valIntArr[i+fromPos]\n+                                + \" actual   = \" + iarr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+             throw new Error(\"Fail\");\n+          }\n+      }\n+      else if (arr instanceof long [])  {\n+        long [] larr = (long [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (valLongArr[i+fromPos] != larr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + valLongArr[i+fromPos]\n+                                + \" actual   = \" + larr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+             throw new Error(\"Fail\");\n+          }\n+      }\n+    }\n+\n+    public static void testByte(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromByteArr, fromPos, toByteArr, toPos, length);\n+       validate(\" Test ByteArr \", toByteArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testChar(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromCharArr, fromPos, toCharArr, toPos, length);\n+       validate(\" Test CharArr \", toCharArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testInt(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromIntArr, fromPos, toIntArr, toPos, length);\n+       validate(\" Test IntArr \", toIntArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testLong(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromLongArr, fromPos, toLongArr, toPos, length);\n+       validate(\" Test LongArr \", toLongArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testByte_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromByteArr, fromPos, toByteArr, toPos, 7);\n+       validate(\" Test Byte constant length 7 \", toByteArr, 7, fromPos, toPos);\n+    }\n+    public static void testByte_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromByteArr, fromPos, toByteArr, toPos, 45);\n+       validate(\" Test Byte constant length 45 \", toByteArr, 45, fromPos, toPos);\n+    }\n+\n+    public static void testChar_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromCharArr, fromPos, toCharArr, toPos, 7);\n+       validate(\" Test Char constant length 7 \", toCharArr, 7, fromPos, toPos);\n+    }\n+    public static void testChar_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromCharArr, fromPos, toCharArr, toPos, 22);\n+       validate(\" Test Char constant length 22 \", toCharArr, 22, fromPos, toPos);\n+    }\n+\n+    public static void testInt_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromIntArr, fromPos, toIntArr, toPos, 7);\n+       validate(\" Test Int constant length 7 \", toIntArr, 7, fromPos, toPos);\n+    }\n+    public static void testInt_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromIntArr, fromPos, toIntArr, toPos, 11);\n+       validate(\" Test Int constant length 11 \", toIntArr, 11, fromPos, toPos);\n+    }\n+\n+    public static void testLong_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromLongArr, fromPos, toLongArr, toPos, 3);\n+       validate(\" Test Long constant length 3 \", toLongArr, 3, fromPos, toPos);\n+    }\n+    public static void testLong_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromLongArr, fromPos, toLongArr, toPos, 6);\n+       validate(\" Test Long constant length 6 \", toLongArr, 6, fromPos, toPos);\n+    }\n+\n+\n+    public static void main(String [] args) {\n+      \/\/ Cases to test each new optimized stub special blocks.\n+      \/\/ Cases to test new PI handling (PI32 and PI64).\n+      \/\/ Cases to test vectorized constant array copies for all primitive types.\n+      \/\/                  LT32B   LT64B  LT96B  LT128B   LT160B   LT192B  LOOP1   LOOP2\n+      int [] lengths =  {   29,    59,    89,    125,     159,      189,   194,   1024 };\n+      Random r = new Random(1024);\n+\n+      setup();\n+\n+      try {\n+        for (int i = 0 ; i < 1000000 ; i++ ) {\n+          int index = r.nextInt(2048);\n+          testByte(lengths[i % lengths.length], index , index+2);\n+          reinit(byte.class);\n+          testByte_constant_LT32B (index , index+2);\n+          reinit(byte.class);\n+          testByte_constant_LT64B (index , index+2);\n+          reinit(byte.class);\n+\n+          testChar(lengths[i % lengths.length] >> 1, index , index+2);\n+          reinit(char.class);\n+          testChar_constant_LT32B (index , index+2);\n+          reinit(char.class);\n+          testChar_constant_LT64B (index , index+2);\n+          reinit(char.class);\n+\n+          testInt(lengths[i % lengths.length]  >> 2, index , index+2);\n+          reinit(int.class);\n+          testInt_constant_LT32B (index , index+2);\n+          reinit(int.class);\n+          testInt_constant_LT64B (index , index+2);\n+          reinit(int.class);\n+\n+          testLong(lengths[i % lengths.length] >> 3, index , index+2);\n+          reinit(long.class);\n+          testLong_constant_LT32B (index , index+2);\n+          reinit(long.class);\n+          testLong_constant_LT64B (index , index+2);\n+          reinit(long.class);\n+        }\n+        System.out.println(\"PASS : \" + validate_ctr);\n+      } catch (Exception e) {\n+        System.out.println(e.getMessage());\n+      }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/arraycopy\/TestArrayCopyConjoint.java","additions":269,"deletions":0,"binary":false,"changes":269,"status":"added"},{"patch":"@@ -0,0 +1,226 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.arraycopy;\n+import java.util.Random;\n+\n+\/**\n+ * @test\n+ * @bug 8251871\n+ * @summary Optimize arrayCopy using AVX-512 masked instructions.\n+ *\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=0 -XX:MaxVectorSize=32 -XX:+UnlockDiagnosticVMOptions\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=0 -XX:MaxVectorSize=64\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=32 -XX:+UnlockDiagnosticVMOptions -XX:MaxVectorSize=32 -XX:+UnlockDiagnosticVMOption\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=32 -XX:+UnlockDiagnosticVMOptions -XX:MaxVectorSize=64\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=64 -XX:MaxVectorSize=64\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=32 -XX:+UnlockDiagnosticVMOptions -XX:MaxVectorSize=32 -XX:+UnlockDiagnosticVMOption -XX:ArrayCopyLoadStoreMaxElem=16\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ * @run main\/othervm\/timeout=600 -XX:-TieredCompilation  -Xbatch -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:UseAVX=3 -XX:+UnlockDiagnosticVMOptions -XX:ArrayCopyPartialInlineSize=64 -XX:MaxVectorSize=64 -XX:ArrayCopyLoadStoreMaxElem=16\n+ *      compiler.arraycopy.TestArrayCopyDisjoint\n+ *\n+ *\/\n+\n+public class TestArrayCopyDisjoint {\n+\n+   public static final int SIZE = 4096;\n+   public static byte[] fromByteArr, toByteArr;\n+   public static char[] fromCharArr, toCharArr;\n+   public static int[] fromIntArr, toIntArr;\n+   public static long[] fromLongArr, toLongArr;\n+\n+   static public void setup() {\n+     \/\/ Both positions aligned\n+     fromByteArr = new byte[SIZE];\n+     toByteArr = new byte[SIZE];\n+     fromCharArr = new char[SIZE];\n+     toCharArr = new char[SIZE];\n+     fromIntArr = new int[SIZE];\n+     toIntArr = new int[SIZE];\n+     fromLongArr = new long[SIZE];\n+     toLongArr = new long[SIZE];\n+\n+     for (int i = 0 ; i < SIZE ; i++) {\n+        fromByteArr[i] = (byte)i;\n+        fromCharArr[i] = (char)i;\n+        fromIntArr[i]  = i;\n+        fromLongArr[i] = i;\n+     }\n+    }\n+\n+    public static int validate_ctr = 0;\n+    public static <E> void validate(String msg, E arr, int length, int fromPos, int toPos) {\n+      validate_ctr++;\n+      if (arr instanceof byte [])  {\n+        byte [] barr = (byte [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (fromByteArr[i+fromPos] != barr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + fromByteArr[i+fromPos]\n+                                + \" actual   = \" + barr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+              throw new Error(\"Fail\");\n+          }\n+      }\n+      else if (arr instanceof char [])  {\n+        char [] carr = (char [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (fromCharArr[i+fromPos] != carr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + fromCharArr[i+fromPos]\n+                                + \" actual   = \" + carr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+              throw new Error(\"Fail\");\n+          }\n+      }\n+      else if (arr instanceof int [])  {\n+        int [] iarr = (int [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (fromIntArr[i+fromPos] != iarr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + fromIntArr[i+fromPos]\n+                                + \" actual   = \" + iarr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+              throw new Error(\"Fail\");\n+          }\n+      }\n+      else if (arr instanceof long [])  {\n+        long [] larr = (long [])arr;\n+        for(int i = 0 ; i < length; i++)\n+          if (fromLongArr[i+fromPos] != larr[i+toPos]) {\n+             System.out.println(msg + \"[\" + arr.getClass() + \"] Result mismtach at i = \" + i\n+                                + \" expected = \" + fromLongArr[i+fromPos]\n+                                + \" actual   = \" + larr[i+toPos]\n+                                + \" fromPos = \" + fromPos\n+                                + \" toPos = \" + toPos);\n+              throw new Error(\"Fail\");\n+          }\n+      }\n+    }\n+\n+    public static void testByte(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromByteArr, fromPos, toByteArr, toPos, length);\n+       validate(\" Test ByteArr \", toByteArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testChar(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromCharArr, fromPos, toCharArr, toPos, length);\n+       validate(\" Test CharArr \", toCharArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testInt(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromIntArr, fromPos, toIntArr, toPos, length);\n+       validate(\" Test IntArr \", toIntArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testLong(int length, int fromPos, int toPos) {\n+       System.arraycopy(fromLongArr, fromPos, toLongArr, toPos, length);\n+       validate(\" Test LongArr \", toLongArr, length, fromPos, toPos);\n+    }\n+\n+    public static void testByte_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromByteArr, fromPos, toByteArr, toPos, 7);\n+       validate(\" Test Byte constant length 7 \", toByteArr, 7, fromPos, toPos);\n+    }\n+    public static void testByte_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromByteArr, fromPos, toByteArr, toPos, 45);\n+       validate(\" Test Byte constant length 45 \", toByteArr, 45, fromPos, toPos);\n+    }\n+\n+    public static void testChar_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromCharArr, fromPos, toCharArr, toPos, 7);\n+       validate(\" Test Char constant length 7 \", toCharArr, 7, fromPos, toPos);\n+    }\n+    public static void testChar_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromCharArr, fromPos, toCharArr, toPos, 22);\n+       validate(\" Test Char constant length 22 \", toCharArr, 22, fromPos, toPos);\n+    }\n+\n+    public static void testInt_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromIntArr, fromPos, toIntArr, toPos, 7);\n+       validate(\" Test Int constant length 7 \", toIntArr, 7, fromPos, toPos);\n+    }\n+    public static void testInt_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromIntArr, fromPos, toIntArr, toPos, 11);\n+       validate(\" Test Int constant length 11 \", toIntArr, 11, fromPos, toPos);\n+    }\n+\n+    public static void testLong_constant_LT32B(int fromPos, int toPos) {\n+       System.arraycopy(fromLongArr, fromPos, toLongArr, toPos, 3);\n+       validate(\" Test Long constant length 3 \", toLongArr, 3, fromPos, toPos);\n+    }\n+    public static void testLong_constant_LT64B(int fromPos, int toPos) {\n+       System.arraycopy(fromLongArr, fromPos, toLongArr, toPos, 6);\n+       validate(\" Test Long constant length 6 \", toLongArr, 6, fromPos, toPos);\n+    }\n+\n+\n+    public static void main(String [] args) {\n+      \/\/ Cases to test each new optimized stub special blocks.\n+      \/\/ Cases to test new PI handling (PI32 and PI64).\n+      \/\/ Cases to test vectorized constant array copies for all primitive types.\n+      \/\/                  LT32B   LT64B  LT96B  LT128B   LT160B   LT192B  LOOP1   LOOP2\n+      int [] lengths =  {   29,    59,    89,    125,     159,      189,   194,   1024 };\n+      Random r = new Random(1024);\n+\n+      setup();\n+\n+      try {\n+        for (int i = 0 ; i < 1000000 ; i++ ) {\n+          testByte(lengths[i % lengths.length], r.nextInt(2048) , r.nextInt(2048));\n+          testByte_constant_LT32B (r.nextInt(2048) , r.nextInt(2048));\n+          testByte_constant_LT64B (r.nextInt(2048) , r.nextInt(2048));\n+\n+          testChar(lengths[i % lengths.length] >> 1, r.nextInt(2048) , r.nextInt(2048));\n+          testChar_constant_LT32B (r.nextInt(2048) , r.nextInt(2048));\n+          testChar_constant_LT64B (r.nextInt(2048) , r.nextInt(2048));\n+\n+          testInt(lengths[i % lengths.length]  >> 2, r.nextInt(2048) , r.nextInt(2048));\n+          testInt_constant_LT32B (r.nextInt(2048) , r.nextInt(2048));\n+          testInt_constant_LT64B (r.nextInt(2048) , r.nextInt(2048));\n+\n+          testLong(lengths[i % lengths.length] >> 3, r.nextInt(2048) , r.nextInt(2048));\n+          testLong_constant_LT32B (r.nextInt(2048) , r.nextInt(2048));\n+          testLong_constant_LT64B (r.nextInt(2048) , r.nextInt(2048));\n+        }\n+        System.out.println(\"PASS : \" + validate_ctr);\n+      } catch (Exception e) {\n+         System.out.println(e.getMessage());\n+      }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/arraycopy\/TestArrayCopyDisjoint.java","additions":226,"deletions":0,"binary":false,"changes":226,"status":"added"}]}