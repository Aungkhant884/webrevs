{"files":[{"patch":"@@ -38,5 +38,1 @@\n-  _transfer_lock(false),\n-  _free_slots_list(),\n-  _pending_slots_list(),\n-  _num_pending_slots(0),\n-  _num_free_slots(0)\n+  _free_slots_list(name, &_segmented_array)\n@@ -53,36 +49,0 @@\n-template <class Slot>\n-bool G1CardSetAllocator<Slot>::try_transfer_pending() {\n-  \/\/ Attempt to claim the lock.\n-  if (Atomic::load_acquire(&_transfer_lock) || \/\/ Skip CAS if likely to fail.\n-      Atomic::cmpxchg(&_transfer_lock, false, true)) {\n-    return false;\n-  }\n-  \/\/ Have the lock; perform the transfer.\n-\n-  \/\/ Claim all the pending slots.\n-  G1CardSetContainer* first = _pending_slots_list.pop_all();\n-\n-  if (first != nullptr) {\n-    \/\/ Prepare to add the claimed slots, and update _num_pending_slots.\n-    G1CardSetContainer* last = first;\n-    Atomic::load_acquire(&_num_pending_slots);\n-\n-    uint count = 1;\n-    for (G1CardSetContainer* next = first->next(); next != nullptr; next = next->next()) {\n-      last = next;\n-      ++count;\n-    }\n-\n-    Atomic::sub(&_num_pending_slots, count);\n-\n-    \/\/ Wait for any in-progress pops to avoid ABA for them.\n-    GlobalCounter::write_synchronize();\n-    \/\/ Add synchronized slots to _free_slots_list.\n-    \/\/ Update count first so there can be no underflow in allocate().\n-    Atomic::add(&_num_free_slots, count);\n-    _free_slots_list.prepend(*first, *last);\n-  }\n-  Atomic::release_store(&_transfer_lock, false);\n-  return true;\n-}\n-\n@@ -92,21 +52,2 @@\n-  \/\/ Desired minimum transfer batch size.  There is relatively little\n-  \/\/ importance to the specific number.  It shouldn't be too big, else\n-  \/\/ we're wasting space when the release rate is low.  If the release\n-  \/\/ rate is high, we might accumulate more than this before being\n-  \/\/ able to start a new transfer, but that's okay.  Also note that\n-  \/\/ the allocation rate and the release rate are going to be fairly\n-  \/\/ similar, due to how the slots are used. - kbarret\n-  uint const trigger_transfer = 10;\n-\n-  uint pending_count = Atomic::add(&_num_pending_slots, 1u, memory_order_relaxed);\n-\n-  G1CardSetContainer* container =  reinterpret_cast<G1CardSetContainer*>(reinterpret_cast<char*>(slot));\n-\n-  container->set_next(nullptr);\n-  assert(container->next() == nullptr, \"precondition\");\n-\n-  _pending_slots_list.push(*container);\n-\n-  if (pending_count > trigger_transfer) {\n-    try_transfer_pending();\n-  }\n+  slot->~Slot();\n+  _free_slots_list.release(slot);\n@@ -117,4 +58,1 @@\n-  _free_slots_list.pop_all();\n-  _pending_slots_list.pop_all();\n-  _num_pending_slots = 0;\n-  _num_free_slots = 0;\n+  _free_slots_list.reset();\n@@ -132,0 +70,1 @@\n+  uint num_pending_slots = (uint)_free_slots_list.pending_count();\n@@ -134,1 +73,1 @@\n-            _num_pending_slots,\n+            num_pending_slots,\n@@ -137,1 +76,1 @@\n-            percent_of(num_allocated_slots - _num_pending_slots, num_available_slots),\n+            percent_of(num_allocated_slots - num_pending_slots, num_available_slots),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.cpp","additions":7,"deletions":68,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/freeListAllocator.hpp\"\n@@ -34,1 +35,0 @@\n-#include \"utilities\/lockFreeStack.hpp\"\n@@ -94,2 +94,0 @@\n-  static G1CardSetContainer* volatile* next_ptr(G1CardSetContainer& slot);\n-  typedef LockFreeStack<G1CardSetContainer, &G1CardSetAllocator::next_ptr> SlotStack;\n@@ -98,13 +96,1 @@\n-  volatile bool _transfer_lock;\n-  SlotStack _free_slots_list;\n-  SlotStack _pending_slots_list;\n-\n-  volatile uint _num_pending_slots;   \/\/ Number of slots in the pending list.\n-  volatile uint _num_free_slots;      \/\/ Number of slots in the free list.\n-\n-  \/\/ Try to transfer slots from _pending_slots_list to _free_slots_list, with a\n-  \/\/ synchronization delay for any in-progress pops from the _free_slots_list\n-  \/\/ to solve ABA here.\n-  bool try_transfer_pending();\n-\n-  uint num_free_slots() const;\n+  FreeListAllocator _free_slots_list;\n@@ -127,2 +113,2 @@\n-      _segmented_array.num_segments() * sizeof(G1CardSetSegment) + _segmented_array.num_available_slots() *\n-                                                                   _segmented_array.slot_size();\n+      _segmented_array.num_segments() * sizeof(G1CardSetSegment) +\n+      _segmented_array.num_available_slots() * _segmented_array.slot_size();\n@@ -132,2 +118,4 @@\n-    return (_segmented_array.num_available_slots() - (_segmented_array.num_allocated_slots() - _num_pending_slots)) *\n-           _segmented_array.slot_size();\n+    uint num_wasted_slots = _segmented_array.num_available_slots() -\n+                            _segmented_array.num_allocated_slots() -\n+                            (uint)_free_slots_list.pending_count();\n+    return num_wasted_slots * _segmented_array.slot_size();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.hpp","additions":8,"deletions":20,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -36,5 +36,0 @@\n-template <class Slot>\n-G1CardSetContainer* volatile* G1CardSetAllocator<Slot>::next_ptr(G1CardSetContainer& slot) {\n-  return slot.next_addr();\n-}\n-\n@@ -43,17 +38,1 @@\n-  assert(_segmented_array.slot_size() > 0, \"instance size not set.\");\n-\n-  if (num_free_slots() > 0) {\n-    \/\/ Pop under critical section to deal with ABA problem\n-    \/\/ Other solutions to the same problem are more complicated (ref counting, HP)\n-    GlobalCounter::CriticalSection cs(Thread::current());\n-\n-    G1CardSetContainer* container = _free_slots_list.pop();\n-    if (container != nullptr) {\n-      Slot* slot = reinterpret_cast<Slot*>(reinterpret_cast<char*>(container));\n-      Atomic::sub(&_num_free_slots, 1u);\n-      guarantee(is_aligned(slot, 8), \"result \" PTR_FORMAT \" not aligned\", p2i(slot));\n-      return slot;\n-    }\n-  }\n-\n-  Slot* slot = _segmented_array.allocate();\n+  Slot* slot = ::new (_free_slots_list.allocate()) Slot();\n@@ -77,5 +56,0 @@\n-template <class Slot>\n-inline uint G1CardSetAllocator<Slot>::num_free_slots() const {\n-  return Atomic::load(&_num_free_slots);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.inline.hpp","additions":1,"deletions":27,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/freeListAllocator.hpp\"\n@@ -184,1 +185,1 @@\n-class G1SegmentedArray {\n+class G1SegmentedArray : public FreeListConfig  {\n@@ -225,1 +226,4 @@\n-  inline Slot* allocate();\n+  inline void* allocate() override;\n+\n+  \/\/ We do not deallocate individual slots\n+  inline void deallocate(void* node) override { ShouldNotReachHere(); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SegmentedArray.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -213,1 +213,1 @@\n-Slot* G1SegmentedArray<Slot, flag>::allocate() {\n+void* G1SegmentedArray<Slot, flag>::allocate() {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SegmentedArray.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,219 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/freeListAllocator.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"utilities\/globalCounter.inline.hpp\"\n+\n+FreeListAllocator::NodeList::NodeList() :\n+  _head(NULL), _tail(NULL), _entry_count(0) {}\n+\n+FreeListAllocator::NodeList::NodeList(FreeNode* head, FreeNode* tail, size_t entry_count) :\n+  _head(head), _tail(tail), _entry_count(entry_count)\n+{\n+  assert((_head == NULL) == (_tail == NULL), \"invariant\");\n+  assert((_head == NULL) == (_entry_count == 0), \"invariant\");\n+}\n+\n+FreeListAllocator::PendingList::PendingList() :\n+  _tail(nullptr), _head(nullptr), _count(0) {}\n+\n+size_t FreeListAllocator::PendingList::add(FreeNode* node) {\n+  assert(node->next() == nullptr, \"precondition\");\n+  FreeNode* old_head = Atomic::xchg(&_head, node);\n+  if (old_head != nullptr) {\n+    node->set_next(old_head);\n+  } else {\n+    assert(_tail == nullptr, \"invariant\");\n+    _tail = node;\n+  }\n+  return Atomic::add(&_count, size_t(1));\n+}\n+\n+typename FreeListAllocator::NodeList FreeListAllocator::PendingList::take_all() {\n+  NodeList result{Atomic::load(&_head), _tail, Atomic::load(&_count)};\n+  Atomic::store(&_head, (FreeNode*)nullptr);\n+  _tail = nullptr;\n+  Atomic::store(&_count, size_t(0));\n+  return result;\n+}\n+\n+size_t FreeListAllocator::PendingList::count() const {\n+  return  Atomic::load(&_count);\n+}\n+\n+FreeListAllocator::FreeListAllocator(const char* name, FreeListConfig* config) :\n+  _config(config),\n+  _free_count(0),\n+  _free_list(),\n+  _transfer_lock(false),\n+  _active_pending_list(0),\n+  _pending_lists()\n+{\n+  strncpy(_name, name, sizeof(_name) - 1);\n+  _name[sizeof(_name) - 1] = '\\0';\n+}\n+\n+void FreeListAllocator::delete_list(FreeNode* list) {\n+  while (list != NULL) {\n+    FreeNode* next = list->next();\n+    DEBUG_ONLY(list->set_next(NULL);)\n+    list->~FreeNode();\n+    _config->deallocate(list);\n+    list = next;\n+  }\n+}\n+\n+FreeListAllocator::~FreeListAllocator() {\n+  uint index = Atomic::load(&_active_pending_list);\n+  NodeList pending_list = _pending_lists[index].take_all();\n+  delete_list(Atomic::load(&pending_list._head));\n+  delete_list(_free_list.pop_all());\n+}\n+\n+\/\/ Free nodes in the allocator could have been allocated out of an arena.\n+\/\/ Therefore, the nodes can be freed at once when entire arena is discarded\n+\/\/ without running destructors for the individual nodes. In such cases, reset\n+\/\/ method should be called before the ~FreeListAllocator(). Calling the reset\n+\/\/ method on nodes not managed by an arena will leak the memory by just dropping\n+\/\/ the nodes to the floor.\n+void FreeListAllocator::reset() {\n+  uint index = Atomic::load(&_active_pending_list);\n+  _pending_lists[index].take_all();\n+  _free_list.pop_all();\n+  _free_count = 0;\n+}\n+\n+size_t FreeListAllocator::free_count() const {\n+  return Atomic::load(&_free_count);\n+}\n+\n+size_t FreeListAllocator::pending_count() const {\n+  uint index = Atomic::load(&_active_pending_list);\n+  return _pending_lists[index].count();;\n+}\n+\n+\/\/ To solve the ABA problem, popping a node from the _free_list is performed within\n+\/\/ a GlobalCounter critical section, and pushing nodes onto the _free_list is done\n+\/\/ after a GlobalCounter synchronization associated with the nodes to be pushed.\n+void* FreeListAllocator::allocate() {\n+  FreeNode* node = nullptr;\n+  if (free_count() > 0) {\n+    \/\/ Protect against ABA; see release().\n+    GlobalCounter::CriticalSection cs(Thread::current());\n+    node = _free_list.pop();\n+  }\n+\n+  if (node != nullptr) {\n+    node->~FreeNode();\n+    \/\/ Decrement count after getting buffer from free list.  This, along\n+    \/\/ with incrementing count before adding to free list, ensures count\n+    \/\/ never underflows.\n+    size_t count = Atomic::sub(&_free_count, 1u);\n+    assert((count + 1) != 0, \"_free_count underflow\");\n+    return node;\n+  } else {\n+    return _config->allocate();\n+  }\n+}\n+\n+\/\/ The release synchronizes on the critical sections before adding to\n+\/\/ the _free_list. But we don't want to make every release have to do a\n+\/\/ synchronize. Instead, we initially place released nodes on the pending list,\n+\/\/ and transfer them to the _free_list in batches. Only one transfer at a time is\n+\/\/ permitted, with a lock bit to control access to that phase. While a transfer\n+\/\/ is in progress, other threads might be adding other nodes to the pending list,\n+\/\/ to be dealt with by some later transfer.\n+void FreeListAllocator::release(void* free_node) {\n+  assert(free_node != nullptr, \"precondition\");\n+  assert(is_aligned(free_node, sizeof(FreeNode)), \"Unaligned addr \" PTR_FORMAT, p2i(free_node));\n+  FreeNode* node = ::new (free_node) FreeNode();\n+\n+  \/\/ The pending list is double-buffered.  Add node to the currently active\n+  \/\/ pending list, within a critical section so a transfer will wait until\n+  \/\/ we're done with what might be the pending list to be transferred.\n+  {\n+    GlobalCounter::CriticalSection cs(Thread::current());\n+    uint index = Atomic::load_acquire(&_active_pending_list);\n+    size_t count = _pending_lists[index].add(node);\n+    if (count <= _config->transfer_threshold()) return;\n+  }\n+  \/\/ Attempt transfer when number pending exceeds the transfer threshold.\n+  try_transfer_pending();\n+}\n+\n+\/\/ Try to transfer nodes from the pending list to _free_list, with a\n+\/\/ synchronization delay for any in-progress pops from the _free_list,\n+\/\/ to solve ABA there.  Return true if performed a (possibly empty)\n+\/\/ transfer, false if blocked from doing so by some other thread's\n+\/\/ in-progress transfer.\n+bool FreeListAllocator::try_transfer_pending() {\n+  \/\/ Attempt to claim the lock.\n+  if (Atomic::load(&_transfer_lock) || \/\/ Skip CAS if likely to fail.\n+      Atomic::cmpxchg(&_transfer_lock, false, true)) {\n+    return false;\n+  }\n+  \/\/ Have the lock; perform the transfer.\n+\n+  \/\/ Change which pending list is active.  Don't need an atomic RMW since\n+  \/\/ we have the lock and we're the only writer.\n+  uint index = Atomic::load(&_active_pending_list);\n+  uint new_active = (index + 1) % ARRAY_SIZE(_pending_lists);\n+  Atomic::release_store(&_active_pending_list, new_active);\n+\n+  \/\/ Wait for all critical sections in the buffer life-cycle to complete.\n+  \/\/ This includes _free_list pops and adding to the now inactive pending\n+  \/\/ list.\n+  GlobalCounter::write_synchronize();\n+\n+  \/\/ Transfer the inactive pending list to _free_list.\n+  NodeList transfer_list = _pending_lists[index].take_all();\n+  size_t count = transfer_list._entry_count;\n+  if (count > 0) {\n+    \/\/ Update count first so no underflow in allocate().\n+    Atomic::add(&_free_count, count);\n+    _free_list.prepend(*transfer_list._head, *transfer_list._tail);\n+    log_trace(gc, freelist)\n+             (\"Transferred %s pending to free: %zu\", name(), count);\n+  }\n+  Atomic::release_store(&_transfer_lock, false);\n+  return true;\n+}\n+\n+size_t FreeListAllocator::reduce_free_list(size_t remove_goal) {\n+  try_transfer_pending();\n+  size_t removed = 0;\n+  for ( ; removed < remove_goal; ++removed) {\n+    FreeNode* node = _free_list.pop();\n+    if (node == NULL) break;\n+    node->~FreeNode();\n+    _config->deallocate(node);\n+  }\n+  size_t new_count = Atomic::sub(&_free_count, removed);\n+  log_debug(gc, freelist)\n+           (\"Reduced %s free list by \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n+            name(), removed, new_count);\n+  return removed;\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/freeListAllocator.cpp","additions":219,"deletions":0,"binary":false,"changes":219,"status":"added"},{"patch":"@@ -0,0 +1,155 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_FREELISTALLOCATOR_HPP\n+#define SHARE_GC_SHARED_FREELISTALLOCATOR_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/padded.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/lockFreeStack.hpp\"\n+\n+class FreeListConfig {\n+  \/\/ Desired minimum transfer batch size.  There is relatively little\n+  \/\/ importance to the specific number.  It shouldn't be too big, else\n+  \/\/ we're wasting space when the release rate is low.  If the release\n+  \/\/ rate is high, we might accumulate more than this before being\n+  \/\/ able to start a new transfer, but that's okay.\n+  const size_t _transfer_threshold;\n+protected:\n+  ~FreeListConfig() = default;\n+public:\n+  explicit FreeListConfig(size_t threshold = 10) : _transfer_threshold(threshold) {}\n+\n+  size_t transfer_threshold() { return _transfer_threshold; }\n+\n+  virtual void* allocate() = 0;\n+\n+  virtual  void deallocate(void* node) = 0;\n+};\n+\n+\/\/ Allocation is based on a lock-free list of nodes. To reduce synchronization\n+\/\/ overhead on the free list between allocation and release calls, the released\n+\/\/ nodes are first placed on a pending list, then transferred to the free list in\n+\/\/ batches. While on the pending list, the nodes are not available for allocation.\n+\/\/ The allocator uses allocation options specified by an instance of\n+\/\/ FreeListConfig. The FreeListConfig includes an allocation method to use in case\n+\/\/ the free list is empty and a deallocation method used to deallocate nodes in\n+\/\/ the free list. Additionally, the FreeListConfig configures the threshold used\n+\/\/ as a minimum batch size for transferring released nodes from the pending list\n+\/\/ to the free list making them available for re-allocation.\n+class FreeListAllocator {\n+  struct FreeNode {\n+    FreeNode* volatile _next;\n+\n+    FreeNode() : _next (nullptr) { }\n+\n+    FreeNode* next() { return Atomic::load(&_next); }\n+\n+    FreeNode* volatile* next_addr() { return &_next; }\n+\n+    void set_next(FreeNode* next) { Atomic::store(&_next, next); }\n+  };\n+\n+  struct NodeList {\n+    FreeNode* _head;     \/\/ First node in list or NULL if empty.\n+    FreeNode* _tail;     \/\/ Last node in list or NULL if empty.\n+    size_t _entry_count; \/\/ Sum of entries in nodes in list.\n+\n+    NodeList();\n+\n+    NodeList(FreeNode* head, FreeNode* tail, size_t entry_count);\n+  };\n+\n+  class PendingList {\n+    FreeNode* _tail;\n+    FreeNode* volatile _head;\n+    volatile size_t _count;\n+\n+    NONCOPYABLE(PendingList);\n+\n+  public:\n+    PendingList();\n+    ~PendingList() = default;\n+\n+    \/\/ Add node to the list.  Returns the number of nodes in the list.\n+    \/\/ Thread-safe against concurrent add operations.\n+    size_t add(FreeNode* node);\n+\n+    size_t count() const;\n+\n+    \/\/ Return the nodes in the list, leaving the list empty.\n+    \/\/ Not thread-safe.\n+    NodeList take_all();\n+  };\n+\n+  static FreeNode* volatile* next_ptr(FreeNode& node) { return node.next_addr(); }\n+  typedef LockFreeStack<FreeNode, &next_ptr> Stack;\n+\n+  FreeListConfig* _config;\n+  char _name[DEFAULT_CACHE_LINE_SIZE - sizeof(FreeListConfig*)];  \/\/ Use name as padding.\n+\n+#define DECLARE_PADDED_MEMBER(Id, Type, Name) \\\n+  Type Name; DEFINE_PAD_MINUS_SIZE(Id, DEFAULT_CACHE_LINE_SIZE, sizeof(Type))\n+  DECLARE_PADDED_MEMBER(1, volatile size_t, _free_count);\n+  DECLARE_PADDED_MEMBER(2, Stack, _free_list);\n+  DECLARE_PADDED_MEMBER(3, volatile bool, _transfer_lock);\n+#undef DECLARE_PADDED_MEMBER\n+\n+  volatile uint _active_pending_list;\n+  PendingList _pending_lists[2];\n+\n+  void delete_list(FreeNode* list);\n+\n+  NONCOPYABLE(FreeListAllocator);\n+\n+public:\n+  FreeListAllocator(const char* name, FreeListConfig* config);\n+\n+  const char* name() const { return _name; }\n+\n+  ~FreeListAllocator();\n+\n+  size_t free_count() const;\n+  size_t pending_count() const;\n+\n+  void* allocate();\n+  void release(void* node);\n+\n+  void reset();\n+  bool try_transfer_pending();\n+\n+  size_t mem_size() const {\n+    return sizeof(*this);\n+  }\n+\n+  \/\/ Deallocate some of the available nodes in the free_list.\n+  \/\/ remove_goal is the target number to remove.  Returns the number\n+  \/\/ actually deallocated, which may be less than the goal if there\n+  \/\/ were fewer available.\n+  size_t reduce_free_list(size_t remove_goal);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_FREELISTALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/freeListAllocator.hpp","additions":155,"deletions":0,"binary":false,"changes":155,"status":"added"},{"patch":"@@ -27,2 +27,0 @@\n-#include \"logging\/log.hpp\"\n-#include \"memory\/allocation.hpp\"\n@@ -30,5 +28,0 @@\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/mutex.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/thread.inline.hpp\"\n-#include \"utilities\/globalCounter.inline.hpp\"\n@@ -48,13 +41,1 @@\n-BufferNode* BufferNode::allocate(size_t size) {\n-  size_t byte_size = size * sizeof(void*);\n-  void* data = NEW_C_HEAP_ARRAY(char, buffer_offset() + byte_size, mtGC);\n-  return new (data) BufferNode;\n-}\n-\n-void BufferNode::deallocate(BufferNode* node) {\n-  node->~BufferNode();\n-  FREE_C_HEAP_ARRAY(char, node);\n-}\n-\n-BufferNode::Allocator::PendingList::PendingList() :\n-  _tail(nullptr), _head(nullptr), _count(0) {}\n+BufferNode::AllocatorConfig::AllocatorConfig(size_t size) : _buffer_size(size) {}\n@@ -62,2 +43,3 @@\n-BufferNode::Allocator::PendingList::~PendingList() {\n-  delete_list(Atomic::load(&_head));\n+void* BufferNode::AllocatorConfig::allocate() {\n+  size_t byte_size = _buffer_size * sizeof(void*);\n+  return NEW_C_HEAP_ARRAY(char, buffer_offset() + byte_size, mtGC);\n@@ -66,18 +48,3 @@\n-size_t BufferNode::Allocator::PendingList::add(BufferNode* node) {\n-  assert(node->next() == nullptr, \"precondition\");\n-  BufferNode* old_head = Atomic::xchg(&_head, node);\n-  if (old_head != nullptr) {\n-    node->set_next(old_head);\n-  } else {\n-    assert(_tail == nullptr, \"invariant\");\n-    _tail = node;\n-  }\n-  return Atomic::add(&_count, size_t(1));\n-}\n-\n-BufferNodeList BufferNode::Allocator::PendingList::take_all() {\n-  BufferNodeList result{Atomic::load(&_head), _tail, Atomic::load(&_count)};\n-  Atomic::store(&_head, (BufferNode*)nullptr);\n-  _tail = nullptr;\n-  Atomic::store(&_count, size_t(0));\n-  return result;\n+void BufferNode::AllocatorConfig::deallocate(void* node) {\n+  assert(node != nullptr, \"precondition\");\n+  FREE_C_HEAP_ARRAY(char, node);\n@@ -87,6 +54,2 @@\n-  _buffer_size(buffer_size),\n-  _pending_lists(),\n-  _active_pending_list(0),\n-  _free_list(),\n-  _free_count(0),\n-  _transfer_lock(false)\n+  _config(buffer_size),\n+  _free_list(name, &_config)\n@@ -94,7 +57,0 @@\n-  strncpy(_name, name, sizeof(_name) - 1);\n-  _name[sizeof(_name) - 1] = '\\0';\n-}\n-\n-BufferNode::Allocator::~Allocator() {\n-  delete_list(_free_list.pop_all());\n-}\n@@ -102,7 +58,0 @@\n-void BufferNode::Allocator::delete_list(BufferNode* list) {\n-  while (list != NULL) {\n-    BufferNode* next = list->next();\n-    DEBUG_ONLY(list->set_next(NULL);)\n-    BufferNode::deallocate(list);\n-    list = next;\n-  }\n@@ -112,1 +61,1 @@\n-  return Atomic::load(&_free_count);\n+  return _free_list.free_count();\n@@ -116,16 +65,1 @@\n-  BufferNode* node;\n-  {\n-    \/\/ Protect against ABA; see release().\n-    GlobalCounter::CriticalSection cs(Thread::current());\n-    node = _free_list.pop();\n-  }\n-  if (node == NULL) {\n-    node = BufferNode::allocate(_buffer_size);\n-  } else {\n-    \/\/ Decrement count after getting buffer from free list.  This, along\n-    \/\/ with incrementing count before adding to free list, ensures count\n-    \/\/ never underflows.\n-    size_t count = Atomic::sub(&_free_count, 1u);\n-    assert((count + 1) != 0, \"_free_count underflow\");\n-  }\n-  return node;\n+  return ::new (_free_list.allocate()) BufferNode();\n@@ -134,9 +68,0 @@\n-\/\/ To solve the ABA problem for lock-free stack pop, allocate does the\n-\/\/ pop inside a critical section, and release synchronizes on the\n-\/\/ critical sections before adding to the _free_list.  But we don't\n-\/\/ want to make every release have to do a synchronize.  Instead, we\n-\/\/ initially place released nodes on the pending list, and transfer\n-\/\/ them to the _free_list in batches.  Only one transfer at a time is\n-\/\/ permitted, with a lock bit to control access to that phase.  While\n-\/\/ a transfer is in progress, other threads might be adding other nodes\n-\/\/ to the pending list, to be dealt with by some later transfer.\n@@ -146,59 +71,2 @@\n-\n-  \/\/ Desired minimum transfer batch size.  There is relatively little\n-  \/\/ importance to the specific number.  It shouldn't be too big, else\n-  \/\/ we're wasting space when the release rate is low.  If the release\n-  \/\/ rate is high, we might accumulate more than this before being\n-  \/\/ able to start a new transfer, but that's okay.  Also note that\n-  \/\/ the allocation rate and the release rate are going to be fairly\n-  \/\/ similar, due to how the buffers are used.\n-  const size_t trigger_transfer = 10;\n-\n-  \/\/ The pending list is double-buffered.  Add node to the currently active\n-  \/\/ pending list, within a critical section so a transfer will wait until\n-  \/\/ we're done with what might be the pending list to be transferred.\n-  {\n-    GlobalCounter::CriticalSection cs(Thread::current());\n-    uint index = Atomic::load_acquire(&_active_pending_list);\n-    size_t count = _pending_lists[index].add(node);\n-    if (count <= trigger_transfer) return;\n-  }\n-  \/\/ Attempt transfer when number pending exceeds the transfer threshold.\n-  try_transfer_pending();\n-}\n-\n-\/\/ Try to transfer nodes from the pending list to _free_list, with a\n-\/\/ synchronization delay for any in-progress pops from the _free_list,\n-\/\/ to solve ABA there.  Return true if performed a (possibly empty)\n-\/\/ transfer, false if blocked from doing so by some other thread's\n-\/\/ in-progress transfer.\n-bool BufferNode::Allocator::try_transfer_pending() {\n-  \/\/ Attempt to claim the lock.\n-  if (Atomic::load(&_transfer_lock) || \/\/ Skip CAS if likely to fail.\n-      Atomic::cmpxchg(&_transfer_lock, false, true)) {\n-    return false;\n-  }\n-  \/\/ Have the lock; perform the transfer.\n-\n-  \/\/ Change which pending list is active.  Don't need an atomic RMW since\n-  \/\/ we have the lock and we're the only writer.\n-  uint index = Atomic::load(&_active_pending_list);\n-  uint new_active = (index + 1) % ARRAY_SIZE(_pending_lists);\n-  Atomic::release_store(&_active_pending_list, new_active);\n-\n-  \/\/ Wait for all critical sections in the buffer life-cycle to complete.\n-  \/\/ This includes _free_list pops and adding to the now inactive pending\n-  \/\/ list.\n-  GlobalCounter::write_synchronize();\n-\n-  \/\/ Transfer the inactive pending list to _free_list.\n-  BufferNodeList transfer_list = _pending_lists[index].take_all();\n-  size_t count = transfer_list._entry_count;\n-  if (count > 0) {\n-    \/\/ Update count first so no underflow in allocate().\n-    Atomic::add(&_free_count, count);\n-    _free_list.prepend(*transfer_list._head, *transfer_list._tail);\n-    log_trace(gc, ptrqueue, freelist)\n-             (\"Transferred %s pending to free: %zu\", name(), count);\n-  }\n-  Atomic::release_store(&_transfer_lock, false);\n-  return true;\n+  node->~BufferNode();\n+  _free_list.release(node);\n@@ -208,12 +76,1 @@\n-  try_transfer_pending();\n-  size_t removed = 0;\n-  for ( ; removed < remove_goal; ++removed) {\n-    BufferNode* node = _free_list.pop();\n-    if (node == NULL) break;\n-    BufferNode::deallocate(node);\n-  }\n-  size_t new_count = Atomic::sub(&_free_count, removed);\n-  log_debug(gc, ptrqueue, freelist)\n-           (\"Reduced %s free list by \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n-            name(), removed, new_count);\n-  return removed;\n+  return _free_list.reduce_free_list(remove_goal);\n","filename":"src\/hotspot\/share\/gc\/shared\/ptrQueue.cpp","additions":14,"deletions":157,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"gc\/shared\/bufferNodeList.hpp\"\n+#include \"gc\/shared\/freeListAllocator.hpp\"\n@@ -135,6 +135,0 @@\n-  \/\/ Allocate a new BufferNode with the \"buffer\" having size elements.\n-  static BufferNode* allocate(size_t size);\n-\n-  \/\/ Free a BufferNode.\n-  static void deallocate(BufferNode* node);\n-\n@@ -166,0 +160,1 @@\n+  class AllocatorConfig;\n@@ -170,21 +165,6 @@\n-\/\/ Allocation is based on a lock-free free list of nodes, linked through\n-\/\/ BufferNode::_next (see BufferNode::Stack).  To solve the ABA problem,\n-\/\/ popping a node from the free list is performed within a GlobalCounter\n-\/\/ critical section, and pushing nodes onto the free list is done after\n-\/\/ a GlobalCounter synchronization associated with the nodes to be pushed.\n-\/\/ This is documented behavior so that other parts of the node life-cycle\n-\/\/ can depend on and make use of it too.\n-class BufferNode::Allocator {\n-  friend class TestSupport;\n-\n-  \/\/ Since we don't expect many instances, and measured >15% speedup\n-  \/\/ on stress gtest, padding seems like a good tradeoff here.\n-#define DECLARE_PADDED_MEMBER(Id, Type, Name) \\\n-  Type Name; DEFINE_PAD_MINUS_SIZE(Id, DEFAULT_CACHE_LINE_SIZE, sizeof(Type))\n-\n-  class PendingList {\n-    BufferNode* _tail;\n-    DECLARE_PADDED_MEMBER(1, BufferNode* volatile, _head);\n-    DECLARE_PADDED_MEMBER(2, volatile size_t, _count);\n-\n-    NONCOPYABLE(PendingList);\n+\/\/ We use BufferNode::AllocatorConfig to set the allocation options for the\n+\/\/ FreeListAllocator.\n+class BufferNode::AllocatorConfig : public FreeListConfig {\n+  const size_t _buffer_size;\n+public:\n+  explicit AllocatorConfig(size_t size);\n@@ -192,3 +172,1 @@\n-  public:\n-    PendingList();\n-    ~PendingList();\n+  ~AllocatorConfig() = default;\n@@ -196,3 +174,1 @@\n-    \/\/ Add node to the list.  Returns the number of nodes in the list.\n-    \/\/ Thread-safe against concurrent add operations.\n-    size_t add(BufferNode* node);\n+  void* allocate() override;\n@@ -200,4 +176,1 @@\n-    \/\/ Return the nodes in the list, leaving the list empty.\n-    \/\/ Not thread-safe.\n-    BufferNodeList take_all();\n-  };\n+  void deallocate(void* node) override;\n@@ -205,7 +178,2 @@\n-  const size_t _buffer_size;\n-  char _name[DEFAULT_CACHE_LINE_SIZE - sizeof(size_t)]; \/\/ Use name as padding.\n-  PendingList _pending_lists[2];\n-  DECLARE_PADDED_MEMBER(1, volatile uint, _active_pending_list);\n-  DECLARE_PADDED_MEMBER(2, Stack, _free_list);\n-  DECLARE_PADDED_MEMBER(3, volatile size_t, _free_count);\n-  DECLARE_PADDED_MEMBER(4, volatile bool, _transfer_lock);\n+  size_t buffer_size() const { return _buffer_size; }\n+};\n@@ -213,1 +181,2 @@\n-#undef DECLARE_PADDED_MEMBER\n+class BufferNode::Allocator {\n+  friend class TestSupport;\n@@ -215,2 +184,2 @@\n-  static void delete_list(BufferNode* list);\n-  bool try_transfer_pending();\n+  AllocatorConfig _config;\n+  FreeListAllocator _free_list;\n@@ -222,1 +191,1 @@\n-  ~Allocator();\n+  ~Allocator() = default;\n@@ -224,2 +193,1 @@\n-  const char* name() const { return _name; }\n-  size_t buffer_size() const { return _buffer_size; }\n+  size_t buffer_size() const { return _config.buffer_size(); }\n@@ -230,0 +198,4 @@\n+  \/\/ If _free_list has items buffered in the pending list, transfer\n+  \/\/ these to make them available for re-allocation.\n+  bool flush_free_list() { return _free_list.try_transfer_pending(); }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/ptrQueue.hpp","additions":23,"deletions":51,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,1 @@\n-    return allocator->try_transfer_pending();\n+    return allocator->flush_free_list();\n@@ -74,7 +74,0 @@\n-  for (size_t i = 0; i < node_count; ++i) {\n-    if (i == 0) {\n-      ASSERT_EQ((BufferNode*)NULL, nodes[i]->next());\n-    } else {\n-      ASSERT_EQ(nodes[i - 1], nodes[i]->next());\n-    }\n-  }\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_ptrQueueBufferAllocator.cpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"}]}