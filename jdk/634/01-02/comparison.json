{"files":[{"patch":"@@ -0,0 +1,3456 @@\n+\/\/ Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, Arm Limited. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+\/\/ This file is automatically generated by running \"m4 aarch64_neon_ad.m4\". Do not edit ----\n+\n+\/\/ AArch64 NEON Architecture Description File\n+\n+\/\/ ====================VECTOR INSTRUCTIONS==================================\n+\n+\/\/ ------------------------------ Load\/store\/reinterpret -----------------------\n+\n+\/\/ Load vector (16 bits)\n+instruct loadV2(vecD dst, memory mem)\n+%{\n+  predicate(n->as_LoadVector()->memory_size() == 2);\n+  match(Set dst (LoadVector mem));\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrh   $dst,$mem\\t# vector (16 bits)\" %}\n+  ins_encode( aarch64_enc_ldrvH(dst, mem) );\n+  ins_pipe(vload_reg_mem64);\n+%}\n+\n+\/\/ Store Vector (16 bits)\n+instruct storeV2(vecD src, memory mem)\n+%{\n+  predicate(n->as_StoreVector()->memory_size() == 2);\n+  match(Set mem (StoreVector mem src));\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"strh   $mem,$src\\t# vector (16 bits)\" %}\n+  ins_encode( aarch64_enc_strvH(src, mem) );\n+  ins_pipe(vstore_reg_mem64);\n+%}\n+\n+instruct reinterpretD(vecD dst)\n+%{\n+  predicate(n->bottom_type()->is_vect()->length_in_bytes() == 8 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == 8);\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(0);\n+  format %{ \" # reinterpret $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct reinterpretX(vecX dst)\n+%{\n+  predicate(n->bottom_type()->is_vect()->length_in_bytes() == 16 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == 16);\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(0);\n+  format %{ \" # reinterpret $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct reinterpretD2X(vecX dst, vecD src)\n+%{\n+  predicate(n->bottom_type()->is_vect()->length_in_bytes() == 16 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == 8);\n+  match(Set dst (VectorReinterpret src));\n+  ins_cost(INSN_COST);\n+  format %{ \" # reinterpret $dst,$src\" %}\n+  ins_encode %{\n+    \/\/ If register is the same, then move is not needed.\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    }\n+  %}\n+  ins_pipe(vlogical64);\n+%}\n+\n+instruct reinterpretX2D(vecD dst, vecX src)\n+%{\n+  predicate(n->bottom_type()->is_vect()->length_in_bytes() == 8 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == 16);\n+  match(Set dst (VectorReinterpret src));\n+  ins_cost(INSN_COST);\n+  format %{ \" # reinterpret $dst,$src\" %}\n+  ins_encode %{\n+    \/\/ If register is the same, then move is not needed.\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    }\n+  %}\n+  ins_pipe(vlogical64);\n+%}\n+\n+\/\/ ------------------------------ Vector cast -------------------------------\n+\n+instruct vcvt4Bto4S(vecD dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"sxtl  $dst, T8H, $src, T8B\\t# convert 4B to 4S vector\" %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt8Bto8S(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 8 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"sxtl  $dst, T8H, $src, T8B\\t# convert 8B to 8S vector\" %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt4Sto4B(vecD dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastS2X src));\n+  format %{ \"xtn  $dst, T8B, $src, T8H\\t# convert 4S to 4B vector\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg), __ T8H);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt8Sto8B(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 8 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastS2X src));\n+  format %{ \"xtn  $dst, T8B, $src, T8H\\t# convert 8S to 8B vector\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg), __ T8H);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt4Sto4I(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorCastS2X src));\n+  format %{ \"sxtl  $dst, T4S, $src, T4H\\t# convert 4S to 4I vector\" %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg), __ T4H);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt4Ito4S(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"xtn  $dst, T4H, $src, T4S\\t# convert 4I to 4S vector\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($src$$reg), __ T4S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt2Ito2L(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"sxtl  $dst, T2D, $src, T2S\\t# convert 2I to 2L vector\" %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg), __ T2S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt2Lto2I(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorCastL2X src));\n+  format %{ \"xtn  $dst, T2S, $src, T2D\\t# convert 2L to 2I vector\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg), __ T2D);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt4Bto4I(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"sxtl  $dst, T8H, $src, T8B\\n\\t\"\n+            \"sxtl  $dst, T4S, $dst, T4H\\t# convert 4B to 4I vector\"\n+  %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvt4Ito4B(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"xtn  $dst, T4H, $src, T4S\\n\\t\"\n+            \"xtn  $dst, T8B, $dst, T8H\\t# convert 4I to 4B vector\"\n+  %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($src$$reg), __ T4S);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg), __ T8H);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvt4Bto4F(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"sxtl  $dst, T8H, $src, T8B\\n\\t\"\n+            \"sxtl  $dst, T4S, $dst, T4H\\n\\t\"\n+            \"scvtfv  T4S, $dst, $dst\\t# convert 4B to 4F vector\"\n+  %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+    __ scvtfv(__ T4S, as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvt4Sto4F(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastS2X src));\n+  format %{ \"sxtl    $dst, T4S, $src, T4H\\n\\t\"\n+            \"scvtfv  T4S, $dst, $dst\\t# convert 4S to 4F vector\"\n+  %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg), __ T4H);\n+    __ scvtfv(__ T4S, as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvt2Ito2D(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"sxtl    $dst, T2D, $src, T2S\\n\\t\"\n+            \"scvtfv  T2D, $dst, $dst\\t# convert 2I to 2D vector\"\n+  %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg), __ T2S);\n+    __ scvtfv(__ T2D, as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvt2Ito2F(vecD dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"scvtfv  T2S, $dst, $src\\t# convert 2I to 2F vector\" %}\n+  ins_encode %{\n+    __ scvtfv(__ T2S, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt4Ito4F(vecX dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"scvtfv  T4S, $dst, $src\\t# convert 4I to 4F vector\" %}\n+  ins_encode %{\n+    __ scvtfv(__ T4S, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt2Lto2D(vecX dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastL2X src));\n+  format %{ \"scvtfv  T2D, $dst, $src\\t# convert 2L to 2D vector\" %}\n+  ins_encode %{\n+    __ scvtfv(__ T2D, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt2Fto2D(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastF2X src));\n+  format %{ \"fcvtl  $dst, T2D, $src, T2S\\t# convert 2F to 2D vector\" %}\n+  ins_encode %{\n+    __ fcvtl(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg), __ T2S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt2Dto2F(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastD2X src));\n+  format %{ \"fcvtn  $dst, T2S, $src, T2D\\t# convert 2D to 2F vector\" %}\n+  ins_encode %{\n+    __ fcvtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg), __ T2D);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vcvt2Lto2F(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastL2X src));\n+  format %{ \"scvtfv  T2D, $dst, $src\\n\\t\"\n+            \"fcvtn   $dst, T2S, $dst, T2D\\t# convert 2L to 2F vector\"\n+  %}\n+  ins_encode %{\n+    __ scvtfv(__ T2D, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    __ fcvtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($dst$$reg), __ T2D);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Reduction -------------------------------\n+\n+instruct reduce_add8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addv  $tmp, T8B, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"addw  $dst, $dst, $isrc\\n\\t\"\n+            \"sxtb  $dst, $dst\\t# add reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T8B, as_FloatRegister($vsrc$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ addw($dst$$Register, $dst$$Register, $isrc$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_add16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addv  $tmp, T16B, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"addw  $dst, $dst, $isrc\\n\\t\"\n+            \"sxtb  $dst, $dst\\t# add reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T16B, as_FloatRegister($vsrc$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ addw($dst$$Register, $dst$$Register, $isrc$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_add4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addv  $tmp, T4H, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"addw  $dst, $dst, $isrc\\n\\t\"\n+            \"sxth  $dst, $dst\\t# add reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T4H, as_FloatRegister($vsrc$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ addw($dst$$Register, $dst$$Register, $isrc$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_add8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addv  $tmp, T8H, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"addw  $dst, $dst, $isrc\\n\\t\"\n+            \"sxth  $dst, $dst\\t# add reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T8H, as_FloatRegister($vsrc$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ addw($dst$$Register, $dst$$Register, $isrc$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_add2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, vecX tmp)\n+%{\n+  match(Set dst (AddReductionVL isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addpd $tmp, $vsrc\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"add   $dst, $isrc, $dst\\t# add reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ addpd(as_FloatRegister($tmp$$reg), as_FloatRegister($vsrc$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ add($dst$$Register, $isrc$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD vtmp1, vecD vtmp2, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP itmp);\n+  format %{ \"ins   $vtmp1, S, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T8B, $vtmp1, $vsrc\\n\\t\"\n+            \"ins   $vtmp2, H, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp2, T8B, $vtmp2, $vtmp1\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxtb  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxtb  $dst, $dst\\t# mul reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp1$$reg), __ S,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T8B,\n+            as_FloatRegister($vtmp1$$reg), as_FloatRegister($vsrc$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ H,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp2$$reg), __ T8B,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX vtmp1, vecX vtmp2, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP itmp);\n+  format %{ \"ins   $vtmp1, D, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T8B, $vtmp1, $vsrc\\n\\t\"\n+            \"ins   $vtmp2, S, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T8B, $vtmp2, $vtmp1\\n\\t\"\n+            \"ins   $vtmp2, H, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp2, T8B, $vtmp2, $vtmp1\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxtb  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxtb  $dst, $dst\\t# mul reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp1$$reg), __ D,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T8B,\n+            as_FloatRegister($vtmp1$$reg), as_FloatRegister($vsrc$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ S,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T8B,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ H,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp2$$reg), __ T8B,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD vtmp, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP itmp);\n+  format %{ \"ins   $vtmp, S, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp, T4H, $vtmp, $vsrc\\n\\t\"\n+            \"umov  $itmp, $vtmp, H, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxth  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp, H, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxth  $dst, $dst\\t# mul reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp$$reg), __ S,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp$$reg), __ T4H,\n+            as_FloatRegister($vtmp$$reg), as_FloatRegister($vsrc$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp$$reg), __ H, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp$$reg), __ H, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX vtmp1, vecX vtmp2, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP itmp);\n+  format %{ \"ins   $vtmp1, D, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T4H, $vtmp1, $vsrc\\n\\t\"\n+            \"ins   $vtmp2, S, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp2, T4H, $vtmp2, $vtmp1\\n\\t\"\n+            \"umov  $itmp, $vtmp2, H, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxth  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp2, H, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxth  $dst, $dst\\t# mul reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp1$$reg), __ D,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T4H,\n+            as_FloatRegister($vtmp1$$reg), as_FloatRegister($vsrc$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ S,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp2$$reg), __ T4H,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ H, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ H, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp)\n+%{\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"mul   $dst, $isrc, $tmp\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"mul   $dst, $dst, $tmp\\t# mul reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ mul($dst$$Register, $isrc$$Register, $tmp$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ mul($dst$$Register, $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"smaxv $tmp, T8B, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc GT\\t# max reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ smaxv(as_FloatRegister($tmp$$reg), __ T8B, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"smaxv $tmp, T16B, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc GT\\t# max reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ smaxv(as_FloatRegister($tmp$$reg), __ T16B, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"smaxv $tmp, T4H, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc GT\\t# max reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ smaxv(as_FloatRegister($tmp$$reg), __ T4H, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"smaxv $tmp, T8H, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc GT\\t# max reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ smaxv(as_FloatRegister($tmp$$reg), __ T8H, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max4I(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"smaxv $tmp, T4S, $vsrc\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc GT\\t# max reduction4I\"\n+  %}\n+  ins_encode %{\n+    __ smaxv(as_FloatRegister($tmp$$reg), __ T4S, as_FloatRegister($vsrc$$reg));\n+    __ umov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"sminv $tmp, T8B, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc LT\\t# min reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ sminv(as_FloatRegister($tmp$$reg), __ T8B, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"sminv $tmp, T16B, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc LT\\t# min reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ sminv(as_FloatRegister($tmp$$reg), __ T16B, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"sminv $tmp, T4H, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc LT\\t# min reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ sminv(as_FloatRegister($tmp$$reg), __ T4H, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"sminv $tmp, T8H, $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, H, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc LT\\t# min reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ sminv(as_FloatRegister($tmp$$reg), __ T8H, as_FloatRegister($vsrc$$reg));\n+    __ smov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ H, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min4I(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"sminv $tmp, T4S, $vsrc\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc LT\\t# min reduction4I\"\n+  %}\n+  ins_encode %{\n+    __ sminv(as_FloatRegister($tmp$$reg), __ T4S, as_FloatRegister($vsrc$$reg));\n+    __ umov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"dup   $tmp, T2D, $vsrc\\n\\t\"\n+            \"smaxv $tmp, T4S, $tmp\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc GT\\t# max reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ dup(as_FloatRegister($tmp$$reg), __ T2D, as_FloatRegister($vsrc$$reg));\n+    __ smaxv(as_FloatRegister($tmp$$reg), __ T4S, as_FloatRegister($tmp$$reg));\n+    __ umov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"dup   $tmp, T2D, $vsrc\\n\\t\"\n+            \"sminv $tmp, T4S, $tmp\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc LT\\t# min reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ dup(as_FloatRegister($tmp$$reg), __ T2D, as_FloatRegister($vsrc$$reg));\n+    __ sminv(as_FloatRegister($tmp$$reg), __ T4S, as_FloatRegister($tmp$$reg));\n+    __ umov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_max2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"cmp   $isrc,$tmp\\n\\t\"\n+            \"csel  $dst, $isrc, $tmp GT\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"cmp   $dst, $tmp\\n\\t\"\n+            \"csel  $dst, $dst, $tmp GT\\t# max reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov(as_Register($tmp$$reg), as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ cmp(as_Register($isrc$$reg), as_Register($tmp$$reg));\n+    __ csel(as_Register($dst$$reg), as_Register($isrc$$reg), as_Register($tmp$$reg), Assembler::GT);\n+    __ umov(as_Register($tmp$$reg), as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ cmp(as_Register($dst$$reg), as_Register($tmp$$reg));\n+    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($tmp$$reg), Assembler::GT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_min2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"cmp   $isrc,$tmp\\n\\t\"\n+            \"csel  $dst, $isrc, $tmp LT\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"cmp   $dst, $tmp\\n\\t\"\n+            \"csel  $dst, $dst, $tmp LT\\t# min reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov(as_Register($tmp$$reg), as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ cmp(as_Register($isrc$$reg), as_Register($tmp$$reg));\n+    __ csel(as_Register($dst$$reg), as_Register($isrc$$reg), as_Register($tmp$$reg), Assembler::LT);\n+    __ umov(as_Register($tmp$$reg), as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ cmp(as_Register($dst$$reg), as_Register($tmp$$reg));\n+    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($tmp$$reg), Assembler::LT);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"andw   $dst, $dst, $tmp\\n\\t\"\n+            \"andw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"andw   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"andw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# and reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ andw($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ andw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ andw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ andw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"orrw   $dst, $dst, $tmp\\n\\t\"\n+            \"orrw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"orrw   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"orrw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# orr reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ orrw($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ orrw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ orrw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ orrw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"eorw   $dst, $dst, $tmp\\n\\t\"\n+            \"eorw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"eorw   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"eorw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# eor reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ eorw($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ eorw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ eorw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ eorw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"andr   $dst, $dst, $tmp\\n\\t\"\n+            \"andr   $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"andw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"andw   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"andw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# and reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ andr($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ andr($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ andw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ andw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ andw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"orr    $dst, $dst, $tmp\\n\\t\"\n+            \"orr    $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"orrw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"orrw   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"orrw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# orr reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ orr ($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ orr ($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ orrw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ orrw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ orrw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"eor    $dst, $dst, $tmp\\n\\t\"\n+            \"eor    $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"eorw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"eorw   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"eorw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# eor reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ eor ($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ eor ($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ eorw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ eorw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ eorw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"andw   $dst, $dst, $tmp\\n\\t\"\n+            \"andw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"andw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# and reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ andw($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ andw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ andw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"orrw   $dst, $dst, $tmp\\n\\t\"\n+            \"orrw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"orrw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# orr reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ orrw($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ orrw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ orrw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"eorw   $dst, $dst, $tmp\\n\\t\"\n+            \"eorw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"eorw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# eor reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ eorw($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ eorw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ eorw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"andr   $dst, $dst, $tmp\\n\\t\"\n+            \"andr   $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"andw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"andw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# and reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ andr($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ andr($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ andw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ andw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"orr    $dst, $dst, $tmp\\n\\t\"\n+            \"orr    $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"orrw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"orrw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# orr reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ orr ($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ orr ($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ orrw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ orrw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"eor    $dst, $dst, $tmp\\n\\t\"\n+            \"eor    $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"eorw   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"eorw   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# eor reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ eor ($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ eor ($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ eorw($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ eorw($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, S, 0\\n\\t\"\n+            \"andw  $dst, $tmp, $isrc\\n\\t\"\n+            \"umov  $tmp, $vsrc, S, 1\\n\\t\"\n+            \"andw  $dst, $tmp, $dst\\t# and reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ andw($dst$$Register, $tmp$$Register, $isrc$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ andw($dst$$Register, $tmp$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, S, 0\\n\\t\"\n+            \"orrw  $dst, $tmp, $isrc\\n\\t\"\n+            \"umov  $tmp, $vsrc, S, 1\\n\\t\"\n+            \"orrw  $dst, $tmp, $dst\\t# orr reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ orrw($dst$$Register, $tmp$$Register, $isrc$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ orrw($dst$$Register, $tmp$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, S, 0\\n\\t\"\n+            \"eorw  $dst, $tmp, $isrc\\n\\t\"\n+            \"umov  $tmp, $vsrc, S, 1\\n\\t\"\n+            \"eorw  $dst, $tmp, $dst\\t# eor reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ eorw($dst$$Register, $tmp$$Register, $isrc$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ eorw($dst$$Register, $tmp$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and4I(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"andr   $dst, $dst, $tmp\\n\\t\"\n+            \"andr   $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"andw   $dst, $isrc, $dst\\t# and reduction4I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ andr($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ andr($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ andw($dst$$Register, $isrc$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr4I(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"orr    $dst, $dst, $tmp\\n\\t\"\n+            \"orr    $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"orrw   $dst, $isrc, $dst\\t# orr reduction4I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ orr ($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ orr ($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ orrw($dst$$Register, $isrc$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor4I(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"eor    $dst, $dst, $tmp\\n\\t\"\n+            \"eor    $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"eorw   $dst, $isrc, $dst\\t# eor reduction4I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ eor ($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ eor ($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ eorw($dst$$Register, $isrc$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_and2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"andr  $dst, $isrc, $tmp\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"andr  $dst, $dst, $tmp\\t# and reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ andr($dst$$Register, $isrc$$Register, $tmp$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ andr($dst$$Register, $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orr2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"orr   $dst, $isrc, $tmp\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"orr   $dst, $dst, $tmp\\t# orr reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ orr ($dst$$Register, $isrc$$Register, $tmp$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ orr ($dst$$Register, $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eor2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"eor   $dst, $isrc, $tmp\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"eor   $dst, $dst, $tmp\\t# eor reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ eor ($dst$$Register, $isrc$$Register, $tmp$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ eor ($dst$$Register, $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector insert ---------------------------------\n+\n+instruct insert8B(vecD dst, vecD src, iRegIorL2I val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T8B, $src, $src\\n\\t\"\n+            \"mov    $dst, T8B, $idx, $val\\t# insert into vector(8B)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T8B, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert16B(vecX dst, vecX src, iRegIorL2I val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"mov    $dst, T16B, $idx, $val\\t# insert into vector(16B)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T16B, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert4S(vecD dst, vecD src, iRegIorL2I val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T8B, $src, $src\\n\\t\"\n+            \"mov    $dst, T4H, $idx, $val\\t# insert into vector(4S)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T4H, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert8S(vecX dst, vecX src, iRegIorL2I val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"mov    $dst, T8H, $idx, $val\\t# insert into vector(8S)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T8H, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert2I(vecD dst, vecD src, iRegIorL2I val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T8B, $src, $src\\n\\t\"\n+            \"mov    $dst, T2S, $idx, $val\\t# insert into vector(2I)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T2S, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert4I(vecX dst, vecX src, iRegIorL2I val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"mov    $dst, T4S, $idx, $val\\t# insert into vector(4I)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T4S, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert2L(vecX dst, vecX src, iRegL val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"mov    $dst, T2D, $idx, $val\\t# insert into vector(2L)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T2D, $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert2F(vecD dst, vecD src, vRegF val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T8B, $src, $src\\n\\t\"\n+            \"ins    $dst, S, $val, $idx, 0\\t# insert into vector(2F)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ S,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert4F(vecX dst, vecX src, vRegF val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"ins    $dst, S, $val, $idx, 0\\t# insert into vector(4F)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ S,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insert2D(vecX dst, vecX src, vRegD val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T16B, $src, $src\\n\\t\"\n+            \"ins    $dst, D, $val, $idx, 0\\t# insert into vector(2D)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ D,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector extract ---------------------------------\n+\n+instruct extract8B(iRegINoSp dst, vecD src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 8);\n+  match(Set dst (ExtractB src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov    $dst, $src, B, $idx\\t# extract from vector(8B)\" %}\n+  ins_encode %{\n+    __ smov($dst$$Register, as_FloatRegister($src$$reg), __ B, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract16B(iRegINoSp dst, vecX src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 16);\n+  match(Set dst (ExtractB src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov    $dst, $src, B, $idx\\t# extract from vector(16B)\" %}\n+  ins_encode %{\n+    __ smov($dst$$Register, as_FloatRegister($src$$reg), __ B, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract4S(iRegINoSp dst, vecD src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 4);\n+  match(Set dst (ExtractS src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov    $dst, $src, H, $idx\\t# extract from vector(4S)\" %}\n+  ins_encode %{\n+    __ smov($dst$$Register, as_FloatRegister($src$$reg), __ H, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract8S(iRegINoSp dst, vecX src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 8);\n+  match(Set dst (ExtractS src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"smov    $dst, $src, H, $idx\\t# extract from vector(8S)\" %}\n+  ins_encode %{\n+    __ smov($dst$$Register, as_FloatRegister($src$$reg), __ H, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract2I(iRegINoSp dst, vecD src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 2);\n+  match(Set dst (ExtractI src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"umov    $dst, $src, S, $idx\\t# extract from vector(2I)\" %}\n+  ins_encode %{\n+    __ umov($dst$$Register, as_FloatRegister($src$$reg), __ S, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract4I(iRegINoSp dst, vecX src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 4);\n+  match(Set dst (ExtractI src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"umov    $dst, $src, S, $idx\\t# extract from vector(4I)\" %}\n+  ins_encode %{\n+    __ umov($dst$$Register, as_FloatRegister($src$$reg), __ S, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract2L(iRegLNoSp dst, vecX src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 2);\n+  match(Set dst (ExtractL src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"umov    $dst, $src, D, $idx\\t# extract from vector(2L)\" %}\n+  ins_encode %{\n+    __ umov($dst$$Register, as_FloatRegister($src$$reg), __ D, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract2F(vRegF dst, vecD src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 2);\n+  match(Set dst (ExtractF src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"ins   $dst, S, $src, 0, $idx\\t# extract from vector(2F)\" %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($dst$$reg), __ S,\n+           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract4F(vRegF dst, vecX src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 4);\n+  match(Set dst (ExtractF src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"ins   $dst, S, $src, 0, $idx\\t# extract from vector(4F)\" %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($dst$$reg), __ S,\n+           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct extract2D(vRegD dst, vecX src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == 2);\n+  match(Set dst (ExtractD src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"ins   $dst, D, $src, 0, $idx\\t# extract from vector(2D)\" %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($dst$$reg), __ D,\n+           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ ------------------------------ Vector comparison ---------------------------------\n+\n+instruct vcmeq8B(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (8B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmeq16B(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (16B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmeq4S(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (4S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmeq8S(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (8S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmeq2I(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (2I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmeq4I(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (4I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmeq2L(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\t# vector cmp (2L)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmeq2F(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmeq  $dst, $src1, $src2\\t# vector cmp (2F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmeq(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmeq4F(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmeq  $dst, $src1, $src2\\t# vector cmp (4F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmeq(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmeq2D(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::eq &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmeq  $dst, $src1, $src2\\t# vector cmp (2D)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmeq(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmgt8B(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (8B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmgt16B(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (16B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmgt4S(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (4S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmgt8S(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (8S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmgt2I(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (2I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmgt4I(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (4I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmgt2L(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector cmp (2L)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmgt2F(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmgt  $dst, $src1, $src2\\t# vector cmp (2F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmgt(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmgt4F(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmgt  $dst, $src1, $src2\\t# vector cmp (4F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmgt(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmgt2D(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::gt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmgt  $dst, $src1, $src2\\t# vector cmp (2D)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmge8B(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (8B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmge16B(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (16B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmge4S(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (4S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmge8S(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (8S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmge2I(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (2I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmge4I(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (4I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmge2L(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src1, $src2\\t# vector cmp (2L)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmge2F(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmge  $dst, $src1, $src2\\t# vector cmp (2F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmge(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmge4F(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmge  $dst, $src1, $src2\\t# vector cmp (4F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmge(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmge2D(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ge &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmge  $dst, $src1, $src2\\t# vector cmp (2D)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmge(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmne8B(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (8B)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne16B(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (16B)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne4S(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (4S)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne8S(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (8S)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne2I(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (2I)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne4I(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (4I)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne2L(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmeq  $dst, $src1, $src2\\n\\t# vector cmp (2L)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmeq(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne2F(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmeq  $dst, $src1, $src2\\n\\t# vector cmp (2F)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmeq(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne4F(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmeq  $dst, $src1, $src2\\n\\t# vector cmp (4F)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmeq(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmne2D(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmeq  $dst, $src1, $src2\\n\\t# vector cmp (2D)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmeq(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcmlt8B(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (8B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmlt16B(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (16B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmlt4S(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (4S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmlt8S(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (8S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmlt2I(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (2I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmlt4I(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (4I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmlt2L(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmgt  $dst, $src2, $src1\\t# vector cmp (2L)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmlt2F(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmgt  $dst, $src2, $src1\\t# vector cmp (2F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmgt(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmlt4F(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmgt  $dst, $src2, $src1\\t# vector cmp (4F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmgt(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmlt2D(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::lt &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmgt  $dst, $src2, $src1\\t# vector cmp (2D)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmle8B(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (8B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmle16B(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (16B)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmle4S(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (4S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmle8S(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (8S)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmle2I(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (2I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmle4I(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (4I)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmle2L(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"cmge  $dst, $src2, $src1\\t# vector cmp (2L)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ cmge(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmle2F(vecD dst, vecD src1, vecD src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmge  $dst, $src2, $src1\\t# vector cmp (2F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmge(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vcmle4F(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmge  $dst, $src2, $src1\\t# vector cmp (4F)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmge(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vcmle2D(vecX dst, vecX src1, vecX src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::le &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"fcmge  $dst, $src2, $src1\\t# vector cmp (2D)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ fcmge(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+\/\/ ------------------------------ Vector mul -----------------------------------\n+\n+instruct vmul2L(vecX dst, vecX src1, vecX src2, iRegLNoSp tmp1, iRegLNoSp tmp2)\n+%{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (MulVL src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp1, TEMP tmp2);\n+  format %{ \"umov   $tmp1, $src1, D, 0\\n\\t\"\n+            \"umov   $tmp2, $src2, D, 0\\n\\t\"\n+            \"mul    $tmp2, $tmp2, $tmp1\\n\\t\"\n+            \"mov    $dst,  T2D,   0, $tmp2\\t# insert into vector(2L)\\n\\t\"\n+            \"umov   $tmp1, $src1, D, 1\\n\\t\"\n+            \"umov   $tmp2, $src2, D, 1\\n\\t\"\n+            \"mul    $tmp2, $tmp2, $tmp1\\n\\t\"\n+            \"mov    $dst,  T2D,   1, $tmp2\\t# insert into vector(2L)\\n\\t\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp1$$Register, as_FloatRegister($src1$$reg), __ D, 0);\n+    __ umov($tmp2$$Register, as_FloatRegister($src2$$reg), __ D, 0);\n+    __ mul(as_Register($tmp2$$reg), as_Register($tmp2$$reg), as_Register($tmp1$$reg));\n+    __ mov(as_FloatRegister($dst$$reg), __ T2D, 0, $tmp2$$Register);\n+    __ umov($tmp1$$Register, as_FloatRegister($src1$$reg), __ D, 1);\n+    __ umov($tmp2$$Register, as_FloatRegister($src2$$reg), __ D, 1);\n+    __ mul(as_Register($tmp2$$reg), as_Register($tmp2$$reg), as_Register($tmp1$$reg));\n+    __ mov(as_FloatRegister($dst$$reg), __ T2D, 1, $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ --------------------------------- Vector not --------------------------------\n+\n+instruct vnot2I(vecD dst, vecD src, immI_M1 m1)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));\n+  ins_cost(INSN_COST);\n+  format %{ \"not  $dst, $src\\t# vector (8B)\" %}\n+  ins_encode %{\n+    __ notr(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vnot4I(vecX dst, vecX src, immI_M1 m1)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));\n+  ins_cost(INSN_COST);\n+  format %{ \"not  $dst, $src\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vnot2L(vecX dst, vecX src, immL_M1 m1)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (XorV src (ReplicateL m1)));\n+  ins_cost(INSN_COST);\n+  format %{ \"not  $dst, $src\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ notr(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ ------------------------------ Vector max\/min -------------------------------\n+\n+instruct vmax8B(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate((n->as_Vector()->length() == 4 || n->as_Vector()->length() == 8) &&\n+             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"maxv  $dst, $src1, $src2\\t# vector (8B)\" %}\n+  ins_encode %{\n+    __ maxv(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vmax16B(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"maxv  $dst, $src1, $src2\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ maxv(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vmax4S(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"maxv  $dst, $src1, $src2\\t# vector (4S)\" %}\n+  ins_encode %{\n+    __ maxv(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vmax8S(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 8 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"maxv  $dst, $src1, $src2\\t# vector (8S)\" %}\n+  ins_encode %{\n+    __ maxv(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vmax2I(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"maxv  $dst, $src1, $src2\\t# vector (2I)\" %}\n+  ins_encode %{\n+    __ maxv(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vmax4I(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"maxv  $dst, $src1, $src2\\t# vector (4I)\" %}\n+  ins_encode %{\n+    __ maxv(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vmin8B(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate((n->as_Vector()->length() == 4 || n->as_Vector()->length() == 8) &&\n+             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"minv  $dst, $src1, $src2\\t# vector (8B)\" %}\n+  ins_encode %{\n+    __ minv(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vmin16B(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"minv  $dst, $src1, $src2\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ minv(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vmin4S(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"minv  $dst, $src1, $src2\\t# vector (4S)\" %}\n+  ins_encode %{\n+    __ minv(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vmin8S(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 8 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"minv  $dst, $src1, $src2\\t# vector (8S)\" %}\n+  ins_encode %{\n+    __ minv(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vmin2I(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"minv  $dst, $src1, $src2\\t# vector (2I)\" %}\n+  ins_encode %{\n+    __ minv(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop64);\n+%}\n+\n+instruct vmin4I(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"minv  $dst, $src1, $src2\\t# vector (4I)\" %}\n+  ins_encode %{\n+    __ minv(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+\n+instruct vmax2L(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP dst);\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector (2L)\\n\\t\"\n+            \"bsl   $dst, $src1, $src2\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ bsl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+instruct vmin2L(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP dst);\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector (2L)\\n\\t\"\n+            \"bsl   $dst, $src2, $src1\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ bsl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}\n+\n+\/\/ --------------------------------- blend (bsl) ----------------------------\n+\n+instruct vbsl8B(vecD dst, vecD src1, vecD src2)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  match(Set dst (VectorBlend (Binary src1 src2) dst));\n+  ins_cost(INSN_COST);\n+  format %{ \"bsl  $dst, $src2, $src1\\t# vector (8B)\" %}\n+  ins_encode %{\n+    __ bsl(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vlogical64);\n+%}\n+\n+instruct vbsl16B(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == 16);\n+  match(Set dst (VectorBlend (Binary src1 src2) dst));\n+  ins_cost(INSN_COST);\n+  format %{ \"bsl  $dst, $src2, $src1\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ bsl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vlogical128);\n+%}\n+\n+\/\/ --------------------------------- Load\/store Mask ----------------------------\n+\n+instruct loadmask8B(vecD dst, vecD src  )\n+%{\n+  predicate(n->as_Vector()->length() == 8 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src ));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# load mask (8B to 8B)\" %}\n+  ins_encode %{\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct loadmask16B(vecX dst, vecX src  )\n+%{\n+  predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src ));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# load mask (16B to 16B)\" %}\n+  ins_encode %{\n+    __ negr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct storemask8B(vecD dst, vecD src , immI_1 size)\n+%{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# store mask (8B to 8B)\" %}\n+  ins_encode %{\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct storemask16B(vecX dst, vecX src , immI_1 size)\n+%{\n+  predicate(n->as_Vector()->length() == 16);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# store mask (16B to 16B)\" %}\n+  ins_encode %{\n+    __ negr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct loadmask4S(vecD dst, vecD src  )\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadMask src ));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\n\\t\"\n+            \"negr  $dst, $dst\\t# load mask (4B to 4H)\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadmask8S(vecX dst, vecD src  )\n+%{\n+  predicate(n->as_Vector()->length() == 8 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadMask src ));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\n\\t\"\n+            \"negr  $dst, $dst\\t# load mask (8B to 8H)\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storemask4S(vecD dst, vecD src , immI_2 size)\n+%{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"xtn  $dst, $src\\n\\t\"\n+            \"negr  $dst, $dst\\t# store mask (4H to 4B)\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg), __ T8H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storemask8S(vecD dst, vecX src , immI_2 size)\n+%{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"xtn  $dst, $src\\n\\t\"\n+            \"negr  $dst, $dst\\t# store mask (8H to 8B)\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg), __ T8H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadmask2I(vecD dst, vecD src  )\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadMask src ));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 2B to 2H\\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 2H to 2S\\n\\t\"\n+            \"negr   $dst, $dst\\t# load mask (2B to 2S)\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadmask4I(vecX dst, vecD src  )\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadMask src ));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 4B to 4H\\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 4H to 4S\\n\\t\"\n+            \"negr   $dst, $dst\\t# load mask (4B to 4S)\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storemask2I(vecD dst, vecD src , immI_4 size)\n+%{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"xtn  $dst, $src\\t# 2S to 2H\\n\\t\"\n+            \"xtn  $dst, $dst\\t# 2H to 2B\\n\\t\"\n+            \"negr   $dst, $dst\\t# store mask (2S to 2B)\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($src$$reg), __ T4S);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg), __ T8H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storemask4I(vecD dst, vecX src , immI_4 size)\n+%{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"xtn  $dst, $src\\t# 4S to 4H\\n\\t\"\n+            \"xtn  $dst, $dst\\t# 4H to 4B\\n\\t\"\n+            \"negr   $dst, $dst\\t# store mask (4S to 4B)\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($src$$reg), __ T4S);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg), __ T8H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadmask2L(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 2B to 2S\\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 2S to 2I\\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 2I to 2L\\n\\t\"\n+            \"neg   $dst, $dst\\t# load mask (2B to 2L)\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($dst$$reg), __ T2S);\n+    __ negr(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storemask2L(vecD dst, vecX src, immI_8 size)\n+%{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"xtn  $dst, $src\\t# 2L to 2I\\n\\t\"\n+            \"xtn  $dst, $dst\\t# 2I to 2S\\n\\t\"\n+            \"xtn  $dst, $dst\\t# 2S to 2B\\n\\t\"\n+            \"neg  $dst, $dst\\t# store mask (2L to 2B)\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg), __ T2D);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($dst$$reg), __ T4S);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg), __ T8H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/-------------------------------- LOAD_IOTA_INDICES----------------------------------\n+\n+instruct loadcon8B(vecD dst, immI0 src)\n+%{\n+  predicate((n->as_Vector()->length() == 2 || n->as_Vector()->length() == 4 ||\n+             n->as_Vector()->length() == 8) &&\n+             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadConst src));\n+  ins_cost(INSN_COST);\n+  format %{ \"ldr $dst, CONSTANT_MEMORY\\t# load iota indices\" %}\n+  ins_encode %{\n+    __ lea(rscratch1, ExternalAddress(StubRoutines::aarch64::vector_iota_indices()));\n+    __ ldrd(as_FloatRegister($dst$$reg), rscratch1);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct loadcon16B(vecX dst, immI0 src)\n+%{\n+  predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadConst src));\n+  ins_cost(INSN_COST);\n+  format %{ \"ldr $dst, CONSTANT_MEMORY\\t# load iota indices\" %}\n+  ins_encode %{\n+    __ lea(rscratch1, ExternalAddress(StubRoutines::aarch64::vector_iota_indices()));\n+    __ ldrq(as_FloatRegister($dst$$reg), rscratch1);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/-------------------------------- LOAD_SHUFFLE ----------------------------------\n+\n+instruct loadshuffle8B(vecD dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"mov  $dst, $src\\t# get 8B shuffle\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct loadshuffle16B(vecX dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"mov  $dst, $src\\t# get 16B shuffle\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct loadshuffle4S(vecD dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 4B to 4H\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct loadshuffle8S(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 8B to 8H\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct loadshuffle4I(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 4B to 4H \\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 4H to 4S\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/-------------------------------- Rearrange -------------------------------------\n+\/\/ Here is an example that rearranges a NEON vector with 4 ints:\n+\/\/ Rearrange V1 int[a0, a1, a2, a3] to V2 int[a2, a3, a0, a1]\n+\/\/   1. Get the indices of V1 and store them as Vi byte[0, 1, 2, 3].\n+\/\/   2. Convert Vi byte[0, 1, 2, 3] to the indices of V2 and also store them as Vi byte[2, 3, 0, 1].\n+\/\/   3. Unsigned extend Long Vi from byte[2, 3, 0, 1] to int[2, 3, 0, 1].\n+\/\/   4. Multiply Vi int[2, 3, 0, 1] with constant int[0x04040404, 0x04040404, 0x04040404, 0x04040404]\n+\/\/      and get tbl base Vm int[0x08080808, 0x0c0c0c0c, 0x00000000, 0x04040404].\n+\/\/   5. Add Vm with constant int[0x03020100, 0x03020100, 0x03020100, 0x03020100]\n+\/\/      and get tbl index Vm int[0x0b0a0908, 0x0f0e0d0c, 0x03020100, 0x07060504]\n+\/\/   6. Use Vm as index register, and use V1 as table register.\n+\/\/      Then get V2 as the result by tbl NEON instructions.\n+\/\/ Notes:\n+\/\/   Step 1 matches VectorLoadConst.\n+\/\/   Step 3 matches VectorLoadShuffle.\n+\/\/   Step 4, 5, 6 match VectorRearrange.\n+\/\/   For VectorRearrange short\/int, the reason why such complex calculation is\n+\/\/   required is because NEON tbl supports bytes table only, so for short\/int, we\n+\/\/   need to lookup 2\/4 bytes as a group. For VectorRearrange long, we use bsl\n+\/\/   to implement rearrange.\n+\n+instruct rearrange8B(vecD dst, vecD src, vecD shuffle)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"tbl $dst, {$dst}, $shuffle\\t# rearrange 8B\" %}\n+  ins_encode %{\n+    __ tbl(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($shuffle$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rearrange16B(vecX dst, vecX src, vecX shuffle)\n+%{\n+  predicate(n->as_Vector()->length() == 16 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"tbl $dst, {$dst}, $shuffle\\t# rearrange 16B\" %}\n+  ins_encode %{\n+    __ tbl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($shuffle$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rearrange4S(vecD dst, vecD src, vecD shuffle, vecD tmp0, vecD tmp1)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp0, TEMP tmp1);\n+  format %{ \"mov   $tmp0, CONSTANT\\t# constant 0x0202020202020202\\n\\t\"\n+            \"mov   $tmp1, CONSTANT\\t# constant 0x0100010001000100\\n\\t\"\n+            \"mulv  $dst, T4H, $shuffle, $tmp0\\n\\t\"\n+            \"addv  $dst, T8B, $dst, $tmp1\\n\\t\"\n+            \"tbl   $dst, {$src}, $dst\\t# rearrange 4S\" %}\n+  ins_encode %{\n+    __ mov(as_FloatRegister($tmp0$$reg), __ T8B, 0x02);\n+    __ mov(as_FloatRegister($tmp1$$reg), __ T4H, 0x0100);\n+    __ mulv(as_FloatRegister($dst$$reg), __ T4H,\n+            as_FloatRegister($shuffle$$reg), as_FloatRegister($tmp0$$reg));\n+    __ addv(as_FloatRegister($dst$$reg), __ T8B,\n+            as_FloatRegister($dst$$reg), as_FloatRegister($tmp1$$reg));\n+    __ tbl(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rearrange8S(vecX dst, vecX src, vecX shuffle, vecX tmp0, vecX tmp1)\n+%{\n+  predicate(n->as_Vector()->length() == 8 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp0, TEMP tmp1);\n+  format %{ \"mov   $tmp0, CONSTANT\\t# constant 0x0202020202020202\\n\\t\"\n+            \"mov   $tmp1, CONSTANT\\t# constant 0x0100010001000100\\n\\t\"\n+            \"mulv  $dst, T8H, $shuffle, $tmp0\\n\\t\"\n+            \"addv  $dst, T16B, $dst, $tmp1\\n\\t\"\n+            \"tbl   $dst, {$src}, $dst\\t# rearrange 8S\" %}\n+  ins_encode %{\n+    __ mov(as_FloatRegister($tmp0$$reg), __ T16B, 0x02);\n+    __ mov(as_FloatRegister($tmp1$$reg), __ T8H, 0x0100);\n+    __ mulv(as_FloatRegister($dst$$reg), __ T8H,\n+            as_FloatRegister($shuffle$$reg), as_FloatRegister($tmp0$$reg));\n+    __ addv(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($dst$$reg), as_FloatRegister($tmp1$$reg));\n+    __ tbl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rearrange4I(vecX dst, vecX src, vecX shuffle, vecX tmp0, vecX tmp1)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp0, TEMP tmp1);\n+  format %{ \"mov   $tmp0, CONSTANT\\t# constant 0x0404040404040404\\n\\t\"\n+            \"mov   $tmp1, CONSTANT\\t# constant 0x0302010003020100\\n\\t\"\n+            \"mulv  $dst, T8H, $shuffle, $tmp0\\n\\t\"\n+            \"addv  $dst, T16B, $dst, $tmp1\\n\\t\"\n+            \"tbl   $dst, {$src}, $dst\\t# rearrange 4I\" %}\n+  ins_encode %{\n+    __ mov(as_FloatRegister($tmp0$$reg), __ T16B, 0x04);\n+    __ mov(as_FloatRegister($tmp1$$reg), __ T4S, 0x03020100);\n+    __ mulv(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($shuffle$$reg), as_FloatRegister($tmp0$$reg));\n+    __ addv(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($dst$$reg), as_FloatRegister($tmp1$$reg));\n+    __ tbl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/-------------------------------- Anytrue\/alltrue -----------------------------\n+\n+instruct anytrue_in_mask8B(iRegINoSp dst, vecD src1, vecD src2, vecD tmp, rFlagsReg cr)\n+%{\n+  predicate(static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2 ));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"addv  $tmp, T8B, $src1\\t# src1 and src2 are the same\\n\\t\"\n+            \"umov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmp   $dst, 0\\n\\t\"\n+            \"cset  $dst\" %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T8B, as_FloatRegister($src1$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::NE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct anytrue_in_mask16B(iRegINoSp dst, vecX src1, vecX src2, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2 ));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"addv  $tmp, T16B, $src1\\t# src1 and src2 are the same\\n\\t\"\n+            \"umov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmp   $dst, 0\\n\\t\"\n+            \"cset  $dst\" %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T16B, as_FloatRegister($src1$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::NE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct alltrue_in_mask8B(iRegINoSp dst, vecD src1, vecD src2, vecD tmp, rFlagsReg cr)\n+%{\n+  predicate(static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2 ));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"andr  $tmp, T8B, $src1, $src2\\t# src2 is maskAllTrue\\n\\t\"\n+            \"notr  $tmp, T8B, $tmp\\n\\t\"\n+            \"addv  $tmp, T8B, $tmp\\n\\t\"\n+            \"umov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmp   $dst, 0\\n\\t\"\n+            \"cset  $dst\" %}\n+  ins_encode %{\n+    __ andr(as_FloatRegister($tmp$$reg), __ T8B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($tmp$$reg), __ T8B, as_FloatRegister($tmp$$reg));\n+    __ addv(as_FloatRegister($tmp$$reg), __ T8B, as_FloatRegister($tmp$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct alltrue_in_mask16B(iRegINoSp dst, vecX src1, vecX src2, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2 ));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"andr  $tmp, T16B, $src1, $src2\\t# src2 is maskAllTrue\\n\\t\"\n+            \"notr  $tmp, T16B, $tmp\\n\\t\"\n+            \"addv  $tmp, T16B, $tmp\\n\\t\"\n+            \"umov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmp   $dst, 0\\n\\t\"\n+            \"cset  $dst\" %}\n+  ins_encode %{\n+    __ andr(as_FloatRegister($tmp$$reg), __ T16B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($tmp$$reg), __ T16B, as_FloatRegister($tmp$$reg));\n+    __ addv(as_FloatRegister($tmp$$reg), __ T16B, as_FloatRegister($tmp$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":3456,"deletions":0,"binary":false,"changes":3456,"status":"added"},{"patch":"@@ -0,0 +1,1424 @@\n+\/\/ Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, Arm Limited. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+dnl Generate the warning\n+\/\/ This file is automatically generated by running \"m4 aarch64_neon_ad.m4\". Do not edit ----\n+dnl\n+\n+\/\/ AArch64 NEON Architecture Description File\n+\n+dnl\n+define(`ORL2I', `ifelse($1,I,orL2I)')dnl\n+dnl\n+define(`error', `__program__:__file__:__line__: Invalid argument ``$1''m4exit(`1')')dnl\n+dnl\n+define(`iTYPE2SIMD',\n+`ifelse($1, `B', `B',\n+        $1, `S', `H',\n+        $1, `I', `S',\n+        $1, `L', `D',\n+        `error($1)')')dnl\n+dnl\n+define(`fTYPE2SIMD',\n+`ifelse($1, `F', `S',\n+        $1, `D', `D',\n+        `error($1)')')dnl\n+dnl\n+define(`TYPE2DATATYPE',\n+`ifelse($1, `B', `BYTE',\n+        $1, `S', `SHORT',\n+        $1, `I', `INT',\n+        $1, `L', `LONG',\n+        $1, `F', `FLOAT',\n+        $1, `D', `DOUBLE',\n+        `error($1)')')dnl\n+dnl\n+\/\/ ====================VECTOR INSTRUCTIONS==================================\n+\n+\/\/ ------------------------------ Load\/store\/reinterpret -----------------------\n+\n+\/\/ Load vector (16 bits)\n+instruct loadV2(vecD dst, memory mem)\n+%{\n+  predicate(n->as_LoadVector()->memory_size() == 2);\n+  match(Set dst (LoadVector mem));\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrh   $dst,$mem\\t# vector (16 bits)\" %}\n+  ins_encode( aarch64_enc_ldrvH(dst, mem) );\n+  ins_pipe(vload_reg_mem64);\n+%}\n+\n+\/\/ Store Vector (16 bits)\n+instruct storeV2(vecD src, memory mem)\n+%{\n+  predicate(n->as_StoreVector()->memory_size() == 2);\n+  match(Set mem (StoreVector mem src));\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"strh   $mem,$src\\t# vector (16 bits)\" %}\n+  ins_encode( aarch64_enc_strvH(src, mem) );\n+  ins_pipe(vstore_reg_mem64);\n+%}\n+dnl\n+define(`REINTERPRET', `\n+instruct reinterpret$1`'(vec$1 dst)\n+%{\n+  predicate(n->bottom_type()->is_vect()->length_in_bytes() == $2 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == $2);\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(0);\n+  format %{ \" # reinterpret $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}')dnl\n+dnl         $1 $2\n+REINTERPRET(D, 8)\n+REINTERPRET(X, 16)\n+dnl\n+define(`REINTERPRET_X', `\n+instruct reinterpret$1`'2$2`'(vec$2 dst, vec$1 src)\n+%{\n+  predicate(n->bottom_type()->is_vect()->length_in_bytes() == $3 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == $4);\n+  match(Set dst (VectorReinterpret src));\n+  ins_cost(INSN_COST);\n+  format %{ \" # reinterpret $dst,$src\" %}\n+  ins_encode %{\n+    \/\/ If register is the same, then move is not needed.\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    }\n+  %}\n+  ins_pipe(vlogical64);\n+%}')dnl\n+dnl           $1 $2 $3  $4\n+REINTERPRET_X(D, X, 16, 8)\n+REINTERPRET_X(X, D, 8,  16)\n+dnl\n+\n+\/\/ ------------------------------ Vector cast -------------------------------\n+dnl\n+define(`VECTOR_CAST_I2I', `\n+instruct vcvt$1$2to$1$3`'(vec$4 dst, vec$5 src)\n+%{\n+  predicate(n->as_Vector()->length() == $1 && n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst (VectorCast$2`'2X src));\n+  format %{ \"$6  $dst, T$8, $src, T$7\\t# convert $1$2 to $1$3 vector\" %}\n+  ins_encode %{\n+    __ $6(as_FloatRegister($dst$$reg), __ T$8, as_FloatRegister($src$$reg), __ T$7);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl             $1 $2 $3 $4 $5 $6    $7  $8\n+VECTOR_CAST_I2I(4, B, S, D, D, sxtl, 8B, 8H)\n+VECTOR_CAST_I2I(8, B, S, X, D, sxtl, 8B, 8H)\n+VECTOR_CAST_I2I(4, S, B, D, D, xtn,  8H, 8B)\n+VECTOR_CAST_I2I(8, S, B, D, X, xtn,  8H, 8B)\n+VECTOR_CAST_I2I(4, S, I, X, D, sxtl, 4H, 4S)\n+VECTOR_CAST_I2I(4, I, S, D, X, xtn,  4S, 4H)\n+VECTOR_CAST_I2I(2, I, L, X, D, sxtl, 2S, 2D)\n+VECTOR_CAST_I2I(2, L, I, D, X, xtn,  2D, 2S)\n+dnl\n+define(`VECTOR_CAST_B2I', `\n+instruct vcvt4$1to4$2`'(vec$3 dst, vec$4 src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n+  match(Set dst (VectorCast$1`'2X src));\n+  format %{ \"$5  $dst, T$7, $src, T$6\\n\\t\"\n+            \"$5  $dst, T$9, $dst, T$8\\t# convert 4$1 to 4$2 vector\"\n+  %}\n+  ins_encode %{\n+    __ $5(as_FloatRegister($dst$$reg), __ T$7, as_FloatRegister($src$$reg), __ T$6);\n+    __ $5(as_FloatRegister($dst$$reg), __ T$9, as_FloatRegister($dst$$reg), __ T$8);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl             $1 $2 $3 $4 $5    $6  $7  $8  $9\n+VECTOR_CAST_B2I(B, I, X, D, sxtl, 8B, 8H, 4H, 4S)\n+VECTOR_CAST_B2I(I, B, D, X, xtn,  4S, 4H, 8H, 8B)\n+\n+instruct vcvt4Bto4F(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"sxtl  $dst, T8H, $src, T8B\\n\\t\"\n+            \"sxtl  $dst, T4S, $dst, T4H\\n\\t\"\n+            \"scvtfv  T4S, $dst, $dst\\t# convert 4B to 4F vector\"\n+  %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+    __ scvtfv(__ T4S, as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+dnl\n+define(`VECTOR_CAST_I2F_L', `\n+instruct vcvt$1$2to$1$3`'(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == $1 && n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst (VectorCast$2`'2X src));\n+  format %{ \"sxtl    $dst, T$5, $src, T$4\\n\\t\"\n+            \"scvtfv  T$5, $dst, $dst\\t# convert $1$2 to $1$3 vector\"\n+  %}\n+  ins_encode %{\n+    __ sxtl(as_FloatRegister($dst$$reg), __ T$5, as_FloatRegister($src$$reg), __ T$4);\n+    __ scvtfv(__ T$5, as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl               $1 $2 $3 $4  $5\n+VECTOR_CAST_I2F_L(4, S, F, 4H, 4S)\n+VECTOR_CAST_I2F_L(2, I, D, 2S, 2D)\n+dnl\n+define(`VECTOR_CAST_I2F', `\n+instruct vcvt$1$2to$1$3`'(vec$4 dst, vec$4 src)\n+%{\n+  predicate(n->as_Vector()->length() == $1 && n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst (VectorCast$2`'2X src));\n+  format %{ \"scvtfv  T$5, $dst, $src\\t# convert $1$2 to $1$3 vector\" %}\n+  ins_encode %{\n+    __ scvtfv(__ T$5, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl             $1 $2 $3 $4 $5\n+VECTOR_CAST_I2F(2, I, F, D, 2S)\n+VECTOR_CAST_I2F(4, I, F, X, 4S)\n+VECTOR_CAST_I2F(2, L, D, X, 2D)\n+dnl\n+define(`VECTOR_CAST_F2F', `\n+instruct vcvt2$1to2$2`'(vec$3 dst, vec$4 src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n+  match(Set dst (VectorCast$1`'2X src));\n+  format %{ \"$5  $dst, T$7, $src, T$6\\t# convert 2$1 to 2$2 vector\" %}\n+  ins_encode %{\n+    __ $5(as_FloatRegister($dst$$reg), __ T$7, as_FloatRegister($src$$reg), __ T$6);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl             $1 $2 $3 $4 $5     $6  $7\n+VECTOR_CAST_F2F(F, D, X, D, fcvtl, 2S, 2D)\n+VECTOR_CAST_F2F(D, F, D, X, fcvtn, 2D, 2S)\n+dnl\n+\n+instruct vcvt2Lto2F(vecD dst, vecX src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastL2X src));\n+  format %{ \"scvtfv  T2D, $dst, $src\\n\\t\"\n+            \"fcvtn   $dst, T2S, $dst, T2D\\t# convert 2L to 2F vector\"\n+  %}\n+  ins_encode %{\n+    __ scvtfv(__ T2D, as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg));\n+    __ fcvtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($dst$$reg), __ T2D);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Reduction -------------------------------\n+dnl\n+define(`REDUCE_ADD_BORS', `\n+instruct reduce_add$1$2`'(iRegINoSp dst, iRegIorL2I isrc, vec$3 vsrc, vec$3 tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addv  $tmp, T$1`'iTYPE2SIMD($2), $vsrc\\n\\t\"\n+            \"smov  $dst, $tmp, iTYPE2SIMD($2), 0\\n\\t\"\n+            \"addw  $dst, $dst, $isrc\\n\\t\"\n+            \"sxt$4  $dst, $dst\\t# add reduction$1$2\"\n+  %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T$1`'iTYPE2SIMD($2), as_FloatRegister($vsrc$$reg));\n+    __ smov($dst$$Register, as_FloatRegister($tmp$$reg), __ iTYPE2SIMD($2), 0);\n+    __ addw($dst$$Register, $dst$$Register, $isrc$$Register);\n+    __ sxt$4($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl             $1  $2 $3 $4\n+REDUCE_ADD_BORS(8,  B, D, b)\n+REDUCE_ADD_BORS(16, B, X, b)\n+REDUCE_ADD_BORS(4,  S, D, h)\n+REDUCE_ADD_BORS(8,  S, X, h)\n+dnl\n+\n+instruct reduce_add2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, vecX tmp)\n+%{\n+  match(Set dst (AddReductionVL isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"addpd $tmp, $vsrc\\n\\t\"\n+            \"umov  $dst, $tmp, D, 0\\n\\t\"\n+            \"add   $dst, $isrc, $dst\\t# add reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ addpd(as_FloatRegister($tmp$$reg), as_FloatRegister($vsrc$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ D, 0);\n+    __ add($dst$$Register, $isrc$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD vtmp1, vecD vtmp2, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP itmp);\n+  format %{ \"ins   $vtmp1, S, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T8B, $vtmp1, $vsrc\\n\\t\"\n+            \"ins   $vtmp2, H, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp2, T8B, $vtmp2, $vtmp1\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxtb  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxtb  $dst, $dst\\t# mul reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp1$$reg), __ S,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T8B,\n+            as_FloatRegister($vtmp1$$reg), as_FloatRegister($vsrc$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ H,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp2$$reg), __ T8B,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX vtmp1, vecX vtmp2, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP itmp);\n+  format %{ \"ins   $vtmp1, D, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T8B, $vtmp1, $vsrc\\n\\t\"\n+            \"ins   $vtmp2, S, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T8B, $vtmp2, $vtmp1\\n\\t\"\n+            \"ins   $vtmp2, H, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp2, T8B, $vtmp2, $vtmp1\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxtb  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp2, B, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxtb  $dst, $dst\\t# mul reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp1$$reg), __ D,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T8B,\n+            as_FloatRegister($vtmp1$$reg), as_FloatRegister($vsrc$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ S,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T8B,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ H,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp2$$reg), __ T8B,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ B, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecD vtmp, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP itmp);\n+  format %{ \"ins   $vtmp, S, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp, T4H, $vtmp, $vsrc\\n\\t\"\n+            \"umov  $itmp, $vtmp, H, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxth  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp, H, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxth  $dst, $dst\\t# mul reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp$$reg), __ S,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp$$reg), __ T4H,\n+            as_FloatRegister($vtmp$$reg), as_FloatRegister($vsrc$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp$$reg), __ H, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp$$reg), __ H, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, vecX vtmp1, vecX vtmp2, iRegINoSp itmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP vtmp1, TEMP vtmp2, TEMP itmp);\n+  format %{ \"ins   $vtmp1, D, $vsrc, 0, 1\\n\\t\"\n+            \"mulv  $vtmp1, T4H, $vtmp1, $vsrc\\n\\t\"\n+            \"ins   $vtmp2, S, $vtmp1, 0, 1\\n\\t\"\n+            \"mulv  $vtmp2, T4H, $vtmp2, $vtmp1\\n\\t\"\n+            \"umov  $itmp, $vtmp2, H, 0\\n\\t\"\n+            \"mulw  $dst, $itmp, $isrc\\n\\t\"\n+            \"sxth  $dst, $dst\\n\\t\"\n+            \"umov  $itmp, $vtmp2, H, 1\\n\\t\"\n+            \"mulw  $dst, $itmp, $dst\\n\\t\"\n+            \"sxth  $dst, $dst\\t# mul reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($vtmp1$$reg), __ D,\n+           as_FloatRegister($vsrc$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp1$$reg), __ T4H,\n+            as_FloatRegister($vtmp1$$reg), as_FloatRegister($vsrc$$reg));\n+    __ ins(as_FloatRegister($vtmp2$$reg), __ S,\n+           as_FloatRegister($vtmp1$$reg), 0, 1);\n+    __ mulv(as_FloatRegister($vtmp2$$reg), __ T4H,\n+            as_FloatRegister($vtmp2$$reg), as_FloatRegister($vtmp1$$reg));\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ H, 0);\n+    __ mulw($dst$$Register, $itmp$$Register, $isrc$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+    __ umov($itmp$$Register, as_FloatRegister($vtmp2$$reg), __ H, 1);\n+    __ mulw($dst$$Register, $itmp$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mul2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp)\n+%{\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"mul   $dst, $isrc, $tmp\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"mul   $dst, $dst, $tmp\\t# mul reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ mul($dst$$Register, $isrc$$Register, $tmp$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ mul($dst$$Register, $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+dnl\n+define(`REDUCE_MAX_MIN_INT', `\n+instruct reduce_$1$2$3`'(iRegINoSp dst, iRegIorL2I isrc, vec$4 vsrc, vec$4 tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst ($5ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"s$1v $tmp, T$2`'iTYPE2SIMD($3), $vsrc\\n\\t\"\n+            \"$6mov  $dst, $tmp, iTYPE2SIMD($3), 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc $7\\t# $1 reduction$2$3\"\n+  %}\n+  ins_encode %{\n+    __ s$1v(as_FloatRegister($tmp$$reg), __ T$2`'iTYPE2SIMD($3), as_FloatRegister($vsrc$$reg));\n+    __ $6mov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ iTYPE2SIMD($3), 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::$7);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2  $3 $4 $5   $6 $7\n+REDUCE_MAX_MIN_INT(max, 8,  B, D, Max, s, GT)\n+REDUCE_MAX_MIN_INT(max, 16, B, X, Max, s, GT)\n+REDUCE_MAX_MIN_INT(max, 4,  S, D, Max, s, GT)\n+REDUCE_MAX_MIN_INT(max, 8,  S, X, Max, s, GT)\n+REDUCE_MAX_MIN_INT(max, 4,  I, X, Max, u, GT)\n+REDUCE_MAX_MIN_INT(min, 8,  B, D, Min, s, LT)\n+REDUCE_MAX_MIN_INT(min, 16, B, X, Min, s, LT)\n+REDUCE_MAX_MIN_INT(min, 4,  S, D, Min, s, LT)\n+REDUCE_MAX_MIN_INT(min, 8,  S, X, Min, s, LT)\n+REDUCE_MAX_MIN_INT(min, 4,  I, X, Min, u, LT)\n+dnl\n+define(`REDUCE_MAX_MIN_2I', `\n+instruct reduce_$1`'2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, vecX tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"dup   $tmp, T2D, $vsrc\\n\\t\"\n+            \"s$1v $tmp, T4S, $tmp\\n\\t\"\n+            \"umov  $dst, $tmp, S, 0\\n\\t\"\n+            \"cmpw  $dst, $isrc\\n\\t\"\n+            \"cselw $dst, $dst, $isrc $3\\t# $1 reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ dup(as_FloatRegister($tmp$$reg), __ T2D, as_FloatRegister($vsrc$$reg));\n+    __ s$1v(as_FloatRegister($tmp$$reg), __ T4S, as_FloatRegister($tmp$$reg));\n+    __ umov(as_Register($dst$$reg), as_FloatRegister($tmp$$reg), __ S, 0);\n+    __ cmpw(as_Register($dst$$reg), as_Register($isrc$$reg));\n+    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($isrc$$reg), Assembler::$3);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl               $1   $2   $3\n+REDUCE_MAX_MIN_2I(max, Max, GT)\n+REDUCE_MAX_MIN_2I(min, Min, LT)\n+dnl\n+define(`REDUCE_MAX_MIN_2L', `\n+instruct reduce_$1`'2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp, rFlagsReg cr)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"cmp   $isrc,$tmp\\n\\t\"\n+            \"csel  $dst, $isrc, $tmp $3\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"cmp   $dst, $tmp\\n\\t\"\n+            \"csel  $dst, $dst, $tmp $3\\t# $1 reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov(as_Register($tmp$$reg), as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ cmp(as_Register($isrc$$reg), as_Register($tmp$$reg));\n+    __ csel(as_Register($dst$$reg), as_Register($isrc$$reg), as_Register($tmp$$reg), Assembler::$3);\n+    __ umov(as_Register($tmp$$reg), as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ cmp(as_Register($dst$$reg), as_Register($tmp$$reg));\n+    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($tmp$$reg), Assembler::$3);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl               $1   $2   $3\n+REDUCE_MAX_MIN_2L(max, Max, GT)\n+REDUCE_MAX_MIN_2L(min, Min, LT)\n+dnl\n+define(`REDUCE_LOGIC_OP_8B', `\n+instruct reduce_$1`'8B(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"$1w   $dst, $dst, $tmp\\n\\t\"\n+            \"$1w   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"$1w   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"$1w   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# $1 reduction8B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ $1w($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ $1w($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ $1w($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ $1w($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2\n+REDUCE_LOGIC_OP_8B(and, And)\n+REDUCE_LOGIC_OP_8B(orr, Or)\n+REDUCE_LOGIC_OP_8B(eor, Xor)\n+define(`REDUCE_LOGIC_OP_16B', `\n+instruct reduce_$1`'16B(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"$3   $dst, $dst, $tmp\\n\\t\"\n+            \"$3   $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"$1w   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"$1w   $dst, $dst, $dst, LSR #8\\n\\t\"\n+            \"$1w   $dst, $isrc, $dst\\n\\t\"\n+            \"sxtb   $dst, $dst\\t# $1 reduction16B\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ $3($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ $3($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ $1w($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ $1w($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 8);\n+    __ $1w($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxtb($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                 $1   $2   $3\n+REDUCE_LOGIC_OP_16B(and, And, andr)\n+REDUCE_LOGIC_OP_16B(orr, Or,  orr )\n+REDUCE_LOGIC_OP_16B(eor, Xor, eor )\n+dnl\n+define(`REDUCE_LOGIC_OP_4S', `\n+instruct reduce_$1`'4S(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, S, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, S, 1\\n\\t\"\n+            \"$1w   $dst, $dst, $tmp\\n\\t\"\n+            \"$1w   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"$1w   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# $1 reduction4S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ $1w($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ $1w($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ $1w($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2\n+REDUCE_LOGIC_OP_4S(and, And)\n+REDUCE_LOGIC_OP_4S(orr, Or)\n+REDUCE_LOGIC_OP_4S(eor, Xor)\n+dnl\n+define(`REDUCE_LOGIC_OP_8S', `\n+instruct reduce_$1`'8S(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"$3   $dst, $dst, $tmp\\n\\t\"\n+            \"$3   $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"$1w   $dst, $dst, $dst, LSR #16\\n\\t\"\n+            \"$1w   $dst, $isrc, $dst\\n\\t\"\n+            \"sxth   $dst, $dst\\t# $1 reduction8S\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ $3($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ $3($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ $1w($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 16);\n+    __ $1w($dst$$Register, $isrc$$Register, $dst$$Register);\n+    __ sxth($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2   $3\n+REDUCE_LOGIC_OP_8S(and, And, andr)\n+REDUCE_LOGIC_OP_8S(orr, Or,  orr )\n+REDUCE_LOGIC_OP_8S(eor, Xor, eor )\n+dnl\n+define(`REDUCE_LOGIC_OP_2I', `\n+instruct reduce_$1`'2I(iRegINoSp dst, iRegIorL2I isrc, vecD vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, S, 0\\n\\t\"\n+            \"$1w  $dst, $tmp, $isrc\\n\\t\"\n+            \"umov  $tmp, $vsrc, S, 1\\n\\t\"\n+            \"$1w  $dst, $tmp, $dst\\t# $1 reduction2I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 0);\n+    __ $1w($dst$$Register, $tmp$$Register, $isrc$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ S, 1);\n+    __ $1w($dst$$Register, $tmp$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2\n+REDUCE_LOGIC_OP_2I(and, And)\n+REDUCE_LOGIC_OP_2I(orr, Or)\n+REDUCE_LOGIC_OP_2I(eor, Xor)\n+dnl\n+define(`REDUCE_LOGIC_OP_4I', `\n+instruct reduce_$1`'4I(iRegINoSp dst, iRegIorL2I isrc, vecX vsrc, iRegINoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov   $tmp, $vsrc, D, 0\\n\\t\"\n+            \"umov   $dst, $vsrc, D, 1\\n\\t\"\n+            \"$3   $dst, $dst, $tmp\\n\\t\"\n+            \"$3   $dst, $dst, $dst, LSR #32\\n\\t\"\n+            \"$1w   $dst, $isrc, $dst\\t# $1 reduction4I\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ umov($dst$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ $3($dst$$Register, $dst$$Register, $tmp$$Register);\n+    __ $3($dst$$Register, $dst$$Register, $dst$$Register, Assembler::LSR, 32);\n+    __ $1w($dst$$Register, $isrc$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2   $3\n+REDUCE_LOGIC_OP_4I(and, And, andr)\n+REDUCE_LOGIC_OP_4I(orr, Or,  orr )\n+REDUCE_LOGIC_OP_4I(eor, Xor, eor )\n+dnl\n+define(`REDUCE_LOGIC_OP_2L', `\n+instruct reduce_$1`'2L(iRegLNoSp dst, iRegL isrc, vecX vsrc, iRegLNoSp tmp)\n+%{\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2ReductionV isrc vsrc));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"umov  $tmp, $vsrc, D, 0\\n\\t\"\n+            \"$3  $dst, $isrc, $tmp\\n\\t\"\n+            \"umov  $tmp, $vsrc, D, 1\\n\\t\"\n+            \"$3  $dst, $dst, $tmp\\t# $1 reduction2L\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 0);\n+    __ $3($dst$$Register, $isrc$$Register, $tmp$$Register);\n+    __ umov($tmp$$Register, as_FloatRegister($vsrc$$reg), __ D, 1);\n+    __ $3($dst$$Register, $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1   $2   $3\n+REDUCE_LOGIC_OP_2L(and, And, andr)\n+REDUCE_LOGIC_OP_2L(orr, Or,  orr )\n+REDUCE_LOGIC_OP_2L(eor, Xor, eor )\n+dnl\n+\n+\/\/ ------------------------------ Vector insert ---------------------------------\n+define(`VECTOR_INSERT_I', `\n+instruct insert$1$2`'(vec$3 dst, vec$3 src, iReg$4`'ORL2I($4) val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"orr    $dst, T$5, $src, $src\\n\\t\"\n+            \"mov    $dst, T$1`'iTYPE2SIMD($2), $idx, $val\\t# insert into vector($1$2)\" %}\n+  ins_encode %{\n+    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+      __ orr(as_FloatRegister($dst$$reg), __ T$5,\n+             as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    }\n+    __ mov(as_FloatRegister($dst$$reg), __ T$1`'iTYPE2SIMD($2), $idx$$constant, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl             $1  $2 $3 $4 $5\n+VECTOR_INSERT_I(8,  B, D, I, 8B)\n+VECTOR_INSERT_I(16, B, X, I, 16B)\n+VECTOR_INSERT_I(4,  S, D, I, 8B)\n+VECTOR_INSERT_I(8,  S, X, I, 16B)\n+VECTOR_INSERT_I(2,  I, D, I, 8B)\n+VECTOR_INSERT_I(4,  I, X, I, 16B)\n+VECTOR_INSERT_I(2,  L, X, L, 16B)\n+dnl\n+define(`VECTOR_INSERT_F', `\n+instruct insert$1`'(vec$2 dst, vec$2 src, vReg$3 val, immI idx)\n+%{\n+  predicate(n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"orr    $dst, T$4, $src, $src\\n\\t\"\n+            \"ins    $dst, $5, $val, $idx, 0\\t# insert into vector($1)\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T$4,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+    __ ins(as_FloatRegister($dst$$reg), __ $5,\n+           as_FloatRegister($val$$reg), $idx$$constant, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl             $1  $2 $3 $4   $5\n+VECTOR_INSERT_F(2F, D, F, 8B,  S)\n+VECTOR_INSERT_F(4F, X, F, 16B, S)\n+VECTOR_INSERT_F(2D, X, D, 16B, D)\n+dnl\n+\n+\/\/ ------------------------------ Vector extract ---------------------------------\n+define(`VECTOR_EXTRACT_I', `\n+instruct extract$1$2`'(iReg$3NoSp dst, vec$4 src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == $1);\n+  match(Set dst (Extract$2 src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"$5mov    $dst, $src, $6, $idx\\t# extract from vector($1$2)\" %}\n+  ins_encode %{\n+    __ $5mov($dst$$Register, as_FloatRegister($src$$reg), __ $6, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl             $1   $2 $3 $4 $5 $6\n+VECTOR_EXTRACT_I(8,  B, I, D, s, B)\n+VECTOR_EXTRACT_I(16, B, I, X, s, B)\n+VECTOR_EXTRACT_I(4,  S, I, D, s, H)\n+VECTOR_EXTRACT_I(8,  S, I, X, s, H)\n+VECTOR_EXTRACT_I(2,  I, I, D, u, S)\n+VECTOR_EXTRACT_I(4,  I, I, X, u, S)\n+VECTOR_EXTRACT_I(2,  L, L, X, u, D)\n+dnl\n+define(`VECTOR_EXTRACT_F', `\n+instruct extract$1$2`'(vReg$2 dst, vec$3 src, immI idx)\n+%{\n+  predicate(n->in(1)->bottom_type()->is_vect()->length() == $1);\n+  match(Set dst (Extract$2 src idx));\n+  ins_cost(INSN_COST);\n+  format %{ \"ins   $dst, $4, $src, 0, $idx\\t# extract from vector($1$2)\" %}\n+  ins_encode %{\n+    __ ins(as_FloatRegister($dst$$reg), __ $4,\n+           as_FloatRegister($src$$reg), 0, $idx$$constant);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl             $1  $2 $3 $4\n+VECTOR_EXTRACT_F(2, F, D, S)\n+VECTOR_EXTRACT_F(4, F, X, S)\n+VECTOR_EXTRACT_F(2, D, X, D)\n+dnl\n+\n+\/\/ ------------------------------ Vector comparison ---------------------------------\n+define(`VECTOR_CMP_EQ_GT_GE', `\n+instruct vcm$1$2$3`'(vec$4 dst, vec$4 src1, vec$4 src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == $2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::$1 &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"$6cm$1  $dst, $src1, $src2\\t# vector cmp ($2$3)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ $6cm$1(as_FloatRegister($dst$$reg), __ T$2$5,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop$7);\n+%}')dnl\n+dnl                $1   $2 $3 $4 $5 $6 $7\n+VECTOR_CMP_EQ_GT_GE(eq, 8, B, D, B,  , 64)\n+VECTOR_CMP_EQ_GT_GE(eq, 16,B, X, B,  , 128)\n+VECTOR_CMP_EQ_GT_GE(eq, 4, S, D, H,  , 64)\n+VECTOR_CMP_EQ_GT_GE(eq, 8, S, X, H,  , 128)\n+VECTOR_CMP_EQ_GT_GE(eq, 2, I, D, S,  , 64)\n+VECTOR_CMP_EQ_GT_GE(eq, 4, I, X, S,  , 128)\n+VECTOR_CMP_EQ_GT_GE(eq, 2, L, X, D,  , 128)\n+VECTOR_CMP_EQ_GT_GE(eq, 2, F, D, S, f, 64)\n+VECTOR_CMP_EQ_GT_GE(eq, 4, F, X, S, f, 128)\n+VECTOR_CMP_EQ_GT_GE(eq, 2, D, X, D, f, 128)\n+VECTOR_CMP_EQ_GT_GE(gt, 8, B, D, B,  , 64)\n+VECTOR_CMP_EQ_GT_GE(gt, 16,B, X, B,  , 128)\n+VECTOR_CMP_EQ_GT_GE(gt, 4, S, D, H,  , 64)\n+VECTOR_CMP_EQ_GT_GE(gt, 8, S, X, H,  , 128)\n+VECTOR_CMP_EQ_GT_GE(gt, 2, I, D, S,  , 64)\n+VECTOR_CMP_EQ_GT_GE(gt, 4, I, X, S,  , 128)\n+VECTOR_CMP_EQ_GT_GE(gt, 2, L, X, D,  , 128)\n+VECTOR_CMP_EQ_GT_GE(gt, 2, F, D, S, f, 64)\n+VECTOR_CMP_EQ_GT_GE(gt, 4, F, X, S, f, 128)\n+VECTOR_CMP_EQ_GT_GE(gt, 2, D, X, D, f, 128)\n+VECTOR_CMP_EQ_GT_GE(ge, 8, B, D, B,  , 64)\n+VECTOR_CMP_EQ_GT_GE(ge, 16,B, X, B,  , 128)\n+VECTOR_CMP_EQ_GT_GE(ge, 4, S, D, H,  , 64)\n+VECTOR_CMP_EQ_GT_GE(ge, 8, S, X, H,  , 128)\n+VECTOR_CMP_EQ_GT_GE(ge, 2, I, D, S,  , 64)\n+VECTOR_CMP_EQ_GT_GE(ge, 4, I, X, S,  , 128)\n+VECTOR_CMP_EQ_GT_GE(ge, 2, L, X, D,  , 128)\n+VECTOR_CMP_EQ_GT_GE(ge, 2, F, D, S, f, 64)\n+VECTOR_CMP_EQ_GT_GE(ge, 4, F, X, S, f, 128)\n+VECTOR_CMP_EQ_GT_GE(ge, 2, D, X, D, f, 128)\n+dnl\n+define(`VECTOR_CMP_NE', `\n+instruct vcmne$1$2`'(vec$3 dst, vec$3 src1, vec$3 src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == $1 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::ne &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"$5cmeq  $dst, $src1, $src2\\n\\t# vector cmp ($1$2)\"\n+            \"not   $dst, $dst\\t\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ $5cmeq(as_FloatRegister($dst$$reg), __ T$1$4,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($dst$$reg), __ T$6, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl           $1 $2 $3 $4 $5 $6\n+VECTOR_CMP_NE(8, B, D, B,  , 8B)\n+VECTOR_CMP_NE(16,B, X, B,  , 16B)\n+VECTOR_CMP_NE(4, S, D, H,  , 8B)\n+VECTOR_CMP_NE(8, S, X, H,  , 16B)\n+VECTOR_CMP_NE(2, I, D, S,  , 8B)\n+VECTOR_CMP_NE(4, I, X, S,  , 16B)\n+VECTOR_CMP_NE(2, L, X, D,  , 16B)\n+VECTOR_CMP_NE(2, F, D, S, f, 8B)\n+VECTOR_CMP_NE(4, F, X, S, f, 16B)\n+VECTOR_CMP_NE(2, D, X, D, f, 16B)\n+dnl\n+define(`VECTOR_CMP_LT_LE', `\n+instruct vcm$1$2$3`'(vec$4 dst, vec$4 src1, vec$4 src2, immI cond)\n+%{\n+  predicate(n->as_Vector()->length() == $2 &&\n+            n->as_VectorMaskCmp()->get_predicate() == BoolTest::$1 &&\n+            n->in(1)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($3));\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"$6cm$7  $dst, $src2, $src1\\t# vector cmp ($2$3)\" %}\n+  ins_cost(INSN_COST);\n+  ins_encode %{\n+    __ $6cm$7(as_FloatRegister($dst$$reg), __ T$2$5,\n+            as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vdop$8);\n+%}')dnl\n+dnl              $1  $2 $3 $4 $5 $6 $7  $8\n+VECTOR_CMP_LT_LE(lt, 8, B, D, B,  , gt, 64)\n+VECTOR_CMP_LT_LE(lt, 16,B, X, B,  , gt, 128)\n+VECTOR_CMP_LT_LE(lt, 4, S, D, H,  , gt, 64)\n+VECTOR_CMP_LT_LE(lt, 8, S, X, H,  , gt, 128)\n+VECTOR_CMP_LT_LE(lt, 2, I, D, S,  , gt, 64)\n+VECTOR_CMP_LT_LE(lt, 4, I, X, S,  , gt, 128)\n+VECTOR_CMP_LT_LE(lt, 2, L, X, D,  , gt, 128)\n+VECTOR_CMP_LT_LE(lt, 2, F, D, S, f, gt, 64)\n+VECTOR_CMP_LT_LE(lt, 4, F, X, S, f, gt, 128)\n+VECTOR_CMP_LT_LE(lt, 2, D, X, D, f, gt, 128)\n+VECTOR_CMP_LT_LE(le, 8, B, D, B,  , ge, 64)\n+VECTOR_CMP_LT_LE(le, 16,B, X, B,  , ge, 128)\n+VECTOR_CMP_LT_LE(le, 4, S, D, H,  , ge, 64)\n+VECTOR_CMP_LT_LE(le, 8, S, X, H,  , ge, 128)\n+VECTOR_CMP_LT_LE(le, 2, I, D, S,  , ge, 64)\n+VECTOR_CMP_LT_LE(le, 4, I, X, S,  , ge, 128)\n+VECTOR_CMP_LT_LE(le, 2, L, X, D,  , ge, 128)\n+VECTOR_CMP_LT_LE(le, 2, F, D, S, f, ge, 64)\n+VECTOR_CMP_LT_LE(le, 4, F, X, S, f, ge, 128)\n+VECTOR_CMP_LT_LE(le, 2, D, X, D, f, ge, 128)\n+dnl\n+\n+\/\/ ------------------------------ Vector mul -----------------------------------\n+\n+instruct vmul2L(vecX dst, vecX src1, vecX src2, iRegLNoSp tmp1, iRegLNoSp tmp2)\n+%{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (MulVL src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp1, TEMP tmp2);\n+  format %{ \"umov   $tmp1, $src1, D, 0\\n\\t\"\n+            \"umov   $tmp2, $src2, D, 0\\n\\t\"\n+            \"mul    $tmp2, $tmp2, $tmp1\\n\\t\"\n+            \"mov    $dst,  T2D,   0, $tmp2\\t# insert into vector(2L)\\n\\t\"\n+            \"umov   $tmp1, $src1, D, 1\\n\\t\"\n+            \"umov   $tmp2, $src2, D, 1\\n\\t\"\n+            \"mul    $tmp2, $tmp2, $tmp1\\n\\t\"\n+            \"mov    $dst,  T2D,   1, $tmp2\\t# insert into vector(2L)\\n\\t\"\n+  %}\n+  ins_encode %{\n+    __ umov($tmp1$$Register, as_FloatRegister($src1$$reg), __ D, 0);\n+    __ umov($tmp2$$Register, as_FloatRegister($src2$$reg), __ D, 0);\n+    __ mul(as_Register($tmp2$$reg), as_Register($tmp2$$reg), as_Register($tmp1$$reg));\n+    __ mov(as_FloatRegister($dst$$reg), __ T2D, 0, $tmp2$$Register);\n+    __ umov($tmp1$$Register, as_FloatRegister($src1$$reg), __ D, 1);\n+    __ umov($tmp2$$Register, as_FloatRegister($src2$$reg), __ D, 1);\n+    __ mul(as_Register($tmp2$$reg), as_Register($tmp2$$reg), as_Register($tmp1$$reg));\n+    __ mov(as_FloatRegister($dst$$reg), __ T2D, 1, $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ --------------------------------- Vector not --------------------------------\n+dnl\n+define(`MATCH_RULE', `ifelse($1, I,\n+`match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));',\n+`match(Set dst (XorV src (ReplicateL m1)));')')dnl\n+dnl\n+define(`VECTOR_NOT', `\n+instruct vnot$1$2`'(vec$3 dst, vec$3 src, imm$2_M1 m1)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == $4);\n+  MATCH_RULE($2)\n+  ins_cost(INSN_COST);\n+  format %{ \"not  $dst, $src\\t# vector ($5)\" %}\n+  ins_encode %{\n+    __ notr(as_FloatRegister($dst$$reg), __ T$5,\n+            as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl        $1 $2 $3 $4  $5\n+VECTOR_NOT(2, I, D, 8,  8B)\n+VECTOR_NOT(4, I, X, 16, 16B)\n+VECTOR_NOT(2, L, X, 16, 16B)\n+undefine(MATCH_RULE)\n+dnl\n+\/\/ ------------------------------ Vector max\/min -------------------------------\n+dnl\n+define(`PREDICATE', `ifelse($1, 8B,\n+`predicate((n->as_Vector()->length() == 4 || n->as_Vector()->length() == 8) &&\n+             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);',\n+`predicate(n->as_Vector()->length() == $2 && n->bottom_type()->is_vect()->element_basic_type() == T_$3);')')dnl\n+dnl\n+define(`VECTOR_MAX_MIN_INT', `\n+instruct v$1$2$3`'(vec$4 dst, vec$4 src1, vec$4 src2)\n+%{\n+  PREDICATE(`$2$3', $2, TYPE2DATATYPE($3))\n+  match(Set dst ($5V src1 src2));\n+  ins_cost(INSN_COST);\n+  format %{ \"$1v  $dst, $src1, $src2\\t# vector ($2$3)\" %}\n+  ins_encode %{\n+    __ $1v(as_FloatRegister($dst$$reg), __ T$2`'iTYPE2SIMD($3),\n+            as_FloatRegister($src1$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(vdop$6);\n+%}')dnl\n+dnl                $1   $2  $3 $4 $5   $6\n+VECTOR_MAX_MIN_INT(max, 8,  B, D, Max, 64)\n+VECTOR_MAX_MIN_INT(max, 16, B, X, Max, 128)\n+VECTOR_MAX_MIN_INT(max, 4,  S, D, Max, 64)\n+VECTOR_MAX_MIN_INT(max, 8,  S, X, Max, 128)\n+VECTOR_MAX_MIN_INT(max, 2,  I, D, Max, 64)\n+VECTOR_MAX_MIN_INT(max, 4,  I, X, Max, 128)\n+VECTOR_MAX_MIN_INT(min, 8,  B, D, Min, 64)\n+VECTOR_MAX_MIN_INT(min, 16, B, X, Min, 128)\n+VECTOR_MAX_MIN_INT(min, 4,  S, D, Min, 64)\n+VECTOR_MAX_MIN_INT(min, 8,  S, X, Min, 128)\n+VECTOR_MAX_MIN_INT(min, 2,  I, D, Min, 64)\n+VECTOR_MAX_MIN_INT(min, 4,  I, X, Min, 128)\n+undefine(PREDICATE)\n+dnl\n+define(`VECTOR_MAX_MIN_LONG', `\n+instruct v$1`'2L`'(vecX dst, vecX src1, vecX src2)\n+%{\n+  predicate(n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2V src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP dst);\n+  format %{ \"cmgt  $dst, $src1, $src2\\t# vector (2L)\\n\\t\"\n+            \"bsl   $dst, $$3, $$4\\t# vector (16B)\" %}\n+  ins_encode %{\n+    __ cmgt(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ bsl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($$3$$reg), as_FloatRegister($$4$$reg));\n+  %}\n+  ins_pipe(vdop128);\n+%}')dnl\n+dnl                $1   $2   $3    $4\n+VECTOR_MAX_MIN_LONG(max, Max, src1, src2)\n+VECTOR_MAX_MIN_LONG(min, Min, src2, src1)\n+dnl\n+\n+\/\/ --------------------------------- blend (bsl) ----------------------------\n+dnl\n+define(`VECTOR_BSL', `\n+instruct vbsl$1B`'(vec$2 dst, vec$2 src1, vec$2 src2)\n+%{\n+  predicate(n->as_Vector()->length_in_bytes() == $1);\n+  match(Set dst (VectorBlend (Binary src1 src2) dst));\n+  ins_cost(INSN_COST);\n+  format %{ \"bsl  $dst, $src2, $src1\\t# vector ($1B)\" %}\n+  ins_encode %{\n+    __ bsl(as_FloatRegister($dst$$reg), __ T$1B,\n+           as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(vlogical$3);\n+%}')dnl\n+dnl        $1  $2 $3\n+VECTOR_BSL(8,  D, 64)\n+VECTOR_BSL(16, X, 128)\n+dnl\n+\n+\/\/ --------------------------------- Load\/store Mask ----------------------------\n+dnl\n+define(`PREDICATE', `ifelse($1, load,\n+`predicate(n->as_Vector()->length() == $2 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);',\n+`predicate(n->as_Vector()->length() == $2);')')dnl\n+dnl\n+define(`VECTOR_LOAD_STORE_MASK_B', `\n+instruct $1mask$2B`'(vec$3 dst, vec$3 src $5 $6)\n+%{\n+  PREDICATE($1, $2)\n+  match(Set dst (Vector$4Mask src $6));\n+  ins_cost(INSN_COST);\n+  format %{ \"negr  $dst, $src\\t# $1 mask ($2B to $2B)\" %}\n+  ins_encode %{\n+    __ negr(as_FloatRegister($dst$$reg), __ T$2B, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl                      $1     $2  $3 $4     $5      $6\n+VECTOR_LOAD_STORE_MASK_B(load,  8,  D, Load)\n+VECTOR_LOAD_STORE_MASK_B(load,  16, X, Load)\n+VECTOR_LOAD_STORE_MASK_B(store, 8,  D, Store, `, immI_1', size)\n+VECTOR_LOAD_STORE_MASK_B(store, 16, X, Store, `, immI_1', size)\n+undefine(PREDICATE)dnl\n+dnl\n+define(`PREDICATE', `ifelse($1, load,\n+`predicate(n->as_Vector()->length() == $2 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);',\n+`predicate(n->as_Vector()->length() == $2);')')dnl\n+dnl\n+define(`VECTOR_LOAD_STORE_MASK_S', `\n+instruct $1mask$2S`'(vec$3 dst, vec$4 src $9 $10)\n+%{\n+  PREDICATE($1, $2)\n+  match(Set dst (Vector$5Mask src $10));\n+  ins_cost(INSN_COST);\n+  format %{ \"$6  $dst, $src\\n\\t\"\n+            \"negr  $dst, $dst\\t# $1 mask ($2$7 to $2$8)\" %}\n+  ins_encode %{\n+    __ $6(as_FloatRegister($dst$$reg), __ T8$8, as_FloatRegister($src$$reg), __ T8$7);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8$8, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                      $1     $2 $3 $4 $5     $6    $7 $8    $9       $10\n+VECTOR_LOAD_STORE_MASK_S(load,  4, D, D, Load,  uxtl, B, H)\n+VECTOR_LOAD_STORE_MASK_S(load,  8, X, D, Load,  uxtl, B, H)\n+VECTOR_LOAD_STORE_MASK_S(store, 4, D, D, Store, xtn,  H, B, `, immI_2', size)\n+VECTOR_LOAD_STORE_MASK_S(store, 8, D, X, Store, xtn,  H, B, `, immI_2', size)\n+undefine(PREDICATE)dnl\n+dnl\n+define(`PREDICATE', `ifelse($1, load,\n+`predicate(n->as_Vector()->length() == $2 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));',\n+`predicate(n->as_Vector()->length() == $2);')')dnl\n+dnl\n+define(`VECTOR_LOAD_STORE_MASK_I', `\n+instruct $1mask$2I`'(vec$3 dst, vec$4 src $12 $13)\n+%{\n+  PREDICATE($1, $2)\n+  match(Set dst (Vector$5Mask src $13));\n+  ins_cost(INSN_COST);\n+  format %{ \"$6  $dst, $src\\t# $2$7 to $2$8\\n\\t\"\n+            \"$6  $dst, $dst\\t# $2$8 to $2$9\\n\\t\"\n+            \"negr   $dst, $dst\\t# $1 mask ($2$7 to $2$9)\" %}\n+  ins_encode %{\n+    __ $6(as_FloatRegister($dst$$reg), __ T$10$8, as_FloatRegister($src$$reg), __ T$10$7);\n+    __ $6(as_FloatRegister($dst$$reg), __ T$11$9, as_FloatRegister($dst$$reg), __ T$11$8);\n+    __ negr(as_FloatRegister($dst$$reg), __ T$11$9, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                      $1     $2 $3 $4 $5     $6    $7 $8 $9 $10$11   $12      $13\n+VECTOR_LOAD_STORE_MASK_I(load,  2, D, D, Load,  uxtl, B, H, S, 8, 4)\n+VECTOR_LOAD_STORE_MASK_I(load,  4, X, D, Load,  uxtl, B, H, S, 8, 4)\n+VECTOR_LOAD_STORE_MASK_I(store, 2, D, D, Store, xtn,  S, H, B, 4, 8, `, immI_4', size)\n+VECTOR_LOAD_STORE_MASK_I(store, 4, D, X, Store, xtn,  S, H, B, 4, 8, `, immI_4', size)\n+undefine(PREDICATE)\n+dnl\n+instruct loadmask2L(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 2 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (VectorLoadMask src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 2B to 2S\\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 2S to 2I\\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 2I to 2L\\n\\t\"\n+            \"neg   $dst, $dst\\t# load mask (2B to 2L)\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($dst$$reg), __ T2S);\n+    __ negr(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storemask2L(vecD dst, vecX src, immI_8 size)\n+%{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(INSN_COST);\n+  format %{ \"xtn  $dst, $src\\t# 2L to 2I\\n\\t\"\n+            \"xtn  $dst, $dst\\t# 2I to 2S\\n\\t\"\n+            \"xtn  $dst, $dst\\t# 2S to 2B\\n\\t\"\n+            \"neg  $dst, $dst\\t# store mask (2L to 2B)\" %}\n+  ins_encode %{\n+    __ xtn(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg), __ T2D);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($dst$$reg), __ T4S);\n+    __ xtn(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg), __ T8H);\n+    __ negr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/-------------------------------- LOAD_IOTA_INDICES----------------------------------\n+dnl\n+define(`PREDICATE', `ifelse($1, 8,\n+`predicate((n->as_Vector()->length() == 2 || n->as_Vector()->length() == 4 ||\n+             n->as_Vector()->length() == 8) &&\n+             n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);',\n+`predicate(n->as_Vector()->length() == 16 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);')')dnl\n+dnl\n+define(`VECTOR_LOAD_CON', `\n+instruct loadcon$1B`'(vec$2 dst, immI0 src)\n+%{\n+  PREDICATE($1)\n+  match(Set dst (VectorLoadConst src));\n+  ins_cost(INSN_COST);\n+  format %{ \"ldr $dst, CONSTANT_MEMORY\\t# load iota indices\" %}\n+  ins_encode %{\n+    __ lea(rscratch1, ExternalAddress(StubRoutines::aarch64::vector_iota_indices()));\n+    __ ldr$3(as_FloatRegister($dst$$reg), rscratch1);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}')dnl\n+dnl             $1  $2 $3\n+VECTOR_LOAD_CON(8,  D, d)\n+VECTOR_LOAD_CON(16, X, q)\n+undefine(PREDICATE)\n+dnl\n+\/\/-------------------------------- LOAD_SHUFFLE ----------------------------------\n+dnl\n+define(`VECTOR_LOAD_SHUFFLE_B', `\n+instruct loadshuffle$1B`'(vec$2 dst, vec$2 src)\n+%{\n+  predicate(n->as_Vector()->length() == $1 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"mov  $dst, $src\\t# get $1B shuffle\" %}\n+  ins_encode %{\n+    __ orr(as_FloatRegister($dst$$reg), __ T$1B,\n+           as_FloatRegister($src$$reg), as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl                   $1  $2\n+VECTOR_LOAD_SHUFFLE_B(8,  D)\n+VECTOR_LOAD_SHUFFLE_B(16, X)\n+dnl\n+define(`VECTOR_LOAD_SHUFFLE_S', `\n+instruct loadshuffle$1S`'(vec$2 dst, vec$3 src)\n+%{\n+  predicate(n->as_Vector()->length() == $1 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# $1B to $1H\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl\n+dnl                   $1 $2 $3\n+VECTOR_LOAD_SHUFFLE_S(4, D, D)\n+VECTOR_LOAD_SHUFFLE_S(8, X, D)\n+dnl\n+\n+instruct loadshuffle4I(vecX dst, vecD src)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorLoadShuffle src));\n+  ins_cost(INSN_COST);\n+  format %{ \"uxtl  $dst, $src\\t# 4B to 4H \\n\\t\"\n+            \"uxtl  $dst, $dst\\t# 4H to 4S\" %}\n+  ins_encode %{\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg), __ T8B);\n+    __ uxtl(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($dst$$reg), __ T4H);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/-------------------------------- Rearrange -------------------------------------\n+\/\/ Here is an example that rearranges a NEON vector with 4 ints:\n+\/\/ Rearrange V1 int[a0, a1, a2, a3] to V2 int[a2, a3, a0, a1]\n+\/\/   1. Get the indices of V1 and store them as Vi byte[0, 1, 2, 3].\n+\/\/   2. Convert Vi byte[0, 1, 2, 3] to the indices of V2 and also store them as Vi byte[2, 3, 0, 1].\n+\/\/   3. Unsigned extend Long Vi from byte[2, 3, 0, 1] to int[2, 3, 0, 1].\n+\/\/   4. Multiply Vi int[2, 3, 0, 1] with constant int[0x04040404, 0x04040404, 0x04040404, 0x04040404]\n+\/\/      and get tbl base Vm int[0x08080808, 0x0c0c0c0c, 0x00000000, 0x04040404].\n+\/\/   5. Add Vm with constant int[0x03020100, 0x03020100, 0x03020100, 0x03020100]\n+\/\/      and get tbl index Vm int[0x0b0a0908, 0x0f0e0d0c, 0x03020100, 0x07060504]\n+\/\/   6. Use Vm as index register, and use V1 as table register.\n+\/\/      Then get V2 as the result by tbl NEON instructions.\n+\/\/ Notes:\n+\/\/   Step 1 matches VectorLoadConst.\n+\/\/   Step 3 matches VectorLoadShuffle.\n+\/\/   Step 4, 5, 6 match VectorRearrange.\n+\/\/   For VectorRearrange short\/int, the reason why such complex calculation is\n+\/\/   required is because NEON tbl supports bytes table only, so for short\/int, we\n+\/\/   need to lookup 2\/4 bytes as a group. For VectorRearrange long, we use bsl\n+\/\/   to implement rearrange.\n+define(`VECTOR_REARRANGE_B', `\n+instruct rearrange$1B`'(vec$2 dst, vec$2 src, vec$2 shuffle)\n+%{\n+  predicate(n->as_Vector()->length() == $1 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst);\n+  format %{ \"tbl $dst, {$dst}, $shuffle\\t# rearrange $1B\" %}\n+  ins_encode %{\n+    __ tbl(as_FloatRegister($dst$$reg), __ T$1B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($shuffle$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1  $2\n+VECTOR_REARRANGE_B(8,  D)\n+VECTOR_REARRANGE_B(16, X)\n+dnl\n+define(`VECTOR_REARRANGE_S', `\n+instruct rearrange$1S`'(vec$2 dst, vec$2 src, vec$2 shuffle, vec$2 tmp0, vec$2 tmp1)\n+%{\n+  predicate(n->as_Vector()->length() == $1 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp0, TEMP tmp1);\n+  format %{ \"mov   $tmp0, CONSTANT\\t# constant 0x0202020202020202\\n\\t\"\n+            \"mov   $tmp1, CONSTANT\\t# constant 0x0100010001000100\\n\\t\"\n+            \"mulv  $dst, T$1H, $shuffle, $tmp0\\n\\t\"\n+            \"addv  $dst, T$3B, $dst, $tmp1\\n\\t\"\n+            \"tbl   $dst, {$src}, $dst\\t# rearrange $1S\" %}\n+  ins_encode %{\n+    __ mov(as_FloatRegister($tmp0$$reg), __ T$3B, 0x02);\n+    __ mov(as_FloatRegister($tmp1$$reg), __ T$1H, 0x0100);\n+    __ mulv(as_FloatRegister($dst$$reg), __ T$1H,\n+            as_FloatRegister($shuffle$$reg), as_FloatRegister($tmp0$$reg));\n+    __ addv(as_FloatRegister($dst$$reg), __ T$3B,\n+            as_FloatRegister($dst$$reg), as_FloatRegister($tmp1$$reg));\n+    __ tbl(as_FloatRegister($dst$$reg), __ T$3B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl                $1 $2 $3\n+VECTOR_REARRANGE_S(4, D, 8)\n+VECTOR_REARRANGE_S(8, X, 16)\n+\n+instruct rearrange4I(vecX dst, vecX src, vecX shuffle, vecX tmp0, vecX tmp1)\n+%{\n+  predicate(n->as_Vector()->length() == 4 &&\n+           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (VectorRearrange src shuffle));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP tmp0, TEMP tmp1);\n+  format %{ \"mov   $tmp0, CONSTANT\\t# constant 0x0404040404040404\\n\\t\"\n+            \"mov   $tmp1, CONSTANT\\t# constant 0x0302010003020100\\n\\t\"\n+            \"mulv  $dst, T8H, $shuffle, $tmp0\\n\\t\"\n+            \"addv  $dst, T16B, $dst, $tmp1\\n\\t\"\n+            \"tbl   $dst, {$src}, $dst\\t# rearrange 4I\" %}\n+  ins_encode %{\n+    __ mov(as_FloatRegister($tmp0$$reg), __ T16B, 0x04);\n+    __ mov(as_FloatRegister($tmp1$$reg), __ T4S, 0x03020100);\n+    __ mulv(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($shuffle$$reg), as_FloatRegister($tmp0$$reg));\n+    __ addv(as_FloatRegister($dst$$reg), __ T16B,\n+            as_FloatRegister($dst$$reg), as_FloatRegister($tmp1$$reg));\n+    __ tbl(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), 1, as_FloatRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/-------------------------------- Anytrue\/alltrue -----------------------------\n+dnl\n+define(`ANYTRUE_IN_MASK', `\n+instruct anytrue_in_mask$1B`'(iRegINoSp dst, vec$2 src1, vec$2 src2, vec$2 tmp, rFlagsReg cr)\n+%{\n+  predicate(static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2 ));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"addv  $tmp, T$1B, $src1\\t# src1 and src2 are the same\\n\\t\"\n+            \"umov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmp   $dst, 0\\n\\t\"\n+            \"cset  $dst\" %}\n+  ins_encode %{\n+    __ addv(as_FloatRegister($tmp$$reg), __ T$1B, as_FloatRegister($src1$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::NE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl             $1  $2\n+ANYTRUE_IN_MASK(8,  D)\n+ANYTRUE_IN_MASK(16, X)\n+dnl\n+define(`ALLTRUE_IN_MASK', `\n+instruct alltrue_in_mask$1B`'(iRegINoSp dst, vec$2 src1, vec$2 src2, vec$2 tmp, rFlagsReg cr)\n+%{\n+  predicate(static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2 ));\n+  ins_cost(INSN_COST);\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"andr  $tmp, T$1B, $src1, $src2\\t# src2 is maskAllTrue\\n\\t\"\n+            \"notr  $tmp, T$1B, $tmp\\n\\t\"\n+            \"addv  $tmp, T$1B, $tmp\\n\\t\"\n+            \"umov  $dst, $tmp, B, 0\\n\\t\"\n+            \"cmp   $dst, 0\\n\\t\"\n+            \"cset  $dst\" %}\n+  ins_encode %{\n+    __ andr(as_FloatRegister($tmp$$reg), __ T$1B,\n+            as_FloatRegister($src1$$reg), as_FloatRegister($src2$$reg));\n+    __ notr(as_FloatRegister($tmp$$reg), __ T$1B, as_FloatRegister($tmp$$reg));\n+    __ addv(as_FloatRegister($tmp$$reg), __ T$1B, as_FloatRegister($tmp$$reg));\n+    __ umov($dst$$Register, as_FloatRegister($tmp$$reg), __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl             $1  $2\n+ALLTRUE_IN_MASK(8,  D)\n+ALLTRUE_IN_MASK(16, X)\n+dnl\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":1424,"deletions":0,"binary":false,"changes":1424,"status":"added"},{"patch":"@@ -0,0 +1,82 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/pretouchTask.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+PretouchTask::PretouchTask(const char* task_name, char* start_address, char* end_address, size_t page_size) :\n+    AbstractGangTask(task_name),\n+    _cur_addr(start_address),\n+    _start_addr(start_address),\n+    _end_addr(end_address),\n+    _page_size(0) {\n+#ifdef LINUX\n+  _page_size = UseTransparentHugePages ? (size_t)os::vm_page_size(): page_size;\n+#else\n+  _page_size = page_size;\n+#endif\n+}\n+\n+size_t PretouchTask::chunk_size() {\n+  return PreTouchParallelChunkSize;\n+}\n+\n+void PretouchTask::work(uint worker_id) {\n+  size_t const actual_chunk_size = MAX2(chunk_size(), _page_size);\n+\n+  while (true) {\n+    char* touch_addr = Atomic::fetch_and_add(&_cur_addr, actual_chunk_size);\n+    if (touch_addr < _start_addr || touch_addr >= _end_addr) {\n+      break;\n+    }\n+\n+    char* end_addr = touch_addr + MIN2(actual_chunk_size, pointer_delta(_end_addr, touch_addr, sizeof(char)));\n+\n+    os::pretouch_memory(touch_addr, end_addr, _page_size);\n+  }\n+}\n+\n+void PretouchTask::pretouch(const char* task_name, char* start_address, char* end_address,\n+                            size_t page_size, WorkGang* pretouch_gang) {\n+  PretouchTask task(task_name, start_address, end_address, page_size);\n+  size_t total_bytes = pointer_delta(end_address, start_address, sizeof(char));\n+\n+  if (pretouch_gang != NULL) {\n+    size_t num_chunks = MAX2((size_t)1, total_bytes \/ MAX2(PretouchTask::chunk_size(), page_size));\n+\n+    uint num_workers = MIN2((uint)num_chunks, pretouch_gang->total_workers());\n+    log_debug(gc, heap)(\"Running %s with %u workers for \" SIZE_FORMAT \" work units pre-touching \" SIZE_FORMAT \"B.\",\n+                        task.name(), num_workers, num_chunks, total_bytes);\n+\n+    pretouch_gang->run_task(&task, num_workers);\n+  } else {\n+    log_debug(gc, heap)(\"Running %s pre-touching \" SIZE_FORMAT \"B.\",\n+                        task.name(), total_bytes);\n+    task.work(0);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/pretouchTask.cpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_PRETOUCH_HPP\n+#define SHARE_GC_SHARED_PRETOUCH_HPP\n+\n+#include \"gc\/shared\/workgroup.hpp\"\n+\n+class PretouchTask : public AbstractGangTask {\n+  char* volatile _cur_addr;\n+  char* const _start_addr;\n+  char* const _end_addr;\n+  size_t _page_size;\n+\n+public:\n+  PretouchTask(const char* task_name, char* start_address, char* end_address, size_t page_size);\n+\n+  virtual void work(uint worker_id);\n+\n+  static size_t chunk_size();\n+\n+  static void pretouch(const char* task_name, char* start_address, char* end_address,\n+                       size_t page_size, WorkGang* pretouch_gang);\n+\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_PRETOUCH_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/pretouchTask.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,348 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"ci\/ciMethod.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"opto\/callGenerator.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/convertnode.hpp\"\n+#include \"opto\/intrinsicnode.hpp\"\n+#include \"opto\/movenode.hpp\"\n+\n+class LibraryIntrinsic : public InlineCallGenerator {\n+  \/\/ Extend the set of intrinsics known to the runtime:\n+ public:\n+ private:\n+  bool             _is_virtual;\n+  bool             _does_virtual_dispatch;\n+  int8_t           _predicates_count;  \/\/ Intrinsic is predicated by several conditions\n+  int8_t           _last_predicate; \/\/ Last generated predicate\n+  vmIntrinsics::ID _intrinsic_id;\n+\n+ public:\n+  LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)\n+    : InlineCallGenerator(m),\n+      _is_virtual(is_virtual),\n+      _does_virtual_dispatch(does_virtual_dispatch),\n+      _predicates_count((int8_t)predicates_count),\n+      _last_predicate((int8_t)-1),\n+      _intrinsic_id(id)\n+  {\n+  }\n+  virtual bool is_intrinsic() const { return true; }\n+  virtual bool is_virtual()   const { return _is_virtual; }\n+  virtual bool is_predicated() const { return _predicates_count > 0; }\n+  virtual int  predicates_count() const { return _predicates_count; }\n+  virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }\n+  virtual JVMState* generate(JVMState* jvms);\n+  virtual Node* generate_predicate(JVMState* jvms, int predicate);\n+  vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }\n+};\n+\n+\n+\/\/ Local helper class for LibraryIntrinsic:\n+class LibraryCallKit : public GraphKit {\n+ private:\n+  LibraryIntrinsic* _intrinsic;     \/\/ the library intrinsic being called\n+  Node*             _result;        \/\/ the result node, if any\n+  int               _reexecute_sp;  \/\/ the stack pointer when bytecode needs to be reexecuted\n+\n+  const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type);\n+\n+ public:\n+  LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)\n+    : GraphKit(jvms),\n+      _intrinsic(intrinsic),\n+      _result(NULL)\n+  {\n+    \/\/ Check if this is a root compile.  In that case we don't have a caller.\n+    if (!jvms->has_method()) {\n+      _reexecute_sp = sp();\n+    } else {\n+      \/\/ Find out how many arguments the interpreter needs when deoptimizing\n+      \/\/ and save the stack pointer value so it can used by uncommon_trap.\n+      \/\/ We find the argument count by looking at the declared signature.\n+      bool ignored_will_link;\n+      ciSignature* declared_signature = NULL;\n+      ciMethod* ignored_callee = caller()->get_method_at_bci(bci(), ignored_will_link, &declared_signature);\n+      const int nargs = declared_signature->arg_size_for_bc(caller()->java_code_at_bci(bci()));\n+      _reexecute_sp = sp() + nargs;  \/\/ \"push\" arguments back on stack\n+    }\n+  }\n+\n+  virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }\n+\n+  ciMethod*         caller()    const    { return jvms()->method(); }\n+  int               bci()       const    { return jvms()->bci(); }\n+  LibraryIntrinsic* intrinsic() const    { return _intrinsic; }\n+  vmIntrinsics::ID  intrinsic_id() const { return _intrinsic->intrinsic_id(); }\n+  ciMethod*         callee()    const    { return _intrinsic->method(); }\n+\n+  bool  try_to_inline(int predicate);\n+  Node* try_to_predicate(int predicate);\n+\n+  void push_result() {\n+    \/\/ Push the result onto the stack.\n+    if (!stopped() && result() != NULL) {\n+      BasicType bt = result()->bottom_type()->basic_type();\n+      push_node(bt, result());\n+    }\n+  }\n+\n+ private:\n+  void fatal_unexpected_iid(vmIntrinsics::ID iid) {\n+    fatal(\"unexpected intrinsic %d: %s\", iid, vmIntrinsics::name_at(iid));\n+  }\n+\n+  void  set_result(Node* n) { assert(_result == NULL, \"only set once\"); _result = n; }\n+  void  set_result(RegionNode* region, PhiNode* value);\n+  Node*     result() { return _result; }\n+\n+  virtual int reexecute_sp() { return _reexecute_sp; }\n+\n+  \/\/ Helper functions to inline natives\n+  Node* generate_guard(Node* test, RegionNode* region, float true_prob);\n+  Node* generate_slow_guard(Node* test, RegionNode* region);\n+  Node* generate_fair_guard(Node* test, RegionNode* region);\n+  Node* generate_negative_guard(Node* index, RegionNode* region,\n+                                \/\/ resulting CastII of index:\n+                                Node* *pos_index = NULL);\n+  Node* generate_limit_guard(Node* offset, Node* subseq_length,\n+                             Node* array_length,\n+                             RegionNode* region);\n+  void  generate_string_range_check(Node* array, Node* offset,\n+                                    Node* length, bool char_count);\n+  Node* generate_current_thread(Node* &tls_output);\n+  Node* load_mirror_from_klass(Node* klass);\n+  Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,\n+                                      RegionNode* region, int null_path,\n+                                      int offset);\n+  Node* load_klass_from_mirror(Node* mirror, bool never_see_null,\n+                               RegionNode* region, int null_path) {\n+    int offset = java_lang_Class::klass_offset();\n+    return load_klass_from_mirror_common(mirror, never_see_null,\n+                                         region, null_path,\n+                                         offset);\n+  }\n+  Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,\n+                                     RegionNode* region, int null_path) {\n+    int offset = java_lang_Class::array_klass_offset();\n+    return load_klass_from_mirror_common(mirror, never_see_null,\n+                                         region, null_path,\n+                                         offset);\n+  }\n+  Node* generate_access_flags_guard(Node* kls,\n+                                    int modifier_mask, int modifier_bits,\n+                                    RegionNode* region);\n+  Node* generate_interface_guard(Node* kls, RegionNode* region);\n+  Node* generate_hidden_class_guard(Node* kls, RegionNode* region);\n+  Node* generate_array_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, false, false);\n+  }\n+  Node* generate_non_array_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, false, true);\n+  }\n+  Node* generate_objArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, true, false);\n+  }\n+  Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, true, true);\n+  }\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n+                                    bool obj_array, bool not_array);\n+  Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);\n+  CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,\n+                                     bool is_virtual = false, bool is_static = false);\n+  CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {\n+    return generate_method_call(method_id, false, true);\n+  }\n+  CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {\n+    return generate_method_call(method_id, true, false);\n+  }\n+  Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);\n+  Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);\n+\n+  Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);\n+  bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);\n+  bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);\n+  bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);\n+  Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,\n+                          RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);\n+  bool inline_string_indexOfChar(StrIntrinsicNode::ArgEnc ae);\n+  bool inline_string_equals(StrIntrinsicNode::ArgEnc ae);\n+  bool inline_string_toBytesU();\n+  bool inline_string_getCharsU();\n+  bool inline_string_copy(bool compress);\n+  bool inline_string_char_access(bool is_store);\n+  Node* round_double_node(Node* n);\n+  bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);\n+  bool inline_math_native(vmIntrinsics::ID id);\n+  bool inline_math(vmIntrinsics::ID id);\n+  bool inline_double_math(vmIntrinsics::ID id);\n+  template <typename OverflowOp>\n+  bool inline_math_overflow(Node* arg1, Node* arg2);\n+  void inline_math_mathExact(Node* math, Node* test);\n+  bool inline_math_addExactI(bool is_increment);\n+  bool inline_math_addExactL(bool is_increment);\n+  bool inline_math_multiplyExactI();\n+  bool inline_math_multiplyExactL();\n+  bool inline_math_multiplyHigh();\n+  bool inline_math_negateExactI();\n+  bool inline_math_negateExactL();\n+  bool inline_math_subtractExactI(bool is_decrement);\n+  bool inline_math_subtractExactL(bool is_decrement);\n+  bool inline_min_max(vmIntrinsics::ID id);\n+  bool inline_notify(vmIntrinsics::ID id);\n+  Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);\n+  \/\/ This returns Type::AnyPtr, RawPtr, or OopPtr.\n+  int classify_unsafe_addr(Node* &base, Node* &offset, BasicType type);\n+  Node* make_unsafe_address(Node*& base, Node* offset, DecoratorSet decorators, BasicType type = T_ILLEGAL, bool can_cast = false);\n+\n+  typedef enum { Relaxed, Opaque, Volatile, Acquire, Release } AccessKind;\n+  DecoratorSet mo_decorator_for_access_kind(AccessKind kind);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  static bool klass_needs_init_guard(Node* kls);\n+  bool inline_unsafe_allocate();\n+  bool inline_unsafe_newArray(bool uninitialized);\n+  bool inline_unsafe_writeback0();\n+  bool inline_unsafe_writebackSync0(bool is_pre);\n+  bool inline_unsafe_copyMemory();\n+  bool inline_native_currentThread();\n+\n+  bool inline_native_time_funcs(address method, const char* funcName);\n+#ifdef JFR_HAVE_INTRINSICS\n+  bool inline_native_classID();\n+  bool inline_native_getEventWriter();\n+#endif\n+  bool inline_native_Class_query(vmIntrinsics::ID id);\n+  bool inline_native_subtype_check();\n+  bool inline_native_getLength();\n+  bool inline_array_copyOf(bool is_copyOfRange);\n+  bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);\n+  bool inline_preconditions_checkIndex();\n+  void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array);\n+  bool inline_native_clone(bool is_virtual);\n+  bool inline_native_Reflection_getCallerClass();\n+  \/\/ Helper function for inlining native object hash method\n+  bool inline_native_hashcode(bool is_virtual, bool is_static);\n+  bool inline_native_getClass();\n+\n+  \/\/ Helper functions for inlining arraycopy\n+  bool inline_arraycopy();\n+  AllocateArrayNode* tightly_coupled_allocation(Node* ptr,\n+                                                RegionNode* slow_region);\n+  JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int& saved_reexecute_sp);\n+  void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp,\n+                                      uint new_idx);\n+\n+  typedef enum { LS_get_add, LS_get_set, LS_cmp_swap, LS_cmp_swap_weak, LS_cmp_exchange } LoadStoreKind;\n+  bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind, AccessKind access_kind);\n+  bool inline_unsafe_fence(vmIntrinsics::ID id);\n+  bool inline_onspinwait();\n+  bool inline_fp_conversions(vmIntrinsics::ID id);\n+  bool inline_number_methods(vmIntrinsics::ID id);\n+  bool inline_reference_get();\n+  bool inline_Class_cast();\n+  bool inline_aescrypt_Block(vmIntrinsics::ID id);\n+  bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);\n+  bool inline_electronicCodeBook_AESCrypt(vmIntrinsics::ID id);\n+  bool inline_counterMode_AESCrypt(vmIntrinsics::ID id);\n+  Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);\n+  Node* inline_electronicCodeBook_AESCrypt_predicate(bool decrypting);\n+  Node* inline_counterMode_AESCrypt_predicate();\n+  Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);\n+  Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);\n+  bool inline_ghash_processBlocks();\n+  bool inline_base64_encodeBlock();\n+  bool inline_digestBase_implCompress(vmIntrinsics::ID id);\n+  bool inline_digestBase_implCompressMB(int predicate);\n+  bool inline_digestBase_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass,\n+                                        bool long_state, address stubAddr, const char *stubName,\n+                                        Node* src_start, Node* ofs, Node* limit);\n+  Node* get_state_from_digest_object(Node *digestBase_object);\n+  Node* get_long_state_from_digest_object(Node *digestBase_object);\n+  Node* inline_digestBase_implCompressMB_predicate(int predicate);\n+  bool inline_encodeISOArray();\n+  bool inline_updateCRC32();\n+  bool inline_updateBytesCRC32();\n+  bool inline_updateByteBufferCRC32();\n+  Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);\n+  bool inline_updateBytesCRC32C();\n+  bool inline_updateDirectByteBufferCRC32C();\n+  bool inline_updateBytesAdler32();\n+  bool inline_updateByteBufferAdler32();\n+  bool inline_multiplyToLen();\n+  bool inline_hasNegatives();\n+  bool inline_squareToLen();\n+  bool inline_mulAdd();\n+  bool inline_montgomeryMultiply();\n+  bool inline_montgomerySquare();\n+  bool inline_bigIntegerShift(bool isRightShift);\n+  bool inline_vectorizedMismatch();\n+  bool inline_fma(vmIntrinsics::ID id);\n+  bool inline_character_compare(vmIntrinsics::ID id);\n+  bool inline_fp_min_max(vmIntrinsics::ID id);\n+\n+  bool inline_profileBoolean();\n+  bool inline_isCompileConstant();\n+\n+  \/\/ Vector API support\n+  bool inline_vector_nary_operation(int n);\n+  bool inline_vector_broadcast_coerced();\n+  bool inline_vector_shuffle_to_vector();\n+  bool inline_vector_shuffle_iota();\n+  bool inline_vector_mem_operation(bool is_store);\n+  bool inline_vector_gather_scatter(bool is_scatter);\n+  bool inline_vector_reduction();\n+  bool inline_vector_test();\n+  bool inline_vector_blend();\n+  bool inline_vector_rearrange();\n+  bool inline_vector_compare();\n+  bool inline_vector_broadcast_int();\n+  bool inline_vector_convert();\n+  bool inline_vector_extract();\n+  bool inline_vector_insert();\n+  Node* box_vector(Node* in, const TypeInstPtr* vbox_type, BasicType bt, int num_elem);\n+  Node* unbox_vector(Node* in, const TypeInstPtr* vbox_type, BasicType bt, int num_elem, bool shuffle_to_vector = false);\n+  Node* shift_count(Node* cnt, int shift_op, BasicType bt, int num_elem);\n+\n+  enum VectorMaskUseType {\n+    VecMaskUseLoad,\n+    VecMaskUseStore,\n+    VecMaskUseAll,\n+    VecMaskNotUsed\n+  };\n+\n+  bool arch_supports_vector(int op, int num_elem, BasicType type, VectorMaskUseType mask_use_type, bool has_scalar_args = false);\n+\n+  void clear_upper_avx() {\n+#ifdef X86\n+    if (UseAVX >= 2) {\n+      C->set_clear_upper_avx(true);\n+    }\n+#endif\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":348,"deletions":0,"binary":false,"changes":348,"status":"added"},{"patch":"@@ -0,0 +1,466 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/phaseX.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/vector.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+void PhaseVector::optimize_vector_boxes() {\n+  Compile::TracePhase tp(\"vector_elimination\", &timers[_t_vector_elimination]);\n+\n+  \/\/ Signal GraphKit it's post-parse phase.\n+  assert(C->inlining_incrementally() == false, \"sanity\");\n+  C->set_inlining_incrementally(true);\n+\n+  C->for_igvn()->clear();\n+  C->initial_gvn()->replace_with(&_igvn);\n+\n+  expand_vunbox_nodes();\n+  scalarize_vbox_nodes();\n+\n+  C->inline_vector_reboxing_calls();\n+\n+  expand_vbox_nodes();\n+  eliminate_vbox_alloc_nodes();\n+\n+  C->set_inlining_incrementally(false);\n+\n+  do_cleanup();\n+}\n+\n+void PhaseVector::do_cleanup() {\n+  if (C->failing())  return;\n+  {\n+    Compile::TracePhase tp(\"vector_pru\", &timers[_t_vector_pru]);\n+    ResourceMark rm;\n+    PhaseRemoveUseless pru(C->initial_gvn(), C->for_igvn());\n+    if (C->failing())  return;\n+  }\n+  {\n+    Compile::TracePhase tp(\"incrementalInline_igvn\", &timers[_t_vector_igvn]);\n+    _igvn = PhaseIterGVN(C->initial_gvn());\n+    _igvn.optimize();\n+    if (C->failing())  return;\n+  }\n+  C->print_method(PHASE_ITER_GVN_BEFORE_EA, 3);\n+}\n+\n+void PhaseVector::scalarize_vbox_nodes() {\n+  if (C->failing())  return;\n+\n+  if (!EnableVectorReboxing) {\n+    return; \/\/ don't scalarize vector boxes\n+  }\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorBox) {\n+      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(n);\n+      scalarize_vbox_node(vbox);\n+      if (C->failing())  return;\n+      C->print_method(PHASE_SCALARIZE_VBOX, vbox, 3);\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+void PhaseVector::expand_vbox_nodes() {\n+  if (C->failing())  return;\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorBox) {\n+      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(n);\n+      expand_vbox_node(vbox);\n+      if (C->failing())  return;\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+void PhaseVector::expand_vunbox_nodes() {\n+  if (C->failing())  return;\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorUnbox) {\n+      VectorUnboxNode* vec_unbox = static_cast<VectorUnboxNode*>(n);\n+      expand_vunbox_node(vec_unbox);\n+      if (C->failing())  return;\n+      C->print_method(PHASE_EXPAND_VUNBOX, vec_unbox, 3);\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+void PhaseVector::eliminate_vbox_alloc_nodes() {\n+  if (C->failing())  return;\n+\n+  int macro_idx = C->macro_count() - 1;\n+  while (macro_idx >= 0) {\n+    Node * n = C->macro_node(macro_idx);\n+    assert(n->is_macro(), \"only macro nodes expected here\");\n+    if (n->Opcode() == Op_VectorBoxAllocate) {\n+      VectorBoxAllocateNode* vbox_alloc = static_cast<VectorBoxAllocateNode*>(n);\n+      eliminate_vbox_alloc_node(vbox_alloc);\n+      if (C->failing())  return;\n+      C->print_method(PHASE_ELIMINATE_VBOX_ALLOC, vbox_alloc, 3);\n+    }\n+    if (C->failing())  return;\n+    macro_idx = MIN2(macro_idx - 1, C->macro_count() - 1);\n+  }\n+}\n+\n+static JVMState* clone_jvms(Compile* C, SafePointNode* sfpt) {\n+  JVMState* new_jvms = sfpt->jvms()->clone_shallow(C);\n+  uint size = sfpt->req();\n+  SafePointNode* map = new SafePointNode(size, new_jvms);\n+  for (uint i = 0; i < size; i++) {\n+    map->init_req(i, sfpt->in(i));\n+  }\n+  new_jvms->set_map(map);\n+  return new_jvms;\n+}\n+\n+void PhaseVector::scalarize_vbox_node(VectorBoxNode* vec_box) {\n+  Node* vec_value = vec_box->in(VectorBoxNode::Value);\n+  PhaseGVN& gvn = *C->initial_gvn();\n+\n+  \/\/ Process merged VBAs\n+\n+  if (EnableVectorAggressiveReboxing) {\n+    Unique_Node_List calls(C->comp_arena());\n+    for (DUIterator_Fast imax, i = vec_box->fast_outs(imax); i < imax; i++) {\n+      Node* use = vec_box->fast_out(i);\n+      if (use->is_CallJava()) {\n+        CallJavaNode* call = use->as_CallJava();\n+        if (call->has_non_debug_use(vec_box) && vec_box->in(VectorBoxNode::Box)->is_Phi()) {\n+          calls.push(call);\n+        }\n+      }\n+    }\n+\n+    while (calls.size() > 0) {\n+      CallJavaNode* call = calls.pop()->as_CallJava();\n+      \/\/ Attach new VBA to the call and use it instead of Phi (VBA ... VBA).\n+\n+      JVMState* jvms = clone_jvms(C, call);\n+      GraphKit kit(jvms);\n+      PhaseGVN& gvn = kit.gvn();\n+\n+      \/\/ Adjust JVMS from post-call to pre-call state: put args on stack\n+      uint nargs = call->method()->arg_size();\n+      kit.ensure_stack(kit.sp() + nargs);\n+      for (uint i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+        kit.push(call->in(i));\n+      }\n+      jvms = kit.sync_jvms();\n+\n+      Node* new_vbox = NULL;\n+      {\n+        PreserveReexecuteState prs(&kit);\n+\n+        kit.jvms()->set_should_reexecute(true);\n+\n+        const TypeInstPtr* vbox_type = vec_box->box_type();\n+        const TypeVect* vect_type = vec_box->vec_type();\n+        Node* vect = vec_box->in(VectorBoxNode::Value);\n+\n+        VectorBoxAllocateNode* alloc = new VectorBoxAllocateNode(C, vbox_type);\n+        kit.set_edges_for_java_call(alloc, \/*must_throw=*\/false, \/*separate_io_proj=*\/true);\n+        kit.make_slow_call_ex(alloc, C->env()->Throwable_klass(), \/*separate_io_proj=*\/true, \/*deoptimize=*\/true);\n+        kit.set_i_o(gvn.transform( new ProjNode(alloc, TypeFunc::I_O) ));\n+        kit.set_all_memory(gvn.transform( new ProjNode(alloc, TypeFunc::Memory) ));\n+        Node* ret = gvn.transform(new ProjNode(alloc, TypeFunc::Parms));\n+\n+        new_vbox = gvn.transform(new VectorBoxNode(C, ret, vect, vbox_type, vect_type));\n+\n+        kit.replace_in_map(vec_box, new_vbox);\n+      }\n+\n+      kit.dec_sp(nargs);\n+      jvms = kit.sync_jvms();\n+\n+      call->set_req(TypeFunc::Control , kit.control());\n+      call->set_req(TypeFunc::I_O     , kit.i_o());\n+      call->set_req(TypeFunc::Memory  , kit.reset_memory());\n+      call->set_req(TypeFunc::FramePtr, kit.frameptr());\n+      call->replace_edge(vec_box, new_vbox);\n+\n+      C->record_for_igvn(call);\n+    }\n+  }\n+\n+  \/\/ Process debug uses at safepoints\n+  Unique_Node_List safepoints(C->comp_arena());\n+\n+  for (DUIterator_Fast imax, i = vec_box->fast_outs(imax); i < imax; i++) {\n+    Node* use = vec_box->fast_out(i);\n+    if (use->is_SafePoint()) {\n+      SafePointNode* sfpt = use->as_SafePoint();\n+      if (!sfpt->is_Call() || !sfpt->as_Call()->has_non_debug_use(vec_box)) {\n+        safepoints.push(sfpt);\n+      }\n+    }\n+  }\n+\n+  while (safepoints.size() > 0) {\n+    SafePointNode* sfpt = safepoints.pop()->as_SafePoint();\n+\n+    uint first_ind = (sfpt->req() - sfpt->jvms()->scloff());\n+    Node* sobj = new SafePointScalarObjectNode(vec_box->box_type(),\n+#ifdef ASSERT\n+                                               NULL,\n+#endif \/\/ ASSERT\n+                                               first_ind, \/*n_fields=*\/1);\n+    sobj->init_req(0, C->root());\n+    sfpt->add_req(vec_value);\n+\n+    sobj = gvn.transform(sobj);\n+\n+    JVMState *jvms = sfpt->jvms();\n+\n+    jvms->set_endoff(sfpt->req());\n+    \/\/ Now make a pass over the debug information replacing any references\n+    \/\/ to the allocated object with \"sobj\"\n+    int start = jvms->debug_start();\n+    int end   = jvms->debug_end();\n+    sfpt->replace_edges_in_range(vec_box, sobj, start, end);\n+\n+    C->record_for_igvn(sfpt);\n+  }\n+}\n+\n+void PhaseVector::expand_vbox_node(VectorBoxNode* vec_box) {\n+  if (vec_box->outcnt() > 0) {\n+    Node* vbox = vec_box->in(VectorBoxNode::Box);\n+    Node* vect = vec_box->in(VectorBoxNode::Value);\n+    Node* result = expand_vbox_node_helper(vbox, vect, vec_box->box_type(), vec_box->vec_type());\n+    C->gvn_replace_by(vec_box, result);\n+    C->print_method(PHASE_EXPAND_VBOX, vec_box, 3);\n+  }\n+  C->remove_macro_node(vec_box);\n+}\n+\n+Node* PhaseVector::expand_vbox_node_helper(Node* vbox,\n+                                           Node* vect,\n+                                           const TypeInstPtr* box_type,\n+                                           const TypeVect* vect_type) {\n+  if (vbox->is_Phi() && vect->is_Phi()) {\n+    assert(vbox->as_Phi()->region() == vect->as_Phi()->region(), \"\");\n+    Node* new_phi = new PhiNode(vbox->as_Phi()->region(), box_type);\n+    for (uint i = 1; i < vbox->req(); i++) {\n+      Node* new_box = expand_vbox_node_helper(vbox->in(i), vect->in(i), box_type, vect_type);\n+      new_phi->set_req(i, new_box);\n+    }\n+    new_phi = C->initial_gvn()->transform(new_phi);\n+    return new_phi;\n+  } else if (vbox->is_Proj() && vbox->in(0)->Opcode() == Op_VectorBoxAllocate) {\n+    VectorBoxAllocateNode* vbox_alloc = static_cast<VectorBoxAllocateNode*>(vbox->in(0));\n+    return expand_vbox_alloc_node(vbox_alloc, vect, box_type, vect_type);\n+  } else {\n+    assert(!vbox->is_Phi(), \"\");\n+    \/\/ TODO: assert that expanded vbox is initialized with the same value (vect).\n+    return vbox; \/\/ already expanded\n+  }\n+}\n+\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+Node* PhaseVector::expand_vbox_alloc_node(VectorBoxAllocateNode* vbox_alloc,\n+                                          Node* value,\n+                                          const TypeInstPtr* box_type,\n+                                          const TypeVect* vect_type) {\n+  JVMState* jvms = clone_jvms(C, vbox_alloc);\n+  GraphKit kit(jvms);\n+  PhaseGVN& gvn = kit.gvn();\n+\n+  ciInstanceKlass* box_klass = box_type->klass()->as_instance_klass();\n+  BasicType bt = vect_type->element_basic_type();\n+  int num_elem = vect_type->length();\n+\n+  bool is_mask = is_vector_mask(box_klass);\n+  if (is_mask && bt != T_BOOLEAN) {\n+    value = gvn.transform(VectorStoreMaskNode::make(gvn, value, bt, num_elem));\n+    \/\/ Although type of mask depends on its definition, in terms of storage everything is stored in boolean array.\n+    bt = T_BOOLEAN;\n+    assert(value->as_Vector()->bottom_type()->is_vect()->element_basic_type() == bt,\n+           \"must be consistent with mask representation\");\n+  }\n+\n+  \/\/ Generate array allocation for the field which holds the values.\n+  const TypeKlassPtr* array_klass = TypeKlassPtr::make(ciTypeArrayKlass::make(bt));\n+  Node* arr = kit.new_array(kit.makecon(array_klass), kit.intcon(num_elem), 1);\n+\n+  \/\/ Store the vector value into the array.\n+  \/\/ (The store should be captured by InitializeNode and turned into initialized store later.)\n+  Node* arr_adr = kit.array_element_address(arr, kit.intcon(0), bt);\n+  const TypePtr* arr_adr_type = arr_adr->bottom_type()->is_ptr();\n+  Node* arr_mem = kit.memory(arr_adr);\n+  Node* vstore = gvn.transform(StoreVectorNode::make(0,\n+                                                     kit.control(),\n+                                                     arr_mem,\n+                                                     arr_adr,\n+                                                     arr_adr_type,\n+                                                     value,\n+                                                     num_elem));\n+  kit.set_memory(vstore, arr_adr_type);\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), vect_type->length_in_bytes()));\n+\n+  \/\/ Generate the allocate for the Vector object.\n+  const TypeKlassPtr* klass_type = box_type->as_klass_type();\n+  Node* klass_node = kit.makecon(klass_type);\n+  Node* vec_obj = kit.new_instance(klass_node);\n+\n+  \/\/ Store the allocated array into object.\n+  ciField* field = ciEnv::current()->vector_VectorPayload_klass()->get_field_by_name(ciSymbol::payload_name(),\n+                                                                                     ciSymbol::object_signature(),\n+                                                                                     false);\n+  assert(field != NULL, \"\");\n+  Node* vec_field = kit.basic_plus_adr(vec_obj, field->offset_in_bytes());\n+  const TypePtr* vec_adr_type = vec_field->bottom_type()->is_ptr();\n+\n+  \/\/ The store should be captured by InitializeNode and turned into initialized store later.\n+  Node* field_store = gvn.transform(kit.access_store_at(vec_obj,\n+                                                            vec_field,\n+                                                            vec_adr_type,\n+                                                            arr,\n+                                                            TypeOopPtr::make_from_klass(field->type()->as_klass()),\n+                                                            T_OBJECT,\n+                                                            IN_HEAP));\n+  kit.set_memory(field_store, vec_adr_type);\n+\n+  kit.replace_call(vbox_alloc, vec_obj, true);\n+  C->remove_macro_node(vbox_alloc);\n+\n+  return vec_obj;\n+}\n+\n+void PhaseVector::expand_vunbox_node(VectorUnboxNode* vec_unbox) {\n+  if (vec_unbox->outcnt() > 0) {\n+    GraphKit kit;\n+    PhaseGVN& gvn = kit.gvn();\n+\n+    Node* obj = vec_unbox->obj();\n+    const TypeInstPtr* tinst = gvn.type(obj)->isa_instptr();\n+    ciInstanceKlass* from_kls = tinst->klass()->as_instance_klass();\n+    BasicType bt = vec_unbox->vect_type()->element_basic_type();\n+    BasicType masktype = bt;\n+    BasicType elem_bt;\n+\n+    if (is_vector_mask(from_kls)) {\n+      bt = T_BOOLEAN;\n+    } else if (is_vector_shuffle(from_kls)) {\n+      if (vec_unbox->is_shuffle_to_vector() == true) {\n+        elem_bt = bt;\n+      }\n+      bt = T_BYTE;\n+    }\n+\n+    ciField* field = ciEnv::current()->vector_VectorPayload_klass()->get_field_by_name(ciSymbol::payload_name(),\n+                                                                                       ciSymbol::object_signature(),\n+                                                                                       false);\n+    assert(field != NULL, \"\");\n+    int offset = field->offset_in_bytes();\n+    Node* vec_adr = kit.basic_plus_adr(obj, offset);\n+\n+    Node* mem = vec_unbox->mem();\n+    Node* ctrl = vec_unbox->in(0);\n+    Node* vec_field_ld = LoadNode::make(gvn,\n+                                        ctrl,\n+                                        mem,\n+                                        vec_adr,\n+                                        vec_adr->bottom_type()->is_ptr(),\n+                                        TypeOopPtr::make_from_klass(field->type()->as_klass()),\n+                                        T_OBJECT,\n+                                        MemNode::unordered);\n+    vec_field_ld = gvn.transform(vec_field_ld);\n+\n+    \/\/ For proper aliasing, attach concrete payload type.\n+    ciKlass* payload_klass = ciTypeArrayKlass::make(bt);\n+    const Type* payload_type = TypeAryPtr::make_from_klass(payload_klass)->cast_to_ptr_type(TypePtr::NotNull);\n+    vec_field_ld = gvn.transform(new CastPPNode(vec_field_ld, payload_type));\n+\n+    Node* adr = kit.array_element_address(vec_field_ld, gvn.intcon(0), bt);\n+    const TypePtr* adr_type = adr->bottom_type()->is_ptr();\n+    const TypeVect* vt = vec_unbox->bottom_type()->is_vect();\n+    int num_elem = vt->length();\n+    Node* vec_val_load = LoadVectorNode::make(0,\n+                                              ctrl,\n+                                              mem,\n+                                              adr,\n+                                              adr_type,\n+                                              num_elem,\n+                                              bt);\n+    vec_val_load = gvn.transform(vec_val_load);\n+\n+    C->set_max_vector_size(MAX2(C->max_vector_size(), vt->length_in_bytes()));\n+\n+    if (is_vector_mask(from_kls) && masktype != T_BOOLEAN) {\n+      assert(vec_unbox->bottom_type()->is_vect()->element_basic_type() == masktype, \"expect mask type consistency\");\n+      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+    } else if (is_vector_shuffle(from_kls)) {\n+      if (vec_unbox->is_shuffle_to_vector() == false) {\n+        assert(vec_unbox->bottom_type()->is_vect()->element_basic_type() == masktype, \"expect shuffle type consistency\");\n+        vec_val_load = gvn.transform(new VectorLoadShuffleNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+      } else if (elem_bt != T_BYTE) {\n+        vec_val_load = gvn.transform(VectorCastNode::make(Op_VectorCastB2X, vec_val_load, elem_bt, num_elem));\n+      }\n+    }\n+\n+    gvn.hash_delete(vec_unbox);\n+    vec_unbox->disconnect_inputs(C);\n+    C->gvn_replace_by(vec_unbox, vec_val_load);\n+  }\n+  C->remove_macro_node(vec_unbox);\n+}\n+\n+void PhaseVector::eliminate_vbox_alloc_node(VectorBoxAllocateNode* vbox_alloc) {\n+  JVMState* jvms = clone_jvms(C, vbox_alloc);\n+  GraphKit kit(jvms);\n+  \/\/ Remove VBA, but leave a safepoint behind.\n+  \/\/ Otherwise, it may end up with a loop without any safepoint polls.\n+  kit.replace_call(vbox_alloc, kit.map(), true);\n+  C->remove_macro_node(vbox_alloc);\n+}\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":466,"deletions":0,"binary":false,"changes":466,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OPTO_VECTOR_HPP\n+#define SHARE_OPTO_VECTOR_HPP\n+\n+#include \"opto\/node.hpp\"\n+#include \"opto\/phaseX.hpp\"\n+#include \"opto\/type.hpp\"\n+#include \"opto\/vectornode.hpp\"\n+\n+class PhaseVector : public Phase {\n+ private:\n+  PhaseIterGVN& _igvn;\n+\n+  void expand_vbox_nodes();\n+  void expand_vbox_node(VectorBoxNode* vec_box);\n+  Node* expand_vbox_node_helper(Node* vbox,\n+                                Node* vect,\n+                                const TypeInstPtr* box_type,\n+                                const TypeVect* vect_type);\n+  Node* expand_vbox_alloc_node(VectorBoxAllocateNode* vbox_alloc,\n+                               Node* value,\n+                               const TypeInstPtr* box_type,\n+                               const TypeVect* vect_type);\n+  void scalarize_vbox_nodes();\n+  void scalarize_vbox_node(VectorBoxNode* vec_box);\n+  void expand_vunbox_nodes();\n+  void expand_vunbox_node(VectorUnboxNode* vec_box);\n+  void eliminate_vbox_alloc_nodes();\n+  void eliminate_vbox_alloc_node(VectorBoxAllocateNode* vbox_alloc);\n+  void do_cleanup();\n+  void scalarize_vector_boxes();\n+  void expand_vector_boxes();\n+\n+ public:\n+  PhaseVector(PhaseIterGVN& igvn) : Phase(Vector), _igvn(igvn) {}\n+  void optimize_vector_boxes();\n+};\n+\n+#endif \/\/ SHARE_OPTO_VECTOR_HPP\n","filename":"src\/hotspot\/share\/opto\/vector.hpp","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -0,0 +1,1594 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"opto\/library_call.hpp\"\n+#include \"opto\/runtime.hpp\"\n+#include \"opto\/vectornode.hpp\"\n+#include \"prims\/vectorSupport.hpp\"\n+\n+bool LibraryCallKit::arch_supports_vector(int sopc, int num_elem, BasicType type, VectorMaskUseType mask_use_type, bool has_scalar_args) {\n+  \/\/ Check that the operation is valid.\n+  if (sopc <= 0) {\n+#ifndef PRODUCT\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** Rejected intrinsification because no valid vector op could be extracted\");\n+    }\n+#endif\n+    return false;\n+  }\n+\n+  \/\/ Check that architecture supports this op-size-type combination.\n+  if (!Matcher::match_rule_supported_vector(sopc, num_elem, type)) {\n+#ifndef PRODUCT\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** Rejected vector op (%s,%s,%d) because architecture does not support it\",\n+                    NodeClassNames[sopc], type2name(type), num_elem);\n+    }\n+#endif\n+    return false;\n+  } else {\n+    assert(Matcher::match_rule_supported(sopc), \"must be supported\");\n+  }\n+\n+  if (!has_scalar_args && VectorNode::is_vector_shift(sopc) &&\n+      Matcher::supports_vector_variable_shifts() == false) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** Rejected vector op (%s,%s,%d) because architecture does not support variable vector shifts\",\n+                    NodeClassNames[sopc], type2name(type), num_elem);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Check whether mask unboxing is supported.\n+  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseLoad) {\n+    if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, type)) {\n+    #ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** Rejected vector mask loading (%s,%s,%d) because architecture does not support it\",\n+                      NodeClassNames[Op_VectorLoadMask], type2name(type), num_elem);\n+      }\n+    #endif\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Check whether mask boxing is supported.\n+  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseStore) {\n+    if (!Matcher::match_rule_supported_vector(Op_VectorStoreMask, num_elem, type)) {\n+    #ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"Rejected vector mask storing (%s,%s,%d) because architecture does not support it\",\n+                      NodeClassNames[Op_VectorStoreMask], type2name(type), num_elem);\n+      }\n+    #endif\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+static bool is_klass_initialized(const TypeInstPtr* vec_klass) {\n+  assert(vec_klass->const_oop()->as_instance()->java_lang_Class_klass(), \"klass instance expected\");\n+  ciInstanceKlass* klass =  vec_klass->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+  return klass->is_initialized();\n+}\n+\n+#ifdef ASSERT\n+static bool is_vector(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorPayload_klass());\n+}\n+\n+static bool check_vbox(const TypeInstPtr* vbox_type) {\n+  assert(vbox_type->klass_is_exact(), \"\");\n+\n+  ciInstanceKlass* ik = vbox_type->klass()->as_instance_klass();\n+  assert(is_vector(ik), \"not a vector\");\n+\n+  ciField* fd1 = ik->get_field_by_name(ciSymbol::ETYPE_name(), ciSymbol::class_signature(), \/* is_static *\/ true);\n+  assert(fd1 != NULL, \"element type info is missing\");\n+\n+  ciConstant val1 = fd1->constant_value();\n+  BasicType elem_bt = val1.as_object()->as_instance()->java_mirror_type()->basic_type();\n+  assert(is_java_primitive(elem_bt), \"element type info is missing\");\n+\n+  ciField* fd2 = ik->get_field_by_name(ciSymbol::VLENGTH_name(), ciSymbol::int_signature(), \/* is_static *\/ true);\n+  assert(fd2 != NULL, \"vector length info is missing\");\n+\n+  ciConstant val2 = fd2->constant_value();\n+  assert(val2.as_int() > 0, \"vector length info is missing\");\n+\n+  return true;\n+}\n+#endif\n+\n+Node* LibraryCallKit::box_vector(Node* vector, const TypeInstPtr* vbox_type,\n+                                 BasicType elem_bt, int num_elem) {\n+  assert(EnableVectorSupport, \"\");\n+  const TypeVect* vec_type = TypeVect::make(elem_bt, num_elem);\n+\n+  VectorBoxAllocateNode* alloc = new VectorBoxAllocateNode(C, vbox_type);\n+  set_edges_for_java_call(alloc, \/*must_throw=*\/false, \/*separate_io_proj=*\/true);\n+  make_slow_call_ex(alloc, env()->Throwable_klass(), \/*separate_io_proj=*\/true);\n+  set_i_o(gvn().transform( new ProjNode(alloc, TypeFunc::I_O) ));\n+  set_all_memory(gvn().transform( new ProjNode(alloc, TypeFunc::Memory) ));\n+  Node* ret = gvn().transform(new ProjNode(alloc, TypeFunc::Parms));\n+\n+  assert(check_vbox(vbox_type), \"\");\n+  VectorBoxNode* vbox = new VectorBoxNode(C, ret, vector, vbox_type, vec_type);\n+  return gvn().transform(vbox);\n+}\n+\n+Node* LibraryCallKit::unbox_vector(Node* v, const TypeInstPtr* vbox_type, BasicType elem_bt, int num_elem, bool shuffle_to_vector) {\n+  assert(EnableVectorSupport, \"\");\n+  const TypeInstPtr* vbox_type_v = gvn().type(v)->is_instptr();\n+  if (vbox_type->klass() != vbox_type_v->klass()) {\n+    return NULL; \/\/ arguments don't agree on vector shapes\n+  }\n+  if (vbox_type_v->maybe_null()) {\n+    return NULL; \/\/ no nulls are allowed\n+  }\n+  assert(check_vbox(vbox_type), \"\");\n+  const TypeVect* vec_type = TypeVect::make(elem_bt, num_elem);\n+  Node* unbox = gvn().transform(new VectorUnboxNode(C, vec_type, v, merged_memory(), shuffle_to_vector));\n+  return unbox;\n+}\n+\n+\/\/ public static\n+\/\/ <VM>\n+\/\/ VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+\/\/            VM vm,\n+\/\/            Function<VM, VM> defaultImpl) {\n+\/\/\n+\/\/ public static\n+\/\/ <VM>\n+\/\/ VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+\/\/             VM vm1, VM vm2,\n+\/\/             BiFunction<VM, VM, VM> defaultImpl) {\n+\/\/\n+\/\/ public static\n+\/\/ <VM>\n+\/\/ VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+\/\/              VM vm1, VM vm2, VM vm3,\n+\/\/              TernaryOperation<VM> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_nary_operation(int n) {\n+  const TypeInt* opr              = gvn().type(argument(0))->is_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(3))->is_int();\n+\n+  if (!opr->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int opc = VectorSupport::vop2ideal(opr->get_con(), elem_bt);\n+  int sopc = VectorNode::opcode(opc, elem_bt);\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, is_vector_mask(vbox_klass) ? VecMaskUseAll : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+                    n, sopc, num_elem, type2name(elem_bt),\n+                    is_vector_mask(vbox_klass) ? 1 : 0);\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* opd1 = NULL; Node* opd2 = NULL; Node* opd3 = NULL;\n+  switch (n) {\n+    case 3: {\n+      opd3 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      if (opd3 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v3=%s\",\n+                        NodeClassNames[argument(6)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      \/\/ fall-through\n+    }\n+    case 2: {\n+      opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      if (opd2 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v2=%s\",\n+                        NodeClassNames[argument(5)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      \/\/ fall-through\n+    }\n+    case 1: {\n+      opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+      if (opd1 == NULL) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** unbox failed v1=%s\",\n+                        NodeClassNames[argument(4)->Opcode()]);\n+        }\n+        return false;\n+      }\n+      break;\n+    }\n+    default: fatal(\"unsupported arity: %d\", n);\n+  }\n+\n+  Node* operation = NULL;\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  switch (n) {\n+    case 1:\n+    case 2: {\n+      operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n+      break;\n+    }\n+    case 3: {\n+      operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n+      break;\n+    }\n+    default: fatal(\"unsupported arity: %d\", n);\n+  }\n+  \/\/ Wrap it up in VectorBox to keep object type information.\n+  Node* vbox = box_vector(operation, vbox_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ <Sh extends VectorShuffle<E>,  E>\n+\/\/  Sh ShuffleIota(Class<?> E, Class<?> ShuffleClass, Vector.Species<E> s, int length,\n+\/\/                  int start, int step, int wrap, ShuffleIotaOperation<Sh, E> defaultImpl)\n+bool LibraryCallKit::inline_vector_shuffle_iota() {\n+  const TypeInstPtr* shuffle_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(3))->is_int();\n+  Node* start                     = argument(4);\n+  const TypeInt* start_val        = gvn().type(start)->is_int();\n+  Node* step                      = argument(5);\n+  const TypeInt* step_val         = gvn().type(step)->is_int();\n+  const TypeInt* wrap             = gvn().type(argument(6))->is_int();\n+\n+  if (!vlen->is_con() || !is_power_of_2(vlen->get_con()) ||\n+      shuffle_klass->const_oop() == NULL || !wrap->is_con()) {\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(shuffle_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  int do_wrap = wrap->get_con();\n+  int num_elem = vlen->get_con();\n+  BasicType elem_bt = T_BYTE;\n+\n+  if (num_elem < 4)\n+    return false;\n+\n+  if (!arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed)) {\n+    return false;\n+  }\n+  if (!arch_supports_vector(Op_AddVB, num_elem, elem_bt, VecMaskNotUsed)) {\n+    return false;\n+  }\n+  if (!arch_supports_vector(Op_AndV, num_elem, elem_bt, VecMaskNotUsed)) {\n+    return false;\n+  }\n+  if (!arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    return false;\n+  }\n+  if (!arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUseStore)) {\n+    return false;\n+  }\n+\n+  const Type * type_bt = Type::get_const_basic_type(elem_bt);\n+  const TypeVect * vt  = TypeVect::make(type_bt, num_elem);\n+\n+  Node* res =  gvn().transform(new VectorLoadConstNode(gvn().makecon(TypeInt::ZERO), vt));\n+\n+  if(!step_val->is_con() || !is_power_of_2(step_val->get_con())) {\n+    Node* bcast_step     = gvn().transform(VectorNode::scalar2vector(step, num_elem, type_bt));\n+    res = gvn().transform(VectorNode::make(Op_MulI, res, bcast_step, num_elem, elem_bt));\n+  } else if (step_val->get_con() > 1) {\n+    Node* cnt = gvn().makecon(TypeInt::make(log2_int(step_val->get_con())));\n+    res = gvn().transform(VectorNode::make(Op_LShiftVB, res, cnt, vt));\n+  }\n+\n+  if (!start_val->is_con() || start_val->get_con() != 0) {\n+    Node* bcast_start    = gvn().transform(VectorNode::scalar2vector(start, num_elem, type_bt));\n+    res = gvn().transform(VectorNode::make(Op_AddI, res, bcast_start, num_elem, elem_bt));\n+  }\n+\n+  Node * mod_val = gvn().makecon(TypeInt::make(num_elem-1));\n+  Node * bcast_mod  = gvn().transform(VectorNode::scalar2vector(mod_val, num_elem, type_bt));\n+  if(do_wrap)  {\n+    \/\/ Wrap the indices greater than lane count.\n+    res = gvn().transform(VectorNode::make(Op_AndI, res, bcast_mod, num_elem, elem_bt));\n+  } else {\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(1));\n+    Node * lane_cnt  = gvn().makecon(TypeInt::make(num_elem));\n+    Node * bcast_lane_cnt = gvn().transform(VectorNode::scalar2vector(lane_cnt, num_elem, type_bt));\n+    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vt));\n+\n+    \/\/ Make the indices greater than lane count as -ve values. This matches the java side implementation.\n+    res = gvn().transform(VectorNode::make(Op_AndI, res, bcast_mod, num_elem, elem_bt));\n+    Node * biased_val = gvn().transform(VectorNode::make(Op_SubI, res, bcast_lane_cnt, num_elem, elem_bt));\n+    res = gvn().transform(new VectorBlendNode(biased_val, res, mask));\n+  }\n+\n+  ciKlass* sbox_klass = shuffle_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* shuffle_box_type = TypeInstPtr::make_exact(TypePtr::NotNull, sbox_klass);\n+\n+  \/\/ Wrap it up in VectorBox to keep object type information.\n+  res = box_vector(res, shuffle_box_type, elem_bt, num_elem);\n+  set_result(res);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ <VM ,Sh extends VectorShuffle<E>, E>\n+\/\/ VM shuffleToVector(Class<VM> VecClass, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n+\/\/                    ShuffleToVectorOperation<VM,Sh,E> defaultImpl)\n+bool LibraryCallKit::inline_vector_shuffle_to_vector() {\n+  const TypeInstPtr* vector_klass  = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* elem_klass    = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* shuffle_klass = gvn().type(argument(2))->is_instptr();\n+  Node* shuffle                    = argument(3);\n+  const TypeInt* vlen              = gvn().type(argument(4))->is_int();\n+\n+  if (!vlen->is_con() || vector_klass->const_oop() == NULL || shuffle_klass->const_oop() == NULL) {\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(shuffle_klass) || !is_klass_initialized(vector_klass) ) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  int num_elem = vlen->get_con();\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  BasicType elem_bt = elem_type->basic_type();\n+\n+  if (num_elem < 4) {\n+    return false;\n+  }\n+\n+  int cast_vopc = VectorCastNode::opcode(T_BYTE); \/\/ from shuffle of type T_BYTE\n+  \/\/ Make sure that cast is implemented to particular type\/size combination.\n+  if (!arch_supports_vector(cast_vopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=cast#%d\/3 vlen2=%d etype2=%s\",\n+        cast_vopc, num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  ciKlass* sbox_klass = shuffle_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* shuffle_box_type = TypeInstPtr::make_exact(TypePtr::NotNull, sbox_klass);\n+\n+  \/\/ Unbox shuffle with true flag to indicate its load shuffle to vector\n+  Node* shuffle_vec = unbox_vector(shuffle, shuffle_box_type, elem_bt, num_elem, true);\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vec_box_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  \/\/ Box vector\n+  Node* res = box_vector(shuffle_vec, vec_box_type, elem_bt, num_elem);\n+  set_result(res);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ <V extends Vector<?,?>>\n+\/\/ V broadcastCoerced(Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/                    long bits,\n+\/\/                    LongFunction<V> defaultImpl)\n+bool LibraryCallKit::inline_vector_broadcast_coerced() {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(1))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(2))->is_int();\n+\n+  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt,\n+                            (is_vector_mask(vbox_klass) ? VecMaskUseStore : VecMaskNotUsed), true \/*has_scalar_args*\/)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=0 op=broadcast vlen=%d etype=%s ismask=%d\",\n+                    num_elem, type2name(elem_bt),\n+                    is_vector_mask(vbox_klass) ? 1 : 0);\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  Node* bits = argument(3); \/\/ long\n+\n+  Node* elem = NULL;\n+  switch (elem_bt) {\n+    case T_BOOLEAN: \/\/ fall-through\n+    case T_BYTE:    \/\/ fall-through\n+    case T_SHORT:   \/\/ fall-through\n+    case T_CHAR:    \/\/ fall-through\n+    case T_INT: {\n+      elem = gvn().transform(new ConvL2INode(bits));\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      elem = gvn().transform(new MoveL2DNode(bits));\n+      break;\n+    }\n+    case T_FLOAT: {\n+      bits = gvn().transform(new ConvL2INode(bits));\n+      elem = gvn().transform(new MoveI2FNode(bits));\n+      break;\n+    }\n+    case T_LONG: {\n+      elem = bits; \/\/ no conversion needed\n+      break;\n+    }\n+    default: fatal(\"%s\", type2name(elem_bt));\n+  }\n+\n+  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n+  broadcast = gvn().transform(broadcast);\n+\n+  Node* box = box_vector(broadcast, vbox_type, elem_bt, num_elem);\n+  set_result(box);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/    <C, V extends Vector<?,?>>\n+\/\/    V load(Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/           Object base, long offset,\n+\/\/           \/* Vector.Mask<E,S> m*\/\n+\/\/           Object container, int index,\n+\/\/           LoadOperation<C, VM> defaultImpl) {\n+\/\/\n+\/\/    <C, V extends Vector<?,?>>\n+\/\/    void store(Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/               Object base, long offset,\n+\/\/               V v, \/*Vector.Mask<E,S> m*\/\n+\/\/               Object container, int index,\n+\/\/               StoreVectorOperation<C, V> defaultImpl) {\n+\n+bool LibraryCallKit::inline_vector_mem_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(1))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(2))->is_int();\n+\n+  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(is_store ? Op_StoreVector : Op_LoadVector, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=no\",\n+                    is_store, is_store ? \"store\" : \"load\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  bool is_mask = is_vector_mask(vbox_klass);\n+\n+  Node* base = argument(3);\n+  Node* offset = ConvL2X(argument(4));\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS;\n+  Node* addr = make_unsafe_address(base, offset, decorators, (is_mask ? T_BOOLEAN : elem_bt), true);\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+\n+  \/\/ Now handle special case where load\/store happens from\/to byte array but element type is not byte.\n+  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  \/\/ Handle loading masks.\n+  \/\/ If there is no consistency between array and vector element types, it must be special byte array case or loading masks\n+  if (arr_type != NULL && !using_byte_array && elem_bt != arr_type->elem()->array_element_basic_type() && !is_mask) {\n+    return false;\n+  }\n+  \/\/ Since we are using byte array, we need to double check that the byte operations are supported by backend.\n+  if (using_byte_array) {\n+    int byte_num_elem = num_elem * type2aelembytes(elem_bt);\n+    if (!arch_supports_vector(is_store ? Op_StoreVector : Op_LoadVector, byte_num_elem, T_BYTE, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d*8 etype=%s\/8 ismask=no\",\n+                      is_store, is_store ? \"store\" : \"load\",\n+                      byte_num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+  }\n+  if (is_mask) {\n+    if (!arch_supports_vector(Op_LoadVector, num_elem, T_BOOLEAN, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s\/mask vlen=%d etype=bit ismask=no\",\n+                      is_store, is_store ? \"store\" : \"load\",\n+                      num_elem);\n+      }\n+      return false; \/\/ not supported\n+    }\n+    if (!is_store) {\n+      if (!arch_supports_vector(Op_LoadVector, num_elem, elem_bt, VecMaskUseLoad)) {\n+        return false; \/\/ not supported\n+      }\n+    } else {\n+         if (!arch_supports_vector(Op_StoreVector, num_elem, elem_bt, VecMaskUseStore)) {\n+           return false; \/\/ not supported\n+         }\n+    }\n+  }\n+\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      return false; \/\/ operand unboxing failed\n+    }\n+    set_all_memory(reset_memory());\n+\n+    \/\/ In case the store needs to happen to byte array, reinterpret the incoming vector to byte vector.\n+    int store_num_elem = num_elem;\n+    if (using_byte_array) {\n+      store_num_elem = num_elem * type2aelembytes(elem_bt);\n+      const TypeVect* to_vect_type = TypeVect::make(T_BYTE, store_num_elem);\n+      val = gvn().transform(new VectorReinterpretNode(val, val->bottom_type()->is_vect(), to_vect_type));\n+    }\n+\n+    Node* vstore = gvn().transform(StoreVectorNode::make(0, control(), memory(addr), addr, addr_type, val, store_num_elem));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    \/\/ When using byte array, we need to load as byte then reinterpret the value. Otherwise, do a simple vector load.\n+    Node* vload = NULL;\n+    if (using_byte_array) {\n+      int load_num_elem = num_elem * type2aelembytes(elem_bt);\n+      vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, load_num_elem, T_BYTE));\n+      const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+      vload = gvn().transform(new VectorReinterpretNode(vload, vload->bottom_type()->is_vect(), to_vect_type));\n+    } else {\n+      \/\/ Special handle for masks\n+      if (is_mask) {\n+          vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, num_elem, T_BOOLEAN));\n+          const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+          vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n+      } else {\n+          vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, num_elem, elem_bt));\n+      }\n+    }\n+    Node* box = box_vector(vload, vbox_type, elem_bt, num_elem);\n+    set_result(box);\n+  }\n+\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/   <C, V extends Vector<?>, W extends IntVector, E, S extends VectorSpecies<E>>\n+\/\/   void loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n+\/\/                    Object base, long offset, \/\/ Unsafe addressing\n+\/\/                    W index_vector,\n+\/\/                    C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+\/\/                    LoadVectorOperationWithMap<C, V, E, S> defaultImpl)\n+\/\/\n+\/\/    <C, V extends Vector<?>, W extends IntVector>\n+\/\/    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n+\/\/                      Object base, long offset,    \/\/ Unsafe addressing\n+\/\/                      W index_vector, V v,\n+\/\/                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n+\/\/                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_gather_scatter(bool is_scatter) {\n+  const TypeInstPtr* vector_klass     = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* elem_klass       = gvn().type(argument(1))->is_instptr();\n+  const TypeInt* vlen                 = gvn().type(argument(2))->is_int();\n+  const TypeInstPtr* vector_idx_klass = gvn().type(argument(3))->is_instptr();\n+\n+  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || vector_idx_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s etype=%s vlen=%s viclass=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+\n+  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(vector_idx_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=no\",\n+                    is_scatter, is_scatter ? \"scatter\" : \"gather\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  \/\/ Check that the vector holding indices is supported by architecture\n+  if (!arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int ismask=no\",\n+                      is_scatter, is_scatter ? \"scatter\" : \"gather\",\n+                      num_elem);\n+      }\n+      return false; \/\/ not supported\n+    }\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+  Node* addr = make_unsafe_address(base, offset, C2_UNSAFE_ACCESS, elem_bt, true);\n+\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+\n+  \/\/ The array must be consistent with vector type\n+  if (arr_type == NULL || (arr_type != NULL && elem_bt != arr_type->elem()->array_element_basic_type())) {\n+    return false;\n+  }\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  ciKlass* vbox_idx_klass = vector_idx_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+\n+  if (vbox_idx_klass == NULL) {\n+    return false;\n+  }\n+\n+  const TypeInstPtr* vbox_idx_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_idx_klass);\n+\n+  Node* index_vect = unbox_vector(argument(7), vbox_idx_type, T_INT, num_elem);\n+  if (index_vect == NULL) {\n+    return false;\n+  }\n+  const TypeVect* vector_type = TypeVect::make(elem_bt, num_elem);\n+  if (is_scatter) {\n+    Node* val = unbox_vector(argument(8), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      return false; \/\/ operand unboxing failed\n+    }\n+    set_all_memory(reset_memory());\n+\n+    Node* vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    Node* vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+\n+    Node* box = box_vector(vload, vbox_type, elem_bt, num_elem);\n+    set_result(box);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ <V extends Vector<?,?>>\n+\/\/ long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/                       V v,\n+\/\/                       Function<V,Long> defaultImpl)\n+\n+bool LibraryCallKit::inline_vector_reduction() {\n+  const TypeInt* opr              = gvn().type(argument(0))->is_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(3))->is_int();\n+\n+  if (!opr->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  int opc  = VectorSupport::vop2ideal(opr->get_con(), elem_bt);\n+  int sopc = ReductionNode::opcode(opc, elem_bt);\n+\n+  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s ismask=no\",\n+                    sopc, num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  Node* opd = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  if (opd == NULL) {\n+    return false; \/\/ operand unboxing failed\n+  }\n+\n+  Node* init = ReductionNode::make_reduction_input(gvn(), opc, elem_bt);\n+  Node* rn = gvn().transform(ReductionNode::make(opc, NULL, init, opd, elem_bt));\n+\n+  Node* bits = NULL;\n+  switch (elem_bt) {\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT: {\n+      bits = gvn().transform(new ConvI2LNode(rn));\n+      break;\n+    }\n+    case T_FLOAT: {\n+      rn   = gvn().transform(new MoveF2INode(rn));\n+      bits = gvn().transform(new ConvI2LNode(rn));\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      bits = gvn().transform(new MoveD2LNode(rn));\n+      break;\n+    }\n+    case T_LONG: {\n+      bits = rn; \/\/ no conversion needed\n+      break;\n+    }\n+    default: fatal(\"%s\", type2name(elem_bt));\n+  }\n+  set_result(bits);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static <V> boolean test(int cond, Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/                                V v1, V v2,\n+\/\/                                BiFunction<V, V, Boolean> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_test() {\n+  const TypeInt* cond             = gvn().type(argument(0))->is_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(3))->is_int();\n+\n+  if (!cond->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: cond=%s vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  BoolTest::mask booltest = (BoolTest::mask)cond->get_con();\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  if (!arch_supports_vector(Op_VectorTest, num_elem, elem_bt, is_vector_mask(vbox_klass) ? VecMaskUseLoad : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=test\/%d vlen=%d etype=%s ismask=%d\",\n+                    cond->get_con(), num_elem, type2name(elem_bt),\n+                    is_vector_mask(vbox_klass));\n+    }\n+    return false;\n+  }\n+\n+  Node* opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  if (opd1 == NULL || opd2 == NULL) {\n+    return false; \/\/ operand unboxing failed\n+  }\n+  Node* test = new VectorTestNode(opd1, opd2, booltest);\n+  test = gvn().transform(test);\n+\n+  set_result(test);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static\n+\/\/ <V extends Vector, M extends Mask>\n+\/\/ V blend(Class<V> vectorClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+\/\/         V v1, V v2, M m,\n+\/\/         VectorBlendOp<V,M> defaultImpl) { ...\n+\/\/\n+bool LibraryCallKit::inline_vector_blend() {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->is_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->is_int();\n+\n+  if (mask_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  BasicType mask_bt = elem_bt;\n+  int num_elem = vlen->get_con();\n+\n+  if (!arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=blend vlen=%d etype=%s ismask=useload\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* v1   = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* v2   = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* mask = unbox_vector(argument(6), mbox_type, mask_bt, num_elem);\n+\n+  if (v1 == NULL || v2 == NULL || mask == NULL) {\n+    return false; \/\/ operand unboxing failed\n+  }\n+\n+  Node* blend = gvn().transform(new VectorBlendNode(v1, v2, mask));\n+\n+  Node* box = box_vector(blend, vbox_type, elem_bt, num_elem);\n+  set_result(box);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/  public static <V extends Vector<E,S>,\n+\/\/          M extends Vector.Mask<E,S>,\n+\/\/          S extends Vector.Shape, E>\n+\/\/  M compare(int cond, Class<V> vectorClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+\/\/            V v1, V v2,\n+\/\/            VectorCompareOp<V,M> defaultImpl) { ...\n+\/\/\n+bool LibraryCallKit::inline_vector_compare() {\n+  const TypeInt*     cond         = gvn().type(argument(0))->is_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->is_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->is_int();\n+\n+  if (!cond->is_con() || vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: cond=%s vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  int num_elem = vlen->get_con();\n+  BasicType elem_bt = elem_type->basic_type();\n+  BasicType mask_bt = elem_bt;\n+\n+  if (!arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUseStore)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* v1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* v2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+\n+  if (v1 == NULL || v2 == NULL) {\n+    return false; \/\/ operand unboxing failed\n+  }\n+  BoolTest::mask pred = (BoolTest::mask)cond->get_con();\n+  ConINode* pred_node = (ConINode*)gvn().makecon(cond);\n+\n+  const TypeVect* vt = TypeVect::make(mask_bt, num_elem);\n+  Node* operation = gvn().transform(new VectorMaskCmpNode(pred, v1, v2, pred_node, vt));\n+\n+  Node* box = box_vector(operation, mbox_type, mask_bt, num_elem);\n+  set_result(box);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static\n+\/\/ <V extends Vector, Sh extends Shuffle>\n+\/\/  V rearrangeOp(Class<V> vectorClass, Class<Sh> shuffleClass, Class< ? > elementType, int vlen,\n+\/\/    V v1, Sh sh,\n+\/\/    VectorSwizzleOp<V, Sh, S, E> defaultImpl) { ...\n+\n+bool LibraryCallKit::inline_vector_rearrange() {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* shuffle_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass = gvn().type(argument(2))->is_instptr();\n+  const TypeInt*     vlen = gvn().type(argument(3))->is_int();\n+\n+  if (shuffle_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n+    elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s sclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(shuffle_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  BasicType shuffle_bt = elem_bt;\n+  int num_elem = vlen->get_con();\n+\n+  if (!arch_supports_vector(Op_VectorLoadShuffle, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=0 op=load\/shuffle vlen=%d etype=%s ismask=no\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  ciKlass* shbox_klass = shuffle_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* shbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, shbox_klass);\n+\n+  Node* v1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* shuffle = unbox_vector(argument(5), shbox_type, shuffle_bt, num_elem);\n+\n+  if (v1 == NULL || shuffle == NULL) {\n+    return false; \/\/ operand unboxing failed\n+  }\n+\n+  Node* rearrange = gvn().transform(new VectorRearrangeNode(v1, shuffle));\n+\n+  Node* box = box_vector(rearrange, vbox_type, elem_bt, num_elem);\n+  set_result(box);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+Node* LibraryCallKit::shift_count(Node* cnt, int shift_op, BasicType bt, int num_elem) {\n+  assert(bt == T_INT || bt == T_LONG || bt == T_SHORT || bt == T_BYTE, \"byte, short, long and int are supported\");\n+  juint mask = (type2aelembytes(bt) * BitsPerByte - 1);\n+  Node* nmask = gvn().transform(ConNode::make(TypeInt::make(mask)));\n+  Node* mcnt = gvn().transform(new AndINode(cnt, nmask));\n+  return gvn().transform(VectorNode::shift_count(shift_op, mcnt, num_elem, bt));\n+}\n+\n+\/\/  public static\n+\/\/  <V extends Vector<?,?>>\n+\/\/  V broadcastInt(int opr, Class<V> vectorClass, Class<?> elementType, int vlen,\n+\/\/                 V v, int i,\n+\/\/                 VectorBroadcastIntOp<V> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_broadcast_int() {\n+  const TypeInt* opr              = gvn().type(argument(0))->is_int();\n+  const TypeInstPtr* vector_klass = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(3))->is_int();\n+\n+  if (!opr->is_con() || vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int opc = VectorSupport::vop2ideal(opr->get_con(), elem_bt);\n+  int sopc = VectorNode::opcode(opc, elem_bt);\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, true \/*has_scalar_args*\/)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s ismask=no\",\n+                    sopc, num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+  Node* opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* opd2 = shift_count(argument(5), opc, elem_bt, num_elem);\n+  if (opd1 == NULL || opd2 == NULL) {\n+    return false;\n+  }\n+  Node* operation = gvn().transform(VectorNode::make(opc, opd1, opd2, num_elem, elem_bt));\n+\n+  Node* vbox = box_vector(operation, vbox_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static <VOUT extends VectorPayload,\n+\/\/                 VIN extends VectorPayload,\n+\/\/                   S extends VectorSpecies>\n+\/\/ VOUT convert(int oprId,\n+\/\/           Class<?> fromVectorClass, Class<?> fromElementType, int fromVLen,\n+\/\/           Class<?>   toVectorClass, Class<?>   toElementType, int   toVLen,\n+\/\/           VIN v, S s,\n+\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_convert() {\n+  const TypeInt*     opr               = gvn().type(argument(0))->is_int();\n+\n+  const TypeInstPtr* vector_klass_from = gvn().type(argument(1))->is_instptr();\n+  const TypeInstPtr* elem_klass_from   = gvn().type(argument(2))->is_instptr();\n+  const TypeInt*     vlen_from         = gvn().type(argument(3))->is_int();\n+\n+  const TypeInstPtr* vector_klass_to   = gvn().type(argument(4))->is_instptr();\n+  const TypeInstPtr* elem_klass_to     = gvn().type(argument(5))->is_instptr();\n+  const TypeInt*     vlen_to           = gvn().type(argument(6))->is_int();\n+\n+  if (!opr->is_con() ||\n+      vector_klass_from->const_oop() == NULL || elem_klass_from->const_oop() == NULL || !vlen_from->is_con() ||\n+      vector_klass_to->const_oop() == NULL || elem_klass_to->const_oop() == NULL || !vlen_to->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: opr=%s vclass_from=%s etype_from=%s vlen_from=%s vclass_to=%s etype_to=%s vlen_to=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()],\n+                    NodeClassNames[argument(5)->Opcode()],\n+                    NodeClassNames[argument(6)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass_from) || !is_klass_initialized(vector_klass_to)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  assert(opr->get_con() == VectorSupport::VECTOR_OP_CAST ||\n+         opr->get_con() == VectorSupport::VECTOR_OP_REINTERPRET, \"wrong opcode\");\n+  bool is_cast = (opr->get_con() == VectorSupport::VECTOR_OP_CAST);\n+\n+  ciKlass* vbox_klass_from = vector_klass_from->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* vbox_klass_to = vector_klass_to->const_oop()->as_instance()->java_lang_Class_klass();\n+  if (is_vector_shuffle(vbox_klass_from) || is_vector_shuffle(vbox_klass_to)) {\n+    return false; \/\/ vector shuffles aren't supported\n+  }\n+  bool is_mask = is_vector_mask(vbox_klass_from);\n+\n+  ciType* elem_type_from = elem_klass_from->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type_from->is_primitive_type()) {\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt_from = elem_type_from->basic_type();\n+  ciType* elem_type_to = elem_klass_to->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type_to->is_primitive_type()) {\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt_to = elem_type_to->basic_type();\n+  if (is_mask && elem_bt_from != elem_bt_to) {\n+    return false; \/\/ type mismatch\n+  }\n+  int num_elem_from = vlen_from->get_con();\n+  int num_elem_to = vlen_to->get_con();\n+\n+  \/\/ Check whether we can unbox to appropriate size. Even with casting, checking for reinterpret is needed\n+  \/\/ since we may need to change size.\n+  if (!arch_supports_vector(Op_VectorReinterpret,\n+                            num_elem_from,\n+                            elem_bt_from,\n+                            is_mask ? VecMaskUseAll : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%s\/1 vlen1=%d etype1=%s ismask=%d\",\n+                    is_cast ? \"cast\" : \"reinterpret\",\n+                    num_elem_from, type2name(elem_bt_from), is_mask);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Check whether we can support resizing\/reinterpreting to the new size.\n+  if (!arch_supports_vector(Op_VectorReinterpret,\n+                            num_elem_to,\n+                            elem_bt_to,\n+                            is_mask ? VecMaskUseAll : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%s\/2 vlen2=%d etype2=%s ismask=%d\",\n+                    is_cast ? \"cast\" : \"reinterpret\",\n+                    num_elem_to, type2name(elem_bt_to), is_mask);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ At this point, we know that both input and output vector registers are supported\n+  \/\/ by the architecture. Next check if the casted type is simply to same type - which means\n+  \/\/ that it is actually a resize and not a cast.\n+  if (is_cast && elem_bt_from == elem_bt_to) {\n+    is_cast = false;\n+  }\n+\n+  const TypeInstPtr* vbox_type_from = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass_from);\n+\n+  Node* opd1 = unbox_vector(argument(7), vbox_type_from, elem_bt_from, num_elem_from);\n+  if (opd1 == NULL) {\n+    return false;\n+  }\n+\n+  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from);\n+  const TypeVect* dst_type = TypeVect::make(elem_bt_to,   num_elem_to);\n+\n+  Node* op = opd1;\n+  if (is_cast) {\n+    assert(!is_mask, \"masks cannot be casted\");\n+    int cast_vopc = VectorCastNode::opcode(elem_bt_from);\n+    \/\/ Make sure that cast is implemented to particular type\/size combination.\n+    if (!arch_supports_vector(cast_vopc, num_elem_to, elem_bt_to, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=1 op=cast#%d\/3 vlen2=%d etype2=%s ismask=%d\",\n+                      cast_vopc,\n+                      num_elem_to, type2name(elem_bt_to), is_mask);\n+      }\n+      return false;\n+    }\n+\n+    if (num_elem_from < num_elem_to) {\n+      \/\/ Since input and output number of elements are not consistent, we need to make sure we\n+      \/\/ properly size. Thus, first make a cast that retains the number of elements from source.\n+      \/\/ In case the size exceeds the arch size, we do the minimum.\n+      int num_elem_for_cast = MIN2(num_elem_from, Matcher::max_vector_size(elem_bt_to));\n+\n+      \/\/ It is possible that arch does not support this intermediate vector size\n+      \/\/ TODO More complex logic required here to handle this corner case for the sizes.\n+      if (!arch_supports_vector(cast_vopc, num_elem_for_cast, elem_bt_to, VecMaskNotUsed)) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** not supported: arity=1 op=cast#%d\/4 vlen1=%d etype2=%s ismask=%d\",\n+                        cast_vopc,\n+                        num_elem_for_cast, type2name(elem_bt_to), is_mask);\n+        }\n+        return false;\n+      }\n+\n+      op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_for_cast));\n+      \/\/ Now ensure that the destination gets properly resized to needed size.\n+      op = gvn().transform(new VectorReinterpretNode(op, op->bottom_type()->is_vect(), dst_type));\n+    } else if (num_elem_from > num_elem_to) {\n+      \/\/ Since number elements from input is larger than output, simply reduce size of input (we are supposed to\n+      \/\/ drop top elements anyway).\n+      int num_elem_for_resize = MAX2(num_elem_to, Matcher::min_vector_size(elem_bt_to));\n+\n+      \/\/ It is possible that arch does not support this intermediate vector size\n+      \/\/ TODO More complex logic required here to handle this corner case for the sizes.\n+      if (!arch_supports_vector(Op_VectorReinterpret,\n+                                num_elem_for_resize,\n+                                elem_bt_from,\n+                                VecMaskNotUsed)) {\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** not supported: arity=1 op=cast\/5 vlen2=%d etype1=%s ismask=%d\",\n+                        num_elem_for_resize, type2name(elem_bt_from), is_mask);\n+        }\n+        return false;\n+      }\n+\n+      op = gvn().transform(new VectorReinterpretNode(op,\n+                                                     src_type,\n+                                                     TypeVect::make(elem_bt_from,\n+                                                                    num_elem_for_resize)));\n+      op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_to));\n+    } else {\n+      \/\/ Since input and output number of elements match, and since we know this vector size is\n+      \/\/ supported, simply do a cast with no resize needed.\n+      op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_to));\n+    }\n+  } else if (Type::cmp(src_type, dst_type) != 0) {\n+    assert(!is_cast, \"must be reinterpret\");\n+    op = gvn().transform(new VectorReinterpretNode(op, src_type, dst_type));\n+  }\n+\n+  const TypeInstPtr* vbox_type_to = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass_to);\n+  Node* vbox = box_vector(op, vbox_type_to, elem_bt_to, num_elem_to);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem_to * type2aelembytes(elem_bt_to))));\n+  return true;\n+}\n+\n+\/\/  public static\n+\/\/  <V extends Vector<?>>\n+\/\/  V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n+\/\/           V vec, int ix, long val,\n+\/\/           VecInsertOp<V> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_insert() {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(1))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(2))->is_int();\n+  const TypeInt* idx              = gvn().type(argument(4))->is_int();\n+\n+  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con() || !idx->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s etype=%s vlen=%s idx=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  if (!arch_supports_vector(Op_VectorInsert, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=insert vlen=%d etype=%s ismask=no\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  Node* opd = unbox_vector(argument(3), vbox_type, elem_bt, num_elem);\n+  if (opd == NULL) {\n+    return false;\n+  }\n+\n+  Node* insert_val = argument(5);\n+  assert(gvn().type(insert_val)->isa_long() != NULL, \"expected to be long\");\n+\n+  \/\/ Convert insert value back to its appropriate type.\n+  switch (elem_bt) {\n+    case T_BYTE:\n+      insert_val = gvn().transform(new ConvL2INode(insert_val));\n+      insert_val = gvn().transform(new CastIINode(insert_val, TypeInt::BYTE));\n+      break;\n+    case T_SHORT:\n+      insert_val = gvn().transform(new ConvL2INode(insert_val));\n+      insert_val = gvn().transform(new CastIINode(insert_val, TypeInt::SHORT));\n+      break;\n+    case T_INT:\n+      insert_val = gvn().transform(new ConvL2INode(insert_val));\n+      break;\n+    case T_FLOAT:\n+      insert_val = gvn().transform(new ConvL2INode(insert_val));\n+      insert_val = gvn().transform(new MoveI2FNode(insert_val));\n+      break;\n+    case T_DOUBLE:\n+      insert_val = gvn().transform(new MoveL2DNode(insert_val));\n+      break;\n+    case T_LONG:\n+      \/\/ no conversion needed\n+      break;\n+    default: fatal(\"%s\", type2name(elem_bt)); break;\n+  }\n+\n+  Node* operation = gvn().transform(VectorInsertNode::make(opd, insert_val, idx->get_con()));\n+\n+  Node* vbox = box_vector(operation, vbox_type, elem_bt, num_elem);\n+  set_result(vbox);\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/  public static\n+\/\/  <V extends Vector<?>>\n+\/\/  long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/               V vec, int ix,\n+\/\/               VecExtractOp<V> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_extract() {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->is_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(1))->is_instptr();\n+  const TypeInt* vlen             = gvn().type(argument(2))->is_int();\n+  const TypeInt* idx              = gvn().type(argument(4))->is_int();\n+\n+  if (vector_klass->const_oop() == NULL || elem_klass->const_oop() == NULL || !vlen->is_con() || !idx->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s etype=%s vlen=%s idx=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+  int vopc = ExtractNode::opcode(elem_bt);\n+  if (!arch_supports_vector(vopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=extract vlen=%d etype=%s ismask=no\",\n+                    num_elem, type2name(elem_bt));\n+    }\n+    return false; \/\/ not supported\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+\n+  Node* opd = unbox_vector(argument(3), vbox_type, elem_bt, num_elem);\n+  if (opd == NULL) {\n+    return false;\n+  }\n+\n+  Node* operation = gvn().transform(ExtractNode::make(opd, idx->get_con(), elem_bt));\n+\n+  Node* bits = NULL;\n+  switch (elem_bt) {\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT: {\n+      bits = gvn().transform(new ConvI2LNode(operation));\n+      break;\n+    }\n+    case T_FLOAT: {\n+      bits = gvn().transform(new MoveF2INode(operation));\n+      bits = gvn().transform(new ConvI2LNode(bits));\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      bits = gvn().transform(new MoveD2LNode(operation));\n+      break;\n+    }\n+    case T_LONG: {\n+      bits = operation; \/\/ no conversion needed\n+      break;\n+    }\n+    default: fatal(\"%s\", type2name(elem_bt));\n+  }\n+\n+  set_result(bits);\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":1594,"deletions":0,"binary":false,"changes":1594,"status":"added"},{"patch":"@@ -0,0 +1,429 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"jni.h\"\n+#include \"jvm.h\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"code\/location.hpp\"\n+#include \"prims\/vectorSupport.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/stackValue.hpp\"\n+\n+#ifdef COMPILER2\n+#include \"opto\/matcher.hpp\" \/\/ Matcher::max_vector_size(BasicType)\n+#endif \/\/ COMPILER2\n+\n+bool VectorSupport::is_vector(Klass* klass) {\n+  return klass->is_subclass_of(SystemDictionary::vector_VectorPayload_klass());\n+}\n+\n+bool VectorSupport::is_vector_mask(Klass* klass) {\n+  return klass->is_subclass_of(SystemDictionary::vector_VectorMask_klass());\n+}\n+\n+bool VectorSupport::is_vector_shuffle(Klass* klass) {\n+  return klass->is_subclass_of(SystemDictionary::vector_VectorShuffle_klass());\n+}\n+\n+BasicType VectorSupport::klass2bt(InstanceKlass* ik) {\n+  assert(ik->is_subclass_of(SystemDictionary::vector_VectorPayload_klass()), \"%s not a VectorPayload\", ik->name()->as_C_string());\n+  fieldDescriptor fd; \/\/ find_field initializes fd if found\n+  \/\/ static final Class<?> ETYPE;\n+  Klass* holder = ik->find_field(vmSymbols::ETYPE_name(), vmSymbols::class_signature(), &fd);\n+\n+  assert(holder != NULL, \"sanity\");\n+  assert(fd.is_static(), \"\");\n+  assert(fd.offset() > 0, \"\");\n+\n+  if (is_vector_shuffle(ik)) {\n+    return T_BYTE;\n+  } else { \/\/ vector and mask\n+    oop value = ik->java_mirror()->obj_field(fd.offset());\n+    BasicType elem_bt = java_lang_Class::as_BasicType(value);\n+    return elem_bt;\n+  }\n+}\n+\n+jint VectorSupport::klass2length(InstanceKlass* ik) {\n+  fieldDescriptor fd; \/\/ find_field initializes fd if found\n+  \/\/ static final int VLENGTH;\n+  Klass* holder = ik->find_field(vmSymbols::VLENGTH_name(), vmSymbols::int_signature(), &fd);\n+\n+  assert(holder != NULL, \"sanity\");\n+  assert(fd.is_static(), \"\");\n+  assert(fd.offset() > 0, \"\");\n+\n+  jint vlen = ik->java_mirror()->int_field(fd.offset());\n+  assert(vlen > 0, \"\");\n+  return vlen;\n+}\n+\n+void VectorSupport::init_vector_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr) {\n+  int elem_size = type2aelembytes(elem_bt);\n+  for (int i = 0; i < num_elem; i++) {\n+    switch (elem_bt) {\n+      case T_BYTE: {\n+        jbyte elem_value = *(jbyte*) (value_addr + i * elem_size);\n+        arr->byte_at_put(i, elem_value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        jshort elem_value = *(jshort*) (value_addr + i * elem_size);\n+        arr->short_at_put(i, elem_value);\n+        break;\n+      }\n+      case T_INT: {\n+        jint elem_value = *(jint*) (value_addr + i * elem_size);\n+        arr->int_at_put(i, elem_value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong elem_value = *(jlong*) (value_addr + i * elem_size);\n+        arr->long_at_put(i, elem_value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        jfloat elem_value = *(jfloat*) (value_addr + i * elem_size);\n+        arr->float_at_put(i, elem_value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        jdouble elem_value = *(jdouble*) (value_addr + i * elem_size);\n+        arr->double_at_put(i, elem_value);\n+        break;\n+      }\n+      default:\n+        fatal(\"unsupported: %s\", type2name(elem_bt));\n+    }\n+  }\n+}\n+\n+void VectorSupport::init_mask_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr) {\n+  int elem_size = type2aelembytes(elem_bt);\n+\n+  for (int i = 0; i < num_elem; i++) {\n+    switch (elem_bt) {\n+      case T_BYTE: {\n+        jbyte elem_value = *(jbyte*) (value_addr + i * elem_size);\n+        arr->bool_at_put(i, elem_value != 0);\n+        break;\n+      }\n+      case T_SHORT: {\n+        jshort elem_value = *(jshort*) (value_addr + i * elem_size);\n+        arr->bool_at_put(i, elem_value != 0);\n+        break;\n+      }\n+      case T_INT:   \/\/ fall-through\n+      case T_FLOAT: {\n+        jint elem_value = *(jint*) (value_addr + i * elem_size);\n+        arr->bool_at_put(i, elem_value != 0);\n+        break;\n+      }\n+      case T_LONG: \/\/ fall-through\n+      case T_DOUBLE: {\n+        jlong elem_value = *(jlong*) (value_addr + i * elem_size);\n+        arr->bool_at_put(i, elem_value != 0);\n+        break;\n+      }\n+      default:\n+        fatal(\"unsupported: %s\", type2name(elem_bt));\n+    }\n+  }\n+}\n+\n+oop VectorSupport::allocate_vector_payload_helper(InstanceKlass* ik, BasicType elem_bt, int num_elem, address value_addr, TRAPS) {\n+\n+  bool is_mask = is_vector_mask(ik);\n+\n+  \/\/ On-heap vector values are represented as primitive arrays.\n+  TypeArrayKlass* tak = TypeArrayKlass::cast(Universe::typeArrayKlassObj(is_mask ? T_BOOLEAN : elem_bt));\n+\n+  typeArrayOop arr = tak->allocate(num_elem, CHECK_NULL); \/\/ safepoint\n+\n+  if (is_mask) {\n+    init_mask_array(arr, elem_bt, num_elem, value_addr);\n+  } else {\n+    init_vector_array(arr, elem_bt, num_elem, value_addr);\n+  }\n+  return arr;\n+}\n+\n+oop VectorSupport::allocate_vector(InstanceKlass* ik, frame* fr, RegisterMap* reg_map, ObjectValue* ov, TRAPS) {\n+  assert(is_vector(ik), \"%s not a vector\", ik->name()->as_C_string());\n+  assert(ov->field_size() == 1, \"%s not a vector\", ik->name()->as_C_string());\n+\n+  \/\/ Vector value in an aligned adjacent tuple (1, 2, 4, 8, or 16 slots).\n+  LocationValue* loc_value = ov->field_at(0)->as_LocationValue();\n+\n+  BasicType elem_bt = klass2bt(ik);\n+  int num_elem = klass2length(ik);\n+\n+  Handle vbox = ik->allocate_instance_handle(CHECK_NULL);\n+\n+  Location loc = loc_value->location();\n+\n+  oop payload = NULL;\n+  if (loc.type() == Location::vector) {\n+    address value_addr = loc.is_register()\n+        \/\/ Value was in a callee-save register\n+        ? reg_map->location(VMRegImpl::as_VMReg(loc.register_number()))\n+        \/\/ Else value was directly saved on the stack. The frame's original stack pointer,\n+        \/\/ before any extension by its callee (due to Compiler1 linkage on SPARC), must be used.\n+        : ((address)fr->unextended_sp()) + loc.stack_offset();\n+    payload = allocate_vector_payload_helper(ik, elem_bt, num_elem, value_addr, CHECK_NULL); \/\/ safepoint\n+  } else {\n+    \/\/ assert(false, \"interesting\");\n+    StackValue* value = StackValue::create_stack_value(fr, reg_map, loc_value);\n+    payload = value->get_obj()();\n+  }\n+  vector_VectorPayload::set_payload(vbox(), payload);\n+  return vbox();\n+}\n+\n+#ifdef COMPILER2\n+int VectorSupport::vop2ideal(jint id, BasicType bt) {\n+  VectorOperation vop = (VectorOperation)id;\n+  switch (vop) {\n+    case VECTOR_OP_ADD: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_AddI;\n+        case T_LONG:   return Op_AddL;\n+        case T_FLOAT:  return Op_AddF;\n+        case T_DOUBLE: return Op_AddD;\n+        default: fatal(\"ADD: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_SUB: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_SubI;\n+        case T_LONG:   return Op_SubL;\n+        case T_FLOAT:  return Op_SubF;\n+        case T_DOUBLE: return Op_SubD;\n+        default: fatal(\"SUB: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_MUL: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_MulI;\n+        case T_LONG:   return Op_MulL;\n+        case T_FLOAT:  return Op_MulF;\n+        case T_DOUBLE: return Op_MulD;\n+        default: fatal(\"MUL: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_DIV: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_DivI;\n+        case T_LONG:   return Op_DivL;\n+        case T_FLOAT:  return Op_DivF;\n+        case T_DOUBLE: return Op_DivD;\n+        default: fatal(\"DIV: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_MIN: {\n+      switch (bt) {\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:    return Op_MinI;\n+        case T_LONG:   return Op_MinL;\n+        case T_FLOAT:  return Op_MinF;\n+        case T_DOUBLE: return Op_MinD;\n+        default: fatal(\"MIN: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_MAX: {\n+      switch (bt) {\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_INT:    return Op_MaxI;\n+        case T_LONG:   return Op_MaxL;\n+        case T_FLOAT:  return Op_MaxF;\n+        case T_DOUBLE: return Op_MaxD;\n+        default: fatal(\"MAX: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_ABS: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_AbsI;\n+        case T_LONG:   return Op_AbsL;\n+        case T_FLOAT:  return Op_AbsF;\n+        case T_DOUBLE: return Op_AbsD;\n+        default: fatal(\"ABS: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_NEG: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_NegI;\n+        case T_FLOAT:  return Op_NegF;\n+        case T_DOUBLE: return Op_NegD;\n+        default: fatal(\"NEG: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_AND: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_AndI;\n+        case T_LONG:   return Op_AndL;\n+        default: fatal(\"AND: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_OR: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_OrI;\n+        case T_LONG:   return Op_OrL;\n+        default: fatal(\"OR: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_XOR: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:    return Op_XorI;\n+        case T_LONG:   return Op_XorL;\n+        default: fatal(\"XOR: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_SQRT: {\n+      switch (bt) {\n+        case T_FLOAT:  return Op_SqrtF;\n+        case T_DOUBLE: return Op_SqrtD;\n+        default: fatal(\"SQRT: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_FMA: {\n+      switch (bt) {\n+        case T_FLOAT:  return Op_FmaF;\n+        case T_DOUBLE: return Op_FmaD;\n+        default: fatal(\"FMA: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_LSHIFT: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:  return Op_LShiftI;\n+        case T_LONG: return Op_LShiftL;\n+        default: fatal(\"LSHIFT: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_RSHIFT: {\n+      switch (bt) {\n+        case T_BYTE:   \/\/ fall-through\n+        case T_SHORT:  \/\/ fall-through\n+        case T_INT:  return Op_RShiftI;\n+        case T_LONG: return Op_RShiftL;\n+        default: fatal(\"RSHIFT: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    case VECTOR_OP_URSHIFT: {\n+      switch (bt) {\n+        case T_BYTE:  return Op_URShiftB;\n+        case T_SHORT: return Op_URShiftS;\n+        case T_INT:   return Op_URShiftI;\n+        case T_LONG:  return Op_URShiftL;\n+        default: fatal(\"URSHIFT: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n+    default: fatal(\"unknown op: %d\", vop);\n+  }\n+  return 0; \/\/ Unimplemented\n+}\n+#endif \/\/ COMPILER2\n+\n+\/**\n+ * Implementation of the jdk.internal.vm.vector.VectorSupport class\n+ *\/\n+\n+JVM_ENTRY(jint, VectorSupport_GetMaxLaneCount(JNIEnv *env, jclass vsclazz, jobject clazz)) {\n+#ifdef COMPILER2\n+  oop mirror = JNIHandles::resolve_non_null(clazz);\n+  if (java_lang_Class::is_primitive(mirror)) {\n+    BasicType bt = java_lang_Class::primitive_type(mirror);\n+    return Matcher::max_vector_size(bt);\n+  }\n+#endif \/\/ COMPILER2\n+  return -1;\n+} JVM_END\n+\n+\/\/ JVM_RegisterVectorSupportMethods\n+\n+#define LANG \"Ljava\/lang\/\"\n+#define CLS LANG \"Class;\"\n+\n+#define CC (char*)  \/*cast a literal from (const char*)*\/\n+#define FN_PTR(f) CAST_FROM_FN_PTR(void*, &f)\n+\n+static JNINativeMethod jdk_internal_vm_vector_VectorSupport_methods[] = {\n+    {CC \"getMaxLaneCount\",   CC \"(\" CLS \")I\", FN_PTR(VectorSupport_GetMaxLaneCount)}\n+};\n+\n+#undef CC\n+#undef FN_PTR\n+\n+#undef LANG\n+#undef CLS\n+\n+\/\/ This function is exported, used by NativeLookup.\n+\n+JVM_ENTRY(void, JVM_RegisterVectorSupportMethods(JNIEnv* env, jclass vsclass)) {\n+  ThreadToNativeFromVM ttnfv(thread);\n+\n+  int ok = env->RegisterNatives(vsclass, jdk_internal_vm_vector_VectorSupport_methods, sizeof(jdk_internal_vm_vector_VectorSupport_methods)\/sizeof(JNINativeMethod));\n+  guarantee(ok == 0, \"register jdk.internal.vm.vector.VectorSupport natives\");\n+} JVM_END\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.cpp","additions":429,"deletions":0,"binary":false,"changes":429,"status":"added"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_PRIMS_VECTORSUPPORT_HPP\n+#define SHARE_PRIMS_VECTORSUPPORT_HPP\n+\n+#include \"jni.h\"\n+#include \"code\/debugInfo.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/registerMap.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+\n+extern \"C\" {\n+  void JNICALL JVM_RegisterVectorSupportMethods(JNIEnv* env, jclass vsclass);\n+}\n+\n+class VectorSupport : AllStatic {\n+ private:\n+  static void init_mask_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr);\n+  static void init_vector_array(typeArrayOop arr, BasicType elem_bt, int num_elem, address value_addr);\n+  static oop  allocate_vector_payload_helper(InstanceKlass* ik, BasicType elem_bt, int num_elem, address value_addr, TRAPS);\n+\n+  static BasicType klass2bt(InstanceKlass* ik);\n+  static jint klass2length(InstanceKlass* ik);\n+\n+ public:\n+\n+   \/\/ Should be aligned with constants in jdk.internal.vm.vector.VectorSupport\n+  enum VectorOperation {\n+    \/\/ Unary\n+    VECTOR_OP_ABS     = 0,\n+    VECTOR_OP_NEG     = 1,\n+    VECTOR_OP_SQRT    = 2,\n+\n+    \/\/ Binary\n+    VECTOR_OP_ADD     = 4,\n+    VECTOR_OP_SUB     = 5,\n+    VECTOR_OP_MUL     = 6,\n+    VECTOR_OP_DIV     = 7,\n+    VECTOR_OP_MIN     = 8,\n+    VECTOR_OP_MAX     = 9,\n+    VECTOR_OP_AND     = 10,\n+    VECTOR_OP_OR      = 11,\n+    VECTOR_OP_XOR     = 12,\n+\n+    \/\/ Ternary\n+    VECTOR_OP_FMA     = 13,\n+\n+    \/\/ Broadcast int\n+    VECTOR_OP_LSHIFT  = 14,\n+    VECTOR_OP_RSHIFT  = 15,\n+    VECTOR_OP_URSHIFT = 16,\n+\n+    \/\/ Convert\n+    VECTOR_OP_CAST        = 17,\n+    VECTOR_OP_REINTERPRET = 18\n+  };\n+\n+  static int vop2ideal(jint vop, BasicType bt);\n+\n+  static oop  allocate_vector(InstanceKlass* holder, frame* fr, RegisterMap* reg_map, ObjectValue* sv, TRAPS);\n+\n+  static bool is_vector(Klass* klass);\n+  static bool is_vector_mask(Klass* klass);\n+  static bool is_vector_shuffle(Klass* klass);\n+};\n+#endif \/\/ SHARE_PRIMS_VECTORSUPPORT_HPP\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.hpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -0,0 +1,215 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_ENUMITERATOR_HPP\n+#define SHARE_UTILITIES_ENUMITERATOR_HPP\n+\n+#include <type_traits>\n+#include <limits>\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+\/\/ Iteration support for enums.\n+\/\/\n+\/\/ E is enum type, U is underlying type of E.\n+\/\/\n+\/\/ case 1:\n+\/\/ enum has sequential enumerators, with E first and E last (inclusive).\n+\/\/\n+\/\/ case 2:\n+\/\/ enum has sequential values, with U start and U end (exclusive).\n+\/\/ WeakProcessorPhases is an example because of oopstorage.\n+\/\/ This can be mapped onto case 1 by casting start\/(end-1).\n+\/\/\n+\/\/ case 3:\n+\/\/ enum has non-sequential non-duplicate enumerators\n+\/\/ Iteration could be supported via array or other sequence of enumerators.\n+\/\/ Don't bother.\n+\/\/\n+\/\/ case 4:\n+\/\/ enum has non-sequential enumerators with duplicate values\n+\/\/ Not clear what iteration should mean in this case.\n+\/\/ Don't bother trying to figure this out.\n+\/\/\n+\/\/\n+\/\/ EnumRange -- defines the range of *one specific* iteration loop.\n+\/\/ EnumIterator -- the current point in the iteration loop.\n+\n+\/\/ Example (see vmSymbols.hpp\/cpp)\n+\/\/\n+\/\/ ENUMERATOR_RANGE(vmSymbolID, vmSymbolID::FIRST_SID, vmSymbolID::LAST_SID)\n+\/\/ constexpr EnumRange<vmSymbolID> vmSymbolsRange;\n+\/\/ using vmSymbolsIterator = EnumIterator<vmSymbolID>;\n+\/\/\n+\/\/ \/* Without range-based for, allowed *\/\n+\/\/ for (vmSymbolsIterator it = vmSymbolsRange.begin(); it != vmSymbolsRange.end(); ++it) {\n+\/\/  vmSymbolID index = *it; ....\n+\/\/ }\n+\/\/\n+\/\/ \/* With range-base for, not allowed by HotSpot coding style yet *\/\n+\/\/ for (vmSymbolID index : vmSymbolsRange) {\n+\/\/    ....\n+\/\/ }\n+\n+\/\/ EnumeratorRange is a traits type supporting iteration over the enumerators of T.\n+\/\/ Specializations must provide static const data members named\n+\/\/ \"_first\" and \"_last\", whose values are the smallest \/ largest\n+\/\/ (resp.) enumerator values for T. For iteration, the enumerators of\n+\/\/ T must have sequential values in that range.\n+template<typename T> struct EnumeratorRange;\n+\n+\/\/ Specialize EnumeratorRange<T>.\n+#define ENUMERATOR_RANGE(T, First, Last)        \\\n+  template<> struct EnumeratorRange<T> {        \\\n+    static constexpr T _first = First;          \\\n+    static constexpr T _last = Last;            \\\n+  };\n+\n+\/\/ A helper class for EnumIterator, computing some additional information the\n+\/\/ iterator uses, based on T and EnumeratorRange.\n+template<typename T>\n+class EnumIterationTraits : AllStatic {\n+  using RangeType = EnumeratorRange<T>;\n+\n+public:\n+  \/\/ The underlying type for T.\n+  using Underlying = std::underlying_type_t<T>;\n+\n+  \/\/ The first enumerator of T.\n+  static constexpr T _first = RangeType::_first;\n+\n+  \/\/ The last enumerator of T.\n+  static constexpr T _last = RangeType::_last;\n+\n+  static_assert(static_cast<Underlying>(_last) <\n+                std::numeric_limits<Underlying>::max(),\n+                \"No one-past-the-end value for enum\");\n+\n+  \/\/ The value of the first enumerator of T.\n+  static constexpr Underlying _start = static_cast<Underlying>(_first);\n+\n+  \/\/ The one-past-the-end value for T.\n+  static constexpr Underlying _end = static_cast<Underlying>(_last) + 1;\n+};\n+\n+template<typename T>\n+class EnumIterator {\n+  using Traits = EnumIterationTraits<T>;\n+\n+  using Underlying = typename Traits::Underlying;\n+  Underlying _value;\n+\n+  constexpr void assert_in_bounds() const {\n+    assert(_value < Traits::_end, \"beyond the end\");\n+  }\n+\n+public:\n+  \/\/ Return a beyond-the-end iterator.\n+  constexpr EnumIterator() : _value(Traits::_end) {}\n+\n+  \/\/ Return an iterator with the indicated value.\n+  constexpr explicit EnumIterator(T value) :\n+    _value(static_cast<Underlying>(value))\n+  {\n+    assert(_value >= Traits::_start, \"out of range\");\n+    assert(_value <= Traits::_end, \"out of range\");\n+  }\n+\n+  \/\/ True if the iterators designate the same enumeration value.\n+  constexpr bool operator==(EnumIterator other) const {\n+    return _value == other._value;\n+  }\n+\n+  \/\/ True if the iterators designate different enumeration values.\n+  constexpr bool operator!=(EnumIterator other) const {\n+    return _value != other._value;\n+  }\n+\n+  \/\/ Return the current value.\n+  \/\/ precondition: this is not beyond the last enumerator.\n+  constexpr T operator*() const {\n+    assert_in_bounds();\n+    return static_cast<T>(_value);\n+  }\n+\n+  \/\/ Step this iterator to the next value.\n+  \/\/ precondition: this is not beyond the last enumerator.\n+  constexpr EnumIterator& operator++() {\n+    assert_in_bounds();\n+    ++_value;\n+    return *this;\n+  }\n+\n+  \/\/ Return a copy and step this iterator to the next value.\n+  \/\/ precondition: this is not beyond the last enumerator.\n+  constexpr EnumIterator operator++(int) {\n+    assert_in_bounds();\n+    EnumIterator result = *this;\n+    ++_value;\n+    return result;\n+  }\n+};\n+\n+template<typename T>\n+class EnumRange {\n+  using Traits = EnumIterationTraits<T>;\n+  using Underlying = typename Traits::Underlying;\n+\n+  Underlying _start;\n+  Underlying _end;\n+\n+public:\n+  using Iterator = EnumIterator<T>;\n+\n+  \/\/ Default constructor gives the full range.\n+  constexpr EnumRange() :\n+    EnumRange(Traits::_first) {}\n+\n+  \/\/ Range from start to the (exclusive) end of the enumerator range.\n+  constexpr explicit EnumRange(T start) :\n+    EnumRange(start, static_cast<T>(Traits::_end)) {}\n+\n+  \/\/ Range from start (inclusive) to end (exclusive).\n+  \/\/ precondition: start <= end.\n+  constexpr EnumRange(T start, T end) :\n+    _start(static_cast<Underlying>(start)),\n+    _end(static_cast<Underlying>(end))\n+  {\n+    assert(Traits::_start <= _start, \"out of range\");\n+    assert(_end <= Traits::_end, \"out of range\");\n+    assert(_start <= _end, \"invalid range\");\n+  }\n+\n+  \/\/ Return an iterator for the start of the range.\n+  constexpr Iterator begin() const {\n+    return Iterator(static_cast<T>(_start));\n+  }\n+\n+  \/\/ Return an iterator for the end of the range.\n+  constexpr Iterator end() const {\n+    return Iterator(static_cast<T>(_end));\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_ENUMITERATOR_HPP\n","filename":"src\/hotspot\/share\/utilities\/enumIterator.hpp","additions":215,"deletions":0,"binary":false,"changes":215,"status":"added"},{"patch":"@@ -0,0 +1,468 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.internal.vm.vector;\n+\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n+import jdk.internal.misc.Unsafe;\n+import jdk.internal.vm.annotation.ForceInline;\n+\n+import java.nio.Buffer;\n+import java.nio.ByteBuffer;\n+import java.util.Objects;\n+import java.util.function.*;\n+\n+public class VectorSupport {\n+    static {\n+        registerNatives();\n+    }\n+\n+    private static final Unsafe U = Unsafe.getUnsafe();\n+\n+    \/\/ Unary\n+    public static final int VECTOR_OP_ABS  = 0;\n+    public static final int VECTOR_OP_NEG  = 1;\n+    public static final int VECTOR_OP_SQRT = 2;\n+\n+    \/\/ Binary\n+    public static final int VECTOR_OP_ADD  = 4;\n+    public static final int VECTOR_OP_SUB  = 5;\n+    public static final int VECTOR_OP_MUL  = 6;\n+    public static final int VECTOR_OP_DIV  = 7;\n+    public static final int VECTOR_OP_MIN  = 8;\n+    public static final int VECTOR_OP_MAX  = 9;\n+\n+    public static final int VECTOR_OP_AND  = 10;\n+    public static final int VECTOR_OP_OR   = 11;\n+    public static final int VECTOR_OP_XOR  = 12;\n+\n+    \/\/ Ternary\n+    public static final int VECTOR_OP_FMA  = 13;\n+\n+    \/\/ Broadcast int\n+    public static final int VECTOR_OP_LSHIFT  = 14;\n+    public static final int VECTOR_OP_RSHIFT  = 15;\n+    public static final int VECTOR_OP_URSHIFT = 16;\n+\n+    public static final int VECTOR_OP_CAST        = 17;\n+    public static final int VECTOR_OP_REINTERPRET = 18;\n+\n+    \/\/ enum BoolTest\n+    public static final int BT_eq = 0;\n+    public static final int BT_ne = 4;\n+    public static final int BT_le = 5;\n+    public static final int BT_ge = 7;\n+    public static final int BT_lt = 3;\n+    public static final int BT_gt = 1;\n+    public static final int BT_overflow = 2;\n+    public static final int BT_no_overflow = 6;\n+\n+    \/\/ BasicType codes, for primitives only:\n+    public static final int\n+        T_FLOAT   = 6,\n+        T_DOUBLE  = 7,\n+        T_BYTE    = 8,\n+        T_SHORT   = 9,\n+        T_INT     = 10,\n+        T_LONG    = 11;\n+\n+    \/* ============================================================================ *\/\n+\n+    public static class VectorSpecies<E> {}\n+\n+    public static class VectorPayload {\n+        private final Object payload; \/\/ array of primitives\n+\n+        public VectorPayload(Object payload) {\n+            this.payload = payload;\n+        }\n+\n+        protected final Object getPayload() {\n+            return VectorSupport.maybeRebox(this).payload;\n+        }\n+    }\n+\n+    public static class Vector<E> extends VectorPayload {\n+        public Vector(Object payload) {\n+            super(payload);\n+        }\n+    }\n+\n+    public static class VectorShuffle<E> extends VectorPayload {\n+        public VectorShuffle(Object payload) {\n+            super(payload);\n+        }\n+    }\n+    public static class VectorMask<E> extends VectorPayload {\n+        public VectorMask(Object payload) {\n+            super(payload);\n+        }\n+    }\n+\n+    \/* ============================================================================ *\/\n+    public interface BroadcastOperation<VM, E, S extends VectorSpecies<E>> {\n+        VM broadcast(long l, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM, E, S extends VectorSpecies<E>>\n+    VM broadcastCoerced(Class<? extends VM> vmClass, Class<E> E, int length,\n+                                  long bits, S s,\n+                                  BroadcastOperation<VM, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.broadcast(bits, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+    public interface ShuffleIotaOperation<E, S extends VectorSpecies<E>> {\n+        VectorShuffle<E> apply(int length, int start, int step, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <E, S extends VectorSpecies<E>>\n+    VectorShuffle<E> shuffleIota(Class<?> E, Class<?> ShuffleClass, S s, int length,\n+                     int start, int step, int wrap, ShuffleIotaOperation<E, S> defaultImpl) {\n+       assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+       return defaultImpl.apply(length, start, step, s);\n+    }\n+\n+    public interface ShuffleToVectorOperation<VM, Sh, E> {\n+       VM apply(Sh s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM ,Sh extends VectorShuffle<E>, E>\n+    VM shuffleToVector(Class<?> VM, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n+                       ShuffleToVectorOperation<VM,Sh,E> defaultImpl) {\n+      assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+      return defaultImpl.apply(s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+    public interface IndexOperation<V extends Vector<E>, E, S extends VectorSpecies<E>> {\n+        V index(V v, int step, S s);\n+    }\n+\n+    \/\/FIXME @IntrinsicCandidate\n+    public static\n+    <V extends Vector<E>, E, S extends VectorSpecies<E>>\n+    V indexVector(Class<? extends V> vClass, Class<E> E, int length,\n+                  V v, int step, S s,\n+                  IndexOperation<V, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.index(v, step, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int length,\n+                          V v,\n+                          Function<V,Long> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VecExtractOp<V> {\n+        long apply(V v1, int idx);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n+                 V vec, int ix,\n+                 VecExtractOp<V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vec, ix);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VecInsertOp<V> {\n+        V apply(V v1, int idx, long val);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n+             V vec, int ix, long val,\n+             VecInsertOp<V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vec, ix, val);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+               VM vm,\n+               Function<VM, VM> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+                VM vm1, VM vm2,\n+                BiFunction<VM, VM, VM> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm1, vm2);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface TernaryOperation<V> {\n+        V apply(V v1, V v2, V v3);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n+                 VM vm1, VM vm2, VM vm3,\n+                 TernaryOperation<VM> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm1, vm2, vm3);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    \/\/ Memory operations\n+\n+    public interface LoadOperation<C, V, E, S extends VectorSpecies<E>> {\n+        V load(C container, int index, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, VM, E, S extends VectorSpecies<E>>\n+    VM load(Class<? extends VM> vmClass, Class<E> E, int length,\n+           Object base, long offset,    \/\/ Unsafe addressing\n+           C container, int index, S s,     \/\/ Arguments for default implementation\n+           LoadOperation<C, VM, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.load(container, index, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>> {\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>, W extends Vector<Integer>, E, S extends VectorSpecies<E>>\n+    V loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n+                  Object base, long offset, \/\/ Unsafe addressing\n+                  W index_vector,\n+                  C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+                  LoadVectorOperationWithMap<C, V, E, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface StoreVectorOperation<C, V extends Vector<?>> {\n+        void store(C container, int index, V v);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>>\n+    void store(Class<?> vectorClass, Class<?> elementType, int length,\n+               Object base, long offset,    \/\/ Unsafe addressing\n+               V v,\n+               C container, int index,      \/\/ Arguments for default implementation\n+               StoreVectorOperation<C, V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface StoreVectorOperationWithMap<C, V extends Vector<?>> {\n+        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C, V extends Vector<?>, W extends Vector<Integer>>\n+    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n+                      Object base, long offset,    \/\/ Unsafe addressing\n+                      W index_vector, V v,\n+                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n+                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.storeWithMap(container, index, v, indexMap, indexM);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static\n+    <VM>\n+    boolean test(int cond, Class<?> vmClass, Class<?> elementType, int length,\n+                 VM vm1, VM vm2,\n+                 BiFunction<VM, VM, Boolean> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(vm1, vm2);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorCompareOp<V,M> {\n+        M apply(int cond, V v1, V v2);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static <V extends Vector<E>,\n+                   M extends VectorMask<E>,\n+                   E>\n+    M compare(int cond, Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n+              V v1, V v2,\n+              VectorCompareOp<V,M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(cond, v1, v2);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorRearrangeOp<V extends Vector<E>,\n+            Sh extends VectorShuffle<E>,\n+            E> {\n+        V apply(V v1, Sh shuffle);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<E>,\n+            Sh extends VectorShuffle<E>,\n+            E>\n+    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<?> elementType, int vlen,\n+                  V v1, Sh sh,\n+                  VectorRearrangeOp<V,Sh, E> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v1, sh);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorBlendOp<V extends Vector<E>,\n+            M extends VectorMask<E>,\n+            E> {\n+        V apply(V v1, V v2, M mask);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V blend(Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n+            V v1, V v2, M m,\n+            VectorBlendOp<V,M, E> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v1, v2, m);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorBroadcastIntOp<V extends Vector<?>> {\n+        V apply(V v, int n);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <V extends Vector<?>>\n+    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<?> elementType, int length,\n+                   V v, int n,\n+                   VectorBroadcastIntOp<V> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v, n);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface VectorConvertOp<VOUT, VIN, S> {\n+        VOUT apply(VIN v, S species);\n+    }\n+\n+    \/\/ Users of this intrinsic assume that it respects\n+    \/\/ REGISTER_ENDIAN, which is currently ByteOrder.LITTLE_ENDIAN.\n+    \/\/ See javadoc for REGISTER_ENDIAN.\n+\n+    @IntrinsicCandidate\n+    public static <VOUT extends VectorPayload,\n+                    VIN extends VectorPayload,\n+                      S extends VectorSpecies<?>>\n+    VOUT convert(int oprId,\n+              Class<?> fromVectorClass, Class<?> fromElementType, int fromVLen,\n+              Class<?>   toVectorClass, Class<?>   toElementType, int   toVLen,\n+              VIN v, S s,\n+              VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.apply(v, s);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    @IntrinsicCandidate\n+    public static <V> V maybeRebox(V v) {\n+        \/\/ The fence is added here to avoid memory aliasing problems in C2 between scalar & vector accesses.\n+        \/\/ TODO: move the fence generation into C2. Generate only when reboxing is taking place.\n+        U.loadFence();\n+        return v;\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    \/\/ query the JVM's supported vector sizes and types\n+    public static native int getMaxLaneCount(Class<?> etype);\n+\n+    \/* ============================================================================ *\/\n+\n+    public static boolean isNonCapturingLambda(Object o) {\n+        return o.getClass().getDeclaredFields().length == 0;\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    private static native int registerNatives();\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":468,"deletions":0,"binary":false,"changes":468,"status":"added"},{"patch":"@@ -0,0 +1,290 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.incubator.vector;\n+\n+import jdk.internal.vm.annotation.ForceInline;\n+\n+import static jdk.incubator.vector.VectorOperators.*;\n+\n+abstract class AbstractMask<E> extends VectorMask<E> {\n+    AbstractMask(boolean[] bits) {\n+        super(bits);\n+    }\n+\n+    \/*package-private*\/\n+    abstract boolean[] getBits();\n+\n+    \/\/ Unary operator\n+\n+    interface MUnOp {\n+        boolean apply(int i, boolean a);\n+    }\n+\n+    abstract AbstractMask<E> uOp(MUnOp f);\n+\n+    \/\/ Binary operator\n+\n+    interface MBinOp {\n+        boolean apply(int i, boolean a, boolean b);\n+    }\n+\n+    abstract AbstractMask<E> bOp(VectorMask<E> o, MBinOp f);\n+\n+    \/*package-private*\/\n+    abstract AbstractSpecies<E> vspecies();\n+\n+    @Override\n+    @ForceInline\n+    public final VectorSpecies<E> vectorSpecies() {\n+        return vspecies();\n+    }\n+\n+    @Override\n+    public boolean laneIsSet(int i) {\n+        return getBits()[i];\n+    }\n+\n+    @Override\n+    public long toLong() {\n+        \/\/ FIXME: This should be an intrinsic.\n+        if (length() > Long.SIZE) {\n+            throw new UnsupportedOperationException(\"too many lanes for one long\");\n+        }\n+        long res = 0;\n+        long set = 1;\n+        boolean[] bits = getBits();\n+        for (int i = 0; i < bits.length; i++) {\n+            res = bits[i] ? res | set : res;\n+            set = set << 1;\n+        }\n+        return res;\n+    }\n+\n+    @Override\n+    public void intoArray(boolean[] bits, int i) {\n+        System.arraycopy(getBits(), 0, bits, i, length());\n+    }\n+\n+    @Override\n+    public boolean[] toArray() {\n+        return getBits().clone();\n+    }\n+\n+    @Override\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    public\n+    <F> VectorMask<F> check(Class<F> elementType) {\n+        if (vectorSpecies().elementType() != elementType) {\n+            throw AbstractSpecies.checkFailed(this, elementType);\n+        }\n+        return (VectorMask<F>) this;\n+    }\n+\n+    @Override\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    public\n+    <F> VectorMask<F> check(VectorSpecies<F> species) {\n+        if (species != vectorSpecies()) {\n+            throw AbstractSpecies.checkFailed(this, species);\n+        }\n+        return (VectorMask<F>) this;\n+    }\n+\n+    @Override\n+    public int trueCount() {\n+        \/\/FIXME: use a population count intrinsic here\n+        int c = 0;\n+        for (boolean i : getBits()) {\n+            if (i) c++;\n+        }\n+        return c;\n+    }\n+\n+    @Override\n+    public int firstTrue() {\n+        \/\/FIXME: use a count trailing zeros intrinsic here\n+        boolean[] bits = getBits();\n+        for (int i = 0; i < bits.length; i++) {\n+            if (bits[i])  return i;\n+        }\n+        return bits.length;\n+    }\n+\n+    @Override\n+    public int lastTrue() {\n+        \/\/FIXME: use a count leading zeros intrinsic here\n+        boolean[] bits = getBits();\n+        for (int i = bits.length-1; i >= 0; i--) {\n+            if (bits[i])  return i;\n+        }\n+        return -1;\n+    }\n+\n+    @Override\n+    public VectorMask<E> eq(VectorMask<E> m) {\n+        \/\/ FIXME: Generate good code here.\n+        return bOp(m, (i, a, b) -> a == b);\n+    }\n+\n+    @Override\n+    public VectorMask<E> andNot(VectorMask<E> m) {\n+        \/\/ FIXME: Generate good code here.\n+        return bOp(m, (i, a, b) -> a && !b);\n+    }\n+\n+    \/*package-private*\/\n+    static boolean anyTrueHelper(boolean[] bits) {\n+        \/\/ FIXME: Maybe use toLong() != 0 here.\n+        for (boolean i : bits) {\n+            if (i) return true;\n+        }\n+        return false;\n+    }\n+\n+    \/*package-private*\/\n+    static boolean allTrueHelper(boolean[] bits) {\n+        \/\/ FIXME: Maybe use not().toLong() == 0 here.\n+        for (boolean i : bits) {\n+            if (!i) return false;\n+        }\n+        return true;\n+    }\n+\n+    @Override\n+    @ForceInline\n+    public VectorMask<E> indexInRange(int offset, int limit) {\n+        int vlength = length();\n+        Vector<E> iota = vectorSpecies().zero().addIndex(1);\n+        VectorMask<E> badMask = checkIndex0(offset, limit, iota, vlength);\n+        return this.andNot(badMask);\n+    }\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    AbstractVector<E>\n+    toVectorTemplate() {\n+        AbstractSpecies<E> vsp = vspecies();\n+        Vector<E> zero = vsp.broadcast(0);\n+        Vector<E> mone = vsp.broadcast(-1);\n+        \/\/ -1 will result in the most significant bit being set in\n+        \/\/ addition to some or all other lane bits.\n+        \/\/ For integral types, *all* lane bits will be set.\n+        \/\/ The bits for -1.0 are like {0b10111*0000*}.\n+        \/\/ FIXME: Use a conversion intrinsic for this operation.\n+        \/\/ https:\/\/bugs.openjdk.java.net\/browse\/JDK-8225740\n+        return (AbstractVector<E>) zero.blend(mone, this);\n+    }\n+\n+    \/**\n+     * Test if a masked memory access at a given offset into an array\n+     * of the given length will stay within the array.\n+     * The per-lane offsets are iota*esize.\n+     *\/\n+    \/*package-private*\/\n+    @ForceInline\n+    void checkIndexByLane(int offset, int alength,\n+                          Vector<E> iota,\n+                          int esize) {\n+        if (VectorIntrinsics.VECTOR_ACCESS_OOB_CHECK == 0) {\n+            return;\n+        }\n+        \/\/ Although the specification is simple, the implementation is\n+        \/\/ tricky, because the value iota*esize might possibly\n+        \/\/ overflow.  So we calculate our test values as scalars,\n+        \/\/ clipping to the range [-1..VLENGTH], and test them against\n+        \/\/ the unscaled iota vector, whose values are in [0..VLENGTH-1].\n+        int vlength = length();\n+        VectorMask<E> badMask;\n+        if (esize == 1) {\n+            badMask = checkIndex0(offset, alength, iota, vlength);\n+        } else if (offset >= 0) {\n+            \/\/ Masked access to multi-byte lanes in byte array.\n+            \/\/ It could be aligned anywhere.\n+            int elemCount = Math.min(vlength, (alength - offset) \/ esize);\n+            badMask = checkIndex0(0, elemCount, iota, vlength);\n+        } else {\n+            \/\/ This requires a split test.\n+            int clipOffset = Math.max(offset, -(vlength * esize));\n+            int elemCount = Math.min(vlength, (alength - clipOffset) \/ esize);\n+            badMask = checkIndex0(0, elemCount, iota, vlength);\n+            clipOffset &= (esize - 1);  \/\/ power of two, so OK\n+            VectorMask<E> badMask2 = checkIndex0(clipOffset \/ esize, vlength,\n+                                                 iota, vlength);\n+            badMask = badMask.or(badMask2);\n+        }\n+        badMask = badMask.and(this);\n+        if (badMask.anyTrue()) {\n+            int badLane = badMask.firstTrue();\n+            throw ((AbstractMask<E>)badMask)\n+                   .checkIndexFailed(offset, badLane, alength, esize);\n+        }\n+    }\n+\n+    private\n+    @ForceInline\n+    VectorMask<E> checkIndex0(int offset, int alength,\n+                              Vector<E> iota, int vlength) {\n+        \/\/ An active lane is bad if its number is greater than\n+        \/\/ alength-offset, since when added to offset it will step off\n+        \/\/ of the end of the array.  To avoid overflow when\n+        \/\/ converting, clip the comparison value to [0..vlength]\n+        \/\/ inclusive.\n+        int indexLimit = Math.max(0, Math.min(alength - offset, vlength));\n+        VectorMask<E> badMask =\n+            iota.compare(GE, iota.broadcast(indexLimit));\n+        if (offset < 0) {\n+            \/\/ An active lane is bad if its number is less than\n+            \/\/ -offset, because when added to offset it will then\n+            \/\/ address an array element at a negative index.  To avoid\n+            \/\/ overflow when converting, clip the comparison value at\n+            \/\/ vlength.  This specific expression works correctly even\n+            \/\/ when offset is Integer.MIN_VALUE.\n+            int firstGoodIndex = -Math.max(offset, -vlength);\n+            VectorMask<E> badMask2 =\n+                iota.compare(LT, iota.broadcast(firstGoodIndex));\n+            if (indexLimit >= vlength) {\n+                badMask = badMask2;  \/\/ 1st badMask is all true\n+            } else {\n+                badMask = badMask.or(badMask2);\n+            }\n+        }\n+        return badMask;\n+    }\n+\n+    private IndexOutOfBoundsException checkIndexFailed(int offset, int lane,\n+                                                       int alength, int esize) {\n+        String msg = String.format(\"Masked range check failed: \"+\n+                                   \"vector mask %s out of bounds at \"+\n+                                   \"index %d+%d in array of length %d\",\n+                                   this, offset, lane * esize, alength);\n+        if (esize != 1) {\n+            msg += String.format(\" (each lane spans %d array elements)\", esize);\n+        }\n+        throw new IndexOutOfBoundsException(msg);\n+    }\n+\n+}\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractMask.java","additions":290,"deletions":0,"binary":false,"changes":290,"status":"added"},{"patch":"@@ -0,0 +1,246 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.incubator.vector;\n+\n+import java.util.function.IntUnaryOperator;\n+import jdk.internal.vm.annotation.ForceInline;\n+\n+abstract class AbstractShuffle<E> extends VectorShuffle<E> {\n+    static final IntUnaryOperator IDENTITY = i -> i;\n+\n+    \/\/ Internal representation allows for a maximum index of 256\n+    \/\/ Values are clipped to [-VLENGTH..VLENGTH-1].\n+\n+    AbstractShuffle(int length, byte[] reorder) {\n+        super(reorder);\n+        assert(length == reorder.length);\n+        assert(indexesInRange(reorder));\n+    }\n+\n+    AbstractShuffle(int length, int[] reorder) {\n+        this(length, reorder, 0);\n+    }\n+\n+    AbstractShuffle(int length, int[] reorder, int offset) {\n+        super(prepare(length, reorder, offset));\n+    }\n+\n+    AbstractShuffle(int length, IntUnaryOperator f) {\n+        super(prepare(length, f));\n+    }\n+\n+    private static byte[] prepare(int length, int[] reorder, int offset) {\n+        byte[] a = new byte[length];\n+        for (int i = 0; i < length; i++) {\n+            int si = reorder[offset + i];\n+            si = partiallyWrapIndex(si, length);\n+            a[i] = (byte) si;\n+        }\n+        return a;\n+    }\n+\n+    private static byte[] prepare(int length, IntUnaryOperator f) {\n+        byte[] a = new byte[length];\n+        for (int i = 0; i < a.length; i++) {\n+            int si = f.applyAsInt(i);\n+            si = partiallyWrapIndex(si, length);\n+            a[i] = (byte) si;\n+        }\n+        return a;\n+    }\n+\n+    byte[] reorder() {\n+        return (byte[])getPayload();\n+    }\n+\n+    \/*package-private*\/\n+    abstract AbstractSpecies<E> vspecies();\n+\n+    @Override\n+    @ForceInline\n+    public final VectorSpecies<E> vectorSpecies() {\n+        return vspecies();\n+    }\n+\n+    @Override\n+    @ForceInline\n+    public void intoArray(int[] a, int offset) {\n+        byte[] reorder = reorder();\n+        int vlen = reorder.length;\n+        for (int i = 0; i < vlen; i++) {\n+            int sourceIndex = reorder[i];\n+            assert(sourceIndex >= -vlen && sourceIndex < vlen);\n+            a[offset + i] = sourceIndex;\n+        }\n+    }\n+\n+    @Override\n+    @ForceInline\n+    public int[] toArray() {\n+        byte[] reorder = reorder();\n+        int[] a = new int[reorder.length];\n+        intoArray(a, 0);\n+        return a;\n+    }\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    AbstractVector<E>\n+    toVectorTemplate() {\n+        \/\/ Note that the values produced by laneSource\n+        \/\/ are already clipped.  At this point we convert\n+        \/\/ them from internal ints (or bytes) into the ETYPE.\n+        \/\/ FIXME: Use a conversion intrinsic for this operation.\n+        \/\/ https:\/\/bugs.openjdk.java.net\/browse\/JDK-8225740\n+        return (AbstractVector<E>) vspecies().fromIntValues(toArray());\n+    }\n+\n+    @ForceInline\n+    public final VectorShuffle<E> checkIndexes() {\n+        \/\/ FIXME: vectorize this\n+        for (int index : reorder()) {\n+            if (index < 0) {\n+                throw checkIndexFailed(index, length());\n+            }\n+        }\n+        return this;\n+    }\n+\n+    @ForceInline\n+    public final VectorShuffle<E> wrapIndexes() {\n+        \/\/ FIXME: vectorize this\n+        byte[] reorder = reorder();\n+        int length = reorder.length;\n+        for (int index : reorder) {\n+            if (index < 0) {\n+                return wrapAndRebuild(reorder);\n+            }\n+        }\n+        return this;\n+    }\n+\n+    @ForceInline\n+    public final VectorShuffle<E> wrapAndRebuild(byte[] oldReorder) {\n+        int length = oldReorder.length;\n+        byte[] reorder = new byte[length];\n+        for (int i = 0; i < length; i++) {\n+            int si = oldReorder[i];\n+            \/\/ FIXME: This does not work unless it's a power of 2.\n+            if ((length & (length - 1)) == 0) {\n+                si += si & length;  \/\/ power-of-two optimization\n+            } else if (si < 0) {\n+                \/\/ non-POT code requires a conditional add\n+                si += length;\n+            }\n+            assert(si >= 0 && si < length);\n+            reorder[i] = (byte) si;\n+        }\n+        return vspecies().dummyVector().shuffleFromBytes(reorder);\n+    }\n+\n+    @ForceInline\n+    public final VectorMask<E> laneIsValid() {\n+        \/\/ FIXME: vectorize this\n+        byte[] reorder = reorder();\n+        int length = reorder.length;\n+        boolean[] bits = new boolean[length];\n+        for (int i = 0; i < length; i++) {\n+            if (reorder[i] >= 0) {\n+                bits[i] = true;\n+            }\n+        }\n+        return vspecies().dummyVector().maskFromArray(bits);\n+    }\n+\n+    @Override\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    public final\n+    <F> VectorShuffle<F> check(VectorSpecies<F> species) {\n+        if (species != vectorSpecies()) {\n+            throw AbstractSpecies.checkFailed(this, species);\n+        }\n+        return (VectorShuffle<F>) this;\n+    }\n+\n+    @Override\n+    @ForceInline\n+    public final int checkIndex(int index) {\n+        return checkIndex0(index, length(), (byte)1);\n+    }\n+\n+    @Override\n+    @ForceInline\n+    public final int wrapIndex(int index) {\n+        return checkIndex0(index, length(), (byte)0);\n+    }\n+\n+    \/** Return invalid indexes partially wrapped\n+     * mod VLENGTH to negative values.\n+     *\/\n+    \/*package-private*\/\n+    @ForceInline\n+    static\n+    int partiallyWrapIndex(int index, int laneCount) {\n+        return checkIndex0(index, laneCount, (byte)-1);\n+    }\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    static int checkIndex0(int index, int laneCount, byte mode) {\n+        int wrapped = VectorIntrinsics.wrapToRange(index, laneCount);\n+        if (mode == 0 || wrapped == index) {\n+            return wrapped;\n+        }\n+        if (mode < 0) {\n+            return wrapped - laneCount;  \/\/ special mode for internal storage\n+        }\n+        throw checkIndexFailed(index, laneCount);\n+    }\n+\n+    private static IndexOutOfBoundsException checkIndexFailed(int index, int laneCount) {\n+        int max = laneCount - 1;\n+        String msg = \"required an index in [0..\"+max+\"] but found \"+index;\n+        return new IndexOutOfBoundsException(msg);\n+    }\n+\n+    static boolean indexesInRange(byte[] reorder) {\n+        int length = reorder.length;\n+        for (byte si : reorder) {\n+            if (si >= length || si < -length) {\n+                boolean assertsEnabled = false;\n+                assert(assertsEnabled = true);\n+                if (assertsEnabled) {\n+                    String msg = (\"index \"+si+\"out of range [\"+length+\"] in \"+\n+                                  java.util.Arrays.toString(reorder));\n+                    throw new AssertionError(msg);\n+                }\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+}\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractShuffle.java","additions":246,"deletions":0,"binary":false,"changes":246,"status":"added"}]}