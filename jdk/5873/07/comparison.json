{"files":[{"patch":"@@ -2062,1 +2062,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -2077,1 +2077,1 @@\n-  if (bottom_type()->isa_vect() != NULL) {\n+  if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2183,0 +2183,3 @@\n+      } else if (dst_lo_rc == rc_predicate) {\n+        __ unspill_sve_predicate(as_PRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo),\n+                                 Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n@@ -2185,2 +2188,18 @@\n-        __ unspill(rscratch1, is64, src_offset);\n-        __ spill(rscratch1, is64, dst_offset);\n+        if (ideal_reg() == Op_RegVectMask) {\n+          __ spill_copy_sve_predicate_stack_to_stack(src_offset, dst_offset,\n+                                                     Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+        } else {\n+          __ unspill(rscratch1, is64, src_offset);\n+          __ spill(rscratch1, is64, dst_offset);\n+        }\n+      }\n+      break;\n+    case rc_predicate:\n+      if (dst_lo_rc == rc_predicate) {\n+        __ sve_mov(as_PRegister(Matcher::_regEncode[dst_lo]), as_PRegister(Matcher::_regEncode[src_lo]));\n+      } else if (dst_lo_rc == rc_stack) {\n+        __ spill_sve_predicate(as_PRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo),\n+                               Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+      } else {\n+        assert(false, \"bad src and dst rc_class combination.\");\n+        ShouldNotReachHere();\n@@ -2207,1 +2226,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2224,0 +2243,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# predicate spill size = %d\", vsize);\n@@ -2383,0 +2406,12 @@\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+    case Op_MaskAll:\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (UseSVE == 0) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -2431,0 +2466,9 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ Only SVE supports masked operations.\n+  if (UseSVE == 0) {\n+    return false;\n+  }\n+  return match_rule_supported(opcode) &&\n+         masked_op_sve_supported(opcode, vlen, bt);\n+}\n+\n@@ -2644,2 +2688,5 @@\n-  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n-    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n+  \/\/ ShiftV src (ShiftCntV con)\n+  \/\/ StoreVector (VectorStoreMask src)\n+  if (is_vshift_con_pattern(n, m) ||\n+      (UseSVE > 0 && m->Opcode() == Op_VectorStoreMask && n->Opcode() == Op_StoreVector)) {\n+    mstack.push(m, Visit);\n@@ -2648,0 +2695,1 @@\n+\n@@ -5506,0 +5554,1 @@\n+  match(pRegGov);\n@@ -8854,0 +8903,11 @@\n+instruct castVVMask(pRegGov dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":67,"deletions":7,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -147,5 +148,1 @@\n-        if (vlen < 4 || length_in_bytes > MaxVectorSize) {\n-          return false;\n-        } else {\n-          return true;\n-        }\n+        return vlen >= 4 && length_in_bytes <= MaxVectorSize;\n@@ -161,0 +158,8 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    if (opcode == Op_VectorRearrange) {\n+      return false;\n+    }\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -297,1 +302,1 @@\n-instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -301,1 +306,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -303,2 +308,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_ldr $dst, $pgtmp, $mem\\t# load vector partial\" %}\n@@ -307,1 +312,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -311,1 +316,1 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -317,1 +322,1 @@\n-instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -321,1 +326,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -323,2 +328,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_str $src, $pgtmp, $mem\\t# store vector partial\" %}\n@@ -327,1 +332,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -331,1 +336,73 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -337,0 +414,134 @@\n+\/\/ maskAll\n+\n+instruct vmaskAll_immI(pRegGov dst, immI src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(pRegGov dst, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and\/or\/xor\n+\n+instruct vmask_and(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_and(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (OrVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (XorVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and_not\n+\n+instruct vmask_and_notI(pRegGov pd, pRegGov pn, pRegGov pm, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_and_notL(pRegGov pd, pRegGov pn, pRegGov pm, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -351,1 +562,1 @@\n-instruct reinterpretResize(vReg dst, vReg src, pRegGov pTmp, rFlagsReg cr) %{\n+instruct reinterpretResize(vReg dst, vReg src, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -355,1 +566,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -365,1 +576,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ B, length_in_bytes_resize);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n@@ -367,1 +578,1 @@\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pTmp$$reg),\n+    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n@@ -373,0 +584,34 @@\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmp(Assembler::EQ, as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -377,1 +622,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -390,1 +635,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -403,1 +648,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -416,1 +661,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -429,1 +674,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -442,1 +687,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -453,0 +698,80 @@\n+\/\/ vector abs - predicated\n+\n+instruct vabsB_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVB dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsS_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVS dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVI dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVL dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -533,0 +858,80 @@\n+\/\/ vector add - predicated\n+\n+instruct vaddB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -578,1 +983,1 @@\n-\/\/ vector not\n+\/\/ vector and - predicated\n@@ -580,1 +985,1 @@\n-instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n+instruct vand_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -582,3 +987,1 @@\n-  match(Set dst (XorV src (ReplicateB m1)));\n-  match(Set dst (XorV src (ReplicateS m1)));\n-  match(Set dst (XorV src (ReplicateI m1)));\n+  match(Set dst_src1 (AndV (Binary dst_src1 src2) pg));\n@@ -586,1 +989,1 @@\n-  format %{ \"sve_not $dst, $src\\t# vector (sve) B\/H\/S\" %}\n+  format %{ \"sve_and $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -588,2 +991,5 @@\n-    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n-               ptrue, as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_and(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -594,1 +1000,3 @@\n-instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n+\/\/ vector or - predicated\n+\n+instruct vor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -596,1 +1004,1 @@\n-  match(Set dst (XorV src (ReplicateL m1)));\n+  match(Set dst_src1 (OrV (Binary dst_src1 src2) pg));\n@@ -598,1 +1006,1 @@\n-  format %{ \"sve_not $dst, $src\\t# vector (sve) D\" %}\n+  format %{ \"sve_orr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -600,2 +1008,5 @@\n-    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n-               ptrue, as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_orr(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -606,0 +1017,1 @@\n+\/\/ vector xor - predicated\n@@ -607,3 +1019,1 @@\n-\/\/ vector and_not\n-\n-instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n+instruct vxor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -611,3 +1021,1 @@\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateI m1))));\n+  match(Set dst_src1 (XorV (Binary dst_src1 src2) pg));\n@@ -615,1 +1023,48 @@\n-  format %{ \"sve_bic $dst, $src1, $src2\\t# vector (sve) B\/H\/S\" %}\n+  format %{ \"sve_eor $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_eor(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector not\n+\n+instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_not $dst, $src\\t# vector (sve) B\/H\/S\" %}\n+  ins_encode %{\n+    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n+               ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (XorV src (ReplicateL m1)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_not $dst, $src\\t# vector (sve) D\" %}\n+  ins_encode %{\n+    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n+               ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and_not\n+\n+instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateI m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $dst, $src1, $src2\\t# vector (sve) B\/H\/S\" %}\n@@ -637,1 +1092,0 @@\n-\n@@ -664,0 +1118,28 @@\n+\/\/ vector float div - predicated\n+\n+instruct vfdivF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vfdivD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -678,1 +1160,1 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n+      assert(is_integral_type(bt), \"unsupported type\");\n@@ -698,1 +1180,1 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n+      assert(is_integral_type(bt), \"unsupported type\");\n@@ -706,0 +1188,42 @@\n+\/\/ vector min\/max - predicated\n+\n+instruct vmin_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MinV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmax_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MaxV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -734,0 +1258,28 @@\n+\/\/ vector fmla - predicated\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -942,1 +1494,0 @@\n-\n@@ -1019,0 +1570,80 @@\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1022,1 +1653,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -1034,1 +1666,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -1045,0 +1678,28 @@\n+\/\/ vector fneg - predicated\n+\n+instruct vnegF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1059,1 +1720,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -1062,4 +1723,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -1068,1 +1728,1 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n@@ -1070,2 +1730,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -1076,3 +1734,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -1080,5 +1736,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -1086,6 +1741,3 @@\n-    Assembler::SIMD_RegVariant size =\n-      __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n@@ -1096,1 +1748,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -1098,2 +1750,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -1101,5 +1752,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -1107,6 +1756,4 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src3$$reg),\n-                   as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n@@ -1119,1 +1766,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -1123,0 +1770,1 @@\n+  effect(KILL cr);\n@@ -1124,14 +1772,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n-  ins_encode %{\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmaskS(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -1139,2 +1774,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n+               ptrue, as_FloatRegister($src$$reg), 0);\n@@ -1145,4 +1780,2 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n@@ -1150,0 +1783,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -1151,21 +1785,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmaskL(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n@@ -1173,4 +1787,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1183,1 +1797,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -1187,1 +1801,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -1189,2 +1803,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -1195,1 +1808,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremask_narrow(vReg dst, pRegGov src, vReg tmp, immI_gt_1 size) %{\n@@ -1200,23 +1813,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (H\/S\/D)\" %}\n@@ -1224,7 +1815,4 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($dst$$reg), size, as_FloatRegister($tmp$$reg));\n@@ -1235,23 +1823,1 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n+\/\/ Combine LoadVector+VectorLoadMask when the vector element type is not T_BYTE\n@@ -1259,5 +1825,4 @@\n-\/\/ load\/store mask vector\n-\n-instruct vloadmask_loadV_byte(vReg dst, vmemA mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n-            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) == 1);\n+instruct vloadmask_loadV(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n@@ -1265,3 +1830,4 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (H\/S\/D)\" %}\n@@ -1269,1 +1835,2 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -1271,3 +1838,2 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n@@ -1275,1 +1841,2 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+               ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -1280,2 +1847,4 @@\n-instruct vloadmask_loadV_non_byte(vReg dst, indirect mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+instruct vloadmask_loadV_partial(pRegGov dst, indirect mem, vReg vtmp, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() > 16 &&\n+            n->as_Vector()->length_in_bytes() < MaxVectorSize &&\n@@ -1284,3 +1853,3 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"vloadmask_loadV $dst, $mem\\t# load vector mask partial (sve) (H\/S\/D)\" %}\n@@ -1288,1 +1857,2 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -1290,3 +1860,4 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n@@ -1294,1 +1865,1 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n@@ -1299,3 +1870,5 @@\n-instruct storeV_vstoremask_byte(vmemA mem, vReg src, vReg tmp, immI_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n+\/\/ Combine VectorStoreMask+StoreVector when the vector element type is not T_BYTE\n+\n+instruct storeV_vstoremask(indirect mem, pRegGov src, vReg tmp, immI_gt_1 esize) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) == MaxVectorSize);\n@@ -1304,3 +1877,3 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (H\/S\/D)\" %}\n@@ -1310,3 +1883,2 @@\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant($esize$$constant);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), size, as_PRegister($src$$reg), 1, false);\n@@ -1320,3 +1892,6 @@\n-instruct storeV_vstoremask_non_byte(indirect mem, vReg src, vReg tmp, immI_gt_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n+instruct storeV_vstoremask_partial(indirect mem, pRegGov src, vReg vtmp,\n+                                   immI_gt_1 esize, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() > 16 &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) < MaxVectorSize);\n@@ -1324,4 +1899,3 @@\n-  effect(TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeV_vstoremask $src, $mem\\t# store vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_cost(6 * SVE_COST);\n@@ -1329,0 +1903,2 @@\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n@@ -1330,6 +1906,5 @@\n-    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n-                          ptrue, T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n@@ -1343,2 +1918,3 @@\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n@@ -1346,1 +1922,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1348,1 +1924,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction (sve) (may extend)\" %}\n@@ -1351,11 +1927,44 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF(vRegF src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD(vRegD src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n@@ -1368,1 +1977,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n@@ -1371,1 +1981,1 @@\n-  ins_cost(SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -1378,11 +1988,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1393,2 +1995,4 @@\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n@@ -1396,1 +2000,17 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n@@ -1398,1 +2018,2 @@\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n@@ -1400,3 +2021,4 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1407,5 +2029,4 @@\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n@@ -1413,1 +2034,2 @@\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n@@ -1417,1 +2039,1 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n@@ -1419,2 +2041,0 @@\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n@@ -1425,0 +2045,1 @@\n+\/\/ vector add reduction - predicated\n@@ -1426,3 +2047,5 @@\n-instruct reduce_addF(vRegF src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n+instruct reduce_addI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1430,1 +2053,1 @@\n-  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated (sve) (may extend)\" %}\n@@ -1432,2 +2055,4 @@\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1438,3 +2063,5 @@\n-instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n+instruct reduce_addL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1442,2 +2069,15 @@\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked(vRegF src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated (sve)\" %}\n@@ -1445,2 +2085,0 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n@@ -1448,1 +2086,1 @@\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1453,3 +2091,4 @@\n-instruct reduce_addD(vRegD src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n+instruct reduce_addD_masked(vRegD src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n@@ -1457,1 +2096,1 @@\n-  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated (sve)\" %}\n@@ -1460,1 +2099,1 @@\n-         ptrue, as_FloatRegister($src2$$reg));\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1465,3 +2104,47 @@\n-instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n+instruct reduce_addI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked_partial(vRegF src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n@@ -1469,0 +2152,16 @@\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_masked_partial(vRegD src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n@@ -1470,1 +2169,2 @@\n-  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated partial (sve)\" %}\n@@ -1474,0 +2174,2 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -1482,2 +2184,3 @@\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1486,1 +2189,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1488,1 +2191,1 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction (sve) (may extend)\" %}\n@@ -1491,11 +2194,19 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1508,1 +2219,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1512,1 +2224,1 @@\n-  ins_cost(SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -1519,11 +2231,3 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1534,3 +2238,5 @@\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n@@ -1538,1 +2244,21 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reduction - predicated\n+\n+instruct reduce_andI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1540,1 +2266,1 @@\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated (sve) (may extend)\" %}\n@@ -1542,3 +2268,4 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1549,6 +2276,6 @@\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_andL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1556,1 +2283,41 @@\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated partial (sve)\" %}\n@@ -1560,4 +2327,5 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1570,2 +2338,3 @@\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1574,1 +2343,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1576,1 +2345,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction (sve) (may extend)\" %}\n@@ -1579,11 +2348,19 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1596,1 +2373,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1600,1 +2378,1 @@\n-  ins_cost(SVE_COST);\n+  ins_cost(2 * SVE_COST);\n@@ -1607,11 +2385,3 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1622,3 +2392,5 @@\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n@@ -1626,1 +2398,21 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reduction - predicated\n+\n+instruct reduce_orI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1628,1 +2420,1 @@\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated (sve) (may extend)\" %}\n@@ -1630,3 +2422,4 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1637,1 +2430,96 @@\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+instruct reduce_orL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reduction\n+\n+instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -1639,1 +2527,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1641,1 +2530,1 @@\n-  match(Set dst (OrReductionV src1 src2));\n+  match(Set dst (XorReductionV src1 src2));\n@@ -1643,2 +2532,23 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction partial (sve)\" %}\n@@ -1648,4 +2558,3 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1656,1 +2565,18 @@\n-\/\/ vector xor reduction\n+\/\/ vector xor reduction - predicated\n+\n+instruct reduce_eorI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1658,5 +2584,6 @@\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_eorL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1664,1 +2591,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated (sve)\" %}\n@@ -1666,12 +2593,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1682,5 +2600,6 @@\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+instruct reduce_eorI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n@@ -1688,2 +2607,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated partial (sve) (may extend)\" %}\n@@ -1695,26 +2614,5 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1725,5 +2623,6 @@\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+instruct reduce_eorL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n@@ -1731,2 +2630,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated partial (sve)\" %}\n@@ -1736,4 +2635,5 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1744,1 +2644,0 @@\n-\n@@ -1747,5 +2646,5 @@\n-instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1753,1 +2652,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1755,1 +2654,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction (sve)\" %}\n@@ -1758,5 +2657,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1767,6 +2664,4 @@\n-instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n@@ -1774,1 +2669,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1776,1 +2671,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxI partial (sve)\" %}\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction (sve)\" %}\n@@ -1778,9 +2673,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1791,3 +2680,6 @@\n-instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1795,3 +2687,3 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction partial (sve)\" %}\n@@ -1799,4 +2691,7 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1809,1 +2704,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1813,2 +2709,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction  partial (sve)\" %}\n@@ -1818,5 +2714,3 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1828,1 +2722,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1833,2 +2728,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction F\" %}\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction (sve)\" %}\n@@ -1836,2 +2730,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n@@ -1845,1 +2738,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1850,1 +2744,1 @@\n-  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# reduce max S partial (sve)\" %}\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction partial (sve)\" %}\n@@ -1854,2 +2748,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1862,1 +2755,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1867,2 +2761,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction D\" %}\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction (sve)\" %}\n@@ -1870,2 +2763,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n@@ -1879,1 +2771,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1884,1 +2777,1 @@\n-  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# reduce max D partial (sve)\" %}\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction partial (sve)\" %}\n@@ -1888,2 +2781,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1895,1 +2787,1 @@\n-\/\/ vector min reduction\n+\/\/ vector max reduction - predicated\n@@ -1897,7 +2789,8 @@\n-instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_maxI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1905,1 +2798,1 @@\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated (sve)\" %}\n@@ -1908,5 +2801,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1917,8 +2808,7 @@\n-instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_maxL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1926,1 +2816,19 @@\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minI partial (sve)\" %}\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated partial (sve)\" %}\n@@ -1932,5 +2840,116 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min reduction\n+\n+instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1941,2 +2960,3 @@\n-instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1945,1 +2965,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1947,1 +2967,19 @@\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction partial (sve)\" %}\n@@ -1949,4 +2987,7 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1959,1 +3000,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1963,2 +3005,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction  partial (sve)\" %}\n@@ -1968,5 +3010,3 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1978,1 +3018,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1983,2 +3024,1 @@\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction F\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction (sve)\" %}\n@@ -1986,2 +3026,1 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n@@ -1995,1 +3034,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -2000,1 +3040,1 @@\n-  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# reduce min S partial (sve)\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction partial (sve)\" %}\n@@ -2004,2 +3044,1 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -2012,1 +3051,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -2017,2 +3057,1 @@\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction D\" %}\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction (sve)\" %}\n@@ -2020,2 +3059,130 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min reduction - predicated\n+\n+instruct reduce_minI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -2027,6 +3194,27 @@\n-instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n+instruct reduce_minF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n@@ -2034,1 +3222,2 @@\n-  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# reduce min D partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated partial (sve)\" %}\n@@ -2038,0 +3227,2 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -2039,1 +3230,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -2451,1 +3642,332 @@\n-instruct vlsrI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrI_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (URShiftVI src (RShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con == 0) {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsr(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (URShiftVL src (RShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con == 0) {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsr(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVB src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con >= 8) {\n+      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ B,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con >= 16) {\n+      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ H,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift - predicated\n+\n+instruct vasrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2453,1 +3975,1 @@\n-  match(Set dst (URShiftVI src (RShiftCntV shift)));\n+  match(Set dst_src (RShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2455,1 +3977,1 @@\n-  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2458,7 +3980,2 @@\n-    if (con == 0) {\n-      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsr(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2469,1 +3986,1 @@\n-instruct vlsrL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vasrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2471,1 +3988,1 @@\n-  match(Set dst (URShiftVL src (RShiftCntV shift)));\n+  match(Set dst_src (RShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2473,1 +3990,1 @@\n-  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2476,7 +3993,2 @@\n-    if (con == 0) {\n-      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsr(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2487,1 +3999,1 @@\n-instruct vlslB_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2489,1 +4001,1 @@\n-  match(Set dst (LShiftVB src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2491,1 +4003,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (B)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n@@ -2494,7 +4006,2 @@\n-    if (con >= 8) {\n-      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ B,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n@@ -2505,1 +4012,1 @@\n-instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2507,1 +4014,1 @@\n-  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2509,1 +4016,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (H)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2512,7 +4019,2 @@\n-    if (con >= 16) {\n-      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ H,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2523,1 +4025,1 @@\n-instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2525,1 +4027,1 @@\n-  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2527,1 +4029,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2530,2 +4032,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2536,1 +4038,1 @@\n-instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2538,1 +4040,1 @@\n-  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2540,1 +4042,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2543,2 +4045,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2549,6 +4051,5 @@\n-instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+instruct vlslB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVB (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n@@ -2556,1 +4057,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n@@ -2561,7 +4064,5 @@\n-instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+instruct vlslS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVS (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2569,1 +4070,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2574,6 +4077,5 @@\n-instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+instruct vlslI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVI (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2581,1 +4083,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2586,6 +4090,5 @@\n-instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+instruct vlslL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVL (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2593,1 +4096,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2601,1 +4106,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2613,1 +4119,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2624,0 +4131,28 @@\n+\/\/ vector sqrt - predicated\n+\n+instruct vsqrtF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsqrtD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2704,1 +4239,1 @@\n-\/\/ vector mask cast\n+\/\/ vector sub - predicated\n@@ -2706,6 +4241,5 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n-            n->bottom_type()->is_vect()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n-  match(Set dst (VectorMaskCast dst));\n-  ins_cost(0);\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+instruct vsubB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n@@ -2713,1 +4247,3 @@\n-    \/\/ empty\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2715,1 +4251,1 @@\n-  ins_pipe(pipe_class_empty);\n+  ins_pipe(pipe_slow);\n@@ -2718,1 +4254,12 @@\n-\/\/ ------------------------------ Vector cast -------------------------------\n+instruct vsubS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2720,5 +4267,3 @@\n-instruct vcvtBtoS(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorCastB2X src));\n+instruct vsubI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) pg));\n@@ -2726,1 +4271,1 @@\n-  format %{ \"sve_sunpklo  $dst, H, $src\\t# convert B to S vector\" %}\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -2728,1 +4273,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2733,8 +4280,5 @@\n-instruct vcvtBtoI(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\t# convert B to I vector\" %}\n+instruct vsubL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -2742,2 +4286,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2748,9 +4293,5 @@\n-instruct vcvtBtoL(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\t# convert B to L vector\" %}\n+instruct vsubF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -2758,3 +4299,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2765,9 +4306,5 @@\n-instruct vcvtBtoF(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\n\\t\"\n-            \"sve_scvtf  $dst, S, $dst, S\\t# convert B to F vector\" %}\n+instruct vsubD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -2775,3 +4312,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg), __ S);\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2782,2 +4319,3 @@\n-instruct vcvtBtoD(vReg dst, vReg src)\n-%{\n+\/\/ ------------------------------ Vector mask cast --------------------------\n+\n+instruct vmaskcast(pRegGov dst_src) %{\n@@ -2785,12 +4323,7 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\n\\t\"\n-            \"sve_scvtf  $dst, D, $dst, D\\t# convert B to D vector\" %}\n-  ins_encode %{\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ D);\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorMaskCast dst_src));\n+  ins_cost(0);\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n+  ins_encode %{\n+    \/\/ empty\n@@ -2798,1 +4331,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(pipe_class_empty);\n@@ -2801,1 +4334,1 @@\n-instruct vcvtStoB(vReg dst, vReg src, vReg tmp)\n+instruct vmaskcast_extend(pRegGov dst, pReg src)\n@@ -2804,6 +4337,6 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastS2X src));\n-  effect(TEMP tmp);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_dup  $tmp, B, 0\\n\\t\"\n-            \"sve_uzp1  $dst, B, $src, tmp\\t# convert S to B vector\" %}\n+            (Matcher::vector_length_in_bytes(n) == 2 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 4 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 8 * Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_extend  $dst, $src\\t# extend predicate $src\" %}\n@@ -2811,2 +4344,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ B, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vmaskcast_extend(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n@@ -2817,1 +4350,1 @@\n-instruct vcvtStoI(vReg dst, vReg src)\n+instruct vmaskcast_narrow(pRegGov dst, pReg src)\n@@ -2820,4 +4353,6 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorCastS2X src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, S, $src\\t# convert S to I vector\" %}\n+            (Matcher::vector_length_in_bytes(n) * 2 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 4 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 8 == Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_narrow  $dst, $src\\t# narrow predicate $src\" %}\n@@ -2825,1 +4360,2 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n+    __ sve_vmaskcast_narrow(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n@@ -2830,1 +4366,3 @@\n-instruct vcvtStoL(vReg dst, vReg src)\n+\/\/ ------------------------------ Vector cast -------------------------------\n+\n+instruct vcvtBtoX_extend(vReg dst, vReg src)\n@@ -2832,3 +4370,2 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (VectorCastS2X src));\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorCastB2X src));\n@@ -2836,2 +4373,1 @@\n-  format %{ \"sve_sunpklo  $dst, S, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\t# convert S to L vector\" %}\n+  format %{ \"sve_vectorcast_b2x  $dst, $src\\t# convert B to X vector (extend)\" %}\n@@ -2839,2 +4375,6 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ B);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -2845,1 +4385,1 @@\n-instruct vcvtStoF(vReg dst, vReg src)\n+instruct vcvtStoB(vReg dst, vReg src, vReg tmp)\n@@ -2848,1 +4388,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n@@ -2850,0 +4390,1 @@\n+  effect(TEMP tmp);\n@@ -2851,2 +4392,1 @@\n-  format %{ \"sve_sunpklo  $dst, S, $src\\n\\t\"\n-            \"sve_scvtf  $dst, S, $dst, S\\t# convert S to F vector\" %}\n+  format %{ \"sve_vectorcast_s2b  $dst, $src\\t# convert H to B vector\" %}\n@@ -2854,2 +4394,2 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg), __ S);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ H, as_FloatRegister($tmp$$reg));\n@@ -2860,1 +4400,1 @@\n-instruct vcvtStoD(vReg dst, vReg src)\n+instruct vcvtStoX_extend(vReg dst, vReg src)\n@@ -2863,1 +4403,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) > 2);\n@@ -2865,4 +4405,2 @@\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, S, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\n\\t\"\n-            \"sve_scvtf  $dst, D, $dst, D\\t# convert S to D vector\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_s2x  $dst, $src\\t# convert H to X vector (extend)\" %}\n@@ -2870,3 +4408,6 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ D);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ H);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -2884,3 +4425,1 @@\n-  format %{ \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $src, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert I to B vector\" %}\n+  format %{ \"sve_vectorcast_i2b  $dst, $src\\t# convert I to B vector\" %}\n@@ -2888,3 +4427,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -2902,2 +4440,1 @@\n-  format %{ \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $src, tmp\\t# convert I to S vector\" %}\n+  format %{ \"sve_vectorcast_i2s $dst, $src\\t# convert I to H vector\" %}\n@@ -2905,2 +4442,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ H,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -2917,1 +4454,1 @@\n-  format %{ \"sve_sunpklo  $dst, D, $src\\t# convert I to L vector\" %}\n+  format %{ \"sve_vectorcast_i2l  $dst, $src\\t# convert I to L vector\" %}\n@@ -2919,1 +4456,1 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n@@ -2930,1 +4467,1 @@\n-  format %{ \"sve_scvtf  $dst, S, $src, S\\t# convert I to F vector\" %}\n+  format %{ \"sve_vectorcast_i2f  $dst, $src\\t# convert I to F vector\" %}\n@@ -2943,2 +4480,1 @@\n-  format %{ \"sve_sunpklo  $dst, D, $src\\n\\t\"\n-            \"sve_scvtf  $dst, D, $dst, D\\t# convert I to D vector\" %}\n+  format %{ \"sve_vectorcast_i2d  $dst, $src\\t# convert I to D vector\" %}\n@@ -2952,21 +4488,1 @@\n-instruct vcvtLtoB(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastL2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $src, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert L to B vector\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtLtoS(vReg dst, vReg src, vReg tmp)\n+instruct vcvtLtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -2974,2 +4490,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  predicate(UseSVE > 0 && is_integral_type(Matcher::vector_element_basic_type(n)));\n@@ -2978,18 +4493,0 @@\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $src, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t# convert L to S vector\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtLtoI(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorCastL2X src));\n-  effect(TEMP tmp);\n@@ -2997,2 +4494,1 @@\n-  format %{ \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $src, tmp\\t# convert L to I vector\" %}\n+  format %{ \"sve_vectorcast_l2x  $dst, $src\\t# convert L to B\/H\/S vector (narrow)\" %}\n@@ -3000,2 +4496,4 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($src$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -3013,3 +4511,1 @@\n-  format %{ \"sve_scvtf  $dst, S, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, $tmp\\t# convert L to F vector\" %}\n+  format %{ \"sve_vectorcast_l2f  $dst, $src\\t# convert L to F vector\" %}\n@@ -3018,2 +4514,3 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n+\n@@ -3030,1 +4527,1 @@\n-  format %{ \"sve_scvtf  $dst, D, $src, D\\t# convert L to D vector\" %}\n+  format %{ \"sve_vectorcast_l2d  $dst, $src\\t# convert L to D vector\" %}\n@@ -3037,21 +4534,1 @@\n-instruct vcvtFtoB(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastF2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\n\\t\"\n-            \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert F to B vector\" %}\n-  ins_encode %{\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtFtoS(vReg dst, vReg src, vReg tmp)\n+instruct vcvtFtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -3060,1 +4537,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT));\n@@ -3064,3 +4542,1 @@\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\n\\t\"\n-            \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\t# convert F to S vector\" %}\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to B\/H vector\" %}\n@@ -3068,0 +4544,2 @@\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n@@ -3069,2 +4547,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -3078,1 +4556,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -3081,1 +4559,1 @@\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\t# convert F to I vector\" %}\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to I vector\" %}\n@@ -3091,39 +4569,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (VectorCastF2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\t# convert F to L vector\" %}\n-  ins_encode %{\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtFtoD(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n-  match(Set dst (VectorCastF2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, D, $src\\n\\t\"\n-            \"sve_fcvt  $dst, D, $dst, S\\t# convert F to D vector\" %}\n-  ins_encode %{\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n-    __ sve_fcvt(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtDtoB(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastD2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert D to B vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(SVE_COST * 2);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to L vector\" %}\n@@ -3131,5 +4574,2 @@\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src$$reg), __ D);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -3140,1 +4580,1 @@\n-instruct vcvtDtoS(vReg dst, vReg src, vReg tmp)\n+instruct vcvtFtoD(vReg dst, vReg src)\n@@ -3143,8 +4583,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorCastD2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t# convert D to S vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_f2d  $dst, $dst\\t# convert F to D vector\" %}\n@@ -3152,4 +4588,2 @@\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src$$reg), __ D);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n+    __ sve_fcvt(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -3160,1 +4594,1 @@\n-instruct vcvtDtoI(vReg dst, vReg src, vReg tmp)\n+instruct vcvtDtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -3163,1 +4597,3 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -3167,3 +4603,1 @@\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, tmp\\t# convert D to I vector\" %}\n+  format %{ \"sve_vectorcast_d2x  $dst, $src\\t# convert D to X vector (narrow)\" %}\n@@ -3171,3 +4605,5 @@\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src$$reg), __ D);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -3184,1 +4620,1 @@\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\t# convert D to L vector\" %}\n+  format %{ \"sve_vectorcast_d2l  $dst, $src\\t# convert D to L vector\" %}\n@@ -3198,3 +4634,1 @@\n-  format %{ \"sve_fcvt  $dst, S, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, $tmp\\t# convert D to F vector\" %}\n+  format %{ \"sve_vectorcast_d2f  $dst, S, $dst\\t# convert D to F vector\" %}\n@@ -3203,2 +4637,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -3208,0 +4642,1 @@\n+\n@@ -3210,1 +4645,1 @@\n-instruct extractB(iRegINoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractB(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3214,1 +4649,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3216,1 +4651,1 @@\n-  format %{ \"sve_extract $dst, B, $pTmp, $src, $idx\\n\\t\"\n+  format %{ \"sve_extract $dst, B, $pgtmp, $src, $idx\\n\\t\"\n@@ -3219,1 +4654,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ B, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n@@ -3226,1 +4661,1 @@\n-instruct extractS(iRegINoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractS(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3230,1 +4665,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3232,1 +4667,1 @@\n-  format %{ \"sve_extract $dst, H, $pTmp, $src, $idx\\n\\t\"\n+  format %{ \"sve_extract $dst, H, $pgtmp, $src, $idx\\n\\t\"\n@@ -3235,1 +4670,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ H, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ H, as_PRegister($pgtmp$$reg),\n@@ -3243,1 +4678,1 @@\n-instruct extractI(iRegINoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractI(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3247,1 +4682,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3249,1 +4684,1 @@\n-  format %{ \"sve_extract $dst, S, $pTmp, $src, $idx\\t# extract from vector(I)\" %}\n+  format %{ \"sve_extract $dst, S, $pgtmp, $src, $idx\\t# extract from vector(I)\" %}\n@@ -3251,1 +4686,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ S, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ S, as_PRegister($pgtmp$$reg),\n@@ -3257,1 +4692,1 @@\n-instruct extractL(iRegLNoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractL(iRegLNoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3261,1 +4696,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3263,1 +4698,1 @@\n-  format %{ \"sve_extract $dst, D, $pTmp, $src, $idx\\t# extract from vector(L)\" %}\n+  format %{ \"sve_extract $dst, D, $pgtmp, $src, $idx\\t# extract from vector(L)\" %}\n@@ -3265,1 +4700,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ D, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ D, as_PRegister($pgtmp$$reg),\n@@ -3271,1 +4706,1 @@\n-instruct extractF(vRegF dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractF(vRegF dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3275,1 +4710,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3277,1 +4712,1 @@\n-  format %{ \"sve_extract $dst, S, $pTmp, $src, $idx\\t# extract from vector(F)\" %}\n+  format %{ \"sve_extract $dst, S, $pgtmp, $src, $idx\\t# extract from vector(F)\" %}\n@@ -3279,1 +4714,1 @@\n-    __ sve_extract(as_FloatRegister($dst$$reg), __ S, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg),\n@@ -3285,1 +4720,1 @@\n-instruct extractD(vRegD dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractD(vRegD dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3289,1 +4724,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3291,1 +4726,1 @@\n-  format %{ \"sve_extract $dst, D, $pTmp, $src, $idx\\t# extract from vector(D)\" %}\n+  format %{ \"sve_extract $dst, D, $pgtmp, $src, $idx\\t# extract from vector(D)\" %}\n@@ -3293,1 +4728,1 @@\n-    __ sve_extract(as_FloatRegister($dst$$reg), __ D, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg),\n@@ -3301,1 +4736,1 @@\n-instruct vtest_alltrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -3303,1 +4738,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3306,1 +4742,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3308,1 +4744,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, 0\\n\\t\"\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n@@ -3311,5 +4747,2 @@\n-    \/\/ \"src2\" is not used for sve.\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src1$$reg), 0);\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3321,1 +4754,1 @@\n-instruct vtest_anytrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n@@ -3323,1 +4756,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3326,1 +4760,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(KILL cr);\n@@ -3328,1 +4762,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, -1\\n\\t\"\n+  format %{ \"sve_ptest $src1\\n\\t\"\n@@ -3332,4 +4766,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src1$$reg), -1);\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n@@ -3341,1 +4772,1 @@\n-instruct vtest_alltrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_alltrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3343,1 +4774,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3346,1 +4778,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3350,1 +4782,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -3353,1 +4784,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -3355,2 +4786,2 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src1$$reg), 0);\n+    __ sve_eors(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3362,1 +4793,1 @@\n-instruct vtest_anytrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_anytrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3364,1 +4795,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3367,1 +4799,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3371,1 +4803,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -3374,1 +4805,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -3376,2 +4807,2 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src1$$reg), -1);\n+    __ sve_ands(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3385,1 +4816,1 @@\n-instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3392,1 +4823,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3394,2 +4825,2 @@\n-  format %{ \"sve_index $dst, -16, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $dst, -16, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3397,1 +4828,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -3402,1 +4833,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -3405,1 +4836,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -3410,1 +4841,1 @@\n-instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3415,1 +4846,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3418,1 +4849,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3420,1 +4851,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -3423,1 +4854,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -3426,1 +4857,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -3431,1 +4862,1 @@\n-instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -3438,1 +4869,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -3440,3 +4871,3 @@\n-  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_dup $dst, $idx\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_dup $dst, $idx\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -3444,1 +4875,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -3450,1 +4881,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -3453,1 +4884,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -3458,1 +4889,1 @@\n-instruct insertL(vReg dst, vReg src, iRegL val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertL(vReg dst, vReg src, iRegL val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3463,1 +4894,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3466,1 +4897,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3468,1 +4899,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (L)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (L)\" %}\n@@ -3471,1 +4902,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ D, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n@@ -3474,1 +4905,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -3479,1 +4910,1 @@\n-instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3484,1 +4915,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3487,1 +4918,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3489,1 +4920,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (D)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (D)\" %}\n@@ -3492,1 +4923,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ D, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n@@ -3495,1 +4926,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -3500,1 +4931,1 @@\n-instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -3505,1 +4936,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -3509,1 +4940,1 @@\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -3511,1 +4942,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -3515,1 +4946,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -3521,1 +4952,1 @@\n-               as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+               as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -3528,3 +4959,2 @@\n-instruct loadshuffleB(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+instruct loadshuffle(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0);\n@@ -3533,1 +4963,1 @@\n-  format %{ \"sve_orr $dst, $src, $src\\t# vector load shuffle (B)\" %}\n+  format %{ \"sve_loadshuffle $dst, $src\\t# vector load shuffle (B\/H\/S\/D)\" %}\n@@ -3535,4 +4965,9 @@\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ sve_orr(as_FloatRegister($dst$$reg),\n-                 as_FloatRegister($src$$reg),\n-                 as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                   as_FloatRegister($src$$reg));\n+      }\n+    } else {\n+      __ sve_vector_extend(as_FloatRegister($dst$$reg),  __ elemType_to_regVariant(bt),\n+                           as_FloatRegister($src$$reg), __ B);\n@@ -3544,46 +4979,0 @@\n-instruct loadshuffleS(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_uunpklo $dst, $src\\t# vector load shuffle (B to H)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleI(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\t# vector load shuffle (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleL(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\t# vector load shuffle (B to D)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3616,1 +5005,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -3631,2 +5020,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -3635,1 +5023,2 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base),\n+                       as_FloatRegister($idx$$reg));\n@@ -3642,1 +5031,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3648,1 +5037,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3650,2 +5039,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -3653,3 +5041,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -3661,1 +5048,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3667,1 +5054,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3669,3 +5056,36 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n@@ -3673,1 +5093,20 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -3675,0 +5114,21 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -3676,1 +5136,1 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -3691,1 +5151,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -3706,2 +5166,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -3709,2 +5168,1 @@\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D,\n-                   as_FloatRegister($idx$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n@@ -3717,1 +5175,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -3719,1 +5177,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3725,1 +5183,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3727,2 +5185,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -3730,1 +5187,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -3732,1 +5189,1 @@\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3738,1 +5195,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3744,1 +5201,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3746,3 +5203,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n@@ -3750,1 +5205,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n@@ -3753,1 +5208,56 @@\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3759,0 +5269,20 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -3814,2 +5344,1 @@\n-\n-instruct vmask_truecount(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_truecount(iRegINoSp dst, pReg src) %{\n@@ -3819,2 +5348,1 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n+  ins_cost(SVE_COST);\n@@ -3823,2 +5351,3 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($src$$reg));\n@@ -3829,1 +5358,1 @@\n-instruct vmask_firsttrue(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_firsttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n@@ -3833,2 +5362,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n+  effect(TEMP ptmp);\n+  ins_cost(2 * SVE_COST);\n@@ -3837,2 +5366,4 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($ptmp$$reg));\n@@ -3843,1 +5374,1 @@\n-instruct vmask_lasttrue(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_lasttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n@@ -3847,2 +5378,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n+  effect(TEMP ptmp);\n+  ins_cost(3 * SVE_COST);\n@@ -3851,2 +5382,2 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($src$$reg), as_PRegister($ptmp$$reg));\n@@ -3857,1 +5388,1 @@\n-instruct vmask_truecount_partial(iRegINoSp dst, vReg src, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vmask_truecount_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n@@ -3862,2 +5393,2 @@\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vmask_truecount $dst, $src\\t# vector mask truecount partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"vmask_truecount_partial $dst, $src\\t# vector mask truecount partial (sve)\" %}\n@@ -3865,4 +5396,4 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ B,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n@@ -3873,1 +5404,1 @@\n-instruct vmask_firsttrue_partial(iRegINoSp dst, vReg src, pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_firsttrue_partial(iRegINoSp dst, pReg src, pReg ptmp1, pReg ptmp2, rFlagsReg cr) %{\n@@ -3877,3 +5408,3 @@\n-  effect(TEMP pgtmp, TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"vmask_firsttrue $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n+  effect(TEMP ptmp1, TEMP ptmp2, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vmask_firsttrue_partial $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n@@ -3881,1 +5412,3 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B,\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp1$$reg), size,\n@@ -3883,2 +5416,2 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister($pgtmp$$reg), as_PRegister($ptmp$$reg));\n+    __ sve_brkb(as_PRegister($ptmp2$$reg), as_PRegister($ptmp1$$reg), as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp1$$reg), as_PRegister($ptmp2$$reg));\n@@ -3889,1 +5422,1 @@\n-instruct vmask_lasttrue_partial(iRegINoSp dst, vReg src, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vmask_lasttrue_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n@@ -3895,108 +5428,1 @@\n-  format %{ \"vmask_lasttrue $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ B,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ ----------------- Vector mask reductions combined with VectorMaskStore ---------------\n-\n-instruct vstoremask_truecount(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (VectorMaskTrueCount (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"vstoremask_truecount $dst, $src\\t# vector mask truecount (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_firsttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (VectorMaskFirstTrue (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vstoremask_firsttrue $dst, $src\\t# vector mask firsttrue (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_lasttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (VectorMaskLastTrue (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"vstoremask_lasttrue $dst, $src\\t# vector mask lasttrue (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_truecount_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskTrueCount (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vstoremask_truecount $dst, $src\\t# vector mask truecount partial (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_firsttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskFirstTrue (VectorStoreMask src esize)));\n-  effect(TEMP pgtmp, TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"vstoremask_firsttrue $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($pgtmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_lasttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskLastTrue (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"vstoremask_lasttrue $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n+  format %{ \"vmask_lasttrue_partial $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n@@ -4004,7 +5430,5 @@\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), ptrue, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n@@ -4013,1 +5437,1 @@\n-%}\n+%}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":2699,"deletions":1275,"binary":false,"changes":3974,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -142,5 +143,1 @@\n-        if (vlen < 4 || length_in_bytes > MaxVectorSize) {\n-          return false;\n-        } else {\n-          return true;\n-        }\n+        return vlen >= 4 && length_in_bytes <= MaxVectorSize;\n@@ -156,0 +153,8 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    if (opcode == Op_VectorRearrange) {\n+      return false;\n+    }\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -234,1 +239,1 @@\n-instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -238,1 +243,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -240,2 +245,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_ldr $dst, $pgtmp, $mem\\t# load vector partial\" %}\n@@ -244,1 +249,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -248,1 +253,1 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -254,1 +259,1 @@\n-instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -258,1 +263,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -260,2 +265,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_str $src, $pgtmp, $mem\\t# store vector partial\" %}\n@@ -264,1 +269,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -268,1 +273,1 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -272,1 +277,141 @@\n-%}dnl\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+dnl\n+dnl MASKALL_IMM($1,   $2  )\n+dnl MASKALL_IMM(type, size)\n+define(`MASKALL_IMM', `\n+instruct vmaskAll_imm$1(pRegGov dst, imm$1 src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    ifelse($1, `I', int, long) con = (ifelse($1, `I', int, long))$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl MASKALL($1,   $2  )\n+dnl MASKALL(type, size)\n+define(`MASKALL', `\n+instruct vmaskAll$1(pRegGov dst, ifelse($1, `I', iRegIorL2I, iRegL) src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ maskAll\n+MASKALL_IMM(I, B\/H\/S)\n+MASKALL(I, B\/H\/S)\n+MASKALL_IMM(L, D)\n+MASKALL(L, D)\n+\n+dnl\n+dnl MASK_LOGICAL_OP($1,        $2,      $3  )\n+dnl MASK_LOGICAL_OP(insn_name, op_name, insn)\n+define(`MASK_LOGICAL_OP', `\n+instruct vmask_$1(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd ($2 pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ $3(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and\/or\/xor\n+MASK_LOGICAL_OP(and, AndVMask, sve_and)\n+MASK_LOGICAL_OP(or, OrVMask, sve_orr)\n+MASK_LOGICAL_OP(xor, XorVMask, sve_eor)\n@@ -274,0 +419,19 @@\n+dnl\n+dnl MASK_LOGICAL_AND_NOT($1,   $2  )\n+dnl MASK_LOGICAL_AND_NOT(type, size)\n+define(`MASK_LOGICAL_AND_NOT', `\n+instruct vmask_and_not$1(pRegGov pd, pRegGov pn, pRegGov pm, imm$1_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and_not\n+MASK_LOGICAL_AND_NOT(I, B\/H\/S)\n+MASK_LOGICAL_AND_NOT(L, D)\n@@ -289,1 +453,1 @@\n-instruct reinterpretResize(vReg dst, vReg src, pRegGov pTmp, rFlagsReg cr) %{\n+instruct reinterpretResize(vReg dst, vReg src, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -293,1 +457,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -303,1 +467,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ B, length_in_bytes_resize);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n@@ -305,1 +469,1 @@\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pTmp$$reg),\n+    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n@@ -310,0 +474,34 @@\n+\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmp(Assembler::EQ, as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -311,3 +509,3 @@\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE($1,        $2,      $3,           $4,   $5,          %6  )\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE(insn_name, op_name, element_type, size, min_vec_len, insn)\n-define(`UNARY_OP_TRUE_PREDICATE_ETYPE', `\n+dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_TRUE_PREDICATE', `\n@@ -316,1 +514,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == $3);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -319,1 +517,1 @@\n-  format %{ \"$6 $dst, $src\\t# vector (sve) ($4)\" %}\n+  format %{ \"$4 $dst, $src\\t# vector (sve) ($3)\" %}\n@@ -321,1 +519,1 @@\n-    __ $6(as_FloatRegister($dst$$reg), __ $4,\n+    __ $4(as_FloatRegister($dst$$reg), __ $3,\n@@ -329,10 +527,34 @@\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsB, AbsVB, T_BYTE,   B, 16, sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsS, AbsVS, T_SHORT,  H, 8,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsI, AbsVI, T_INT,    S, 4,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsL, AbsVL, T_LONG,   D, 2,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsF, AbsVF, T_FLOAT,  S, 4,  sve_fabs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsD, AbsVD, T_DOUBLE, D, 2,  sve_fabs)\n-dnl\n-dnl BINARY_OP_UNPREDICATED($1,        $2       $3,   $4           $5  )\n-dnl BINARY_OP_UNPREDICATED(insn_name, op_name, size, min_vec_len, insn)\n-define(`BINARY_OP_UNPREDICATED', `\n+UNARY_OP_TRUE_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_TRUE_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl UNARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src, $pg, $dst_src\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\/\/ vector abs - predicated\n+UNARY_OP_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl\n+dnl BINARY_OP_UNPREDICATE($1,        $2       $3,   $4           $5  )\n+dnl BINARY_OP_UNPREDICATE(insn_name, op_name, size, min_vec_len, insn)\n+define(`BINARY_OP_UNPREDICATE', `\n@@ -351,1 +573,18 @@\n-\n+dnl\n+dnl\n+dnl BINARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl BINARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`BINARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src1$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -353,9 +592,18 @@\n-BINARY_OP_UNPREDICATED(vaddB, AddVB, B, 16, sve_add)\n-BINARY_OP_UNPREDICATED(vaddS, AddVS, H, 8,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddI, AddVI, S, 4,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddL, AddVL, D, 2,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddF, AddVF, S, 4,  sve_fadd)\n-BINARY_OP_UNPREDICATED(vaddD, AddVD, D, 2,  sve_fadd)\n-dnl\n-dnl BINARY_OP_UNSIZED($1,        $2,      $3,          $4  )\n-dnl BINARY_OP_UNSIZED(insn_name, op_name, min_vec_len, insn)\n+BINARY_OP_UNPREDICATE(vaddB, AddVB, B, 16, sve_add)\n+BINARY_OP_UNPREDICATE(vaddS, AddVS, H, 8,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddI, AddVI, S, 4,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddL, AddVL, D, 2,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddF, AddVF, S, 4,  sve_fadd)\n+BINARY_OP_UNPREDICATE(vaddD, AddVD, D, 2,  sve_fadd)\n+\n+\/\/ vector add - predicated\n+BINARY_OP_PREDICATE(vaddB, AddVB, B, sve_add)\n+BINARY_OP_PREDICATE(vaddS, AddVS, H, sve_add)\n+BINARY_OP_PREDICATE(vaddI, AddVI, S, sve_add)\n+BINARY_OP_PREDICATE(vaddL, AddVL, D, sve_add)\n+BINARY_OP_PREDICATE(vaddF, AddVF, S, sve_fadd)\n+BINARY_OP_PREDICATE(vaddD, AddVD, D, sve_fadd)\n+\n+dnl\n+dnl BINARY_OP_UNSIZED($1,        $2,      $3  )\n+dnl BINARY_OP_UNSIZED(insn_name, op_name, insn)\n@@ -367,1 +615,1 @@\n-  format %{ \"$4  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"$3  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -369,1 +617,1 @@\n-    __ $4(as_FloatRegister($dst$$reg),\n+    __ $3(as_FloatRegister($dst$$reg),\n@@ -375,1 +623,1 @@\n-\n+dnl\n@@ -377,1 +625,1 @@\n-BINARY_OP_UNSIZED(vand, AndV, 16, sve_and)\n+BINARY_OP_UNSIZED(vand, AndV, sve_and)\n@@ -380,1 +628,1 @@\n-BINARY_OP_UNSIZED(vor, OrV, 16, sve_orr)\n+BINARY_OP_UNSIZED(vor, OrV, sve_orr)\n@@ -383,1 +631,28 @@\n-BINARY_OP_UNSIZED(vxor, XorV, 16, sve_eor)\n+BINARY_OP_UNSIZED(vxor, XorV, sve_eor)\n+\n+dnl BINARY_LOGIC_OP_PREDICATE($1,        $2,      $3  )\n+dnl BINARY_LOGIC_OP_PREDICATE(insn_name, op_name, insn)\n+define(`BINARY_LOGIC_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ $3(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector and - predicated\n+BINARY_LOGIC_OP_PREDICATE(vand, AndV, sve_and)\n+\n+\/\/ vector or - predicated\n+BINARY_LOGIC_OP_PREDICATE(vor, OrV, sve_orr)\n+\n+\/\/ vector xor - predicated\n+BINARY_LOGIC_OP_PREDICATE(vxor, XorV, sve_eor)\n@@ -409,1 +684,1 @@\n-\n+dnl\n@@ -450,1 +725,1 @@\n-\n+dnl\n@@ -455,1 +730,3 @@\n-\/\/ vector min\/max\n+\/\/ vector float div - predicated\n+BINARY_OP_PREDICATE(vfdivF, DivVF, S, sve_fdiv)\n+BINARY_OP_PREDICATE(vfdivD, DivVD, D, sve_fdiv)\n@@ -457,1 +734,5 @@\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX($1     , $2, $3   , $4  )\n+dnl VMINMAX(op_name, op, finsn, insn)\n+define(`VMINMAX', `\n+instruct v$1(vReg dst_src1, vReg src2) %{\n@@ -459,1 +740,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src1 ($2 dst_src1 src2));\n@@ -461,1 +742,1 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -466,1 +747,1 @@\n-      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n@@ -469,2 +750,2 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n@@ -475,1 +756,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max\n+VMINMAX(min, MinV, sve_fmin, sve_smin)\n+VMINMAX(max, MaxV, sve_fmax, sve_smax)\n@@ -477,1 +762,5 @@\n-instruct vmax(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX_PREDICATE($1     , $2, $3   , $4  )\n+dnl VMINMAX_PREDICATE(op_name, op, finsn, insn)\n+define(`VMINMAX_PREDICATE', `\n+instruct v$1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -479,1 +768,1 @@\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n@@ -481,1 +770,1 @@\n-  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n@@ -486,2 +775,2 @@\n-      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -489,3 +778,3 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -495,1 +784,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max - predicated\n+VMINMAX_PREDICATE(min, MinV, sve_fmin, sve_smin)\n+VMINMAX_PREDICATE(max, MaxV, sve_fmax, sve_smax)\n@@ -518,0 +811,21 @@\n+dnl\n+dnl VFMLA_PREDICATE($1,   $2  )\n+dnl VFMLA_PREDICATE(type, size)\n+define(`VFMLA_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmla$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fmla - predicated\n+VFMLA_PREDICATE(F, S)\n+VFMLA_PREDICATE(D, D)\n+\n@@ -648,1 +962,1 @@\n-\n+dnl\n@@ -654,2 +968,10 @@\n-BINARY_OP_UNPREDICATED(vmulF, MulVF, S, 4, sve_fmul)\n-BINARY_OP_UNPREDICATED(vmulD, MulVD, D, 2, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulF, MulVF, S, 4, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulD, MulVD, D, 2, sve_fmul)\n+\n+\/\/ vector mul - predicated\n+BINARY_OP_PREDICATE(vmulB, MulVB, B, sve_mul)\n+BINARY_OP_PREDICATE(vmulS, MulVS, H, sve_mul)\n+BINARY_OP_PREDICATE(vmulI, MulVI, S, sve_mul)\n+BINARY_OP_PREDICATE(vmulL, MulVL, D, sve_mul)\n+BINARY_OP_PREDICATE(vmulF, MulVF, S, sve_fmul)\n+BINARY_OP_PREDICATE(vmulD, MulVD, D, sve_fmul)\n@@ -657,16 +979,0 @@\n-dnl\n-dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4,            $5  )\n-dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, min_vec_bytes, insn)\n-define(`UNARY_OP_TRUE_PREDICATE', `\n-instruct $1(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst ($2 src));\n-  ins_cost(SVE_COST);\n-  format %{ \"$5 $dst, $src\\t# vector (sve) ($3)\" %}\n-  ins_encode %{\n-    __ $5(as_FloatRegister($dst$$reg), __ $3,\n-         ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n@@ -674,2 +980,6 @@\n-UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, 16, sve_fneg)\n-UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, 16, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, sve_fneg)\n+\n+\/\/ vector fneg - predicated\n+UNARY_OP_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_PREDICATE(vnegD, NegVD, D, sve_fneg)\n@@ -691,1 +1001,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -694,4 +1004,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -700,1 +1009,1 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n@@ -702,2 +1011,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -708,3 +1015,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -712,5 +1017,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -718,6 +1022,3 @@\n-    Assembler::SIMD_RegVariant size =\n-      __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n@@ -728,1 +1029,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -730,2 +1031,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -733,5 +1033,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -739,6 +1037,4 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src3$$reg),\n-                   as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n@@ -751,1 +1047,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -755,0 +1051,1 @@\n+  effect(KILL cr);\n@@ -756,14 +1053,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n-  ins_encode %{\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmaskS(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -771,2 +1055,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n+               ptrue, as_FloatRegister($src$$reg), 0);\n@@ -777,4 +1061,2 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n@@ -782,0 +1064,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -783,21 +1066,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmaskL(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n@@ -805,4 +1068,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -815,1 +1078,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -819,1 +1082,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -821,2 +1084,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -827,1 +1089,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremask_narrow(vReg dst, pRegGov src, vReg tmp, immI_gt_1 size) %{\n@@ -832,3 +1094,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (H\/S\/D)\" %}\n@@ -836,6 +1096,4 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($dst$$reg), size, as_FloatRegister($tmp$$reg));\n@@ -846,20 +1104,1 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n+\/\/ Combine LoadVector+VectorLoadMask when the vector element type is not T_BYTE\n@@ -867,10 +1106,9 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+instruct vloadmask_loadV(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (H\/S\/D)\" %}\n@@ -878,9 +1116,8 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+               ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -890,8 +1127,6 @@\n-dnl\n-dnl\n-dnl VLOADMASK_LOADV($1,    $2  )\n-dnl VLOADMASK_LOADV(esize, cond)\n-define(`VLOADMASK_LOADV', `\n-instruct vloadmask_loadV_$1(vReg dst, ifelse($1, `byte', vmemA, indirect) mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n-            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) $2);\n+\n+instruct vloadmask_loadV_partial(pRegGov dst, indirect mem, vReg vtmp, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() > 16 &&\n+            n->as_Vector()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n@@ -899,3 +1134,3 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"vloadmask_loadV $dst, $mem\\t# load vector mask partial (sve) (H\/S\/D)\" %}\n@@ -903,1 +1138,2 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -905,3 +1141,4 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n@@ -909,1 +1146,1 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n@@ -912,11 +1149,7 @@\n-%}')dnl\n-dnl\n-define(`ARGLIST',\n-`ifelse($1, `byte', vmemA, indirect) mem, vReg src, vReg tmp, ifelse($1, `byte', immI_1, immI_gt_1) esize')\n-dnl\n-dnl STOREV_VSTOREMASK($1,  )\n-dnl STOREV_VSTOREMASK(esize)\n-define(`STOREV_VSTOREMASK', `\n-instruct storeV_vstoremask_$1(ARGLIST($1)) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n+%}\n+\n+\/\/ Combine VectorStoreMask+StoreVector when the vector element type is not T_BYTE\n+\n+instruct storeV_vstoremask(indirect mem, pRegGov src, vReg tmp, immI_gt_1 esize) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) == MaxVectorSize);\n@@ -925,3 +1158,3 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (H\/S\/D)\" %}\n@@ -931,3 +1164,2 @@\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant($esize$$constant);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), size, as_PRegister($src$$reg), 1, false);\n@@ -939,10 +1171,1 @@\n-%}')dnl\n-undefine(ARGLIST)dnl\n-dnl\n-\/\/ load\/store mask vector\n-VLOADMASK_LOADV(byte, == 1)\n-VLOADMASK_LOADV(non_byte, > 1)\n-STOREV_VSTOREMASK(byte)\n-STOREV_VSTOREMASK(non_byte)\n-\n-\/\/ vector add reduction\n+%}\n@@ -950,4 +1173,36 @@\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct storeV_vstoremask_partial(indirect mem, pRegGov src, vReg vtmp,\n+                                   immI_gt_1 esize, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() > 16 &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) < MaxVectorSize);\n+  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeV_vstoremask $src, $mem\\t# store vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+dnl\n+dnl REDUCE_I($1,        $2     )\n+dnl REDUCE_I(insn_name, op_name)\n+define(`REDUCE_I', `\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -955,1 +1210,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve) (may extend)\" %}\n@@ -958,11 +1213,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -971,3 +1218,29 @@\n-%}\n-\n-instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl\n+dnl REDUCE_L($1,        $2    )\n+dnl REDUCE_L(insn_name, op_name)\n+define(`REDUCE_L', `\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PARTIAL', `\n+instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -975,2 +1248,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -978,2 +1256,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve) (may extend)\" %}\n@@ -985,25 +1263,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1012,3 +1268,6 @@\n-%}\n-\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PARTIAL', `\n+instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1016,2 +1275,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -1019,2 +1283,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction partial (sve)\" %}\n@@ -1024,4 +1288,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1030,2 +1293,1 @@\n-%}\n-\n+%}')dnl\n@@ -1036,3 +1298,4 @@\n-instruct $1($3 src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1($3 src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1052,3 +1315,4 @@\n-instruct $1($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1_partial($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1057,1 +1321,1 @@\n-  format %{ \"sve_reduce_add$2 $src1_dst, $src1_dst, $src2\\t# add$2 reduction partial (sve) ($4)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $src1_dst, $src2\\t# $1 reduction partial (sve) ($4)\" %}\n@@ -1067,12 +1331,13 @@\n-REDUCE_ADDF(reduce_addF, F, vRegF, S)\n-REDUCE_ADDF_PARTIAL(reduce_addF_partial, F, vRegF, S)\n-REDUCE_ADDF(reduce_addD, D, vRegD, D)\n-REDUCE_ADDF_PARTIAL(reduce_addD_partial, D, vRegD, D)\n-\n-\/\/ vector and reduction\n-\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+dnl\n+dnl REDUCE_I_PREDICATE($1,        $2     )\n+dnl REDUCE_I_PREDICATE(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1080,1 +1345,1 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve) (may extend)\" %}\n@@ -1083,11 +1348,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1096,8 +1353,14 @@\n-%}\n-\n-instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE($1,        $2    )\n+dnl REDUCE_L_PREDICATE(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1105,1 +1368,24 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PREDICATE_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve) (may extend)\" %}\n@@ -1111,26 +1397,5 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1139,7 +1404,14 @@\n-%}\n-\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1147,2 +1419,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n@@ -1152,4 +1424,5 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1158,9 +1431,9 @@\n-%}\n-\n-\/\/ vector or reduction\n-\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE', `\n+instruct reduce_$1_masked($3 src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n@@ -1168,1 +1441,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated (sve)\" %}\n@@ -1170,12 +1443,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1184,8 +1447,10 @@\n-%}\n-\n-instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE_PARTIAL', `\n+instruct reduce_$1_masked_partial($3 src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n@@ -1193,1 +1458,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated partial (sve)\" %}\n@@ -1195,3 +1460,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n@@ -1199,11 +1462,4 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1212,1 +1468,2 @@\n-%}\n+%}')dnl\n+dnl\n@@ -1214,14 +1471,19 @@\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n+\/\/ vector add reduction\n+REDUCE_I(add, AddReductionVI)\n+REDUCE_L(add, AddReductionVL)\n+REDUCE_ADDF(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PARTIAL(addD, AddReductionVD, vRegD, D)\n+\n+\/\/ vector add reduction - predicated\n+REDUCE_I_PREDICATE(add, AddReductionVI)\n+REDUCE_L_PREDICATE(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PREDICATE_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PREDICATE_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addD, AddReductionVD, vRegD, D)\n@@ -1229,18 +1491,23 @@\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n+\/\/ vector and reduction\n+REDUCE_I(and, AndReductionV)\n+REDUCE_L(and, AndReductionV)\n+REDUCE_I_PARTIAL(and, AndReductionV)\n+REDUCE_L_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector and reduction - predicated\n+REDUCE_I_PREDICATE(and, AndReductionV)\n+REDUCE_L_PREDICATE(and, AndReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(and, AndReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector or reduction\n+REDUCE_I(or, OrReductionV)\n+REDUCE_L(or, OrReductionV)\n+REDUCE_I_PARTIAL(or, OrReductionV)\n+REDUCE_L_PARTIAL(or, OrReductionV)\n+\n+\/\/ vector or reduction - predicated\n+REDUCE_I_PREDICATE(or, OrReductionV)\n+REDUCE_L_PREDICATE(or, OrReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(or, OrReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(or, OrReductionV)\n@@ -1249,0 +1516,4 @@\n+REDUCE_I(eor, XorReductionV)\n+REDUCE_L(eor, XorReductionV)\n+REDUCE_I_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PARTIAL(eor, XorReductionV)\n@@ -1250,5 +1521,17 @@\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+\/\/ vector xor reduction - predicated\n+REDUCE_I_PREDICATE(eor, XorReductionV)\n+REDUCE_L_PREDICATE(eor, XorReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(eor, XorReductionV)\n+\n+dnl\n+dnl REDUCE_MAXMIN_I($1,        $2     )\n+dnl REDUCE_MAXMIN_I(insn_name, op_name)\n+define(`REDUCE_MAXMIN_I', `\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1256,1 +1539,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve)\" %}\n@@ -1259,11 +1542,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1272,3 +1547,25 @@\n-%}\n-\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L($1,        $2     )\n+dnl REDUCE_MAXMIN_L(insn_name, op_name)\n+define(`REDUCE_MAXMIN_L', `\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_I_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PARTIAL', `\n+instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -1276,3 +1573,5 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 src1 src2));\n@@ -1280,2 +1579,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve)\" %}\n@@ -1287,11 +1586,3 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1300,18 +1591,6 @@\n-%}\n-\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PARTIAL', `\n+instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1319,3 +1598,4 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 src1 src2));\n@@ -1323,2 +1603,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction  partial (sve)\" %}\n@@ -1328,4 +1608,3 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1334,2 +1613,1 @@\n-%}\n-\n+%}')dnl\n@@ -1337,10 +1615,11 @@\n-dnl REDUCE_MAXMIN_I($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I(min_max, op_mame, cmp)\n-define(`REDUCE_MAXMIN_I', `\n-instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst ($2 src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+dnl REDUCE_MAXMIN_I_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1348,1 +1627,1 @@\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1B\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve)\" %}\n@@ -1351,5 +1630,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1360,8 +1637,10 @@\n-dnl REDUCE_MAXMIN_L($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L(min_max, op_name, cmp)\n-define(`REDUCE_MAXMIN_L', `\n-instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst ($2 src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+dnl REDUCE_MAXMIN_L_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1369,1 +1648,1 @@\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n@@ -1371,4 +1650,3 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1379,10 +1657,10 @@\n-dnl REDUCE_MAXMIN_I_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_mame, cmp)\n-define(`REDUCE_MAXMIN_I_PARTIAL', `\n-instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst ($2 src1 src2));\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1390,2 +1668,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1I partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve)\" %}\n@@ -1397,5 +1675,5 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1406,8 +1684,9 @@\n-dnl REDUCE_MAXMIN_L_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name, cmp)\n-define(`REDUCE_MAXMIN_L_PARTIAL', `\n-instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst ($2 src1 src2));\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1415,2 +1694,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n@@ -1420,5 +1699,5 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1433,1 +1712,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1438,2 +1718,1 @@\n-  format %{ \"sve_f$1v $dst, $src2 # vector (sve) ($4)\\n\\t\"\n-            \"f$1s $dst, $dst, $src1\\t# $1 reduction $2\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction (sve)\" %}\n@@ -1441,2 +1720,1 @@\n-    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src2$$reg));\n@@ -1448,1 +1726,0 @@\n-dnl\n@@ -1454,1 +1731,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1459,1 +1737,1 @@\n-  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# reduce $1 $4 partial (sve)\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction partial (sve)\" %}\n@@ -1463,0 +1741,40 @@\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE', `\n+instruct reduce_$1$2_masked($5 dst, $5 src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE_PARTIAL', `\n+instruct reduce_$1$2_masked_partial($5 dst, $5 src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -1464,1 +1782,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1469,1 +1787,0 @@\n-\n@@ -1471,4 +1788,4 @@\n-REDUCE_MAXMIN_I(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV, GT)\n+REDUCE_MAXMIN_I(max, MaxReductionV)\n+REDUCE_MAXMIN_L(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV)\n@@ -1480,0 +1797,10 @@\n+\/\/ vector max reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_FMINMAX_PREDICATE(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(max, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, D, T_DOUBLE, D, vRegD)\n+\n@@ -1481,4 +1808,4 @@\n-REDUCE_MAXMIN_I(min, MinReductionV, LT)\n-REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV, LT)\n+REDUCE_MAXMIN_I(min, MinReductionV)\n+REDUCE_MAXMIN_L(min, MinReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV)\n@@ -1490,0 +1817,10 @@\n+\/\/ vector min reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_FMINMAX_PREDICATE(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(min, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, D, T_DOUBLE, D, vRegD)\n+\n@@ -1667,3 +2004,42 @@\n-\/\/ vector sqrt\n-UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, 16, sve_fsqrt)\n-UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, 16, sve_fsqrt)\n+\/\/ vector shift - predicated\n+BINARY_OP_PREDICATE(vasrB, RShiftVB,  B, sve_asr)\n+BINARY_OP_PREDICATE(vasrS, RShiftVS,  H, sve_asr)\n+BINARY_OP_PREDICATE(vasrI, RShiftVI,  S, sve_asr)\n+BINARY_OP_PREDICATE(vasrL, RShiftVL,  D, sve_asr)\n+BINARY_OP_PREDICATE(vlslB, LShiftVB,  B, sve_lsl)\n+BINARY_OP_PREDICATE(vlslS, LShiftVS,  H, sve_lsl)\n+BINARY_OP_PREDICATE(vlslI, LShiftVI,  S, sve_lsl)\n+BINARY_OP_PREDICATE(vlslL, LShiftVL,  D, sve_lsl)\n+BINARY_OP_PREDICATE(vlsrB, URShiftVB, B, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrS, URShiftVS, H, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrI, URShiftVI, S, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrL, URShiftVL, D, sve_lsr)\n+dnl\n+dnl VSHIFT_IMM_PREDICATED($1,        $2,      $3,       $4,   $5,   $6  )\n+dnl VSHIFT_IMM_PREDICATED(insn_name, op_name, op_name2, type, size, insn)\n+define(`VSHIFT_IMM_PREDICATED', `\n+instruct $1_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 (Binary dst_src ($3 shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$6 $dst_src, $pg, $dst_src, $shift\\t# vector (sve) ($4)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con ifelse(index(`$1', `vlsl'), 0, `>=', `>') 0 && con < $5, \"invalid shift immediate\");\n+    __ $6(as_FloatRegister($dst_src$$reg), __ $4, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+VSHIFT_IMM_PREDICATED(vasrB, RShiftVB,  RShiftCntV, B, 8,  sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrS, RShiftVS,  RShiftCntV, H, 16, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrI, RShiftVI,  RShiftCntV, S, 32, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrL, RShiftVL,  RShiftCntV, D, 64, sve_asr)\n+VSHIFT_IMM_PREDICATED(vlsrB, URShiftVB, RShiftCntV, B, 8,  sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrS, URShiftVS, RShiftCntV, H, 16, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrI, URShiftVI, RShiftCntV, S, 32, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrL, URShiftVL, RShiftCntV, D, 64, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlslB, LShiftVB,  LShiftCntV, B, 8,  sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslS, LShiftVS,  LShiftCntV, H, 16, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslI, LShiftVI,  LShiftCntV, S, 32, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslL, LShiftVL,  LShiftCntV, D, 64, sve_lsl)\n@@ -1671,7 +2047,3 @@\n-\/\/ vector sub\n-BINARY_OP_UNPREDICATED(vsubB, SubVB, B, 16, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubS, SubVS, H, 8, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubI, SubVI, S, 4, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubL, SubVL, D, 2, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubF, SubVF, S, 4, sve_fsub)\n-BINARY_OP_UNPREDICATED(vsubD, SubVD, D, 2, sve_fsub)\n+\/\/ vector sqrt\n+UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n@@ -1679,1 +2051,3 @@\n-\/\/ vector mask cast\n+\/\/ vector sqrt - predicated\n+UNARY_OP_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n@@ -1681,2 +2055,21 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+\/\/ vector sub\n+BINARY_OP_UNPREDICATE(vsubB, SubVB, B, 16, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubS, SubVS, H, 8, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubI, SubVI, S, 4, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubL, SubVL, D, 2, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubF, SubVF, S, 4, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubD, SubVD, D, 2, sve_fsub)\n+\n+\/\/ vector sub - predicated\n+BINARY_OP_PREDICATE(vsubB, SubVB, B, sve_sub)\n+BINARY_OP_PREDICATE(vsubS, SubVS, H, sve_sub)\n+BINARY_OP_PREDICATE(vsubI, SubVI, S, sve_sub)\n+BINARY_OP_PREDICATE(vsubL, SubVL, D, sve_sub)\n+BINARY_OP_PREDICATE(vsubF, SubVF, S, sve_fsub)\n+BINARY_OP_PREDICATE(vsubD, SubVD, D, sve_fsub)\n+\n+\/\/ ------------------------------ Vector mask cast --------------------------\n+\n+instruct vmaskcast(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n@@ -1684,1 +2077,1 @@\n-  match(Set dst (VectorMaskCast dst));\n+  match(Set dst_src (VectorMaskCast dst_src));\n@@ -1686,1 +2079,1 @@\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n@@ -1693,0 +2086,33 @@\n+instruct vmaskcast_extend(pRegGov dst, pReg src)\n+%{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_length_in_bytes(n) == 2 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 4 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 8 * Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_extend  $dst, $src\\t# extend predicate $src\" %}\n+  ins_encode %{\n+    __ sve_vmaskcast_extend(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcast_narrow(pRegGov dst, pReg src)\n+%{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_length_in_bytes(n) * 2 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 4 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 8 == Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_narrow  $dst, $src\\t# narrow predicate $src\" %}\n+  ins_encode %{\n+    __ sve_vmaskcast_narrow(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+dnl\n+\n@@ -1696,1 +2122,1 @@\n-define(`VECTOR_CAST_EXTEND1', `\n+define(`VECTOR_CAST_X2X', `\n@@ -1703,1 +2129,1 @@\n-  format %{ \"sve_$3  $dst, $4, $src\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_$5  $dst, $src\\t# convert $1 to $2 vector\" %}\n@@ -1705,1 +2131,1 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n+    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n@@ -1709,0 +2135,1 @@\n+\n@@ -1710,0 +2137,1 @@\n+dnl Start of vector cast rules\n@@ -1711,2 +2139,1 @@\n-define(`VECTOR_CAST_EXTEND2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+instruct vcvtBtoX_extend(vReg dst, vReg src)\n@@ -1714,3 +2141,2 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorCastB2X src));\n@@ -1718,2 +2144,1 @@\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_b2x  $dst, $src\\t# convert B to X vector (extend)\" %}\n@@ -1721,2 +2146,6 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ B);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -1725,5 +2154,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_EXTEND3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtStoB(vReg dst, vReg src, vReg tmp)\n@@ -1732,6 +2159,5 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\n\\t\"\n-            \"sve_$3  $dst, $6, $dst\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastS2X src));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_s2b  $dst, $src\\t# convert H to B vector\" %}\n@@ -1739,3 +2165,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ H, as_FloatRegister($tmp$$reg));\n@@ -1744,5 +2169,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_NARROW1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtStoX_extend(vReg dst, vReg src)\n@@ -1751,3 +2174,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP tmp);\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) > 2);\n+  match(Set dst (VectorCastS2X src));\n@@ -1755,2 +2177,1 @@\n-  format %{ \"sve_$3  $tmp, $4, 0\\n\\t\"\n-            \"sve_$5  $dst, $4, $src, tmp\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_s2x  $dst, $src\\t# convert H to X vector (extend)\" %}\n@@ -1758,2 +2179,6 @@\n-    __ sve_$3(as_FloatRegister($tmp$$reg), __ $4, 0);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ H);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -1762,5 +2187,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_NARROW2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtItoB(vReg dst, vReg src, vReg tmp)\n@@ -1769,2 +2192,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastI2X src));\n@@ -1773,3 +2196,1 @@\n-  format %{ \"sve_$3  $tmp, $4, 0\\n\\t\"\n-            \"sve_$5  $dst, $4, $src, tmp\\n\\t\"\n-            \"sve_$5  $dst, $6, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_i2b  $dst, $src\\t# convert I to B vector\" %}\n@@ -1777,3 +2198,2 @@\n-    __ sve_$3(as_FloatRegister($tmp$$reg), __ $4, 0);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -1782,5 +2202,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_NARROW3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtItoS(vReg dst, vReg src, vReg tmp)\n@@ -1789,8 +2207,5 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_$3  $tmp, $4, 0\\n\\t\"\n-            \"sve_$5  $dst, $4, $src, tmp\\n\\t\"\n-            \"sve_$5  $dst, $6, $dst, tmp\\n\\t\"\n-            \"sve_$5  $dst, $7, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorCastI2X src));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_i2s $dst, $src\\t# convert I to H vector\" %}\n@@ -1798,4 +2213,2 @@\n-    __ sve_$3(as_FloatRegister($tmp$$reg), __ $4, 0);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $7, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ H,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -1804,5 +2217,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_I2F_EXTEND2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtItoL(vReg dst, vReg src)\n@@ -1811,6 +2222,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\n\\t\"\n-            \"sve_$6  $dst, $5, $dst, $5\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorCastI2X src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_vectorcast_i2l  $dst, $src\\t# convert I to L vector\" %}\n@@ -1818,3 +2227,1 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n-    __ sve_$6(as_FloatRegister($dst$$reg), __ $5, ptrue, as_FloatRegister($dst$$reg), __ $5);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n@@ -1823,2 +2230,1 @@\n-%}')dnl\n-dnl\n+%}\n@@ -1826,2 +2232,4 @@\n-define(`VECTOR_CAST_I2F_EXTEND3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+dnl vcvtItoF\n+VECTOR_CAST_X2X(I, F, scvtf, S, i2f)\n+\n+instruct vcvtItoD(vReg dst, vReg src)\n@@ -1830,7 +2238,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\n\\t\"\n-            \"sve_$3  $dst, $6, $dst\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, $6\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastI2X src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_i2d  $dst, $src\\t# convert I to D vector\" %}\n@@ -1838,4 +2243,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, ptrue, as_FloatRegister($dst$$reg), __ $6);\n+    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_scvtf(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ D);\n@@ -1844,5 +2247,19 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_X2F_NARROW1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtLtoX_narrow(vReg dst, vReg src, vReg tmp)\n+%{\n+  predicate(UseSVE > 0 && is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastL2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_l2x  $dst, $src\\t# convert L to B\/H\/S vector (narrow)\" %}\n+  ins_encode %{\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($src$$reg), __ D, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtLtoF(vReg dst, vReg src, vReg tmp)\n@@ -1851,2 +2268,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastL2X src));\n@@ -1855,3 +2272,1 @@\n-  format %{ \"sve_$3  $dst, $4, $src, $5\\n\\t\"\n-            \"sve_$6  $tmp, $7, 0\\n\\t\"\n-            \"sve_$8  $dst, $7, $dst, $tmp\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_l2f  $dst, $src\\t# convert L to F vector\" %}\n@@ -1859,3 +2274,4 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $5);\n-    __ sve_$6(as_FloatRegister($tmp$$reg), __ $7, 0);\n-    __ sve_$8(as_FloatRegister($dst$$reg), __ $7, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_scvtf(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n+\n@@ -1864,2 +2280,1 @@\n-%}')dnl\n-dnl\n+%}\n@@ -1867,2 +2282,4 @@\n-define(`VECTOR_CAST_X2X', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+dnl vcvtLtoD\n+VECTOR_CAST_X2X(L, D, scvtf, D, l2d)\n+\n+instruct vcvtFtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -1871,4 +2288,6 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT));\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to B\/H vector\" %}\n@@ -1876,1 +2295,5 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -1879,5 +2302,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_X2F_EXTEND1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtFtoI(vReg dst, vReg src)\n@@ -1886,5 +2307,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$5  $dst, $4, $dst, $6\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to I vector\" %}\n@@ -1892,2 +2312,1 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($dst$$reg), __ $6);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n@@ -1896,5 +2315,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_F2X_NARROW1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtFtoL(vReg dst, vReg src)\n@@ -1903,7 +2320,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $tmp, $6, 0\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, tmp\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(SVE_COST * 2);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to L vector\" %}\n@@ -1911,3 +2325,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($tmp$$reg), __ $6, 0);\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -1916,5 +2329,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_F2X_NARROW2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtFtoD(vReg dst, vReg src)\n@@ -1923,8 +2334,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $tmp, $6, 0\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, tmp\\n\\t\"\n-            \"sve_$7  $dst, $8, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_f2d  $dst, $dst\\t# convert F to D vector\" %}\n@@ -1932,4 +2339,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($tmp$$reg), __ $6, 0);\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $8, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n+    __ sve_fcvt(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -1938,5 +2343,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_F2X_EXTEND1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtDtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -1945,5 +2348,7 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $dst, $6, $dst\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_vectorcast_d2x  $dst, $src\\t# convert D to X vector (narrow)\" %}\n@@ -1951,2 +2356,5 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -1955,2 +2363,1 @@\n-%}')dnl\n-dnl\n+%}\n@@ -1958,2 +2365,4 @@\n-define(`VECTOR_CAST_F2X_NARROW3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+dnl vcvtDtoL\n+VECTOR_CAST_X2X(D, L, fcvtzs, D, d2l)\n+\n+instruct vcvtDtoF(vReg dst, vReg src, vReg tmp)\n@@ -1962,2 +2371,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastD2X src));\n@@ -1965,6 +2374,2 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $tmp, $6, 0\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, tmp\\n\\t\"\n-            \"sve_$7  $dst, $8, $dst, tmp\\n\\t\"\n-            \"sve_$7  $dst, $9, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_vectorcast_d2f  $dst, S, $dst\\t# convert D to F vector\" %}\n@@ -1972,5 +2377,3 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($tmp$$reg), __ $6, 0);\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $8, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $9, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_fcvt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -1979,37 +2382,2 @@\n-%}')dnl\n-dnl\n-VECTOR_CAST_EXTEND1(B, S, sunpklo, H)\n-VECTOR_CAST_EXTEND2(B, I, sunpklo, H, S)\n-VECTOR_CAST_EXTEND3(B, L, sunpklo, H, S, D)\n-VECTOR_CAST_I2F_EXTEND2(B, F, sunpklo, H, S, scvtf)\n-VECTOR_CAST_I2F_EXTEND3(B, D, sunpklo, H, S, D, scvtf)\n-dnl\n-VECTOR_CAST_NARROW1(S, B, dup, B, uzp1)\n-VECTOR_CAST_EXTEND1(S, I, sunpklo, S)\n-VECTOR_CAST_EXTEND2(S, L, sunpklo, S, D)\n-VECTOR_CAST_X2F_EXTEND1(S, F, sunpklo, S, scvtf, S)\n-VECTOR_CAST_I2F_EXTEND2(S, D, sunpklo, S, D, scvtf)\n-dnl\n-VECTOR_CAST_NARROW2(I, B, dup, H, uzp1, B)\n-VECTOR_CAST_NARROW1(I, S, dup, H, uzp1)\n-VECTOR_CAST_EXTEND1(I, L, sunpklo, D)\n-VECTOR_CAST_X2X(I, F, scvtf, S)\n-VECTOR_CAST_X2F_EXTEND1(I, D, sunpklo, D, scvtf, D)\n-dnl\n-VECTOR_CAST_NARROW3(L, B, dup, S, uzp1, H, B)\n-VECTOR_CAST_NARROW2(L, S, dup, S, uzp1, H)\n-VECTOR_CAST_NARROW1(L, I, dup, S, uzp1)\n-VECTOR_CAST_X2F_NARROW1(L, F, scvtf, S, D, dup, S, uzp1)\n-VECTOR_CAST_X2X(L, D, scvtf, D)\n-dnl\n-VECTOR_CAST_F2X_NARROW2(F, B, fcvtzs, S, dup, H, uzp1, B)\n-VECTOR_CAST_F2X_NARROW1(F, S, fcvtzs, S, dup, H, uzp1)\n-VECTOR_CAST_X2X(F, I, fcvtzs, S)\n-VECTOR_CAST_F2X_EXTEND1(F, L, fcvtzs, S, sunpklo, D)\n-VECTOR_CAST_X2F_EXTEND1(F, D, sunpklo, D, fcvt, S)\n-dnl\n-VECTOR_CAST_F2X_NARROW3(D, B, fcvtzs, D, dup, S, uzp1, H, B)\n-VECTOR_CAST_F2X_NARROW2(D, S, fcvtzs, D, dup, S, uzp1, H)\n-VECTOR_CAST_F2X_NARROW1(D, I, fcvtzs, D, dup, S, uzp1)\n-VECTOR_CAST_X2X(D, L, fcvtzs, D)\n-VECTOR_CAST_X2F_NARROW1(D, F, fcvt, S, D, dup, S, uzp1)\n+%}\n+\n@@ -2020,1 +2388,1 @@\n-instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2024,1 +2392,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -2026,1 +2394,1 @@\n-  format %{ \"sve_extract $dst, $3, $pTmp, $src, $idx\\n\\t\"\n+  format %{ \"sve_extract $dst, $3, $pgtmp, $src, $idx\\n\\t\"\n@@ -2029,1 +2397,1 @@\n-    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pgtmp$$reg),\n@@ -2041,1 +2409,1 @@\n-instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2045,1 +2413,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -2047,1 +2415,1 @@\n-  format %{ \"sve_extract $dst, $3, $pTmp, $src, $idx\\t# extract from vector($1)\" %}\n+  format %{ \"sve_extract $dst, $3, $pgtmp, $src, $idx\\t# extract from vector($1)\" %}\n@@ -2049,1 +2417,1 @@\n-    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pgtmp$$reg),\n@@ -2061,5 +2429,2 @@\n-dnl\n-dnl VTEST($1,      $2,   $3,  $4  )\n-dnl VTEST(op_name, pred, imm, cond)\n-define(`VTEST', `\n-instruct vtest_$1`'(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -2067,2 +2432,21 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::$2);\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n+            \"csetw $dst, EQ\\t# VectorTest (sve) - alltrue\" %}\n+  ins_encode %{\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n+%{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n@@ -2070,1 +2454,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(KILL cr);\n@@ -2072,2 +2456,2 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, $3\\n\\t\"\n-            \"csetw $dst, $4\\t# VectorTest (sve) - $1\" %}\n+  format %{ \"sve_ptest $src1\\n\\t\"\n+            \"csetw $dst, NE\\t# VectorTest (sve) - anytrue\" %}\n@@ -2076,5 +2460,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src1$$reg), $3);\n-    __ csetw(as_Register($dst$$reg), Assembler::$4);\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::NE);\n@@ -2083,4 +2464,1 @@\n-%}')dnl\n-dnl\n-VTEST(alltrue, overflow, 0, EQ)\n-VTEST(anytrue, ne,      -1, NE)\n+%}\n@@ -2089,2 +2467,2 @@\n-dnl VTEST_PARTIAL($1,      $2,   $3,  $4  )\n-dnl VTEST_PARTIAL(op_name, pred, imm, cond)\n+dnl VTEST_PARTIAL($1,      $2,   $3,   $4  )\n+dnl VTEST_PARTIAL(op_name, pred, inst, cond)\n@@ -2092,1 +2470,1 @@\n-instruct vtest_$1_partial`'(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_$1_partial`'(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -2094,1 +2472,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -2097,1 +2476,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2101,1 +2480,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -2104,1 +2482,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -2106,2 +2484,2 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src1$$reg), $3);\n+    __ $3(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -2113,2 +2491,2 @@\n-VTEST_PARTIAL(alltrue, overflow, 0, EQ)\n-VTEST_PARTIAL(anytrue, ne,      -1, NE)\n+VTEST_PARTIAL(alltrue, overflow, sve_eors, EQ)\n+VTEST_PARTIAL(anytrue, ne,       sve_ands, NE)\n@@ -2118,1 +2496,1 @@\n-instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2125,1 +2503,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -2127,2 +2505,2 @@\n-  format %{ \"sve_index $dst, -16, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $dst, -16, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2130,1 +2508,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -2135,1 +2513,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -2138,1 +2516,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -2143,1 +2521,1 @@\n-instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2148,1 +2526,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -2151,1 +2529,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2153,1 +2531,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -2156,1 +2534,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -2159,1 +2537,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -2164,1 +2542,1 @@\n-instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -2171,1 +2549,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -2173,3 +2551,3 @@\n-  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_dup $dst, $idx\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_dup $dst, $idx\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -2177,1 +2555,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -2183,1 +2561,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -2186,1 +2564,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -2193,1 +2571,1 @@\n-instruct insert$1`'(vReg dst, vReg src, $2 val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insert$1`'(vReg dst, vReg src, $2 val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2198,1 +2576,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -2201,1 +2579,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2203,1 +2581,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector ($1)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector ($1)\" %}\n@@ -2206,1 +2584,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ $3, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ $3, ptrue,\n@@ -2209,1 +2587,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ $3, as_PRegister($pTmp$$reg), as_$4($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ $3, as_PRegister($pgtmp$$reg), as_$4($val$$reg));\n@@ -2217,1 +2595,1 @@\n-instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -2222,1 +2600,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -2226,1 +2604,1 @@\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -2228,1 +2606,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -2232,1 +2610,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -2238,1 +2616,1 @@\n-               as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+               as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -2245,3 +2623,2 @@\n-instruct loadshuffleB(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+instruct loadshuffle(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0);\n@@ -2250,1 +2627,1 @@\n-  format %{ \"sve_orr $dst, $src, $src\\t# vector load shuffle (B)\" %}\n+  format %{ \"sve_loadshuffle $dst, $src\\t# vector load shuffle (B\/H\/S\/D)\" %}\n@@ -2252,4 +2629,9 @@\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ sve_orr(as_FloatRegister($dst$$reg),\n-                 as_FloatRegister($src$$reg),\n-                 as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                   as_FloatRegister($src$$reg));\n+      }\n+    } else {\n+      __ sve_vector_extend(as_FloatRegister($dst$$reg),  __ elemType_to_regVariant(bt),\n+                           as_FloatRegister($src$$reg), __ B);\n@@ -2261,46 +2643,0 @@\n-instruct loadshuffleS(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_uunpklo $dst, $src\\t# vector load shuffle (B to H)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleI(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\t# vector load shuffle (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleL(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\t# vector load shuffle (B to D)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2333,1 +2669,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -2348,2 +2684,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -2352,1 +2687,2 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base),\n+                       as_FloatRegister($idx$$reg));\n@@ -2359,1 +2695,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2365,1 +2701,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2367,2 +2703,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -2370,3 +2705,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -2378,1 +2712,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2384,1 +2718,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2386,3 +2720,55 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n@@ -2390,1 +2776,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -2392,0 +2778,21 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -2393,1 +2800,1 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -2408,1 +2815,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -2423,2 +2830,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -2426,2 +2832,1 @@\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D,\n-                   as_FloatRegister($idx$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n@@ -2434,1 +2839,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -2436,1 +2841,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2442,1 +2847,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2444,2 +2849,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -2447,1 +2851,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -2449,1 +2853,1 @@\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -2455,1 +2859,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2461,1 +2865,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2463,3 +2867,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n@@ -2467,1 +2869,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n@@ -2470,1 +2872,56 @@\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -2476,0 +2933,20 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2516,5 +2993,2 @@\n-dnl\n-dnl VMASK_REDUCTION($1,     $2,      $3  )\n-dnl VMASK_REDUCTION(suffix, op_name, cost)\n-define(`VMASK_REDUCTION', `\n-instruct vmask_$1(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+\/\/ ---------------------------- Vector mask reductions ---------------------------\n+instruct vmask_truecount(iRegINoSp dst, pReg src) %{\n@@ -2523,4 +2997,3 @@\n-  match(Set dst ($2 src));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vmask_$1 $dst, $src\\t# vector mask $1 (sve)\" %}\n+  match(Set dst (VectorMaskTrueCount src));\n+  ins_cost(SVE_COST);\n+  format %{ \"vmask_truecount $dst, $src\\t# vector mask truecount (sve)\" %}\n@@ -2528,2 +3001,3 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($src$$reg));\n@@ -2532,11 +3006,3 @@\n-%}')dnl\n-dnl\n-\/\/ ---------------------------- Vector mask reductions ---------------------------\n-VMASK_REDUCTION(truecount, VectorMaskTrueCount, 2)\n-VMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 3)\n-VMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 4)\n-dnl\n-dnl VMASK_REDUCTION_PARTIAL($1,     $2,      $3  )\n-dnl VMASK_REDUCTION_PARTIAL(suffix, op_name, cost)\n-define(`VMASK_REDUCTION_PARTIAL', `\n-instruct vmask_$1_partial(iRegINoSp dst, vReg src, pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n+%}\n+\n+instruct vmask_firsttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n@@ -2544,5 +3010,5 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst ($2 src));\n-  effect(TEMP ifelse($1, `firsttrue', `pgtmp, TEMP ptmp', `ptmp'), KILL cr);\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vmask_$1 $dst, $src\\t# vector mask $1 partial (sve)\" %}\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (VectorMaskFirstTrue src));\n+  effect(TEMP ptmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"vmask_firsttrue $dst, $src\\t# vector mask firsttrue (sve)\" %}\n@@ -2550,4 +3016,4 @@\n-    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), __ B,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($ptmp$$reg));\n@@ -2556,5 +3022,1 @@\n-%}')dnl\n-dnl\n-VMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 3)\n-VMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 4)\n-VMASK_REDUCTION_PARTIAL(lasttrue,  VectorMaskLastTrue, 5)\n+%}\n@@ -2562,5 +3024,15 @@\n-dnl\n-dnl VSTOREMASK_REDUCTION($1,     $2,      $3  )\n-dnl VSTOREMASK_REDUCTION(suffix, op_name, cost)\n-define(`VSTOREMASK_REDUCTION', `\n-instruct vstoremask_$1(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_lasttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (VectorMaskLastTrue src));\n+  effect(TEMP ptmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vmask_lasttrue $dst, $src\\t# vector mask lasttrue (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($src$$reg), as_PRegister($ptmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_truecount_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n@@ -2568,2 +3040,2 @@\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst ($2 (VectorStoreMask src esize)));\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (VectorMaskTrueCount src));\n@@ -2571,2 +3043,2 @@\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vstoremask_$1 $dst, $src\\t# vector mask $1 (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"vmask_truecount_partial $dst, $src\\t# vector mask truecount partial (sve)\" %}\n@@ -2574,5 +3046,4 @@\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n@@ -2581,22 +3052,13 @@\n-%}')dnl\n-dnl\n-\/\/ ----------------- Vector mask reductions combined with VectorMaskStore ---------------\n-VSTOREMASK_REDUCTION(truecount, VectorMaskTrueCount, 2)\n-VSTOREMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 3)\n-VSTOREMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 4)\n-dnl\n-dnl VSTOREMASK_REDUCTION_PARTIAL($1,     $2,      $3  )\n-dnl VSTOREMASK_REDUCTION_PARTIAL(suffix, op_name, cost)\n-define(`VSTOREMASK_REDUCTION_PARTIAL', `\n-instruct vstoremask_$1_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst ($2 (VectorStoreMask src esize)));\n-  effect(TEMP ifelse($1, `firsttrue', `pgtmp, TEMP ptmp', `ptmp'), KILL cr);\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vstoremask_$1 $dst, $src\\t# vector mask $1 partial (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), variant,\n+%}\n+\n+instruct vmask_firsttrue_partial(iRegINoSp dst, pReg src, pReg ptmp1, pReg ptmp2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (VectorMaskFirstTrue src));\n+  effect(TEMP ptmp1, TEMP ptmp2, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vmask_firsttrue_partial $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp1$$reg), size,\n@@ -2604,2 +3066,2 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    __ sve_brkb(as_PRegister($ptmp2$$reg), as_PRegister($ptmp1$$reg), as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp1$$reg), as_PRegister($ptmp2$$reg));\n@@ -2608,5 +3070,18 @@\n-%}')dnl\n-dnl\n-VSTOREMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 3)\n-VSTOREMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 4)\n-VSTOREMASK_REDUCTION_PARTIAL(lasttrue,  VectorMaskLastTrue, 5)\n+%}\n+\n+instruct vmask_lasttrue_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (VectorMaskLastTrue src));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(5 * SVE_COST);\n+  format %{ \"vmask_lasttrue_partial $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), ptrue, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}dnl\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":1594,"deletions":1119,"binary":false,"changes":2713,"status":"modified"},{"patch":"@@ -586,1 +586,1 @@\n-  static bool offset_ok_for_sve_immed(long offset, int shift, int vl \/* sve vector length *\/) {\n+  static bool offset_ok_for_sve_immed(int64_t offset, int shift, int vl \/* sve vector length *\/) {\n@@ -2979,0 +2979,26 @@\n+  void sve_shift_imm_encoding(SIMD_RegVariant T, int shift, bool isSHR,\n+                              int& tszh, int& tszl_imm) {\n+    \/* The encodings for the tszh:tszl:imm3 fields\n+     * for shift right is calculated as:\n+     *   0001 xxx       B, shift = 16  - UInt(tszh:tszl:imm3)\n+     *   001x xxx       H, shift = 32  - UInt(tszh:tszl:imm3)\n+     *   01xx xxx       S, shift = 64  - UInt(tszh:tszl:imm3)\n+     *   1xxx xxx       D, shift = 128 - UInt(tszh:tszl:imm3)\n+     * for shift left is calculated as:\n+     *   0001 xxx       B, shift = UInt(tszh:tszl:imm3) - 8\n+     *   001x xxx       H, shift = UInt(tszh:tszl:imm3) - 16\n+     *   01xx xxx       S, shift = UInt(tszh:tszl:imm3) - 32\n+     *   1xxx xxx       D, shift = UInt(tszh:tszl:imm3) - 64\n+     *\/\n+    assert(T != Q, \"Invalid register variant\");\n+    if (isSHR) {\n+      assert(((1 << (T + 3)) >= shift) && (shift > 0) , \"Invalid shift value\");\n+    } else {\n+      assert(((1 << (T + 3)) > shift) && (shift >= 0) , \"Invalid shift value\");\n+    }\n+    int cVal = (1 << ((T + 3) + (isSHR ? 1 : 0)));\n+    int encodedShift = isSHR ? cVal - shift : cVal + shift;\n+    tszh = encodedShift >> 5;\n+    tszl_imm = encodedShift & 0x1f;\n+  }\n+\n@@ -2990,0 +3016,1 @@\n+  INSN(sve_and,  0b00000100, 0b011010000); \/\/ vector and\n@@ -2992,1 +3019,1 @@\n-  INSN(sve_cnt,  0b00000100, 0b011010101)  \/\/ count non-zero bits\n+  INSN(sve_cnt,  0b00000100, 0b011010101); \/\/ count non-zero bits\n@@ -2994,0 +3021,1 @@\n+  INSN(sve_eor,  0b00000100, 0b011001000); \/\/ vector eor\n@@ -3000,0 +3028,1 @@\n+  INSN(sve_orr,  0b00000100, 0b011000000); \/\/ vector or\n@@ -3042,1 +3071,1 @@\n-  INSN(sve_fmla,  0b01100101, 1, 0b000); \/\/ floating-point fused multiply-add: Zda = Zda + Zn * Zm\n+  INSN(sve_fmla,  0b01100101, 1, 0b000); \/\/ floating-point fused multiply-add, writing addend: Zda = Zda + Zn * Zm\n@@ -3046,0 +3075,1 @@\n+  INSN(sve_fmad,  0b01100101, 1, 0b100); \/\/ floating-point fused multiply-add, writing multiplicand: Zda = Zm + Zda * Zn\n@@ -3067,22 +3097,2 @@\n-    \/* The encodings for the tszh:tszl:imm3 fields (bits 23:22 20:19 18:16)     \\\n-     * for shift right is calculated as:                                        \\\n-     *   0001 xxx       B, shift = 16  - UInt(tszh:tszl:imm3)                   \\\n-     *   001x xxx       H, shift = 32  - UInt(tszh:tszl:imm3)                   \\\n-     *   01xx xxx       S, shift = 64  - UInt(tszh:tszl:imm3)                   \\\n-     *   1xxx xxx       D, shift = 128 - UInt(tszh:tszl:imm3)                   \\\n-     * for shift left is calculated as:                                         \\\n-     *   0001 xxx       B, shift = UInt(tszh:tszl:imm3) - 8                     \\\n-     *   001x xxx       H, shift = UInt(tszh:tszl:imm3) - 16                    \\\n-     *   01xx xxx       S, shift = UInt(tszh:tszl:imm3) - 32                    \\\n-     *   1xxx xxx       D, shift = UInt(tszh:tszl:imm3) - 64                    \\\n-     *\/                                                                         \\\n-    assert(T != Q, \"Invalid register variant\");                                 \\\n-    if (isSHR) {                                                                \\\n-      assert(((1 << (T + 3)) >= shift) && (shift > 0) , \"Invalid shift value\"); \\\n-    } else {                                                                    \\\n-      assert(((1 << (T + 3)) > shift) && (shift >= 0) , \"Invalid shift value\"); \\\n-    }                                                                           \\\n-    int cVal = (1 << ((T + 3) + (isSHR ? 1 : 0)));                              \\\n-    int encodedShift = isSHR ? cVal - shift : cVal + shift;                     \\\n-    int tszh = encodedShift >> 5;                                               \\\n-    int tszl_imm = encodedShift & 0x1f;                                         \\\n+    int tszh, tszl_imm;                                                         \\\n+    sve_shift_imm_encoding(T, shift, isSHR, tszh, tszl_imm);                    \\\n@@ -3099,0 +3109,15 @@\n+\/\/ SVE bitwise shift by immediate (predicated)\n+#define INSN(NAME, opc, isSHR)                                                  \\\n+  void NAME(FloatRegister Zdn, SIMD_RegVariant T, PRegister Pg, int shift) {    \\\n+    starti;                                                                     \\\n+    int tszh, tszl_imm;                                                         \\\n+    sve_shift_imm_encoding(T, shift, isSHR, tszh, tszl_imm);                    \\\n+    f(0b00000100, 31, 24), f(tszh, 23, 22), f(0b00, 21, 20), f(opc, 19, 16);    \\\n+    f(0b100, 15, 13), pgrf(Pg, 10), f(tszl_imm, 9, 5), rf(Zdn, 0);              \\\n+  }\n+\n+  INSN(sve_asr, 0b0000, \/* isSHR = *\/ true);\n+  INSN(sve_lsl, 0b0011, \/* isSHR = *\/ false);\n+  INSN(sve_lsr, 0b0001, \/* isSHR = *\/ true);\n+#undef INSN\n+\n@@ -3210,0 +3235,18 @@\n+\/\/ SVE predicate logical operations\n+#define INSN(NAME, op1, op2, op3) \\\n+  void NAME(PRegister Pd, PRegister Pg, PRegister Pn, PRegister Pm) { \\\n+    starti;                                                           \\\n+    f(0b00100101, 31, 24), f(op1, 23, 22), f(0b00, 21, 20);           \\\n+    prf(Pm, 16), f(0b01, 15, 14), prf(Pg, 10), f(op2, 9);             \\\n+    prf(Pn, 5), f(op3, 4), prf(Pd, 0);                                \\\n+  }\n+\n+  INSN(sve_and,  0b00, 0b0, 0b0);\n+  INSN(sve_ands, 0b01, 0b0, 0b0);\n+  INSN(sve_eor,  0b00, 0b1, 0b0);\n+  INSN(sve_eors, 0b01, 0b1, 0b0);\n+  INSN(sve_orr,  0b10, 0b0, 0b0);\n+  INSN(sve_orrs, 0b11, 0b0, 0b0);\n+  INSN(sve_bic,  0b00, 0b0, 0b1);\n+#undef INSN\n+\n@@ -3243,0 +3286,7 @@\n+  \/\/ SVE predicate test\n+  void sve_ptest(PRegister Pg, PRegister Pn) {\n+    starti;\n+    f(0b001001010101000011, 31, 14), prf(Pg, 10), f(0, 9), prf(Pn, 5), f(0, 4, 0);\n+  }\n+\n+  \/\/ SVE predicate initialize\n@@ -3249,0 +3299,28 @@\n+  \/\/ SVE predicate zero\n+  void sve_pfalse(PRegister pd) {\n+    starti;\n+    f(0b00100101, 31, 24), f(0b00, 23, 22), f(0b011000111001, 21, 10);\n+    f(0b000000, 9, 4), prf(pd, 0);\n+  }\n+\n+\/\/ SVE load\/store predicate register\n+#define INSN(NAME, op1)                                                  \\\n+  void NAME(PRegister Pt, const Address &a)  {                           \\\n+    starti;                                                              \\\n+    assert(a.index() == noreg, \"invalid address variant\");               \\\n+    f(op1, 31, 29), f(0b0010110, 28, 22), sf(a.offset() >> 3, 21, 16),   \\\n+    f(0b000, 15, 13), f(a.offset() & 0x7, 12, 10), srf(a.base(), 5),     \\\n+    f(0, 4), prf(Pt, 0);                                                 \\\n+  }\n+\n+  INSN(sve_ldr, 0b100); \/\/ LDR (predicate)\n+  INSN(sve_str, 0b111); \/\/ STR (predicate)\n+#undef INSN\n+\n+  \/\/ SVE move predicate register\n+  void sve_mov(PRegister Pd, PRegister Pn) {\n+    starti;\n+    f(0b001001011000, 31, 20), prf(Pn, 16), f(0b01, 15, 14), prf(Pn, 10);\n+    f(0, 9), prf(Pn, 5), f(0, 4), prf(Pd, 0);\n+  }\n+\n@@ -3351,0 +3429,12 @@\n+\/\/ SVE unpack predicate elements\n+#define INSN(NAME, op) \\\n+  void NAME(PRegister Pd, PRegister Pn) { \\\n+    starti;                                                          \\\n+    f(0b000001010011000, 31, 17), f(op, 16), f(0b0100000, 15, 9);    \\\n+    prf(Pn, 5), f(0b0, 4), prf(Pd, 0);                               \\\n+  }\n+\n+  INSN(sve_punpkhi, 0b1); \/\/ Unpack and widen high half of predicate\n+  INSN(sve_punpklo, 0b0); \/\/ Unpack and widen low half of predicate\n+#undef INSN\n+\n@@ -3364,0 +3454,13 @@\n+\/\/ SVE permute predicate elements\n+#define INSN(NAME, op) \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pn, PRegister Pm) {             \\\n+    starti;                                                                            \\\n+    assert(T != Q, \"invalid size\");                                                    \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b10, 21, 20), prf(Pm, 16);                 \\\n+    f(0b01001, 15, 11), f(op, 10), f(0b0, 9), prf(Pn, 5), f(0b0, 4), prf(Pd, 0);       \\\n+  }\n+\n+  INSN(sve_uzp1, 0b0); \/\/ Concatenate even elements from two predicates\n+  INSN(sve_uzp2, 0b1); \/\/ Concatenate odd elements from two predicates\n+#undef INSN\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":128,"deletions":25,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -975,2 +975,121 @@\n-void C2_MacroAssembler::sve_vmask_reduction(int opc, Register dst, SIMD_RegVariant size, FloatRegister src,\n-                                            PRegister pg, PRegister pn, int length) {\n+\/\/ Get index of the last mask lane that is set\n+void C2_MacroAssembler::sve_vmask_lasttrue(Register dst, BasicType bt, PRegister src, PRegister ptmp) {\n+  SIMD_RegVariant size = elemType_to_regVariant(bt);\n+  sve_rev(ptmp, size, src);\n+  sve_brkb(ptmp, ptrue, ptmp, false);\n+  sve_cntp(dst, size, ptrue, ptmp);\n+  movw(rscratch1, MaxVectorSize \/ type2aelembytes(bt) - 1);\n+  subw(dst, rscratch1, dst);\n+}\n+\n+void C2_MacroAssembler::sve_vector_extend(FloatRegister dst, SIMD_RegVariant dst_size,\n+                                          FloatRegister src, SIMD_RegVariant src_size) {\n+  assert(dst_size > src_size && dst_size <= D && src_size <= S, \"invalid element size\");\n+  if (src_size == B) {\n+    switch (dst_size) {\n+    case H:\n+      sve_sunpklo(dst, H, src);\n+      break;\n+    case S:\n+      sve_sunpklo(dst, H, src);\n+      sve_sunpklo(dst, S, dst);\n+      break;\n+    case D:\n+      sve_sunpklo(dst, H, src);\n+      sve_sunpklo(dst, S, dst);\n+      sve_sunpklo(dst, D, dst);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  } else if (src_size == H) {\n+    if (dst_size == S) {\n+      sve_sunpklo(dst, S, src);\n+    } else { \/\/ D\n+      sve_sunpklo(dst, S, src);\n+      sve_sunpklo(dst, D, dst);\n+    }\n+  } else if (src_size == S) {\n+    sve_sunpklo(dst, D, src);\n+  }\n+}\n+\n+\/\/ Vector narrow from src to dst with specified element sizes.\n+\/\/ High part of dst vector will be filled with zero.\n+void C2_MacroAssembler::sve_vector_narrow(FloatRegister dst, SIMD_RegVariant dst_size,\n+                                          FloatRegister src, SIMD_RegVariant src_size,\n+                                          FloatRegister tmp) {\n+  assert(dst_size < src_size && dst_size <= S && src_size <= D, \"invalid element size\");\n+  sve_dup(tmp, src_size, 0);\n+  if (src_size == D) {\n+    switch (dst_size) {\n+    case S:\n+      sve_uzp1(dst, S, src, tmp);\n+      break;\n+    case H:\n+      sve_uzp1(dst, S, src, tmp);\n+      sve_uzp1(dst, H, dst, tmp);\n+      break;\n+    case B:\n+      sve_uzp1(dst, S, src, tmp);\n+      sve_uzp1(dst, H, dst, tmp);\n+      sve_uzp1(dst, B, dst, tmp);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  } else if (src_size == S) {\n+    if (dst_size == H) {\n+      sve_uzp1(dst, H, src, tmp);\n+    } else { \/\/ B\n+      sve_uzp1(dst, H, src, tmp);\n+      sve_uzp1(dst, B, dst, tmp);\n+    }\n+  } else if (src_size == H) {\n+    sve_uzp1(dst, B, src, tmp);\n+  }\n+}\n+\n+\/\/ Extend src predicate to dst predicate with the same lane count but larger\n+\/\/ element size, e.g. 64Byte -> 512Long\n+void C2_MacroAssembler::sve_vmaskcast_extend(PRegister dst, PRegister src,\n+                                             uint dst_element_length_in_bytes,\n+                                             uint src_element_length_in_bytes) {\n+  if (dst_element_length_in_bytes == 2 * src_element_length_in_bytes) {\n+    sve_punpklo(dst, src);\n+  } else if (dst_element_length_in_bytes == 4 * src_element_length_in_bytes) {\n+    sve_punpklo(dst, src);\n+    sve_punpklo(dst, dst);\n+  } else if (dst_element_length_in_bytes == 8 * src_element_length_in_bytes) {\n+    sve_punpklo(dst, src);\n+    sve_punpklo(dst, dst);\n+    sve_punpklo(dst, dst);\n+  } else {\n+    assert(false, \"unsupported\");\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Narrow src predicate to dst predicate with the same lane count but\n+\/\/ smaller element size, e.g. 512Long -> 64Byte\n+void C2_MacroAssembler::sve_vmaskcast_narrow(PRegister dst, PRegister src,\n+                                             uint dst_element_length_in_bytes, uint src_element_length_in_bytes) {\n+  \/\/ The insignificant bits in src predicate are expected to be zero.\n+  if (dst_element_length_in_bytes * 2 == src_element_length_in_bytes) {\n+    sve_uzp1(dst, B, src, src);\n+  } else if (dst_element_length_in_bytes * 4 == src_element_length_in_bytes) {\n+    sve_uzp1(dst, H, src, src);\n+    sve_uzp1(dst, B, dst, dst);\n+  } else if (dst_element_length_in_bytes * 8 == src_element_length_in_bytes) {\n+    sve_uzp1(dst, S, src, src);\n+    sve_uzp1(dst, H, dst, dst);\n+    sve_uzp1(dst, B, dst, dst);\n+  } else {\n+    assert(false, \"unsupported\");\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::sve_reduce_integral(int opc, Register dst, BasicType bt, Register src1,\n+                                            FloatRegister src2, PRegister pg, FloatRegister tmp) {\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported element type\");\n@@ -978,2 +1097,3 @@\n-  \/\/ The conditional flags will be clobbered by this function\n-  sve_cmp(Assembler::NE, pn, size, pg, src, 0);\n+  assert_different_registers(src1, dst);\n+  \/\/ Register \"dst\" and \"tmp\" are to be clobbered, and \"src1\" and \"src2\" should be preserved.\n+  Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n@@ -981,2 +1101,38 @@\n-    case Op_VectorMaskTrueCount:\n-      sve_cntp(dst, size, ptrue, pn);\n+    case Op_AddReductionVI: {\n+      sve_uaddv(tmp, size, pg, src2);\n+      smov(dst, tmp, size, 0);\n+      if (bt == T_BYTE) {\n+        addw(dst, src1, dst, ext::sxtb);\n+      } else if (bt == T_SHORT) {\n+        addw(dst, src1, dst, ext::sxth);\n+      } else {\n+        addw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_AddReductionVL: {\n+      sve_uaddv(tmp, size, pg, src2);\n+      umov(dst, tmp, size, 0);\n+      add(dst, dst, src1);\n+      break;\n+    }\n+    case Op_AndReductionV: {\n+      sve_andv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        andr(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        andw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_OrReductionV: {\n+      sve_orv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        orr(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        orrw(dst, dst, src1);\n+      }\n@@ -984,3 +1140,23 @@\n-    case Op_VectorMaskFirstTrue:\n-      sve_brkb(pn, pg, pn, false);\n-      sve_cntp(dst, size, ptrue, pn);\n+    }\n+    case Op_XorReductionV: {\n+      sve_eorv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        eor(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        eorw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_MaxReductionV: {\n+      sve_smaxv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        cmp(dst, src1);\n+        csel(dst, dst, src1, Assembler::GT);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        cmpw(dst, src1);\n+        cselw(dst, dst, src1, Assembler::GT);\n+      }\n@@ -988,6 +1164,12 @@\n-    case Op_VectorMaskLastTrue:\n-      sve_rev(pn, size, pn);\n-      sve_brkb(pn, ptrue, pn, false);\n-      sve_cntp(dst, size, ptrue, pn);\n-      movw(rscratch1, length - 1);\n-      subw(dst, rscratch1, dst);\n+    }\n+    case Op_MinReductionV: {\n+      sve_sminv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        cmp(dst, src1);\n+        csel(dst, dst, src1, Assembler::LT);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        cmpw(dst, src1);\n+        cselw(dst, dst, src1, Assembler::LT);\n+      }\n@@ -995,0 +1177,1 @@\n+    }\n@@ -999,0 +1182,8 @@\n+\n+  if (opc == Op_AndReductionV || opc == Op_OrReductionV || opc == Op_XorReductionV) {\n+    if (bt == T_BYTE) {\n+      sxtb(dst, dst);\n+    } else if (bt == T_SHORT) {\n+      sxth(dst, dst);\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":206,"deletions":15,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -64,2 +64,16 @@\n-  void sve_vmask_reduction(int opc, Register dst, SIMD_RegVariant size, FloatRegister src,\n-                           PRegister pg, PRegister pn, int length = MaxVectorSize);\n+  void sve_vmask_lasttrue(Register dst, BasicType bt, PRegister src, PRegister ptmp);\n+\n+  void sve_vector_extend(FloatRegister dst, SIMD_RegVariant dst_size,\n+                         FloatRegister src, SIMD_RegVariant src_size);\n+\n+  void sve_vector_narrow(FloatRegister dst, SIMD_RegVariant dst_size,\n+                         FloatRegister src, SIMD_RegVariant src_size, FloatRegister tmp);\n+\n+  void sve_vmaskcast_extend(PRegister dst, PRegister src,\n+                            uint dst_element_length_in_bytes, uint src_element_lenght_in_bytes);\n+\n+  void sve_vmaskcast_narrow(PRegister dst, PRegister src,\n+                            uint dst_element_length_in_bytes, uint src_element_lenght_in_bytes);\n+\n+  void sve_reduce_integral(int opc, Register dst, BasicType bt, Register src1,\n+                           FloatRegister src2, PRegister pg, FloatRegister tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -318,0 +318,1 @@\n+  PRegSet               _p_regs;\n@@ -331,0 +332,2 @@\n+        } else if (vm_reg->is_PRegister()) {\n+          _p_regs += PRegSet::of(vm_reg->as_PRegister());\n@@ -344,1 +347,2 @@\n-      _fp_regs() {\n+      _fp_regs(),\n+      _p_regs() {\n@@ -352,0 +356,1 @@\n+    __ push_p(_p_regs, sp);\n@@ -356,0 +361,1 @@\n+    __ pop_p(_p_regs, sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1981,1 +1981,1 @@\n-\/\/ Return the number of dwords poped\n+\/\/ Return the number of dwords popped\n@@ -2040,0 +2040,74 @@\n+\/\/ Return the number of dwords pushed\n+int MacroAssembler::push_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  unsigned char regs[PRegisterImpl::number_of_saved_registers];\n+  int count = 0;\n+  for (int reg = 0; reg < PRegisterImpl::number_of_saved_registers; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_push_bytes = align_up(sve_predicate_size_in_slots *\n+                                  VMRegImpl::stack_slot_size * count, 16);\n+  sub(stack, stack, total_push_bytes);\n+  for (int i = 0; i < count; i++) {\n+    sve_str(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  return total_push_bytes \/ 8;\n+}\n+\n+\/\/ Return the number of dwords popped\n+int MacroAssembler::pop_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  unsigned char regs[PRegisterImpl::number_of_saved_registers];\n+  int count = 0;\n+  for (int reg = 0; reg < PRegisterImpl::number_of_saved_registers; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_pop_bytes = align_up(sve_predicate_size_in_slots *\n+                                 VMRegImpl::stack_slot_size * count, 16);\n+  for (int i = count - 1; i >= 0; i--) {\n+    sve_ldr(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  add(stack, stack, total_pop_bytes);\n+  return total_pop_bytes \/ 8;\n+}\n+\n@@ -2498,1 +2572,1 @@\n-                                    int sve_vector_size_in_bytes) {\n+                                    int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n@@ -2515,0 +2589,6 @@\n+  if (save_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    sub(sp, sp, total_predicate_in_bytes);\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      sve_str(as_PRegister(i), Address(sp, i));\n+    }\n+  }\n@@ -2518,1 +2598,7 @@\n-                                   int sve_vector_size_in_bytes) {\n+                                   int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n+  if (restore_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    for (int i = PRegisterImpl::number_of_saved_registers - 1; i >= 0; i--) {\n+      sve_ldr(as_PRegister(i), Address(sp, i));\n+    }\n+    add(sp, sp, total_predicate_in_bytes);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":89,"deletions":3,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -458,0 +458,3 @@\n+  int push_p(unsigned int bitset, Register stack);\n+  int pop_p(unsigned int bitset, Register stack);\n+\n@@ -469,0 +472,3 @@\n+  void push_p(PRegSet regs, Register stack) { if (regs.bits()) push_p(regs.bits(), stack); }\n+  void pop_p(PRegSet regs, Register stack) { if (regs.bits()) pop_p(regs.bits(), stack); }\n+\n@@ -868,1 +874,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                      int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -870,1 +876,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                     int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -1364,0 +1370,1 @@\n+\n@@ -1367,0 +1374,4 @@\n+  void spill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_str(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1377,0 +1388,1 @@\n+\n@@ -1380,0 +1392,4 @@\n+  void unspill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_ldr(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1402,0 +1418,6 @@\n+  void spill_copy_sve_predicate_stack_to_stack(int src_offset, int dst_offset,\n+                                               int sve_predicate_reg_size_in_bytes) {\n+    sve_ldr(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, src_offset));\n+    sve_str(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, dst_offset));\n+    reinitialize_ptrue();\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -37,1 +37,2 @@\n-  = ConcreteRegisterImpl::max_fpr + PRegisterImpl::number_of_registers;\n+  = ConcreteRegisterImpl::max_fpr +\n+    PRegisterImpl::number_of_registers * PRegisterImpl::max_slots_per_register;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -246,0 +246,5 @@\n+    \/\/ p0-p7 are governing predicates for load\/store and arithmetic, but p7 is\n+    \/\/ preserved as an all-true predicate in OpenJDK. And since we don't support\n+    \/\/ non-governing predicate registers allocation for non-temp register, the\n+    \/\/ predicate registers to be saved are p0-p6.\n+    number_of_saved_registers = number_of_governing_registers - 1,\n@@ -380,0 +385,1 @@\n+typedef AbstractRegSet<PRegister> PRegSet;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -104,1 +104,4 @@\n-  int v0_offset_in_bytes(void)   { return 0; }\n+  int v0_offset_in_bytes();\n+\n+  \/\/ Total stack size in bytes for saving sve predicate registers.\n+  int total_sve_predicate_in_bytes();\n@@ -142,1 +145,1 @@\n-  int r0_offset = (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n+  int r0_offset = v0_offset_in_bytes() + (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n@@ -146,0 +149,20 @@\n+int RegisterSaver::v0_offset_in_bytes() {\n+  \/\/ The floating point registers are located above the predicate registers if\n+  \/\/ they are present in the stack frame pushed by save_live_registers(). So the\n+  \/\/ offset depends on the saved total predicate vectors in the stack frame.\n+  return (total_sve_predicate_in_bytes() \/ VMRegImpl::stack_slot_size) * BytesPerInt;\n+}\n+\n+int RegisterSaver::total_sve_predicate_in_bytes() {\n+#ifdef COMPILER2\n+  if (_save_vectors && Matcher::supports_scalable_vector()) {\n+    \/\/ The number of total predicate bytes is unlikely to be a multiple\n+    \/\/ of 16 bytes so we manually align it up.\n+    return align_up(Matcher::scalable_predicate_reg_slots() *\n+                    VMRegImpl::stack_slot_size *\n+                    PRegisterImpl::number_of_saved_registers, 16);\n+  }\n+#endif\n+  return 0;\n+}\n+\n@@ -150,0 +173,3 @@\n+  int sve_predicate_size_in_slots = 0;\n+  int total_predicate_in_bytes = total_sve_predicate_in_bytes();\n+  int total_predicate_in_slots = total_predicate_in_bytes \/ VMRegImpl::stack_slot_size;\n@@ -153,2 +179,5 @@\n-  sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n-  sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+  if (use_sve) {\n+    sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n+    sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n@@ -159,1 +188,0 @@\n-    int vect_words = 0;\n@@ -167,3 +195,4 @@\n-    vect_words = FloatRegisterImpl::number_of_registers * extra_save_slots_per_register \/\n-                 VMRegImpl::slots_per_word;\n-    additional_frame_words += vect_words;\n+    int extra_vector_bytes = extra_save_slots_per_register *\n+                             VMRegImpl::stack_slot_size *\n+                             FloatRegisterImpl::number_of_registers;\n+    additional_frame_words += ((extra_vector_bytes + total_predicate_in_bytes) \/ wordSize);\n@@ -187,1 +216,1 @@\n-  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes);\n+  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes, total_predicate_in_bytes);\n@@ -204,2 +233,1 @@\n-      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots),\n-                                r->as_VMReg());\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots), r->as_VMReg());\n@@ -213,1 +241,1 @@\n-      sp_offset = use_sve ? (sve_vector_size_in_slots * i) :\n+      sp_offset = use_sve ? (total_predicate_in_slots + sve_vector_size_in_slots * i) :\n@@ -218,2 +246,9 @@\n-    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n-                              r->as_VMReg());\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+  }\n+\n+  if (_save_vectors && use_sve) {\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      PRegister r = as_PRegister(i);\n+      int sp_offset = sve_predicate_size_in_slots * i;\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+    }\n@@ -228,1 +263,1 @@\n-                   Matcher::scalable_vector_reg_size(T_BYTE));\n+                   Matcher::scalable_vector_reg_size(T_BYTE), total_sve_predicate_in_bytes());\n@@ -241,0 +276,2 @@\n+\/\/ The SVE supported min vector size is 8 bytes and we need to save\n+\/\/ predicate registers when the vector size is 8 bytes as well.\n@@ -242,1 +279,1 @@\n-  return size > 8;\n+  return size > 8 || (UseSVE > 0 && size >= 8);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":53,"deletions":16,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -986,0 +986,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2180,0 +2180,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1539,0 +1539,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2461,0 +2461,7 @@\n+void Assembler::kmovbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x90, (0xC0 | encode));\n+}\n+\n@@ -2508,1 +2515,1 @@\n-  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -2574,0 +2581,98 @@\n+void Assembler::knotbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n+void Assembler::korbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kandbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kanddl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::knotdl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n@@ -2621,0 +2726,21 @@\n+void Assembler::ktestdl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestwl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestbl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n@@ -2635,0 +2761,46 @@\n+void Assembler::kxnorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftlbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftrwl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrdl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrql(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n@@ -4115,18 +4287,0 @@\n-void Assembler::evpmovd2m(KRegister kdst, XMMRegister src, int vector_len) {\n-  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x39, (0xC0 | encode));\n-}\n-\n-void Assembler::evpmovq2m(KRegister kdst, XMMRegister src, int vector_len) {\n-  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x39, (0xC0 | encode));\n-}\n-\n@@ -7422,1 +7576,0 @@\n-  assert(VM_Version::supports_evex(), \"\");\n@@ -7424,0 +7577,1 @@\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -7434,0 +7588,112 @@\n+void Assembler::evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W1 EF \/r\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xDB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEB, (0xC0 | encode));\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEB);\n+  emit_operand(dst, src);\n+}\n+\n@@ -7978,6 +8244,10 @@\n-\/\/ duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL\n-void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(UseAVX >= 2, \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x58, (0xC0 | encode));\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFC, (0xC0 | encode));\n@@ -7986,3 +8256,1 @@\n-void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n-  assert(dst != xnoreg, \"sanity\");\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n@@ -7990,11 +8258,1448 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n-  \/\/ swap src<->dst for encoding\n-  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int8(0x58);\n-  emit_operand(dst, src);\n-}\n-\n-\/\/ duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL\n-void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFC);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFD, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFD);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD4, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD4);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF9, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF9);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD5, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD5);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x51, (0xC0 | encode));\n+}\n+\n+void Assembler::evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x51);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x51, (0xC0 | encode));\n+}\n+\n+void Assembler::evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x51);\n+  emit_operand(dst, src);\n+}\n+\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1C, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1D, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1E, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1F, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x12, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x10, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x38);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3C, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+\/\/ duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL\n+void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX >= 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+\/\/ duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL\n+void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n@@ -9370,0 +11075,96 @@\n+void Assembler::evprord(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm0->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprorq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm0->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprorvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::evprorvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::evprold(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm1->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprolq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm1->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprolvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n+void Assembler::evprolvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n@@ -9493,0 +11294,24 @@\n+void Assembler::evpmovq2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovd2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovw2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x29, (0xC0 | encode));\n+}\n+\n@@ -9501,0 +11326,31 @@\n+void Assembler::evpmovm2q(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2d(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2w(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2b(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":1896,"deletions":40,"binary":false,"changes":1936,"status":"modified"},{"patch":"@@ -1465,0 +1465,14 @@\n+  void kandbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kanddl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void korbl(KRegister dst, KRegister src1, KRegister src2);\n+  void korwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kordl(KRegister dst, KRegister src1, KRegister src2);\n+  void korql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void kxorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxordl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorql(KRegister dst, KRegister src1, KRegister src2);\n@@ -1467,0 +1481,1 @@\n+  void kmovbl(KRegister dst, KRegister src);\n@@ -1480,0 +1495,1 @@\n+  void knotbl(KRegister dst, KRegister src);\n@@ -1481,0 +1497,1 @@\n+  void knotdl(KRegister dst, KRegister src);\n@@ -1488,0 +1505,6 @@\n+  void kxnorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kshiftlbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrwl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrdl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrql(KRegister dst, KRegister src, int imm8);\n@@ -1492,0 +1515,3 @@\n+  void ktestdl(KRegister dst, KRegister src);\n+  void ktestwl(KRegister dst, KRegister src);\n+  void ktestbl(KRegister dst, KRegister src);\n@@ -2155,3 +2181,0 @@\n-  void evpmovd2m(KRegister kdst, XMMRegister src, int vector_len);\n-  void evpmovq2m(KRegister kdst, XMMRegister src, int vector_len);\n-\n@@ -2249,0 +2272,130 @@\n+  \/\/ Leaf level assembler routines for masked operations.\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+\n+  void evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evprold(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprolq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprolvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprolvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprord(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprorq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprorvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprorvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+\n@@ -2367,1 +2520,0 @@\n-  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2380,3 +2532,0 @@\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n-\n@@ -2388,1 +2537,0 @@\n-  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2530,0 +2678,7 @@\n+  void evpmovw2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovd2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovq2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovm2b(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2w(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2d(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2q(XMMRegister dst, KRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":163,"deletions":8,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -1464,0 +1464,13 @@\n+void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp,\n+                                         Register tmp, bool novlbwdq, int vlen_enc) {\n+  if (novlbwdq) {\n+    vpmovsxbd(xtmp, src, vlen_enc);\n+    evpcmpd(dst, k0, xtmp, ExternalAddress(StubRoutines::x86::vector_int_mask_cmp_bits()),\n+            Assembler::eq, true, vlen_enc, tmp);\n+  } else {\n+    vpxor(xtmp, xtmp, xtmp, vlen_enc);\n+    vpsubb(xtmp, xtmp, src, vlen_enc);\n+    evpmovb2m(dst, xtmp, vlen_enc);\n+  }\n+}\n+\n@@ -3830,0 +3843,212 @@\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, int imm8, bool merge, int vlen_enc) {\n+  switch(ideal_opc) {\n+    case Op_LShiftVS:\n+      Assembler::evpsllw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_LShiftVI:\n+      Assembler::evpslld(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_LShiftVL:\n+      Assembler::evpsllq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVS:\n+      Assembler::evpsraw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVI:\n+      Assembler::evpsrad(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVL:\n+      Assembler::evpsraq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVS:\n+      Assembler::evpsrlw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVI:\n+      Assembler::evpsrld(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVL:\n+      Assembler::evpsrlq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RotateRightV:\n+      evrord(eType, dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RotateLeftV:\n+      evrold(eType, dst, mask, src1, imm8, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc,\n+                                    bool is_varshift) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SqrtVF:\n+      evsqrtps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SqrtVD:\n+      evsqrtpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AbsVB:\n+      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVS:\n+      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVI:\n+      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVL:\n+      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_VectorRearrange:\n+      evperm(eType, dst, mask, src2, src1, merge, vlen_enc); break;\n+    case Op_LShiftVS:\n+      evpsllw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVI:\n+      evpslld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVL:\n+      evpsllq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVS:\n+      evpsraw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVI:\n+      evpsrad(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVL:\n+      evpsraq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVS:\n+      evpsrlw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVI:\n+      evpsrld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVL:\n+      evpsrlq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RotateLeftV:\n+      evrold(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_RotateRightV:\n+      evrord(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, Address src2, bool merge, int vlen_enc) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                                  KRegister src1, KRegister src2) {\n+  BasicType etype = T_ILLEGAL;\n+  switch(mask_len) {\n+    case 2:\n+    case 4:\n+    case 8:  etype = T_BYTE; break;\n+    case 16: etype = T_SHORT; break;\n+    case 32: etype = T_INT; break;\n+    case 64: etype = T_LONG; break;\n+    default: fatal(\"Unsupported type\"); break;\n+  }\n+  assert(etype != T_ILLEGAL, \"\");\n+  switch(ideal_opc) {\n+    case Op_AndVMask:\n+      kand(etype, dst, src1, src2); break;\n+    case Op_OrVMask:\n+      kor(etype, dst, src1, src2); break;\n+    case Op_XorVMask:\n+      kxor(etype, dst, src1, src2); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n@@ -3831,7 +4056,12 @@\n-void C2_MacroAssembler::vector_mask_operation(int opc, Register dst, XMMRegister mask, XMMRegister xtmp,\n-                                              Register tmp, KRegister ktmp, int masklen, int vec_enc) {\n-  assert(VM_Version::supports_avx512vlbw(), \"\");\n-  vpxor(xtmp, xtmp, xtmp, vec_enc);\n-  vpsubb(xtmp, xtmp, mask, vec_enc);\n-  evpmovb2m(ktmp, xtmp, vec_enc);\n-  kmovql(tmp, ktmp);\n+void C2_MacroAssembler::vector_mask_operation(int opc, Register dst, KRegister mask,\n+                                              Register tmp, int masklen, int masksize,\n+                                              int vec_enc) {\n+  if(VM_Version::supports_avx512bw()) {\n+    kmovql(tmp, mask);\n+  } else {\n+    assert(masklen <= 16, \"\");\n+    kmovwl(tmp, mask);\n+  }\n+  if (masksize < 16) {\n+    andq(tmp, (((jlong)1 << masklen) - 1));\n+  }\n@@ -3857,1 +4087,2 @@\n-                                              XMMRegister xtmp1, Register tmp, int masklen, int vec_enc) {\n+                                              XMMRegister xtmp1, Register tmp, int masklen, int masksize,\n+                                              int vec_enc) {\n@@ -3862,1 +4093,1 @@\n-  if (masklen < 64) {\n+  if (masksize < 16) {\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":240,"deletions":9,"binary":false,"changes":249,"status":"modified"},{"patch":"@@ -145,0 +145,2 @@\n+  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, Register tmp, bool novlbwdq, int vlen_enc);\n+\n@@ -225,2 +227,1 @@\n-  void vector_mask_operation(int opc, Register dst, XMMRegister mask, XMMRegister xtmp, Register tmp,\n-                             KRegister ktmp, int masklen, int vec_enc);\n+  void vector_mask_operation(int opc, Register dst, KRegister mask, Register tmp, int masklen, int masksize, int vec_enc);\n@@ -229,1 +230,1 @@\n-                             Register tmp, int masklen, int vec_enc);\n+                             Register tmp, int masklen, int masksize, int vec_enc);\n@@ -276,0 +277,14 @@\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                   bool merge, int vlen_enc, bool is_varshift = false);\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, Address src2,\n+                   bool merge, int vlen_enc);\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                   XMMRegister src1, int imm8, bool merge, int vlen_enc);\n+\n+  void masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                 KRegister src1, KRegister src2);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":18,"deletions":3,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -8270,0 +8270,373 @@\n+void MacroAssembler::knot(uint masklen, KRegister dst, KRegister src, KRegister ktmp, Register rtmp) {\n+  switch(masklen) {\n+    case 2:\n+       knotbl(dst, src);\n+       movl(rtmp, 3);\n+       kmovbl(ktmp, rtmp);\n+       kandbl(dst, ktmp, dst);\n+       break;\n+    case 4:\n+       knotbl(dst, src);\n+       movl(rtmp, 15);\n+       kmovbl(ktmp, rtmp);\n+       kandbl(dst, ktmp, dst);\n+       break;\n+    case 8:\n+       knotbl(dst, src);\n+       break;\n+    case 16:\n+       knotwl(dst, src);\n+       break;\n+    case 32:\n+       knotdl(dst, src);\n+       break;\n+    case 64:\n+       knotql(dst, src);\n+       break;\n+    default:\n+      fatal(\"Unexpected vector length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kand(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kandbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kandwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kanddl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kandql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       korbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       korwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       korql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kxorbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kxorwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kxordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kxorql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evpermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evpermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src1, KRegister src2) {\n+   masklen = masklen < 8 ? 8 : masklen;\n+   ktest(masklen, src1, src2);\n+   setb(Assembler::notZero, dst);\n+   movzbl(dst, dst);\n+}\n+\n+void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch) {\n+  if (masklen < 8) {\n+    knotbl(kscratch, src2);\n+    kortestbl(src1, kscratch);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  } else {\n+    ktest(masklen, src1, src2);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  }\n+}\n+\n+void MacroAssembler::kortest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen) {\n+    case 8:\n+       kortestbl(src1, src2);\n+       break;\n+    case 16:\n+       kortestwl(src1, src2);\n+       break;\n+    case 32:\n+       kortestdl(src1, src2);\n+       break;\n+    case 64:\n+       kortestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+\n+void MacroAssembler::ktest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen)  {\n+    case 8:\n+       ktestbl(src1, src2);\n+       break;\n+    case 16:\n+       ktestwl(src1, src2);\n+       break;\n+    case 32:\n+       ktestdl(src1, src2);\n+       break;\n+    case 64:\n+       ktestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprold(dst, mask, src, shift, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprolq(dst, mask, src, shift, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprord(dst, mask, src, shift, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprorq(dst, mask, src, shift, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprolvd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprolvq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprorvd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprorvq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":373,"deletions":0,"binary":false,"changes":373,"status":"modified"},{"patch":"@@ -1341,0 +1341,69 @@\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpslld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrad(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n@@ -1630,1 +1699,27 @@\n-  \/\/ Data\n+  \/\/ AVX-512 mask operations.\n+  void kand(BasicType etype, KRegister dst, KRegister src1, KRegister src2);\n+  void kor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void knot(uint masklen, KRegister dst, KRegister src, KRegister ktmp = knoreg, Register rtmp = noreg);\n+  void kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void kortest(uint masklen, KRegister src1, KRegister src2);\n+  void ktest(uint masklen, KRegister src1, KRegister src2);\n+\n+  void evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc);\n+  void evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc);\n+  void evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc);\n+  void evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc);\n+\n+  void alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch);\n+  void anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":96,"deletions":1,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -4004,0 +4004,1 @@\n+    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x00000001);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -7679,0 +7679,1 @@\n+    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+address StubRoutines::x86::_vector_int_mask_cmp_bits = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+  static address _vector_int_mask_cmp_bits;\n@@ -292,0 +293,4 @@\n+  static address vector_int_mask_cmp_bits() {\n+    return _vector_int_mask_cmp_bits;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -887,0 +887,1 @@\n+  static bool supports_avx512bwdq()   { return (supports_evex() && supports_avx512bw() && supports_avx512dq()); }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1377,0 +1377,1 @@\n+  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n@@ -1559,0 +1560,1 @@\n+    case Op_VectorMaskToLong:\n@@ -1805,0 +1807,2 @@\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n@@ -1806,1 +1810,1 @@\n-      if(bt == T_BYTE || bt == T_SHORT) {\n+      if(is_subword_type(bt)) {\n@@ -1817,0 +1821,11 @@\n+    case Op_MaskAll:\n+      if (!is_LP64 || !VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -1826,0 +1841,142 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n+    return false;\n+  }\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+      if(!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorLoadMask:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_SqrtVF:\n+    case Op_SqrtVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_floating_point_type(bt)) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorRearrange:\n+      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MaskAll:\n+      return true;\n+\n+    default:\n+      return false;\n+  }\n+}\n+\n@@ -1890,1 +2047,1 @@\n-  return new TypeVectMask(TypeInt::BOOL, length);\n+  return new TypeVectMask(elemTy, length);\n@@ -3313,0 +3470,1 @@\n+\n@@ -3314,0 +3472,73 @@\n+instruct reinterpret_mask(kReg dst) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n) == Matcher::vector_length(n->in(1))); \/\/ dst == src\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(125);\n+  format %{ \"vector_reinterpret $dst\\t!\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_W2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_SHORT &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_W2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_SHORT);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2w($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_D2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_INT ||\n+             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_FLOAT) &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_D2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_INT);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2d($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_Q2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_LONG ||\n+             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_DOUBLE) &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_Q2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_LONG);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2q($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3316,1 +3547,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); \/\/ dst == src\n+  predicate(!n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); \/\/ dst == src\n@@ -3351,0 +3583,1 @@\n+            !n->bottom_type()->isa_vectmask() &&\n@@ -3366,0 +3599,1 @@\n+            !n->bottom_type()->isa_vectmask() &&\n@@ -3383,1 +3617,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); \/\/ src > dst\n+  predicate(!n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); \/\/ src > dst\n@@ -3585,1 +3820,1 @@\n-  predicate(Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n@@ -3610,1 +3845,1 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == 64);\n+  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n@@ -3613,1 +3848,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t! using $tmp and k2 as TEMP\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t! using $tmp and ktmp as TEMP\" %}\n@@ -3629,0 +3864,18 @@\n+instruct evgather_masked(vec dst, memory mem, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx mask)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp);\n+  format %{ \"load_vector_gather_masked $dst, $mem, $idx, $mask\\t! using $tmp and ktmp as TEMP\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"sanity\");\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    \/\/ Note: Since gather instruction partially updates the opmask register used\n+    \/\/ for predication hense moving mask operand to a temporary.\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3652,0 +3905,18 @@\n+instruct scatter_masked(memory mem, vec src, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx mask))));\n+  effect(TEMP tmp, TEMP ktmp);\n+  format %{ \"store_vector_scatter_masked $mem, $idx, $src, $mask\\t!\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(Matcher::vector_length_in_bytes(this, $src) >= 16, \"sanity\");\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    \/\/ Note: Since scatter instruction partially updates the opmask register used\n+    \/\/ for predication hense moving mask operand to a temporary.\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -3897,1 +4168,1 @@\n-  predicate(UseAVX > 0);\n+  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) >= 16);\n@@ -5863,0 +6134,1 @@\n+  ins_cost(400);\n@@ -5875,0 +6147,1 @@\n+  ins_cost(400);\n@@ -5887,0 +6160,1 @@\n+  ins_cost(400);\n@@ -5899,0 +6173,1 @@\n+  ins_cost(400);\n@@ -6907,1 +7182,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n@@ -6924,1 +7200,1 @@\n-instruct evcmpFD(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n@@ -6926,0 +7202,1 @@\n+            n->bottom_type()->isa_vectmask() == NULL &&\n@@ -6945,0 +7222,19 @@\n+instruct evcmpFD(kReg dst, vec src1, vec src2, immI8 cond) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            is_floating_point_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1 T_FLOAT, T_DOUBLE\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vector_compare_evex $dst,$src1,$src2,$cond\\t!\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+    KRegister mask = k0; \/\/ The comparison itself is not being masked.\n+    if (Matcher::vector_element_basic_type(this, $src1) == T_FLOAT) {\n+      __ evcmpps($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    } else {\n+      __ evcmppd($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6946,1 +7242,1 @@\n-  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -6964,1 +7260,1 @@\n-  predicate((UseAVX == 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -6983,1 +7279,1 @@\n-  predicate((UseAVX == 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7000,3 +7296,2 @@\n-instruct evcmp(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n-  predicate(UseAVX > 2 &&\n-            (VM_Version::supports_avx512vl() ||\n+instruct vcmpu64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+  predicate((n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7018,0 +7313,33 @@\n+    switch (src1_elem_bt) {\n+      case T_INT: {\n+        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      case T_LONG: {\n+        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct evcmp(kReg dst, vec src1, vec src2, immI8 cond) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vector_compared_evex $dst,$src1,$src2,$cond\\t!\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"required\");\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+    bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n@@ -7020,2 +7348,1 @@\n-        __ evpcmpb($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqub($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpb($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7025,2 +7352,1 @@\n-        __ evpcmpw($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquw($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpw($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7030,2 +7356,1 @@\n-        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpd($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7035,2 +7360,1 @@\n-        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpq($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7189,0 +7513,1 @@\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL &&\n@@ -7202,0 +7527,1 @@\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL &&\n@@ -7214,1 +7540,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == 64);\n+  predicate(Matcher::vector_length_in_bytes(n) == 64 &&\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL);\n@@ -7227,0 +7554,16 @@\n+\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+  predicate(n->in(2)->bottom_type()->isa_vectmask() &&\n+            (!is_subword_type(Matcher::vector_element_basic_type(n)) ||\n+             VM_Version::supports_avx512bw()));\n+  match(Set dst (VectorBlend (Binary src1 src2) mask));\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n+  effect(TEMP scratch);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ evpblend(elem_bt, $dst$$XMMRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7231,0 +7574,1 @@\n+  ins_cost(450);\n@@ -7246,0 +7590,1 @@\n+  ins_cost(450);\n@@ -7262,0 +7607,1 @@\n+  ins_cost(250);\n@@ -7276,0 +7622,1 @@\n+  ins_cost(450);\n@@ -7348,1 +7695,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7353,1 +7701,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_lt16 $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n@@ -7363,2 +7711,3 @@\n-instruct vptest_alltrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_alltrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7369,1 +7718,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_ge16  $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n@@ -7379,6 +7728,8 @@\n-instruct vptest_alltrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 64 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+instruct vptest_alltrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n->in(1)) < 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr, TEMP kscratch);\n+  format %{ \"vptest_alltrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7386,4 +7737,24 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::carrySet, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, $kscratch$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vptest_alltrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n->in(1)) >= 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_alltrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, knoreg);\n@@ -7394,0 +7765,1 @@\n+\n@@ -7395,1 +7767,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7400,1 +7773,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_lt16 $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n@@ -7410,2 +7783,3 @@\n-instruct vptest_anytrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_anytrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7416,1 +7790,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_ge16 $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7426,2 +7800,2 @@\n-instruct vptest_anytrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 64 &&\n+instruct vptest_anytrue_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n@@ -7429,3 +7803,3 @@\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7433,4 +7807,5 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::notZero, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint  masklen = Matcher::vector_length(this, $src1);\n+    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7442,1 +7817,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n@@ -7447,1 +7823,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t! using $vtmp as TEMP\" %}\n+  format %{ \"cmpvptest_anytrue_lt16 $src1,$src2\\t! using $vtmp as TEMP\" %}\n@@ -7455,2 +7831,3 @@\n-instruct cmpvptest_anytrue(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n+instruct cmpvptest_anytrue_ge16(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n@@ -7460,1 +7837,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_ge16 $src1,$src2\\t!\" %}\n@@ -7468,2 +7845,2 @@\n-instruct cmpvptest_anytrue_evex(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero, kReg ktmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 &&\n+instruct cmpvptest_anytrue_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n@@ -7472,2 +7849,1 @@\n-  effect(TEMP ktmp);\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_evex $src1,$src2\\t!\" %}\n@@ -7475,2 +7851,6 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    masklen = masklen < 8 ? 8 : masklen;\n+    __ ktest(masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7485,1 +7865,1 @@\n-  predicate(!VM_Version::supports_avx512vlbw());\n+  predicate(n->bottom_type()->isa_vectmask() == NULL && !VM_Version::supports_avx512vlbw());\n@@ -7488,1 +7868,1 @@\n-  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  format %{ \"vector_loadmask_byte $dst, $src\\n\\t\" %}\n@@ -7492,1 +7872,0 @@\n-\n@@ -7498,2 +7877,2 @@\n-instruct loadMask_evex(vec dst, vec src) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n+instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI tmp) %{\n+  predicate(n->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());\n@@ -7501,2 +7880,2 @@\n-  effect(TEMP dst);\n-  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  effect(TEMP xtmp, TEMP tmp);\n+  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp and $tmp as TEMP\" %}\n@@ -7504,2 +7883,5 @@\n-    int vlen_in_bytes = Matcher::vector_length_in_bytes(this);\n-    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                        $tmp$$Register, true, Assembler::AVX_512bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -7507,1 +7889,9 @@\n-    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, false);\n+instruct loadMask_evex(kReg dst, vec src,  vec xtmp) %{\n+  predicate(n->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_loadmask_byte $dst, $src\\t! using $xtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(in(1));\n+    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                        noreg, false, vlen_enc);\n@@ -7514,2 +7904,2 @@\n-instruct storeMask1B(vec dst, vec src, immI_1 size) %{\n-  predicate(Matcher::vector_length(n) < 64 || VM_Version::supports_avx512vlbw());\n+instruct vstoreMask1B(vec dst, vec src, immI_1 size) %{\n+  predicate(Matcher::vector_length(n) < 64 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7517,1 +7907,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7519,2 +7909,3 @@\n-    assert(UseSSE >= 3, \"required\");\n-    if (Matcher::vector_length_in_bytes(this) <= 16) {\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 16 && UseAVX <= 2) {\n+      assert(UseSSE >= 3, \"required\");\n@@ -7523,1 +7914,1 @@\n-      assert(UseAVX >= 2, \"required\");\n+      assert(UseAVX > 0, \"required\");\n@@ -7531,2 +7922,2 @@\n-instruct storeMask2B(vec dst, vec src, immI_2 size) %{\n-  predicate(Matcher::vector_length(n) <= 8);\n+instruct vstoreMask2B(vec dst, vec src, vec xtmp, immI_2 size) %{\n+  predicate(Matcher::vector_length(n) <= 16 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7534,14 +7925,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\n\\t\" %}\n-  ins_encode %{\n-    assert(UseSSE >= 3, \"required\");\n-    __ pabsw($dst$$XMMRegister, $src$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vstoreMask2B(vec dst, vec src, immI_2 size) %{\n-  predicate(Matcher::vector_length(n) == 16 && !VM_Version::supports_avx512bw());\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP dst);\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7550,3 +7929,12 @@\n-    __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n-    __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister,vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 8) {\n+      assert(UseSSE >= 3, \"required\");\n+      __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);\n+      __ pabsw($dst$$XMMRegister, $src$$XMMRegister);\n+      __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 0, \"required\");\n+      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n+      __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -7557,2 +7945,2 @@\n-instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n-  predicate(VM_Version::supports_avx512bw());\n+instruct vstoreMask4B(vec dst, vec src, vec xtmp, immI_4 size) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) <= 8 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7560,1 +7948,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp);\n@@ -7562,4 +7951,16 @@\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+    int vlen_enc = Assembler::AVX_128bit;\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 4) {\n+      assert(UseSSE >= 3, \"required\");\n+      __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);\n+      __ pabsd($dst$$XMMRegister, $src$$XMMRegister);\n+      __ packusdw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+      __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 0, \"required\");\n+      __ vpxor($xtmp$$XMMRegister, $xtmp$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);\n+      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n+      __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);\n+      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -7570,2 +7971,2 @@\n-instruct storeMask4B(vec dst, vec src, immI_4 size) %{\n-  predicate(Matcher::vector_length(n) <= 4 && UseAVX <= 2);\n+instruct storeMask8B(vec dst, vec src, vec xtmp, immI_8 size) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 2);\n@@ -7573,1 +7974,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7576,3 +7978,5 @@\n-    __ pabsd($dst$$XMMRegister, $src$$XMMRegister);\n-    __ packssdw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n+    __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);\n+    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x8);\n+    __ pabsd($dst$$XMMRegister, $dst$$XMMRegister);\n+    __ packusdw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);\n@@ -7583,2 +7987,2 @@\n-instruct vstoreMask4B(vec dst, vec src, immI_4 size) %{\n-  predicate(Matcher::vector_length(n) == 8 && UseAVX <= 2);\n+instruct storeMask8B_avx(vec dst, vec src, immI_8 size, vec vtmp) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 4);\n@@ -7586,2 +7990,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  effect(TEMP dst);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s], using $vtmp as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP vtmp);\n@@ -7590,3 +7994,6 @@\n-    __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n-    __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    __ vpshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);\n+    __ vextracti128($vtmp$$XMMRegister, $dst$$XMMRegister, 0x1);\n+    __ vblendps($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0xC, vlen_enc);\n+    __ vpxor($vtmp$$XMMRegister, $vtmp$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+    __ vpackssdw($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n@@ -7598,2 +8005,2 @@\n-instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n-  predicate(UseAVX > 2);\n+instruct vstoreMask4B_evex_novectmask(vec dst, vec src, immI_4 size) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7601,1 +8008,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7614,2 +8021,2 @@\n-instruct storeMask8B(vec dst, vec src, immI_8 size) %{\n-  predicate(Matcher::vector_length(n) == 2 && UseAVX <= 2);\n+instruct vstoreMask8B_evex_novectmask(vec dst, vec src, immI_8 size) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7617,1 +8024,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7619,6 +8026,8 @@\n-    assert(UseSSE >= 3, \"required\");\n-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x8);\n-    __ packssdw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ pabsb($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n+    int src_vlen_enc = vector_length_encoding(this, $src);\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    if (!VM_Version::supports_avx512vl()) {\n+      src_vlen_enc = Assembler::AVX_512bit;\n+    }\n+    __ evpmovqb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n@@ -7628,5 +8037,5 @@\n-instruct storeMask8B_avx(vec dst, vec src, immI_8 size, legVec vtmp) %{\n-  predicate(Matcher::vector_length(n) == 4 && UseAVX <= 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t! using $vtmp as TEMP\" %}\n-  effect(TEMP dst, TEMP vtmp);\n+instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size, rRegI tmp) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vector_store_mask $dst, $mask \\t! elem size is $size byte[s]\" %}\n@@ -7634,7 +8043,4 @@\n-    int vlen_enc = Assembler::AVX_128bit;\n-    __ vpshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);\n-    __ vextracti128($vtmp$$XMMRegister, $dst$$XMMRegister, 0x1);\n-    __ vblendps($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0xC, vlen_enc);\n-    __ vpackssdw($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    assert(Matcher::vector_length_in_bytes(this, $mask) == 64, \"\");\n+    __ evmovdqul($dst$$XMMRegister, $mask$$KRegister, ExternalAddress(vector_int_mask_cmp_bits()),\n+                 false, Assembler::AVX_512bit, $tmp$$Register);\n+    __ evpmovdb($dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_512bit);\n@@ -7645,4 +8051,5 @@\n-instruct vstoreMask8B_evex(vec dst, vec src, immI_8 size) %{\n-  predicate(UseAVX > 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+instruct vstoreMask_evex(vec dst, kReg mask, immI size) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vector_store_mask $dst, $mask \\t! elem size is $size byte[s]\" %}\n@@ -7650,1 +8057,0 @@\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n@@ -7652,4 +8058,1 @@\n-    if (!VM_Version::supports_avx512vl()) {\n-      src_vlen_enc = Assembler::AVX_512bit;\n-    }\n-    __ evpmovqb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ evpmovm2b($dst$$XMMRegister, $mask$$KRegister, dst_vlen_enc);\n@@ -7661,0 +8064,11 @@\n+instruct vmaskcast_evex(kReg dst) %{\n+  predicate(Matcher::vector_length(n) == Matcher::vector_length(n->in(1)));\n+  match(Set dst (VectorMaskCast dst));\n+  ins_cost(0);\n+  format %{ \"vector_mask_cast $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(empty);\n+%}\n+\n@@ -8230,2 +8644,49 @@\n-instruct vmask_truecount_evex(rRegI dst, vec mask, rRegL tmp, kReg ktmp, vec xtmp) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n+instruct vmask_tolong_evex(rRegL dst, kReg mask, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorMaskToLong mask));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"vector_tolong_evex $dst, $mask \\t! vector mask tolong\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n+    if (VM_Version::supports_avx512vlbw()) {\n+      __ kmovql($dst$$Register, $mask$$KRegister);\n+    } else {\n+      assert(mask_len <= 16, \"\");\n+      __ kmovwl($dst$$Register, $mask$$KRegister);\n+    }\n+    \/\/ Mask generated out of partial vector comparisons\/replicate\/mask manipulation\n+    \/\/ operations needs to be clipped.\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    if (mask_size < 16) {\n+      __ andq($dst$$Register, (((jlong)1 << mask_len) - 1));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_tolong_avx(rRegL dst, vec mask, vec xtmp, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() == NULL &&\n+            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n+  match(Set dst (VectorMaskToLong mask));\n+  format %{ \"vector_tolong_avx $dst, $mask \\t! using $xtmp as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp, KILL cr);\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n+    __ vpxor($xtmp$$XMMRegister, $xtmp$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);\n+    __ vpsubb($xtmp$$XMMRegister, $xtmp$$XMMRegister, $mask$$XMMRegister, vlen_enc);\n+    __ vpmovmskb($dst$$Register, $xtmp$$XMMRegister, vlen_enc);\n+    \/\/ Mask generated out of partial vector comparisons\/replicate\/mask manipulation\n+    \/\/ operations needs to be clipped.\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    if (mask_size < 16) {\n+      __ andq($dst$$Register, (((jlong)1 << mask_len) - 1));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_truecount_evex(rRegI dst, kReg mask, rRegL tmp, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask());\n@@ -8233,2 +8694,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP xtmp);\n-  format %{ \"vector_truecount_evex $mask \\t! vector mask true count\" %}\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"vector_truecount_evex $dst, $mask \\t! using $tmp as TEMP\" %}\n@@ -8237,1 +8698,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8239,2 +8700,4 @@\n-    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,\n-                             $tmp$$Register, $ktmp$$KRegister, mask_len, vlen_enc);\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n+    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister, $tmp$$Register,\n+                             mask_len, mask_size, vlen_enc);\n@@ -8245,6 +8708,5 @@\n-instruct vmask_first_or_last_true_evex(rRegI dst, vec mask, rRegL tmp, kReg ktmp, vec xtmp, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n-  match(Set dst (VectorMaskFirstTrue mask));\n-  match(Set dst (VectorMaskLastTrue mask));\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP xtmp, KILL cr);\n-  format %{ \"vector_mask_first_or_last_true_evex $mask \\t! vector first\/last true location\" %}\n+instruct vmask_truecount_avx(rRegI dst, vec mask, rRegL tmp, vec xtmp, vec xtmp1, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() == NULL);\n+  match(Set dst (VectorMaskTrueCount mask));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, TEMP xtmp1, KILL cr);\n+  format %{ \"vector_truecount_avx $dst, $mask \\t! using $tmp, $xtmp and $xtmp1 as TEMP\" %}\n@@ -8253,1 +8715,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8255,0 +8717,2 @@\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n@@ -8256,1 +8720,1 @@\n-                             $tmp$$Register, $ktmp$$KRegister, mask_len, vlen_enc);\n+                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, mask_size, vlen_enc);\n@@ -8261,5 +8725,6 @@\n-instruct vmask_truecount_avx(rRegI dst, vec mask, rRegL tmp, vec xtmp, vec xtmp1) %{\n-  predicate(!VM_Version::supports_avx512vlbw());\n-  match(Set dst (VectorMaskTrueCount mask));\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, TEMP xtmp1);\n-  format %{ \"vector_truecount_avx $mask \\t! vector mask true count\" %}\n+instruct vmask_first_or_last_true_evex(rRegI dst, kReg mask, rRegL tmp, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorMaskFirstTrue mask));\n+  match(Set dst (VectorMaskLastTrue mask));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"vector_mask_first_or_last_true_evex $dst, $mask \\t! using $tmp as TEMP\" %}\n@@ -8268,1 +8733,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8270,2 +8735,4 @@\n-    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,\n-                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, vlen_enc);\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n+    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister, $tmp$$Register, mask_len,\n+                             mask_size, vlen_enc);\n@@ -8277,1 +8744,1 @@\n-  predicate(!VM_Version::supports_avx512vlbw());\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -8281,1 +8748,1 @@\n-  format %{ \"vector_mask_first_or_last_true_avx $mask \\t! vector first\/last true location\" %}\n+  format %{ \"vector_mask_first_or_last_true_avx $dst, $mask \\t! using $tmp, $xtmp and $xtmp1 as TEMP\" %}\n@@ -8284,1 +8751,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8286,0 +8753,2 @@\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n@@ -8287,1 +8756,1 @@\n-                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, vlen_enc);\n+                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, mask_size, vlen_enc);\n@@ -8293,0 +8762,669 @@\n+\/\/ ---------------------------------- Vector Masked Operations ------------------------------------\n+\n+instruct vadd_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (AddVB (Binary dst src2) mask));\n+  match(Set dst (AddVS (Binary dst src2) mask));\n+  match(Set dst (AddVI (Binary dst src2) mask));\n+  match(Set dst (AddVL (Binary dst src2) mask));\n+  match(Set dst (AddVF (Binary dst src2) mask));\n+  match(Set dst (AddVD (Binary dst src2) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2, $mask\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vadd_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (AddVB (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2, $mask\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (XorV (Binary dst src2) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2, $mask\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (XorV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2, $mask\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (OrV (Binary dst src2) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2, $mask\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (OrV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2, $mask\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (AndV (Binary dst src2) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2, $mask\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (AndV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2, $mask\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (SubVB (Binary dst src2) mask));\n+  match(Set dst (SubVS (Binary dst src2) mask));\n+  match(Set dst (SubVI (Binary dst src2) mask));\n+  match(Set dst (SubVL (Binary dst src2) mask));\n+  match(Set dst (SubVF (Binary dst src2) mask));\n+  match(Set dst (SubVD (Binary dst src2) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2, $mask\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (SubVB (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2, $mask\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MulVS (Binary dst src2) mask));\n+  match(Set dst (MulVI (Binary dst src2) mask));\n+  match(Set dst (MulVL (Binary dst src2) mask));\n+  match(Set dst (MulVF (Binary dst src2) mask));\n+  match(Set dst (MulVD (Binary dst src2) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2, $mask\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MulVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2, $mask\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsqrt_reg_masked(vec dst, kReg mask) %{\n+  match(Set dst (SqrtVF dst mask));\n+  match(Set dst (SqrtVD dst mask));\n+  ins_cost(100);\n+  format %{ \"vpsqrt_masked $dst, $mask\\t! sqrt masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (DivVF (Binary dst src2) mask));\n+  match(Set dst (DivVD (Binary dst src2) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2, $mask\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (DivVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (DivVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2, $mask\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vrol_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (RotateLeftV (Binary dst shift) mask));\n+  match(Set dst (RotateRightV (Binary dst shift) mask));\n+  format %{ \"vprotate_imm_masked $dst, $dst, $shift, $mask\\t! rotate masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrol_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (RotateLeftV (Binary dst src2) mask));\n+  match(Set dst (RotateRightV (Binary dst src2) mask));\n+  format %{ \"vrotate_masked $dst, $dst, $src2, $mask\\t! rotate masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst (LShiftCntV shift)) mask));\n+  match(Set dst (LShiftVI (Binary dst (LShiftCntV shift)) mask));\n+  match(Set dst (LShiftVL (Binary dst (LShiftCntV shift)) mask));\n+  format %{ \"vplshift_imm_masked $dst, $dst, $shift, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (LShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (LShiftVL (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (RShiftVI (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (RShiftVL (Binary dst (RShiftCntV shift)) mask));\n+  format %{ \"vprshift_imm_masked $dst, $dst, $shift, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (RShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (RShiftVL (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (URShiftVI (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (URShiftVL (Binary dst (RShiftCntV shift)) mask));\n+  format %{ \"vpurshift_imm_masked $dst, $dst, $shift, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (URShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (URShiftVL (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MaxV (Binary dst src2) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2, $mask\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MaxV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2, $mask\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MinV (Binary dst src2) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2, $mask\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MinV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2, $mask\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrearrangev_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (VectorRearrange (Binary dst src2) mask));\n+  format %{ \"vprearrange_masked $dst, $dst, $src2, $mask\\t! rearrange masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, false, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vabs_masked(vec dst, kReg mask) %{\n+  match(Set dst (AbsVB dst mask));\n+  match(Set dst (AbsVS dst mask));\n+  match(Set dst (AbsVI dst mask));\n+  match(Set dst (AbsVL dst mask));\n+  format %{ \"vabs_masked $dst, $mask \\t! vabs masked operation\" %}\n+  ins_cost(100);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_reg_masked(vec dst, vec src2, vec src3, kReg mask) %{\n+  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_mem_masked(vec dst, vec src2, memory src3, kReg mask) %{\n+  match(Set dst (FmaVF (Binary dst src2) (Binary (LoadVector src3) mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary (LoadVector src3) mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond mask)));\n+  effect(TEMP scratch);\n+  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n+    switch (src1_elem_bt) {\n+      case T_BYTE: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpb($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_SHORT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpw($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_INT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_LONG: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpq($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmpps($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmppd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt)); break;\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+#ifdef _LP64\n+instruct mask_all_evexI_imm(kReg dst, immI cnt, rRegL tmp) %{\n+  match(Set dst (MaskAll cnt));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $cnt \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ movq($tmp$$Register, $cnt$$constant);\n+      __ kmovql($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ movq($tmp$$Register, $cnt$$constant);\n+      __ kmovwl($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexI(kReg dst, rRegI src, rRegL tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $src \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ movslq($tmp$$Register, $src$$Register);\n+      __ kmovql($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ kmovwl($dst$$KRegister, $src$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ kmovql($dst$$KRegister, $src$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ kmovwl($dst$$KRegister, $src$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_not_immLT8(kReg dst, kReg src, rRegI rtmp, kReg ktmp, immI_M1 cnt) %{\n+  predicate(Matcher::vector_length(n) < 8 && VM_Version::supports_avx512dq());\n+  match(Set dst (XorVMask src (MaskAll cnt)));\n+  effect(TEMP_DEF dst, TEMP rtmp, TEMP ktmp);\n+  format %{ \"mask_not_LT8 $dst, $src, $cnt \\t!using $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    uint masklen = Matcher::vector_length(this);\n+    __ knot(masklen, $dst$$KRegister, $src$$KRegister, $ktmp$$KRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_not_imm(kReg dst, kReg src, immI_M1 cnt) %{\n+  predicate((Matcher::vector_length(n) == 8 && VM_Version::supports_avx512dq()) ||\n+            (Matcher::vector_length(n) == 16) ||\n+            (Matcher::vector_length(n) > 16 && VM_Version::supports_avx512bw()));\n+  match(Set dst (XorVMask src (MaskAll cnt)));\n+  format %{ \"mask_not $dst, $src, $cnt \\t! mask not operation\" %}\n+  ins_encode %{\n+    uint masklen = Matcher::vector_length(this);\n+    __ knot(masklen, $dst$$KRegister, $src$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n+instruct mask_opers_evex(kReg dst, kReg src1, kReg src2, kReg kscratch) %{\n+  match(Set dst (AndVMask src1 src2));\n+  match(Set dst (OrVMask src1 src2));\n+  match(Set dst (XorVMask src1 src2));\n+  effect(TEMP kscratch);\n+  format %{ \"mask_opers_evex $dst, $src1, $src2\\t! using $kscratch as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this);\n+    masklen = (masklen < 16 && !VM_Version::supports_avx512dq()) ? 16 : masklen;\n+    __ masked_op(this->ideal_Opcode(), masklen, $dst$$KRegister, $src1$$KRegister, $src2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castMM(kReg dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":1318,"deletions":180,"binary":false,"changes":1498,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -272,0 +272,1 @@\n+  if( strcmp(opType,\"LoadVectorGatherMasked\")==0 )  return Form::idealV;\n@@ -290,0 +291,1 @@\n+  if( strcmp(opType,\"StoreVectorScatterMasked\")==0 )  return Form::idealV;\n","filename":"src\/hotspot\/share\/adlc\/forms.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2284,0 +2284,1 @@\n+  if (strcmp(name, \"RegVectMask\") == 0) size = globalAD->get_preproc_def(\"AARCH64\") ? 1 : 2;\n@@ -3517,1 +3518,2 @@\n-    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"StoreVector\", \"LoadVector\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorGatherMasked\", \"StoreVectorScatterMasked\",\n@@ -3821,1 +3823,1 @@\n-\/\/-------------------------- has_commutative_op -------------------------------\n+\/\/-------------------------- count_commutative_op -------------------------------\n@@ -3827,1 +3829,0 @@\n-    \"AddVB\",\"AddVS\",\"AddVI\",\"AddVL\",\"AddVF\",\"AddVD\",\n@@ -3829,1 +3830,0 @@\n-    \"AndV\",\n@@ -3831,1 +3831,0 @@\n-    \"MaxV\", \"MinV\",\n@@ -3833,1 +3832,0 @@\n-    \"MulVB\",\"MulVS\",\"MulVI\",\"MulVL\",\"MulVF\",\"MulVD\",\n@@ -3835,3 +3833,1 @@\n-    \"OrV\",\n-    \"XorI\",\"XorL\",\n-    \"XorV\"\n+    \"XorI\",\"XorL\"\n@@ -3839,1 +3835,0 @@\n-  int cnt = sizeof(commut_op_list)\/sizeof(char*);\n@@ -3841,1 +3836,8 @@\n-  if( _lChild && _rChild && (_lChild->_lChild || _rChild->_lChild) ) {\n+  static const char *commut_vector_op_list[] = {\n+    \"AddVB\", \"AddVS\", \"AddVI\", \"AddVL\", \"AddVF\", \"AddVD\",\n+    \"MulVB\", \"MulVS\", \"MulVI\", \"MulVL\", \"MulVF\", \"MulVD\",\n+    \"AndV\", \"OrV\", \"XorV\",\n+    \"MaxV\", \"MinV\"\n+  };\n+\n+  if (_lChild && _rChild && (_lChild->_lChild || _rChild->_lChild)) {\n@@ -3844,1 +3846,1 @@\n-    if( _rChild->_lChild == NULL && _rChild->_rChild == NULL ) {\n+    if (_rChild->_lChild == NULL && _rChild->_rChild == NULL) {\n@@ -3847,3 +3849,3 @@\n-      if ( form ) {\n-        OperandForm  *oper = form->is_operand();\n-        if( oper && oper->interface_type(globals) == Form::constant_interface )\n+      if (form) {\n+        OperandForm *oper = form->is_operand();\n+        if (oper && oper->interface_type(globals) == Form::constant_interface)\n@@ -3853,5 +3855,19 @@\n-    if( !is_const ) {\n-      for( int i=0; i<cnt; i++ ) {\n-        if( strcmp(_opType, commut_op_list[i]) == 0 ) {\n-          count++;\n-          _commutative_id = count; \/\/ id should be > 0\n+\n+    if (!is_const) {\n+      int scalar_cnt = sizeof(commut_op_list)\/sizeof(char*);\n+      int vector_cnt = sizeof(commut_vector_op_list)\/sizeof(char*);\n+      bool matched = false;\n+\n+      \/\/ Check the commutative vector op first. It's noncommutative if\n+      \/\/ the current node is a masked vector op, since a mask value\n+      \/\/ is added to the original vector node's input list and the original\n+      \/\/ first two inputs are packed into one BinaryNode. So don't swap\n+      \/\/ if one of the operands is a BinaryNode.\n+      for (int i = 0; i < vector_cnt; i++) {\n+        if (strcmp(_opType, commut_vector_op_list[i]) == 0) {\n+          if (strcmp(_lChild->_opType, \"Binary\") != 0 &&\n+              strcmp(_rChild->_opType, \"Binary\") != 0) {\n+            count++;\n+            _commutative_id = count; \/\/ id should be > 0\n+          }\n+          matched = true;\n@@ -3861,0 +3877,12 @@\n+\n+      \/\/ Then check the scalar op if the current op is not in\n+      \/\/ the commut_vector_op_list.\n+      if (!matched) {\n+        for (int i = 0; i < scalar_cnt; i++) {\n+          if (strcmp(_opType, commut_op_list[i]) == 0) {\n+            count++;\n+            _commutative_id = count; \/\/ id should be > 0\n+            break;\n+          }\n+        }\n+      }\n@@ -3863,1 +3891,1 @@\n-  if( _lChild )\n+  if (_lChild)\n@@ -3865,1 +3893,1 @@\n-  if( _rChild )\n+  if (_rChild)\n@@ -4091,0 +4119,1 @@\n+        strcmp(opType,\"MaskAll\")==0 ||\n@@ -4202,1 +4231,1 @@\n-    \"LoadVectorGather\", \"StoreVectorScatter\",\n+    \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorGatherMasked\", \"StoreVectorScatterMasked\",\n@@ -4209,0 +4238,2 @@\n+    \/\/ Next are vector mask ops.\n+    \"MaskAll\", \"AndVMask\", \"OrVMask\", \"XorVMask\", \"VectorMaskCast\",\n@@ -4211,2 +4242,1 @@\n-    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\",\n-    \"VectorMaskCast\"\n+    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\"\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":55,"deletions":25,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -822,1 +822,8 @@\n-   do_signature(vector_unary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/util\/function\/Function;)Ljava\/lang\/Object;\") \\\n+   do_signature(vector_unary_op_sig, \"(I\"                                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;Ljava\/lang\/Class;\"                                                                     \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$UnaryOperation;)\"                                                 \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                         \\\n@@ -826,2 +833,10 @@\n-   do_signature(vector_binary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                              \\\n-                                       \"Ljava\/util\/function\/BiFunction;)Ljava\/lang\/Object;\")                                                   \\\n+   do_signature(vector_binary_op_sig, \"(I\"                                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"I\"                                                                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$BinaryOperation;)\"                                               \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                 \\\n@@ -831,2 +846,11 @@\n-   do_signature(vector_ternary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                             \\\n-                                        \"Ljava\/lang\/Object;Ljdk\/internal\/vm\/vector\/VectorSupport$TernaryOperation;)Ljava\/lang\/Object;\")        \\\n+   do_signature(vector_ternary_op_sig, \"(I\"                                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"I\"                                                                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                        \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                        \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                        \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$TernaryOperation;)\"                                             \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                       \\\n@@ -836,2 +860,7 @@\n-   do_signature(vector_broadcast_coerced_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;IJLjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"      \\\n-                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$BroadcastOperation;)Ljava\/lang\/Object;\")                 \\\n+   do_signature(vector_broadcast_coerced_sig, \"(Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"I\"                                                                                             \\\n+                                               \"J\"                                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                          \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$BroadcastOperation;)\"                                    \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                         \\\n@@ -841,2 +870,6 @@\n-   do_signature(vector_shuffle_step_iota_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"        \\\n-                                               \"IIIILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleIotaOperation;)Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\") \\\n+   do_signature(vector_shuffle_step_iota_sig, \"(Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                          \\\n+                                               \"IIII\"                                                                                          \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$ShuffleIotaOperation;)\"                                  \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\")                                         \\\n@@ -846,2 +879,6 @@\n-   do_signature(vector_shuffle_to_vector_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\" \\\n-                                               \"ILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleToVectorOperation;)Ljava\/lang\/Object;\")          \\\n+   do_signature(vector_shuffle_to_vector_sig, \"(Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"                                          \\\n+                                               \"ILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleToVectorOperation;)\"                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                \\\n@@ -851,2 +888,10 @@\n-   do_signature(vector_load_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjava\/lang\/Object;\"                                \\\n-                                     \"ILjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;Ljdk\/internal\/vm\/vector\/VectorSupport$LoadOperation;)Ljava\/lang\/Object;\") \\\n+   do_signature(vector_load_op_sig, \"(Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"J\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                                    \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$LoadOperation;)\"                                                   \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                   \\\n@@ -855,0 +900,15 @@\n+  do_intrinsic(_VectorLoadMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_load_masked_op_name, vector_load_masked_op_sig, F_S)          \\\n+   do_signature(vector_load_masked_op_sig, \"(Ljava\/lang\/Class;\"                                                                                \\\n+                                            \"Ljava\/lang\/Class;\"                                                                                \\\n+                                            \"Ljava\/lang\/Class;\"                                                                                \\\n+                                            \"I\"                                                                                                \\\n+                                            \"Ljava\/lang\/Object;\"                                                                               \\\n+                                            \"J\"                                                                                                \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                \\\n+                                            \"Ljava\/lang\/Object;\"                                                                               \\\n+                                            \"I\"                                                                                                \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                             \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$LoadVectorMaskedOperation;)\"                                \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                   \\\n+   do_name(vector_load_masked_op_name,     \"loadMasked\")                                                                                       \\\n+                                                                                                                                               \\\n@@ -856,2 +916,8 @@\n-   do_signature(vector_store_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$Vector;\"    \\\n-                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)V\")                      \\\n+   do_signature(vector_store_op_sig, \"(Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljava\/lang\/Object;\"                                                                                     \\\n+                                      \"J\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)\"                        \\\n+                                      \"V\")                                                                                                     \\\n@@ -860,2 +926,25 @@\n-  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S) \\\n-   do_signature(vector_reduction_coerced_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljava\/util\/function\/Function;)J\") \\\n+  do_intrinsic(_VectorStoreMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_store_masked_op_name, vector_store_masked_op_sig, F_S)       \\\n+   do_signature(vector_store_masked_op_sig, \"(Ljava\/lang\/Class;\"                                                                               \\\n+                                             \"Ljava\/lang\/Class;\"                                                                               \\\n+                                             \"Ljava\/lang\/Class;\"                                                                               \\\n+                                             \"I\"                                                                                               \\\n+                                             \"Ljava\/lang\/Object;\"                                                                              \\\n+                                             \"J\"                                                                                               \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                   \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                               \\\n+                                             \"Ljava\/lang\/Object;\"                                                                              \\\n+                                             \"I\"                                                                                               \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorMaskedOperation;)\"                              \\\n+                                             \"V\")                                                                                              \\\n+   do_name(vector_store_masked_op_name,     \"storeMasked\")                                                                                     \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S)\\\n+   do_signature(vector_reduction_coerced_sig, \"(I\"                                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"I\"                                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                 \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$ReductionOperation;)\"                                    \\\n+                                               \"J\")                                                                                            \\\n@@ -865,1 +954,8 @@\n-   do_signature(vector_test_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;Ljava\/util\/function\/BiFunction;)Z\") \\\n+   do_signature(vector_test_sig, \"(I\"                                                                                                          \\\n+                                  \"Ljava\/lang\/Class;\"                                                                                          \\\n+                                  \"Ljava\/lang\/Class;\"                                                                                          \\\n+                                  \"I\"                                                                                                          \\\n+                                  \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                          \\\n+                                  \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                          \\\n+                                  \"Ljava\/util\/function\/BiFunction;)\"                                                                           \\\n+                                  \"Z\")                                                                                                         \\\n@@ -869,3 +965,9 @@\n-   do_signature(vector_blend_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                      \\\n-                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\" \\\n-                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBlendOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")       \\\n+   do_signature(vector_blend_sig, \"(Ljava\/lang\/Class;\"                                                                                         \\\n+                                   \"Ljava\/lang\/Class;\"                                                                                         \\\n+                                   \"Ljava\/lang\/Class;\"                                                                                         \\\n+                                   \"I\"                                                                                                         \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                             \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                             \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                         \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBlendOp;)\"                                                     \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                            \\\n@@ -875,3 +977,9 @@\n-   do_signature(vector_compare_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                   \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\" \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"           \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorCompareOp;\" \")\" \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\") \\\n+   do_signature(vector_compare_sig, \"(I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;Ljava\/lang\/Class;\"                                                                      \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorCompareOp;)\"                                                 \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\")                                                      \\\n@@ -881,3 +989,10 @@\n-   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                  \\\n-                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"     \\\n-                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorRearrangeOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\") \\\n+   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"I\"                                                                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                         \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorRearrangeOp;)\"                                             \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                        \\\n@@ -887,3 +1002,7 @@\n-   do_signature(vector_extract_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                     \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                          \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecExtractOp;)J\")                                                  \\\n+   do_signature(vector_extract_sig, \"(Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecExtractOp;)\"                                                    \\\n+                                     \"J\")                                                                                                      \\\n@@ -893,3 +1012,7 @@\n-   do_signature(vector_insert_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                      \\\n-                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;IJ\"                                                          \\\n-                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecInsertOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")        \\\n+   do_signature(vector_insert_sig, \"(Ljava\/lang\/Class;\"                                                                                        \\\n+                                    \"Ljava\/lang\/Class;\"                                                                                        \\\n+                                    \"I\"                                                                                                        \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                            \\\n+                                    \"IJ\"                                                                                                       \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecInsertOp;)\"                                                      \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                           \\\n@@ -899,3 +1022,10 @@\n-   do_signature(vector_broadcast_int_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                              \\\n-                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                    \\\n-                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBroadcastIntOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\") \\\n+   do_signature(vector_broadcast_int_sig, \"(I\"                                                                                                 \\\n+                                           \"Ljava\/lang\/Class;\"                                                                                 \\\n+                                           \"Ljava\/lang\/Class;\"                                                                                 \\\n+                                           \"Ljava\/lang\/Class;\"                                                                                 \\\n+                                           \"I\"                                                                                                 \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                     \\\n+                                           \"I\"                                                                                                 \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                 \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBroadcastIntOp;)\"                                      \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                    \\\n@@ -905,2 +1035,7 @@\n-   do_signature(vector_convert_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                    \\\n-                                     \"Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                     \\\n+   do_signature(vector_convert_sig, \"(I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n@@ -909,1 +1044,2 @@\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorConvertOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\") \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorConvertOp;)\"                                                 \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                   \\\n@@ -913,2 +1049,7 @@\n-    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                    \\\n-                                     \"Ljava\/lang\/Object;J\"                                                                                     \\\n+    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"J\"                                                                                                       \\\n@@ -916,1 +1057,3 @@\n-                                     \"Ljava\/lang\/Object;I[II\"                                                                                  \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"I[II\"                                                                                                    \\\n@@ -923,5 +1066,13 @@\n-    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                   \\\n-                                      \"Ljava\/lang\/Object;J\"                                                                                    \\\n-                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"             \\\n-                                      \"Ljava\/lang\/Object;I[II\"                                                                                 \\\n-                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperationWithMap;)V\")                                  \\\n+    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Object;\"                                                                                     \\\n+                                      \"J\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;\"                                    \\\n+                                      \"I[II\"                                                                                                   \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperationWithMap;)\"                                    \\\n+                                      \"V\")                                                                                                     \\\n@@ -931,1 +1082,2 @@\n-   do_alias(vector_rebox_sig, object_object_signature)                                                                                         \\\n+    do_signature(vector_rebox_sig, \"(Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;)\"                                                    \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                    \\\n@@ -935,2 +1087,7 @@\n-    do_signature(vector_mask_oper_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;\"                                               \\\n-                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMaskOp;)I\")                                               \\\n+    do_signature(vector_mask_oper_sig, \"(I\"                                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"I\"                                                                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMaskOp;)\"                                                 \\\n+                                        \"J\")                                                                                                   \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":207,"deletions":50,"binary":false,"changes":257,"status":"modified"},{"patch":"@@ -688,0 +688,1 @@\n+  case vmIntrinsics::_VectorLoadMaskedOp:\n@@ -689,0 +690,1 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2398,21 +2398,32 @@\n-  if (EnableVectorReboxing && can_reshape && progress == NULL) {\n-    PhaseIterGVN* igvn = phase->is_IterGVN();\n-\n-    bool all_inputs_are_equiv_vboxes = true;\n-    for (uint i = 1; i < req(); ++i) {\n-      Node* n = in(i);\n-      if (in(i)->Opcode() != Op_VectorBox) {\n-        all_inputs_are_equiv_vboxes = false;\n-        break;\n-      }\n-      \/\/ Check that vector type of vboxes is equivalent\n-      if (i != 1) {\n-        if (Type::cmp(in(i-0)->in(VectorBoxNode::Value)->bottom_type(),\n-                      in(i-1)->in(VectorBoxNode::Value)->bottom_type()) != 0) {\n-          all_inputs_are_equiv_vboxes = false;\n-          break;\n-        }\n-        if (Type::cmp(in(i-0)->in(VectorBoxNode::Box)->bottom_type(),\n-                      in(i-1)->in(VectorBoxNode::Box)->bottom_type()) != 0) {\n-          all_inputs_are_equiv_vboxes = false;\n-          break;\n+  if (EnableVectorReboxing && can_reshape && progress == NULL && type()->isa_oopptr()) {\n+    progress = merge_through_phi(this, phase->is_IterGVN());\n+  }\n+\n+  return progress;              \/\/ Return any progress\n+}\n+\n+Node* PhiNode::clone_through_phi(Node* root_phi, const Type* t, uint c, PhaseIterGVN* igvn) {\n+  Node_Stack stack(1);\n+  VectorSet  visited;\n+  Node_List  node_map;\n+\n+  stack.push(root_phi, 1); \/\/ ignore control\n+  visited.set(root_phi->_idx);\n+\n+  Node* new_phi = new PhiNode(root_phi->in(0), t);\n+  node_map.map(root_phi->_idx, new_phi);\n+\n+  while (stack.is_nonempty()) {\n+    Node* n   = stack.node();\n+    uint  idx = stack.index();\n+    assert(n->is_Phi(), \"not a phi\");\n+    if (idx < n->req()) {\n+      stack.set_index(idx + 1);\n+      Node* def = n->in(idx);\n+      if (def == NULL) {\n+        continue; \/\/ ignore dead path\n+      } else if (def->is_Phi()) { \/\/ inner node\n+        Node* new_phi = node_map[n->_idx];\n+        if (!visited.test_set(def->_idx)) { \/\/ not visited yet\n+          node_map.map(def->_idx, new PhiNode(def->in(0), t));\n+          stack.push(def, 1); \/\/ ignore control\n@@ -2420,0 +2431,9 @@\n+        Node* new_in = node_map[def->_idx];\n+        new_phi->set_req(idx, new_in);\n+      } else if (def->Opcode() == Op_VectorBox) { \/\/ leaf\n+        assert(n->is_Phi(), \"not a phi\");\n+        Node* new_phi = node_map[n->_idx];\n+        new_phi->set_req(idx, def->in(c));\n+      } else {\n+        assert(false, \"not optimizeable\");\n+        return NULL;\n@@ -2421,0 +2441,4 @@\n+    } else {\n+      Node* new_phi = node_map[n->_idx];\n+      igvn->register_new_node_with_optimizer(new_phi, n);\n+      stack.pop();\n@@ -2422,0 +2446,3 @@\n+  }\n+  return new_phi;\n+}\n@@ -2423,8 +2450,34 @@\n-    if (all_inputs_are_equiv_vboxes) {\n-      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(in(1));\n-      PhiNode* new_vbox_phi = new PhiNode(r, vbox->box_type());\n-      PhiNode* new_vect_phi = new PhiNode(r, vbox->vec_type());\n-      for (uint i = 1; i < req(); ++i) {\n-        VectorBoxNode* old_vbox = static_cast<VectorBoxNode*>(in(i));\n-        new_vbox_phi->set_req(i, old_vbox->in(VectorBoxNode::Box));\n-        new_vect_phi->set_req(i, old_vbox->in(VectorBoxNode::Value));\n+Node* PhiNode::merge_through_phi(Node* root_phi, PhaseIterGVN* igvn) {\n+  Node_Stack stack(1);\n+  VectorSet  visited;\n+\n+  stack.push(root_phi, 1); \/\/ ignore control\n+  visited.set(root_phi->_idx);\n+\n+  VectorBoxNode* cached_vbox = NULL;\n+  while (stack.is_nonempty()) {\n+    Node* n   = stack.node();\n+    uint  idx = stack.index();\n+    if (idx < n->req()) {\n+      stack.set_index(idx + 1);\n+      Node* in = n->in(idx);\n+      if (in == NULL) {\n+        continue; \/\/ ignore dead path\n+      } else if (in->isa_Phi()) {\n+        if (!visited.test_set(in->_idx)) {\n+          stack.push(in, 1); \/\/ ignore control\n+        }\n+      } else if (in->Opcode() == Op_VectorBox) {\n+        VectorBoxNode* vbox = static_cast<VectorBoxNode*>(in);\n+        if (cached_vbox == NULL) {\n+          cached_vbox = vbox;\n+        } else if (vbox->vec_type() != cached_vbox->vec_type()) {\n+          \/\/ TODO: vector type mismatch can be handled with additional reinterpret casts\n+          assert(Type::cmp(vbox->vec_type(), cached_vbox->vec_type()) != 0, \"inconsistent\");\n+          return NULL; \/\/ not optimizable: vector type mismatch\n+        } else if (vbox->box_type() != cached_vbox->box_type()) {\n+          assert(Type::cmp(vbox->box_type(), cached_vbox->box_type()) != 0, \"inconsistent\");\n+          return NULL; \/\/ not optimizable: box type mismatch\n+        }\n+      } else {\n+        return NULL; \/\/ not optimizable: neither Phi nor VectorBox\n@@ -2432,3 +2485,2 @@\n-      igvn->register_new_node_with_optimizer(new_vbox_phi, this);\n-      igvn->register_new_node_with_optimizer(new_vect_phi, this);\n-      progress = new VectorBoxNode(igvn->C, new_vbox_phi, new_vect_phi, vbox->box_type(), vbox->vec_type());\n+    } else {\n+      stack.pop();\n@@ -2437,2 +2489,6 @@\n-\n-  return progress;              \/\/ Return any progress\n+  assert(cached_vbox != NULL, \"sanity\");\n+  const TypeInstPtr* btype = cached_vbox->box_type();\n+  const TypeVect*    vtype = cached_vbox->vec_type();\n+  Node* new_vbox_phi = clone_through_phi(root_phi, btype, VectorBoxNode::Box,   igvn);\n+  Node* new_vect_phi = clone_through_phi(root_phi, vtype, VectorBoxNode::Value, igvn);\n+  return new VectorBoxNode(igvn->C, new_vbox_phi, new_vect_phi, btype, vtype);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":90,"deletions":34,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -146,0 +146,3 @@\n+  static Node* clone_through_phi(Node* root_phi, const Type* t, uint c, PhaseIterGVN* igvn);\n+  static Node* merge_through_phi(Node* root_phi, PhaseIterGVN* igvn);\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  if( _is_predicate ) tty->print(\"Predicate \");\n@@ -641,1 +642,2 @@\n-      } else if (lrg.num_regs() == 1) {\n+      } else if ((lrg.num_regs() == 1 && !lrg.is_scalable()) ||\n+                 (lrg.is_scalable() && lrg.scalable_reg_slots() == 1)) {\n@@ -656,9 +658,13 @@\n-          OptoReg::Name lo = OptoReg::add(hi, (1-num_regs)); \/\/ Find lo\n-          \/\/ We have to use pair [lo,lo+1] even for wide vectors because\n-          \/\/ the rest of code generation works only with pairs. It is safe\n-          \/\/ since for registers encoding only 'lo' is used.\n-          \/\/ Second reg from pair is used in ScheduleAndBundle on SPARC where\n-          \/\/ vector max size is 8 which corresponds to registers pair.\n-          \/\/ It is also used in BuildOopMaps but oop operations are not\n-          \/\/ vectorized.\n-          set2(i, lo);\n+          if (num_regs == 1) {\n+            set1(i, hi);\n+          } else {\n+            OptoReg::Name lo = OptoReg::add(hi, (1 - num_regs)); \/\/ Find lo\n+            \/\/ We have to use pair [lo,lo+1] even for wide vectors\/vmasks because\n+            \/\/ the rest of code generation works only with pairs. It is safe\n+            \/\/ since for registers encoding only 'lo' is used.\n+            \/\/ Second reg from pair is used in ScheduleAndBundle with vector max\n+            \/\/ size 8 which corresponds to registers pair.\n+            \/\/ It is also used in BuildOopMaps but oop operations are not\n+            \/\/ vectorized.\n+            set2(i, lo);\n+          }\n@@ -827,0 +833,14 @@\n+\n+        if (ireg == Op_RegVectMask) {\n+          assert(Matcher::has_predicated_vectors(), \"predicated vector should be supported\");\n+          lrg._is_predicate = 1;\n+          if (Matcher::supports_scalable_vector()) {\n+            lrg._is_scalable = 1;\n+            \/\/ For scalable predicate, when it is allocated in physical register,\n+            \/\/ num_regs is RegMask::SlotsPerRegVectMask for reg mask,\n+            \/\/ which may not be the actual physical register size.\n+            \/\/ If it is allocated in stack, we need to get the actual\n+            \/\/ physical length of scalable predicate register.\n+            lrg.set_scalable_reg_slots(Matcher::scalable_predicate_reg_slots());\n+          }\n+        }\n@@ -828,1 +848,1 @@\n-               ireg == Op_RegD || ireg == Op_RegL  || ireg == Op_RegVectMask,\n+               ireg == Op_RegD || ireg == Op_RegL || ireg == Op_RegVectMask,\n@@ -922,0 +942,2 @@\n+          assert(Matcher::has_predicated_vectors(), \"sanity\");\n+          assert(RegMask::num_registers(Op_RegVectMask) == RegMask::SlotsPerRegVectMask, \"sanity\");\n@@ -1374,0 +1396,5 @@\n+    } else if (lrg._is_predicate) {\n+      assert(num_regs == RegMask::SlotsPerRegVectMask, \"scalable predicate register\");\n+      num_regs = lrg.scalable_reg_slots();\n+      mask.clear_to_sets(num_regs);\n+      return mask.find_first_set(lrg, num_regs);\n@@ -1420,1 +1447,1 @@\n-  if (lrg._is_vector || lrg.num_regs() == 2) {\n+  if (lrg._is_vector || lrg.num_regs() == 2 || lrg.is_scalable()) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":39,"deletions":12,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -166,2 +166,2 @@\n-      \/\/ Should only be a vector for now, but it could also be a RegVectMask in future.\n-      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA), \"unexpected scalable reg\");\n+      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA) ||\n+             _is_predicate && (_num_regs == RegMask::SlotsPerRegVectMask), \"unexpected scalable reg\");\n@@ -198,0 +198,1 @@\n+         _is_predicate:1,       \/\/ True if in mask\/predicate registers\n","filename":"src\/hotspot\/share\/opto\/chaitin.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -417,0 +417,1 @@\n+macro(LoadVectorGatherMasked)\n@@ -419,0 +420,1 @@\n+macro(StoreVectorScatterMasked)\n@@ -427,0 +429,1 @@\n+macro(VectorMaskToLong)\n@@ -477,0 +480,4 @@\n+macro(MaskAll)\n+macro(AndVMask)\n+macro(OrVMask)\n+macro(XorVMask)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2145,1 +2145,2 @@\n-    for_igvn()->clear();\n+    Unique_Node_List* old_worklist = for_igvn();\n+    old_worklist->clear();\n@@ -2155,1 +2156,1 @@\n-    set_for_igvn(save_for_igvn);\n+    set_for_igvn(old_worklist); \/\/ new_worklist is dead beyond this point\n@@ -2369,0 +2370,1 @@\n+         n->req() == 2 &&\n@@ -2376,1 +2378,1 @@\n-      return true;\n+      return n->req() == 2;\n@@ -3435,0 +3437,2 @@\n+  case Op_LoadVectorGatherMasked:\n+  case Op_StoreVectorScatterMasked:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -705,1 +705,0 @@\n-        case Op_StoreVectorScatter:\n@@ -707,0 +706,2 @@\n+        case Op_StoreVectorScatter:\n+        case Op_StoreVectorScatterMasked:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -656,0 +656,2 @@\n+  case vmIntrinsics::_VectorLoadMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store*\/false);\n@@ -658,0 +660,2 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store=*\/true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -322,0 +322,1 @@\n+  bool inline_vector_mem_masked_operation(bool is_store);\n@@ -335,4 +336,5 @@\n-    VecMaskUseLoad,\n-    VecMaskUseStore,\n-    VecMaskUseAll,\n-    VecMaskNotUsed\n+    VecMaskUseLoad  = 1 << 0,\n+    VecMaskUseStore = 1 << 1,\n+    VecMaskUseAll   = VecMaskUseLoad | VecMaskUseStore,\n+    VecMaskUsePred  = 1 << 2,\n+    VecMaskNotUsed  = 1 << 3\n@@ -342,1 +344,1 @@\n-  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args = false);\n+  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, VectorMaskUseType mask_use_type, bool has_scalar_args = false);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -437,0 +437,18 @@\n+const int Matcher::scalable_predicate_reg_slots() {\n+  assert(Matcher::has_predicated_vectors() && Matcher::supports_scalable_vector(),\n+        \"scalable predicate vector should be supported\");\n+  int vector_reg_bit_size = Matcher::scalable_vector_reg_size(T_BYTE) << LogBitsPerByte;\n+  \/\/ We assume each predicate register is one-eighth of the size of\n+  \/\/ scalable vector register, one mask bit per vector byte.\n+  int predicate_reg_bit_size = vector_reg_bit_size >> 3;\n+  \/\/ Compute number of slots which is required when scalable predicate\n+  \/\/ register is spilled. E.g. if scalable vector register is 640 bits,\n+  \/\/ predicate register is 80 bits, which is 2.5 * slots.\n+  \/\/ We will round up the slot number to power of 2, which is required\n+  \/\/ by find_first_set().\n+  int slots = predicate_reg_bit_size & (BitsPerInt - 1)\n+              ? (predicate_reg_bit_size >> LogBitsPerInt) + 1\n+              : predicate_reg_bit_size >> LogBitsPerInt;\n+  return round_up_power_of_2(slots);\n+}\n+\n@@ -545,0 +563,2 @@\n+  } else {\n+    *idealreg2spillmask[Op_RegVectMask] = RegMask::Empty;\n@@ -617,0 +637,13 @@\n+    \/\/ Exclude last input arg stack slots to avoid spilling vector register there,\n+    \/\/ otherwise RegVectMask spills could stomp over stack slots in caller frame.\n+    for (; (in >= init_in) && (k < scalable_predicate_reg_slots()); k++) {\n+      scalable_stack_mask.Remove(in);\n+      in = OptoReg::add(in, -1);\n+    }\n+\n+    \/\/ For RegVectMask\n+    scalable_stack_mask.clear_to_sets(scalable_predicate_reg_slots());\n+    assert(scalable_stack_mask.is_AllStack(), \"should be infinite stack\");\n+    *idealreg2spillmask[Op_RegVectMask] = *idealreg2regmask[Op_RegVectMask];\n+    idealreg2spillmask[Op_RegVectMask]->OR(scalable_stack_mask);\n+\n@@ -2231,0 +2264,1 @@\n+    case Op_VectorLoadMask:\n@@ -2276,0 +2310,15 @@\n+  if (n->is_predicated_vector()) {\n+    \/\/ Restructure into binary trees for Matching.\n+    if (n->req() == 4) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, n->in(3));\n+      n->del_req(3);\n+    } else if (n->req() == 5) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, new BinaryNode(n->in(3), n->in(4)));\n+      n->del_req(4);\n+      n->del_req(3);\n+    }\n+    return;\n+  }\n+\n@@ -2415,0 +2464,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -2421,0 +2471,9 @@\n+    case Op_StoreVectorScatterMasked: {\n+      Node* pair = new BinaryNode(n->in(MemNode::ValueIn+1), n->in(MemNode::ValueIn+2));\n+      n->set_req(MemNode::ValueIn+1, pair);\n+      n->del_req(MemNode::ValueIn+2);\n+      pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+      n->set_req(MemNode::ValueIn, pair);\n+      n->del_req(MemNode::ValueIn+1);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -332,0 +332,2 @@\n+  static const bool match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt);\n+\n@@ -348,0 +350,2 @@\n+  \/\/ Actual max scalable predicate register length.\n+  static const int scalable_predicate_reg_slots();\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1138,1 +1138,1 @@\n-      if (store_Opcode() == Op_StoreVector) {\n+      if (st->is_StoreVector()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+class VectorUnboxNode;\n@@ -176,0 +177,1 @@\n+class VectorReinterpretNode;\n@@ -710,0 +712,2 @@\n+        DEFINE_CLASS_ID(VectorUnbox, Vector, 1)\n+        DEFINE_CLASS_ID(VectorReinterpret, Vector, 2)\n@@ -781,1 +785,2 @@\n-    Flag_for_post_loop_opts_igvn     = 1 << 15,\n+    Flag_is_predicated_vector        = 1 << 15,\n+    Flag_for_post_loop_opts_igvn     = 1 << 16,\n@@ -936,0 +941,3 @@\n+  DEFINE_CLASS_QUERY(VectorMaskCmp)\n+  DEFINE_CLASS_QUERY(VectorUnbox)\n+  DEFINE_CLASS_QUERY(VectorReinterpret);\n@@ -940,1 +948,0 @@\n-  DEFINE_CLASS_QUERY(VectorMaskCmp)\n@@ -991,0 +998,2 @@\n+  bool is_predicated_vector() const { return (_flags & Flag_is_predicated_vector) != 0; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -312,1 +312,1 @@\n-      assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      assert(val->ideal_reg() == Op_VecA || val->ideal_reg() == Op_RegVectMask, \"scalable register\");\n@@ -316,1 +316,1 @@\n-        n_regs = RegMask::SlotsPerVecA;\n+        n_regs = lrgs(val_idx)._is_predicate ? RegMask::SlotsPerRegVectMask : RegMask::SlotsPerVecA;\n@@ -321,2 +321,1 @@\n-      if (lrgs(val_idx).is_scalable()) {\n-        assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      if (lrgs(val_idx).is_scalable() && val->ideal_reg() == Op_VecA) {\n","filename":"src\/hotspot\/share\/opto\/postaloc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -240,1 +240,1 @@\n-  if (lrg.is_scalable()) {\n+  if (lrg.is_scalable() && lrg._is_vector) {\n","filename":"src\/hotspot\/share\/opto\/regmask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1000,1 +1000,0 @@\n-\n@@ -2362,1 +2361,4 @@\n-const TypeVect* TypeVect::make(const Type *elem, uint length) {\n+const TypeVect* TypeVect::make(const Type *elem, uint length, bool is_mask) {\n+  if (is_mask) {\n+    return makemask(elem, length);\n+  }\n@@ -2388,1 +2390,3 @@\n-  if (Matcher::has_predicated_vectors()) {\n+  BasicType elem_bt = elem->array_element_basic_type();\n+  if (Matcher::has_predicated_vectors() &&\n+      Matcher::match_rule_supported_vector_masked(Op_VectorLoadMask, length, elem_bt)) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -807,1 +807,1 @@\n-  static const TypeVect *make(const BasicType elem_bt, uint length) {\n+  static const TypeVect *make(const BasicType elem_bt, uint length, bool is_mask = false) {\n@@ -809,1 +809,1 @@\n-    return make(get_const_basic_type(elem_bt), length);\n+    return make(get_const_basic_type(elem_bt), length, is_mask);\n@@ -812,1 +812,1 @@\n-  static const TypeVect *make(const Type* elem, uint length);\n+  static const TypeVect *make(const Type* elem, uint length, bool is_mask = false);\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -341,1 +341,4 @@\n-  if (is_mask && bt != T_BOOLEAN) {\n+  \/\/ If boxed mask value is present in a predicate register, it must be\n+  \/\/ spilled to a vector though a VectorStoreMaskOperation before actual StoreVector\n+  \/\/ operation to vector payload field.\n+  if (is_mask && (value->bottom_type()->isa_vectmask() || bt != T_BOOLEAN)) {\n@@ -457,1 +460,1 @@\n-      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::makemask(masktype, num_elem)));\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -62,40 +62,78 @@\n-bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args) {\n-    bool is_supported = true;\n-    \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n-    \/\/ since in this case shift needs to be broadcasted.\n-    if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n-         (has_scalar_args &&\n-           !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n-      is_supported = false;\n-    }\n-\n-    int lshiftopc, rshiftopc;\n-    switch(elem_bt) {\n-      case T_BYTE:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftB;\n-        break;\n-      case T_SHORT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftS;\n-        break;\n-      case T_INT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftI;\n-        break;\n-      case T_LONG:\n-        lshiftopc = Op_LShiftL;\n-        rshiftopc = Op_URShiftL;\n-        break;\n-      default:\n-        assert(false, \"Unexpected type\");\n-    }\n-    int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n-    int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n-    if (!is_supported &&\n-        arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n-      is_supported = true;\n-    }\n-    return is_supported;\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt,\n+                                                 VectorMaskUseType mask_use_type, bool has_scalar_args) {\n+  bool is_supported = true;\n+\n+  \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n+  \/\/ since in this case shift needs to be broadcasted.\n+  if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n+       (has_scalar_args &&\n+         !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+    is_supported = false;\n+  }\n+\n+  if (is_supported) {\n+    \/\/ Check whether mask unboxing is supported.\n+    if ((mask_use_type & VecMaskUseLoad) != 0) {\n+      if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** Rejected vector mask loading (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[Op_VectorLoadMask], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+\n+    if ((mask_use_type & VecMaskUsePred) != 0) {\n+      if (!Matcher::has_predicated_vectors() ||\n+          !Matcher::match_rule_supported_vector_masked(opc, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[opc], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+  }\n+\n+  int lshiftopc, rshiftopc;\n+  switch(elem_bt) {\n+    case T_BYTE:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftB;\n+      break;\n+    case T_SHORT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftS;\n+      break;\n+    case T_INT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftI;\n+      break;\n+    case T_LONG:\n+      lshiftopc = Op_LShiftL;\n+      rshiftopc = Op_URShiftL;\n+      break;\n+    default:\n+      assert(false, \"Unexpected type\");\n+  }\n+  int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n+  int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n+  if (!is_supported &&\n+      arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n+    is_supported = true;\n+  }\n+  return is_supported;\n@@ -118,1 +156,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -133,1 +171,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -158,1 +196,1 @@\n-    if(!arch_supports_vector_rotate(sopc, num_elem, type, has_scalar_args)) {\n+    if(!arch_supports_vector_rotate(sopc, num_elem, type, mask_use_type, has_scalar_args)) {\n@@ -216,1 +254,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseLoad) {\n+  if ((mask_use_type & VecMaskUseLoad) != 0) {\n@@ -229,1 +267,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseStore) {\n+  if ((mask_use_type & VecMaskUseStore) != 0) {\n@@ -241,6 +279,12 @@\n-  return true;\n-}\n-\n-static bool is_vector_mask(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n-}\n+  if ((mask_use_type & VecMaskUsePred) != 0) {\n+    if (!Matcher::has_predicated_vectors() ||\n+        !Matcher::match_rule_supported_vector_masked(sopc, num_elem, type)) {\n+    #ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                      NodeClassNames[sopc], type2name(type), num_elem);\n+      }\n+    #endif\n+      return false;\n+    }\n+  }\n@@ -248,2 +292,1 @@\n-static bool is_vector_shuffle(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+  return true;\n@@ -262,4 +305,6 @@\n-\/\/ <VM>\n-\/\/ VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/            VM vm,\n-\/\/            Function<VM, VM> defaultImpl) {\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V unaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<E> elementType,\n+\/\/           int length, V v, M m,\n+\/\/           UnaryOperation<V, M> defaultImpl)\n@@ -268,4 +313,6 @@\n-\/\/ <VM>\n-\/\/ VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/             VM vm1, VM vm2,\n-\/\/             BiFunction<VM, VM, VM> defaultImpl) {\n+\/\/ <V,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V binaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<E> elementType,\n+\/\/            int length, V v1, V v2, M m,\n+\/\/            BinaryOperation<V, M> defaultImpl)\n@@ -274,4 +321,6 @@\n-\/\/ <VM>\n-\/\/ VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/              VM vm1, VM vm2, VM vm3,\n-\/\/              TernaryOperation<VM> defaultImpl) {\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V ternaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<E> elementType,\n+\/\/             int length, V v1, V v2, V v3, M m,\n+\/\/             TernaryOperation<V, M> defaultImpl)\n@@ -282,2 +331,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -291,2 +341,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -296,0 +346,1 @@\n+\n@@ -309,0 +360,28 @@\n+\n+  \/\/ \"argument(n + 5)\" should be the mask object. We assume it is \"null\" when no mask\n+  \/\/ is used to control this operation.\n+  const Type* vmask_type = gvn().type(argument(n + 5));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -331,0 +410,4 @@\n+  if (is_vector_mask(vbox_klass)) {\n+    assert(!is_masked_op, \"mask operations do not need mask to control\");\n+  }\n+\n@@ -353,3 +436,4 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if ((sopc != 0) &&\n-      !arch_supports_vector(sopc, num_elem, elem_bt, is_vector_mask(vbox_klass) ? VecMaskUseAll : VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  VectorMaskUseType mask_use_type = is_vector_mask(vbox_klass) ? VecMaskUseAll\n+                                      : is_masked_op ? VecMaskUseLoad : VecMaskNotUsed;\n+  if ((sopc != 0) && !arch_supports_vector(sopc, num_elem, elem_bt, mask_use_type)) {\n@@ -357,1 +441,1 @@\n-      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d is_masked_op=%d\",\n@@ -359,1 +443,1 @@\n-                    is_vector_mask(vbox_klass) ? 1 : 0);\n+                    is_vector_mask(vbox_klass) ? 1 : 0, is_masked_op ? 1 : 0);\n@@ -364,0 +448,10 @@\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && sopc != 0 && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=0 is_masked_op=1\",\n+                    n, sopc, num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -367,1 +461,1 @@\n-      opd3 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      opd3 = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n@@ -371,1 +465,1 @@\n-                        NodeClassNames[argument(6)->Opcode()]);\n+                        NodeClassNames[argument(7)->Opcode()]);\n@@ -378,1 +472,1 @@\n-      opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      opd2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n@@ -382,1 +476,1 @@\n-                        NodeClassNames[argument(5)->Opcode()]);\n+                        NodeClassNames[argument(6)->Opcode()]);\n@@ -389,1 +483,1 @@\n-      opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+      opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -393,1 +487,1 @@\n-                        NodeClassNames[argument(4)->Opcode()]);\n+                        NodeClassNames[argument(5)->Opcode()]);\n@@ -402,0 +496,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(n + 5), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(n + 5)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -416,1 +525,1 @@\n-    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_klass));\n@@ -420,1 +529,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, vt, is_vector_mask(vbox_klass));\n@@ -424,1 +533,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, opd3, vt);\n@@ -430,0 +539,12 @@\n+\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n+\n@@ -438,1 +559,1 @@\n-\/\/  Sh ShuffleIota(Class<?> E, Class<?> ShuffleClass, Vector.Species<E> s, int length,\n+\/\/  Sh ShuffleIota(Class<?> E, Class<?> shuffleClass, Vector.Species<E> s, int length,\n@@ -512,1 +633,1 @@\n-    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(1));\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::ge));\n@@ -515,1 +636,2 @@\n-    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vt));\n+    const TypeVect* vmask_type = TypeVect::makemask(elem_bt, num_elem);\n+    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vmask_type));\n@@ -534,1 +656,1 @@\n-\/\/ int maskReductionCoerced(int oper, Class<? extends M> maskClass, Class<?> elemClass,\n+\/\/ long maskReductionCoerced(int oper, Class<? extends M> maskClass, Class<?> elemClass,\n@@ -579,2 +701,8 @@\n-  Node* store_mask = gvn().transform(VectorStoreMaskNode::make(gvn(), mask_vec, elem_bt, num_elem));\n-  Node* maskoper = gvn().transform(VectorMaskOpNode::make(store_mask, TypeInt::INT, mopc));\n+  if (mask_vec->bottom_type()->isa_vectmask() == NULL) {\n+    mask_vec = gvn().transform(VectorStoreMaskNode::make(gvn(), mask_vec, elem_bt, num_elem));\n+  }\n+  const Type* maskoper_ty = mopc == Op_VectorMaskToLong ? (const Type*)TypeLong::LONG : (const Type*)TypeInt::INT;\n+  Node* maskoper = gvn().transform(VectorMaskOpNode::make(mask_vec, maskoper_ty, mopc));\n+  if (mopc != Op_VectorMaskToLong) {\n+    maskoper = ConvI2L(maskoper);\n+  }\n@@ -587,3 +715,7 @@\n-\/\/ <VM ,Sh extends VectorShuffle<E>, E>\n-\/\/ VM shuffleToVector(Class<VM> VecClass, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n-\/\/                    ShuffleToVectorOperation<VM,Sh,E> defaultImpl)\n+\/\/ public static\n+\/\/ <V,\n+\/\/  Sh extends VectorShuffle<E>,\n+\/\/  E>\n+\/\/ V shuffleToVector(Class<? extends Vector<E>> vclass, Class<E> elementType,\n+\/\/                   Class<? extends Sh> shuffleClass, Sh s, int length,\n+\/\/                   ShuffleToVectorOperation<V, Sh, E> defaultImpl)\n@@ -648,4 +780,7 @@\n-\/\/ <V extends Vector<?,?>>\n-\/\/ V broadcastCoerced(Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/                    long bits,\n-\/\/                    LongFunction<V> defaultImpl)\n+\/\/ public static\n+\/\/ <M,\n+\/\/  S extends VectorSpecies<E>,\n+\/\/  E>\n+\/\/ M broadcastCoerced(Class<? extends M> vmClass, Class<E> elementType, int length,\n+\/\/                    long bits, S s,\n+\/\/                    BroadcastOperation<M, E, S> defaultImpl)\n@@ -698,1 +833,0 @@\n-\n@@ -725,1 +859,1 @@\n-  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n+  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt), is_vector_mask(vbox_klass));\n@@ -750,6 +884,9 @@\n-\/\/    <C, V extends Vector<?,?>>\n-\/\/    V load(Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/           Object base, long offset,\n-\/\/           \/* Vector.Mask<E,S> m*\/\n-\/\/           Object container, int index,\n-\/\/           LoadOperation<C, VM> defaultImpl) {\n+\/\/ public static\n+\/\/ <C,\n+\/\/  VM,\n+\/\/  E,\n+\/\/  S extends VectorSpecies<E>>\n+\/\/ VM load(Class<? extends VM> vmClass, Class<E> elementType, int length,\n+\/\/         Object base, long offset,    \/\/ Unsafe addressing\n+\/\/         C container, int index, S s,     \/\/ Arguments for default implementation\n+\/\/         LoadOperation<C, VM, E, S> defaultImpl)\n@@ -757,6 +894,8 @@\n-\/\/    <C, V extends Vector<?,?>>\n-\/\/    void store(Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/               Object base, long offset,\n-\/\/               V v, \/*Vector.Mask<E,S> m*\/\n-\/\/               Object container, int index,\n-\/\/               StoreVectorOperation<C, V> defaultImpl) {\n+\/\/ public static\n+\/\/ <C,\n+\/\/  V extends Vector<?>>\n+\/\/ void store(Class<?> vectorClass, Class<?> elementType, int length,\n+\/\/            Object base, long offset,    \/\/ Unsafe addressing\n+\/\/            V v,\n+\/\/            C container, int index,      \/\/ Arguments for default implementation\n+\/\/            StoreVectorOperation<C, V> defaultImpl)\n@@ -817,2 +956,4 @@\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+\n+  \/\/ The memory barrier checks are based on ones for unsafe access.\n+  \/\/ This is not 1-1 implementation.\n+  const Type *const base_type = gvn().type(base);\n@@ -823,0 +964,9 @@\n+  const bool in_native = TypePtr::NULL_PTR == base_type; \/\/ base always null\n+  const bool in_heap   = !TypePtr::NULL_PTR->higher_equal(base_type); \/\/ base never null\n+\n+  const bool is_mixed_access = !in_heap && !in_native;\n+\n+  const bool is_mismatched_access = in_heap && (addr_type->isa_aryptr() == NULL);\n+\n+  const bool needs_cpu_membar = is_mixed_access || is_mismatched_access;\n+\n@@ -880,1 +1030,1 @@\n-  if (can_access_non_heap) {\n+  if (needs_cpu_membar) {\n@@ -915,2 +1065,1 @@\n-        const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n-        vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n+        vload = gvn().transform(new VectorLoadMaskNode(vload, TypeVect::makemask(elem_bt, num_elem)));\n@@ -927,0 +1076,237 @@\n+  if (needs_cpu_membar) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static\n+\/\/ <C,\n+\/\/  V extends Vector<?>,\n+\/\/  E,\n+\/\/  S extends VectorSpecies<E>,\n+\/\/  M extends VectorMask<E>>\n+\/\/ V loadMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/              int length, Object base, long offset, M m,\n+\/\/              C container, int index, S s,  \/\/ Arguments for default implementation\n+\/\/              LoadVectorMaskedOperation<C, V, S, M> defaultImpl) {\n+\/\/\n+\/\/ public static\n+\/\/ <C,\n+\/\/  V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ void storeMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/                  int length, Object base, long offset,\n+\/\/                  V v, M m,\n+\/\/                  C container, int index,  \/\/ Arguments for default implementation\n+\/\/                  StoreVectorMaskedOperation<C, V, M, E> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_mem_masked_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+\n+  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+\n+  \/\/ Save state and restore on bailout\n+  uint old_sp = sp();\n+  SafePointNode* old_map = clone_map();\n+\n+  Node* addr = make_unsafe_address(base, offset, elem_bt, true);\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+\n+  \/\/ Now handle special case where load\/store happens from\/to byte array but element type is not byte.\n+  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  \/\/ If there is no consistency between array and vector element types, it must be special byte array case\n+  if (arr_type != NULL && !using_byte_array && !elem_consistent_with_arr(elem_bt, arr_type)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s atype=%s\",\n+                    is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                    num_elem, type2name(elem_bt), type2name(arr_type->elem()->array_element_basic_type()));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  int mem_num_elem = using_byte_array ? num_elem * type2aelembytes(elem_bt) : num_elem;\n+  BasicType mem_elem_bt = using_byte_array ? T_BYTE : elem_bt;\n+  bool use_predicate = arch_supports_vector(is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked,\n+                                            mem_num_elem, mem_elem_bt,\n+                                            (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred));\n+  \/\/ Masked vector store operation needs the architecture predicate feature. We need to check\n+  \/\/ whether the predicated vector operation is supported by backend.\n+  if (is_store && !use_predicate) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=storeMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ This only happens for masked vector load. If predicate is not supported, then check whether\n+  \/\/ the normal vector load and blend operations are supported by backend.\n+  if (!use_predicate && (!arch_supports_vector(Op_LoadVector, mem_num_elem, mem_elem_bt, VecMaskNotUsed) ||\n+      !arch_supports_vector(Op_VectorBlend, mem_num_elem, mem_elem_bt, VecMaskUseLoad))) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=loadMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Since we are using byte array, we need to double check that the vector reinterpret operation\n+  \/\/ with byte type is supported by backend.\n+  if (using_byte_array) {\n+    if (!arch_supports_vector(Op_VectorReinterpret, mem_num_elem, T_BYTE, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s using_byte_array=1\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Since it needs to unbox the mask, we need to double check that the related load operations\n+  \/\/ for mask are supported by backend.\n+  if (!arch_supports_vector(Op_LoadVector, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(!is_vector_mask(vbox_klass) && is_vector_mask(mbox_klass), \"Invalid class type\");\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* mask = unbox_vector(is_store ? argument(8) : argument(7), mbox_type, elem_bt, num_elem);\n+  if (mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_store ? NodeClassNames[argument(8)->Opcode()]\n+                             : NodeClassNames[argument(7)->Opcode()]);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false; \/\/ operand unboxing failed\n+    }\n+    set_all_memory(reset_memory());\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the incoming vector to byte vector.\n+      const TypeVect* to_vect_type = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      val = gvn().transform(new VectorReinterpretNode(val, val->bottom_type()->is_vect(), to_vect_type));\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+    Node* vstore = gvn().transform(new StoreVectorMaskedNode(control(), memory(addr), addr, val, addr_type, mask));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    Node* vload = NULL;\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+\n+    if (use_predicate) {\n+      \/\/ Generate masked load vector node if predicate feature is supported.\n+      const TypeVect* vt = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      vload = gvn().transform(new LoadVectorMaskedNode(control(), memory(addr), addr, addr_type, vt, mask));\n+    } else {\n+      \/\/ Use the vector blend to implement the masked load vector. The biased elements are zeros.\n+      Node* zero = gvn().transform(gvn().zerocon(mem_elem_bt));\n+      zero = gvn().transform(VectorNode::scalar2vector(zero, mem_num_elem, Type::get_const_basic_type(mem_elem_bt)));\n+      vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, mem_num_elem, mem_elem_bt));\n+      vload = gvn().transform(new VectorBlendNode(zero, vload, mask));\n+    }\n+\n+    if (using_byte_array) {\n+      const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+      vload = gvn().transform(new VectorReinterpretNode(vload, vload->bottom_type()->is_vect(), to_vect_type));\n+    }\n+\n+    Node* box = box_vector(vload, vbox_type, elem_bt, num_elem);\n+    set_result(box);\n+  }\n+\n+  old_map->destruct(&_gvn);\n+\n@@ -935,6 +1321,12 @@\n-\/\/   <C, V extends Vector<?>, W extends IntVector, E, S extends VectorSpecies<E>>\n-\/\/   void loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n-\/\/                    Object base, long offset, \/\/ Unsafe addressing\n-\/\/                    W index_vector,\n-\/\/                    C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n-\/\/                    LoadVectorOperationWithMap<C, V, E, S> defaultImpl)\n+\/\/ <C,\n+\/\/  V extends Vector<?>,\n+\/\/  W extends Vector<Integer>,\n+\/\/  S extends VectorSpecies<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V loadWithMap(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType, int length,\n+\/\/               Class<? extends Vector<Integer>> vectorIndexClass,\n+\/\/               Object base, long offset, \/\/ Unsafe addressing\n+\/\/               W index_vector, M m,\n+\/\/               C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+\/\/               LoadVectorOperationWithMap<C, V, E, S, M> defaultImpl)\n@@ -942,6 +1334,10 @@\n-\/\/    <C, V extends Vector<?>, W extends IntVector>\n-\/\/    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-\/\/                      Object base, long offset,    \/\/ Unsafe addressing\n-\/\/                      W index_vector, V v,\n-\/\/                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n-\/\/                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+\/\/  <C,\n+\/\/   V extends Vector<E>,\n+\/\/   W extends Vector<Integer>,\n+\/\/   M extends VectorMask<E>,\n+\/\/   E>\n+\/\/  void storeWithMap(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/                    int length, Class<? extends Vector<Integer>> vectorIndexClass, Object base, long offset,    \/\/ Unsafe addressing\n+\/\/                    W index_vector, V v, M m,\n+\/\/                    C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n+\/\/                    StoreVectorOperationWithMap<C, V, M, E> defaultImpl)\n@@ -951,3 +1347,4 @@\n-  const TypeInstPtr* elem_klass       = gvn().type(argument(1))->isa_instptr();\n-  const TypeInt*     vlen             = gvn().type(argument(2))->isa_int();\n-  const TypeInstPtr* vector_idx_klass = gvn().type(argument(3))->isa_instptr();\n+  const TypeInstPtr* mask_klass       = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass       = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen             = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* vector_idx_klass = gvn().type(argument(4))->isa_instptr();\n@@ -960,1 +1357,0 @@\n-                    NodeClassNames[argument(1)->Opcode()],\n@@ -962,1 +1358,2 @@\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -973,0 +1370,1 @@\n+\n@@ -980,0 +1378,1 @@\n+\n@@ -983,5 +1382,43 @@\n-  if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=no\",\n-                    is_scatter, is_scatter ? \"scatter\" : \"gather\",\n-                    num_elem, type2name(elem_bt));\n+  const Type* vmask_type = gvn().type(is_scatter ? argument(10) : argument(9));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(1)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Check whether the predicated gather\/scatter node is supported by architecture.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n+                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=1\",\n+                      is_scatter, is_scatter ? \"scatterMasked\" : \"gatherMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+  } else {\n+    \/\/ Check whether the normal gather\/scatter node is supported for non-masked operation.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=0\",\n+                      is_scatter, is_scatter ? \"scatter\" : \"gather\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -989,1 +1426,0 @@\n-    return false; \/\/ not supported\n@@ -995,1 +1431,1 @@\n-        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int ismask=no\",\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int is_masked_op=%d\",\n@@ -997,1 +1433,1 @@\n-                      num_elem);\n+                      num_elem, is_masked_op ? 1 : 0);\n@@ -1000,1 +1436,1 @@\n-    }\n+  }\n@@ -1002,2 +1438,2 @@\n-  Node* base = argument(4);\n-  Node* offset = ConvL2X(argument(5));\n+  Node* base = argument(5);\n+  Node* offset = ConvL2X(argument(6));\n@@ -1025,0 +1461,1 @@\n+\n@@ -1027,1 +1464,0 @@\n-\n@@ -1029,1 +1465,0 @@\n-\n@@ -1037,2 +1472,1 @@\n-\n-  Node* index_vect = unbox_vector(argument(7), vbox_idx_type, T_INT, num_elem);\n+  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n@@ -1044,0 +1478,18 @@\n+\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(is_scatter ? argument(10) : argument(9), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_scatter ? NodeClassNames[argument(10)->Opcode()]\n+                               : NodeClassNames[argument(9)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n@@ -1046,1 +1498,1 @@\n-    Node* val = unbox_vector(argument(8), vbox_type, elem_bt, num_elem);\n+    Node* val = unbox_vector(argument(9), vbox_type, elem_bt, num_elem);\n@@ -1054,1 +1506,6 @@\n-    Node* vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    Node* vstore = NULL;\n+    if (mask != NULL) {\n+      vstore = gvn().transform(new StoreVectorScatterMaskedNode(control(), memory(addr), addr, addr_type, val, index_vect, mask));\n+    } else {\n+      vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    }\n@@ -1057,2 +1514,6 @@\n-    Node* vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n-\n+    Node* vload = NULL;\n+    if (mask != NULL) {\n+      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+    } else {\n+      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+    }\n@@ -1069,5 +1530,7 @@\n-\/\/ <V extends Vector<?,?>>\n-\/\/ long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/                       V v,\n-\/\/                       Function<V,Long> defaultImpl)\n-\n+\/\/ public static\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ long reductionCoerced(int oprId, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                       Class<E> elementType, int length, V v, M m,\n+\/\/                       ReductionOperation<V, M> defaultImpl)\n@@ -1077,2 +1540,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1086,2 +1550,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1104,0 +1568,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(6));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1106,1 +1596,0 @@\n-\n@@ -1110,2 +1599,12 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, is_masked_op ? VecMaskUseLoad : VecMaskNotUsed)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=%d\",\n+                    sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n@@ -1113,1 +1612,1 @@\n-      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s ismask=no\",\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=1\",\n@@ -1122,1 +1621,1 @@\n-  Node* opd = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* opd = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1127,0 +1626,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(6), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(6)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1128,1 +1642,16 @@\n-  Node* rn = gvn().transform(ReductionNode::make(opc, NULL, init, opd, elem_bt));\n+  Node* value = NULL;\n+  if (mask == NULL) {\n+    assert(!is_masked_op, \"Masked op needs the mask value never null\");\n+    value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+  } else {\n+    if (use_predicate) {\n+      value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+      value->add_req(mask);\n+      value->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      Node* reduce_identity = gvn().transform(VectorNode::scalar2vector(init, num_elem, Type::get_const_basic_type(elem_bt)));\n+      value = gvn().transform(new VectorBlendNode(reduce_identity, opd, mask));\n+      value = ReductionNode::make(opc, NULL, init, value, elem_bt);\n+    }\n+  }\n+  value = gvn().transform(value);\n@@ -1135,1 +1664,1 @@\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      bits = gvn().transform(new ConvI2LNode(value));\n@@ -1139,2 +1668,2 @@\n-      rn   = gvn().transform(new MoveF2INode(rn));\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      value = gvn().transform(new MoveF2INode(value));\n+      bits  = gvn().transform(new ConvI2LNode(value));\n@@ -1144,1 +1673,1 @@\n-      bits = gvn().transform(new MoveD2LNode(rn));\n+      bits = gvn().transform(new MoveD2LNode(value));\n@@ -1148,1 +1677,1 @@\n-      bits = rn; \/\/ no conversion needed\n+      bits = value; \/\/ no conversion needed\n@@ -1160,1 +1689,1 @@\n-\/\/                                BiFunction<V, V, Boolean> defaultImpl) {\n+\/\/                                BiFunction<V, V, Boolean> defaultImpl)\n@@ -1221,2 +1750,4 @@\n-\/\/ <V extends Vector, M extends Mask>\n-\/\/ V blend(Class<V> vectorClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V blend(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType, int vlen,\n@@ -1224,2 +1755,1 @@\n-\/\/         VectorBlendOp<V,M> defaultImpl) { ...\n-\/\/\n+\/\/         VectorBlendOp<V, M, E> defaultImpl)\n@@ -1292,7 +1822,7 @@\n-\/\/  public static <V extends Vector<E,S>,\n-\/\/          M extends Vector.Mask<E,S>,\n-\/\/          S extends Vector.Shape, E>\n-\/\/  M compare(int cond, Class<V> vectorClass, Class<M> maskClass, Class<?> elementType, int vlen,\n-\/\/            V v1, V v2,\n-\/\/            VectorCompareOp<V,M> defaultImpl) { ...\n-\/\/\n+\/\/  public static\n+\/\/  <V extends Vector<E>,\n+\/\/   M extends VectorMask<E>,\n+\/\/   E>\n+\/\/  M compare(int cond, Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType, int vlen,\n+\/\/            V v1, V v2, M m,\n+\/\/            VectorCompareOp<V,M> defaultImpl)\n@@ -1366,0 +1896,19 @@\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  Node* mask = is_masked_op ? unbox_vector(argument(7), mbox_type, elem_bt, num_elem) : NULL;\n+  if (is_masked_op && mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: mask = null arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  bool use_predicate = is_masked_op && arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_AndV, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -1372,2 +1921,14 @@\n-  const TypeVect* vt = TypeVect::make(mask_bt, num_elem);\n-  Node* operation = gvn().transform(new VectorMaskCmpNode(pred, v1, v2, pred_node, vt));\n+  const TypeVect* vmask_type = TypeVect::makemask(mask_bt, num_elem);\n+  Node* operation = new VectorMaskCmpNode(pred, v1, v2, pred_node, vmask_type);\n+\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = VectorNode::make(Op_AndV, operation, mask, vmask_type);\n+    }\n+  }\n+\n+  operation = gvn().transform(operation);\n@@ -1382,5 +1943,7 @@\n-\/\/ <V extends Vector, Sh extends Shuffle>\n-\/\/  V rearrangeOp(Class<V> vectorClass, Class<Sh> shuffleClass, Class< ? > elementType, int vlen,\n-\/\/    V v1, Sh sh,\n-\/\/    VectorSwizzleOp<V, Sh, S, E> defaultImpl) { ...\n-\n+\/\/ <V extends Vector<E>,\n+\/\/  Sh extends VectorShuffle<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<M> maskClass, Class<E> elementType, int vlen,\n+\/\/               V v1, Sh sh, M m,\n+\/\/               VectorRearrangeOp<V, Sh, M, E> defaultImpl)\n@@ -1390,2 +1953,3 @@\n-  const TypeInstPtr* elem_klass    = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen          = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass    = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass    = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen          = gvn().type(argument(4))->isa_int();\n@@ -1393,1 +1957,1 @@\n-  if (vector_klass == NULL || shuffle_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (vector_klass == NULL  || shuffle_klass == NULL ||  elem_klass == NULL || vlen == NULL) {\n@@ -1396,2 +1960,4 @@\n-  if (shuffle_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n-    elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (shuffle_klass->const_oop() == NULL ||\n+      vector_klass->const_oop()  == NULL ||\n+      elem_klass->const_oop()    == NULL ||\n+      !vlen->is_con()) {\n@@ -1402,2 +1968,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1407,1 +1973,2 @@\n-  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(shuffle_klass)) {\n+  if (!is_klass_initialized(vector_klass)  ||\n+      !is_klass_initialized(shuffle_klass)) {\n@@ -1431,1 +1998,7 @@\n-  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed)) {\n+\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  bool use_predicate = is_masked_op;\n+  if (is_masked_op &&\n+      (mask_klass == NULL ||\n+       mask_klass->const_oop() == NULL ||\n+       !is_klass_initialized(mask_klass))) {\n@@ -1433,2 +2006,15 @@\n-      tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n-                    num_elem, type2name(elem_bt));\n+      tty->print_cr(\"  ** mask_klass argument not initialized\");\n+    }\n+  }\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, checkFlags)) {\n+    use_predicate = false;\n+    if(!is_masked_op ||\n+       (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed) ||\n+        !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)     ||\n+        !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -1436,1 +2022,0 @@\n-    return false; \/\/ not supported\n@@ -1444,2 +2029,2 @@\n-  Node* v1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n-  Node* shuffle = unbox_vector(argument(5), shbox_type, shuffle_bt, num_elem);\n+  Node* v1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* shuffle = unbox_vector(argument(6), shbox_type, shuffle_bt, num_elem);\n@@ -1451,1 +2036,28 @@\n-  Node* rearrange = gvn().transform(new VectorRearrangeNode(v1, shuffle));\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=3 op=shuffle\/rearrange vlen=%d etype=%s ismask=useload is_masked_op=1\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* rearrange = new VectorRearrangeNode(v1, shuffle);\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      rearrange->add_req(mask);\n+      rearrange->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      const TypeVect* vt = v1->bottom_type()->is_vect();\n+      rearrange = gvn().transform(rearrange);\n+      Node* zero = gvn().makecon(Type::get_zero_type(elem_bt));\n+      Node* zerovec = gvn().transform(VectorNode::scalar2vector(zero, num_elem, Type::get_const_basic_type(elem_bt)));\n+      rearrange = new VectorBlendNode(zerovec, rearrange, mask);\n+    }\n+  }\n+  rearrange = gvn().transform(rearrange);\n@@ -1517,5 +2129,7 @@\n-\/\/  <V extends Vector<?,?>>\n-\/\/  V broadcastInt(int opr, Class<V> vectorClass, Class<?> elementType, int vlen,\n-\/\/                 V v, int i,\n-\/\/                 VectorBroadcastIntOp<V> defaultImpl) {\n-\/\/\n+\/\/  <V extends Vector<E>,\n+\/\/   M extends VectorMask<E>,\n+\/\/   E>\n+\/\/  V broadcastInt(int opr, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                 Class<E> elementType, int length,\n+\/\/                 V v, int n, M m,\n+\/\/                 VectorBroadcastIntOp<V, M> defaultImpl)\n@@ -1525,2 +2139,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1536,2 +2151,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1547,0 +2162,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(7));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1554,1 +2195,1 @@\n-  BasicType elem_bt = elem_type->basic_type();\n+\n@@ -1556,0 +2197,1 @@\n+  BasicType elem_bt = elem_type->basic_type();\n@@ -1557,0 +2199,1 @@\n+\n@@ -1559,0 +2202,1 @@\n+\n@@ -1565,0 +2209,1 @@\n+\n@@ -1572,1 +2217,2 @@\n-  Node* cnt  = argument(5);\n+\n+  Node* cnt  = argument(6);\n@@ -1581,4 +2227,15 @@\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s ismask=no\",\n-                    sopc, num_elem, type2name(elem_bt));\n+\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  bool use_predicate = is_masked_op;\n+\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, checkFlags, has_scalar_args)) {\n+    use_predicate = false;\n+    if (!is_masked_op ||\n+        (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) ||\n+         !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad))) {\n+\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s is_masked_op=%d\",\n+                      sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+      }\n+      return false; \/\/ not supported\n@@ -1586,1 +2243,0 @@\n-    return false; \/\/ not supported\n@@ -1588,1 +2244,2 @@\n-  Node* opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+\n+  Node* opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1603,0 +2260,1 @@\n+\n@@ -1606,1 +2264,0 @@\n-  Node* operation = gvn().transform(VectorNode::make(opc, opd1, opd2, num_elem, elem_bt));\n@@ -1608,0 +2265,24 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\", NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* operation = VectorNode::make(opc, opd1, opd2, num_elem, elem_bt);\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n@@ -1621,1 +2302,1 @@\n-\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl)\n@@ -1682,3 +2363,0 @@\n-  if (is_mask && (type2aelembytes(elem_bt_from) != type2aelembytes(elem_bt_to))) {\n-    return false; \/\/ elem size mismatch\n-  }\n@@ -1730,2 +2408,13 @@\n-  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from);\n-  const TypeVect* dst_type = TypeVect::make(elem_bt_to,   num_elem_to);\n+  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from, is_mask);\n+  const TypeVect* dst_type = TypeVect::make(elem_bt_to, num_elem_to, is_mask);\n+\n+  \/\/ Safety check to prevent casting if source mask is of type vector\n+  \/\/ and destination mask of type predicate vector and vice-versa.\n+  \/\/ From X86 standpoint, this case will only arise over KNL target,\n+  \/\/ where certain masks (depending on the species) are either propagated\n+  \/\/ through a vector or predicate register.\n+  if (is_mask &&\n+      ((src_type->isa_vectmask() == NULL && dst_type->isa_vectmask()) ||\n+       (dst_type->isa_vectmask() == NULL && src_type->isa_vectmask()))) {\n+    return false;\n+  }\n@@ -1735,2 +2424,6 @@\n-    assert(!is_mask, \"masks cannot be casted\");\n-    int cast_vopc = VectorCastNode::opcode(elem_bt_from);\n+    BasicType new_elem_bt_to = elem_bt_to;\n+    BasicType new_elem_bt_from = elem_bt_from;\n+    if (is_mask && is_floating_point_type(elem_bt_from)) {\n+      new_elem_bt_from = elem_bt_from == T_FLOAT ? T_INT : T_LONG;\n+    }\n+    int cast_vopc = VectorCastNode::opcode(new_elem_bt_from);\n@@ -1790,3 +2483,26 @@\n-      \/\/ Since input and output number of elements match, and since we know this vector size is\n-      \/\/ supported, simply do a cast with no resize needed.\n-      op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_to));\n+      if (is_mask) {\n+        if ((dst_type->isa_vectmask() && src_type->isa_vectmask()) ||\n+            (type2aelembytes(elem_bt_from) == type2aelembytes(elem_bt_to))) {\n+          op = gvn().transform(new VectorMaskCastNode(op, dst_type));\n+        } else {\n+          \/\/ Special handling for casting operation involving floating point types.\n+          \/\/ Case A) F -> X :=  F -> VectorMaskCast (F->I\/L [NOP]) -> VectorCast[I\/L]2X\n+          \/\/ Case B) X -> F :=  X -> VectorCastX2[I\/L] -> VectorMaskCast ([I\/L]->F [NOP])\n+          \/\/ Case C) F -> F :=  VectorMaskCast (F->I\/L [NOP]) -> VectorCast[I\/L]2[L\/I] -> VectotMaskCast (L\/I->F [NOP])\n+          if (is_floating_point_type(elem_bt_from)) {\n+            const TypeVect* new_src_type = TypeVect::make(new_elem_bt_from, num_elem_to, is_mask);\n+            op = gvn().transform(new VectorMaskCastNode(op, new_src_type));\n+          }\n+          if (is_floating_point_type(elem_bt_to)) {\n+            new_elem_bt_to = elem_bt_to == T_FLOAT ? T_INT : T_LONG;\n+          }\n+          op = gvn().transform(VectorCastNode::make(cast_vopc, op, new_elem_bt_to, num_elem_to));\n+          if (new_elem_bt_to != elem_bt_to) {\n+            op = gvn().transform(new VectorMaskCastNode(op, dst_type));\n+          }\n+        }\n+      } else {\n+        \/\/ Since input and output number of elements match, and since we know this vector size is\n+        \/\/ supported, simply do a cast with no resize needed.\n+        op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_to));\n+      }\n@@ -1807,2 +2523,3 @@\n-\/\/  <V extends Vector<?>>\n-\/\/  V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n+\/\/  <V extends Vector<E>,\n+\/\/   E>\n+\/\/  V insert(Class<? extends V> vectorClass, Class<E> elementType, int vlen,\n@@ -1810,2 +2527,1 @@\n-\/\/           VecInsertOp<V> defaultImpl) {\n-\/\/\n+\/\/           VecInsertOp<V> defaultImpl)\n@@ -1900,2 +2616,3 @@\n-\/\/  <V extends Vector<?>>\n-\/\/  long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/  <V extends Vector<E>,\n+\/\/   E>\n+\/\/  long extract(Class<? extends V> vectorClass, Class<E> elementType, int vlen,\n@@ -1903,2 +2620,1 @@\n-\/\/               VecExtractOp<V> defaultImpl) {\n-\/\/\n+\/\/               VecExtractOp<V> defaultImpl)\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":958,"deletions":242,"binary":false,"changes":1200,"status":"modified"},{"patch":"@@ -369,2 +369,2 @@\n-bool VectorNode::is_vshift_cnt(Node* n) {\n-  switch (n->Opcode()) {\n+bool VectorNode::is_vshift_cnt_opcode(int opc) {\n+  switch (opc) {\n@@ -379,0 +379,4 @@\n+bool VectorNode::is_vshift_cnt(Node* n) {\n+  return is_vshift_cnt_opcode(n->Opcode());\n+}\n+\n@@ -445,0 +449,25 @@\n+VectorNode* VectorNode::make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n+  guarantee(vopc > 0, \"vopc must be > 0\");\n+  const TypeVect* vmask_type = TypeVect::makemask(bt, vlen);\n+  switch (vopc) {\n+    case Op_AndV:\n+      if (Matcher::match_rule_supported_vector_masked(Op_AndVMask, vlen, bt)) {\n+        return new AndVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new AndVNode(n1, n2, vmask_type);\n+    case Op_OrV:\n+      if (Matcher::match_rule_supported_vector_masked(Op_OrVMask, vlen, bt)) {\n+        return new OrVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new OrVNode(n1, n2, vmask_type);\n+    case Op_XorV:\n+      if (Matcher::match_rule_supported_vector_masked(Op_XorVMask, vlen, bt)) {\n+        return new XorVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new XorVNode(n1, n2, vmask_type);\n+    default:\n+      fatal(\"Unsupported mask vector creation for '%s'\", NodeClassNames[vopc]);\n+      return NULL;\n+  }\n+}\n+\n@@ -446,1 +475,1 @@\n-VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt) {\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask) {\n@@ -449,0 +478,5 @@\n+\n+  if (is_mask) {\n+    return make_mask_node(vopc, n1, n2, vt->length(), vt->element_basic_type());\n+  }\n+\n@@ -555,1 +589,1 @@\n-VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t) {\n+VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask) {\n@@ -557,2 +591,7 @@\n-  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen)\n-                                          : TypeVect::make(bt, vlen);\n+  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen, is_mask)\n+                                          : TypeVect::make(bt, vlen, is_mask);\n+\n+  if (is_mask && Matcher::match_rule_supported_vector(Op_MaskAll, vlen, bt)) {\n+    return new MaskAllNode(s, vt);\n+  }\n+\n@@ -1009,1 +1048,1 @@\n-  if (out_bt == T_BOOLEAN) {\n+  if (!Matcher::has_predicated_vectors() && out_bt == T_BOOLEAN) {\n@@ -1012,0 +1051,1 @@\n+\n@@ -1108,0 +1148,1 @@\n+          return gvn.makecon(TypeInt::make(max_jbyte));\n@@ -1109,0 +1150,1 @@\n+          return gvn.makecon(TypeInt::make(max_jshort));\n@@ -1123,0 +1165,1 @@\n+          return gvn.makecon(TypeInt::make(min_jbyte));\n@@ -1124,0 +1167,1 @@\n+          return gvn.makecon(TypeInt::make(min_jshort));\n@@ -1316,0 +1360,1 @@\n+          const TypeVect* vmask_type = TypeVect::makemask(out_vt->element_basic_type(), out_vt->length());\n@@ -1321,1 +1366,1 @@\n-            return new VectorMaskCastNode(value, out_vt);\n+            return new VectorMaskCastNode(value, vmask_type);\n@@ -1325,1 +1370,1 @@\n-          return new VectorLoadMaskNode(value, out_vt);\n+          return new VectorLoadMaskNode(value, vmask_type);\n@@ -1383,0 +1428,2 @@\n+    case Op_VectorMaskToLong:\n+      return new VectorMaskToLongNode(mask, ty);\n@@ -1389,1 +1436,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":56,"deletions":10,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -69,1 +69,3 @@\n-  virtual uint ideal_reg() const { return Matcher::vector_ideal_reg(vect_type()->length_in_bytes()); }\n+  virtual uint ideal_reg() const {\n+    return type()->ideal_reg();\n+  }\n@@ -71,1 +73,1 @@\n-  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t);\n+  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask = false);\n@@ -74,1 +76,1 @@\n-  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt);\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask = false);\n@@ -77,0 +79,1 @@\n+  static VectorNode* make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt);\n@@ -79,0 +82,3 @@\n+\n+  static bool is_vshift_cnt_opcode(int opc);\n+\n@@ -801,2 +807,2 @@\n-                                                     idx == MemNode::ValueIn ||\n-                                                     idx == MemNode::ValueIn + 1; }\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1; }\n@@ -811,1 +817,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -831,1 +837,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -845,0 +851,39 @@\n+\/\/-------------------------------LoadVectorGatherMaskedNode---------------------------------\n+\/\/ Load Vector from memory via index map under the influence of a predicate register(mask).\n+class LoadVectorGatherMaskedNode : public LoadVectorNode {\n+ public:\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+    : LoadVectorNode(c, mem, adr, at, vt) {\n+    init_class_id(Class_LoadVector);\n+    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+    add_req(indices);\n+    add_req(mask);\n+    assert(req() == MemNode::ValueIn + 2, \"match_edge expects that last input is in MemNode::ValueIn+1\");\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                   idx == MemNode::ValueIn ||\n+                                                   idx == MemNode::ValueIn + 1; }\n+};\n+\n+\/\/------------------------------StoreVectorScatterMaskedNode--------------------------------\n+\/\/ Store Vector into memory via index map under the influence of a predicate register(mask).\n+class StoreVectorScatterMaskedNode : public StoreVectorNode {\n+  public:\n+   StoreVectorScatterMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* indices, Node* mask)\n+     : StoreVectorNode(c, mem, adr, at, val) {\n+     init_class_id(Class_StoreVector);\n+     assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+     assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+     add_req(indices);\n+     add_req(mask);\n+     assert(req() == MemNode::ValueIn + 3, \"match_edge expects that last input is in MemNode::ValueIn+2\");\n+   }\n+   virtual int Opcode() const;\n+   virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1 ||\n+                                                    idx == MemNode::ValueIn + 2; }\n+};\n@@ -859,1 +904,0 @@\n-\n@@ -881,1 +925,1 @@\n-    assert(mask->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN, \"\");\n+    assert(Matcher::has_predicated_vectors() || mask->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN, \"\");\n@@ -916,0 +960,36 @@\n+class VectorMaskToLongNode : public VectorMaskOpNode {\n+ public:\n+  VectorMaskToLongNode(Node* mask, const Type* ty):\n+    VectorMaskOpNode(mask, ty, Op_VectorMaskToLong) {}\n+  virtual int Opcode() const;\n+  virtual uint  ideal_reg() const { return Op_RegL; }\n+};\n+\n+\/\/-------------------------- Vector mask broadcast -----------------------------------\n+class MaskAllNode : public VectorNode {\n+ public:\n+  MaskAllNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical and --------------------------------\n+class AndVMaskNode : public VectorNode {\n+ public:\n+  AndVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical or ---------------------------------\n+class OrVMaskNode : public VectorNode {\n+ public:\n+  OrVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical xor --------------------------------\n+class XorVMaskNode : public VectorNode {\n+ public:\n+  XorVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -1187,1 +1267,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  virtual  uint size_of() const { return sizeof(VectorMaskCmpNode); }\n@@ -1197,0 +1277,1 @@\n+    assert((BoolTest::mask)predicate_node->get_int() == predicate, \"Unmatched predicates\");\n@@ -1308,1 +1389,0 @@\n-    assert(type2aelembytes(in_vt->element_basic_type()) == type2aelembytes(vt->element_basic_type()), \"element size must match\");\n@@ -1318,0 +1398,1 @@\n+\n@@ -1319,1 +1400,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  uint size_of() const { return sizeof(VectorReinterpretNode); }\n@@ -1322,1 +1403,6 @@\n-      : VectorNode(in, dst_vt), _src_vt(src_vt) { }\n+     : VectorNode(in, dst_vt), _src_vt(src_vt) {\n+     assert((!dst_vt->isa_vectmask() && !src_vt->isa_vectmask()) ||\n+            (type2aelembytes(src_vt->element_basic_type()) >= type2aelembytes(dst_vt->element_basic_type())),\n+            \"unsupported mask widening reinterpretation\");\n+     init_class_id(Class_VectorReinterpret);\n+  }\n@@ -1324,0 +1410,1 @@\n+  const TypeVect* src_type() { return _src_vt; }\n@@ -1456,0 +1543,1 @@\n+    init_class_id(Class_VectorUnbox);\n@@ -1485,1 +1573,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":101,"deletions":14,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -433,0 +433,12 @@\n+    case VECTOR_OP_MASK_TOLONG: {\n+      switch (bt) {\n+        case T_BYTE:  \/\/ fall-through\n+        case T_SHORT: \/\/ fall-through\n+        case T_INT:   \/\/ fall-through\n+        case T_LONG:  \/\/ fall-through\n+        case T_FLOAT: \/\/ fall-through\n+        case T_DOUBLE: return Op_VectorMaskToLong;\n+        default: fatal(\"MASK_TOLONG: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+    VECTOR_OP_MASK_TOLONG    = 22,\n@@ -87,2 +88,2 @@\n-    VECTOR_OP_LROTATE = 22,\n-    VECTOR_OP_RROTATE = 23,\n+    VECTOR_OP_LROTATE = 23,\n+    VECTOR_OP_RROTATE = 24,\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1848,0 +1848,4 @@\n+  declare_c2_type(MaskAllNode, VectorNode)                                \\\n+  declare_c2_type(AndVMaskNode, VectorNode)                               \\\n+  declare_c2_type(OrVMaskNode, VectorNode)                                \\\n+  declare_c2_type(XorVMaskNode, VectorNode)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -323,0 +323,11 @@\n+  void truncate_to(int idx) {\n+    for (int i = 0, j = idx; j < length(); i++, j++) {\n+      at_put(i, at(j));\n+    }\n+    trunc_to(length() - idx);\n+  }\n+\n+  void truncate_from(int idx) {\n+    trunc_to(idx);\n+  }\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -356,0 +356,3 @@\n+        static final long BYTE_BUFFER_IS_READ_ONLY\n+                = UNSAFE.objectFieldOffset(ByteBuffer.class, \"isReadOnly\");\n+\n@@ -376,0 +379,5 @@\n+    @ForceInline\n+    public static boolean isReadOnly(ByteBuffer bb) {\n+        return UNSAFE.getBoolean(bb, BufferAccess.BYTE_BUFFER_IS_READ_ONLY);\n+    }\n+\n@@ -382,1 +390,1 @@\n-                          VectorSupport.LoadOperation<ByteBuffer, V, E, S> defaultImpl) {\n+                          VectorSupport.LoadOperation<ByteBuffer, V, S> defaultImpl) {\n@@ -403,1 +411,1 @@\n-                          VectorSupport.LoadOperation<ByteBuffer, V, E, S> defaultImpl) {\n+                          VectorSupport.LoadOperation<ByteBuffer, V, S> defaultImpl) {\n@@ -409,0 +417,2 @@\n+            final byte[] base = (byte[]) BufferAccess.bufferBase(bb);\n+\n@@ -410,1 +420,44 @@\n-                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset),\n+                      base, BufferAccess.bufferAddress(bb, offset),\n+                      bb, offset, s,\n+                      defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n+\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                               int length, ByteBuffer bb, int offset, M m, S s,\n+                               VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, S, M> defaultImpl) {\n+        try {\n+            return loadFromByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    bb, offset, m,\n+                    s,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope, Class<? extends V> vmClass,\n+                                     Class<M> maskClass, Class<E> e, int length,\n+                                     ByteBuffer bb, int offset, M m,\n+                                     S s,\n+                                     VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, S, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            return VectorSupport.loadMasked(vmClass, maskClass, e, length,\n+                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset), m,\n@@ -451,0 +504,2 @@\n+            final byte[] base = (byte[]) BufferAccess.bufferBase(bb);\n+\n@@ -452,0 +507,43 @@\n+                                base, BufferAccess.bufferAddress(bb, offset),\n+                                v,\n+                                bb, offset,\n+                                defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n+\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                                   int length, V v, M m,\n+                                   ByteBuffer bb, int offset,\n+                                   VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            storeIntoByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    v, m,\n+                    bb, offset,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope,\n+                                         Class<? extends V> vmClass, Class<M> maskClass,\n+                                         Class<E> e, int length, V v, M m,\n+                                         ByteBuffer bb, int offset,\n+                                         VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            VectorSupport.storeMasked(vmClass, maskClass, e, length,\n@@ -453,1 +551,1 @@\n-                    v,\n+                    v, m,\n@@ -461,1 +559,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/X-ScopedMemoryAccess.java.template","additions":102,"deletions":5,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+    public static final int VECTOR_OP_MASK_TOLONG    = 22;\n@@ -74,2 +75,2 @@\n-    public static final int VECTOR_OP_LROTATE = 22;\n-    public static final int VECTOR_OP_RROTATE = 23;\n+    public static final int VECTOR_OP_LROTATE = 23;\n+    public static final int VECTOR_OP_RROTATE = 24;\n@@ -159,1 +160,2 @@\n-    public interface BroadcastOperation<VM, E, S extends VectorSpecies<E>> {\n+    public interface BroadcastOperation<VM extends VectorPayload,\n+                                        S extends VectorSpecies<?>> {\n@@ -165,4 +167,7 @@\n-    <VM, E, S extends VectorSpecies<E>>\n-    VM broadcastCoerced(Class<? extends VM> vmClass, Class<E> E, int length,\n-                                  long bits, S s,\n-                                  BroadcastOperation<VM, E, S> defaultImpl) {\n+    <VM extends VectorPayload,\n+     S extends VectorSpecies<E>,\n+     E>\n+    VM broadcastCoerced(Class<? extends VM> vmClass, Class<E> eClass,\n+                        int length,\n+                        long bits, S s,\n+                        BroadcastOperation<VM, S> defaultImpl) {\n@@ -174,2 +179,3 @@\n-    public interface ShuffleIotaOperation<E, S extends VectorSpecies<E>> {\n-        VectorShuffle<E> apply(int length, int start, int step, S s);\n+    public interface ShuffleIotaOperation<S extends VectorSpecies<?>,\n+                                          SH extends VectorShuffle<?>> {\n+        SH apply(int length, int start, int step, S s);\n@@ -180,3 +186,7 @@\n-    <E, S extends VectorSpecies<E>>\n-    VectorShuffle<E> shuffleIota(Class<?> E, Class<?> ShuffleClass, S s, int length,\n-                     int start, int step, int wrap, ShuffleIotaOperation<E, S> defaultImpl) {\n+    <E,\n+     S extends VectorSpecies<E>,\n+     SH extends VectorShuffle<E>>\n+    SH shuffleIota(Class<E> eClass, Class<? extends SH> shClass, S s,\n+                   int length,\n+                   int start, int step, int wrap,\n+                   ShuffleIotaOperation<S, SH> defaultImpl) {\n@@ -187,2 +197,3 @@\n-    public interface ShuffleToVectorOperation<VM, Sh, E> {\n-       VM apply(Sh s);\n+    public interface ShuffleToVectorOperation<V extends Vector<?>,\n+                                              SH extends VectorShuffle<?>> {\n+       V apply(SH sh);\n@@ -193,3 +204,6 @@\n-    <VM ,Sh extends VectorShuffle<E>, E>\n-    VM shuffleToVector(Class<?> VM, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n-                       ShuffleToVectorOperation<VM,Sh,E> defaultImpl) {\n+    <V extends Vector<E>,\n+     SH extends VectorShuffle<E>,\n+     E>\n+    V shuffleToVector(Class<? extends Vector<E>> vClass, Class<E> eClass, Class<? extends SH> shClass, SH sh,\n+                      int length,\n+                      ShuffleToVectorOperation<V, SH> defaultImpl) {\n@@ -197,1 +211,1 @@\n-      return defaultImpl.apply(s);\n+      return defaultImpl.apply(sh);\n@@ -201,1 +215,2 @@\n-    public interface IndexOperation<V extends Vector<E>, E, S extends VectorSpecies<E>> {\n+    public interface IndexOperation<V extends Vector<?>,\n+                                    S extends VectorSpecies<?>> {\n@@ -207,2 +222,5 @@\n-    <V extends Vector<E>, E, S extends VectorSpecies<E>>\n-    V indexVector(Class<? extends V> vClass, Class<E> E, int length,\n+    <V extends Vector<E>,\n+     E,\n+     S extends VectorSpecies<E>>\n+    V indexVector(Class<? extends V> vClass, Class<E> eClass,\n+                  int length,\n@@ -210,1 +228,1 @@\n-                  IndexOperation<V, E, S> defaultImpl) {\n+                  IndexOperation<V, S> defaultImpl) {\n@@ -217,0 +235,5 @@\n+    public interface ReductionOperation<V extends Vector<?>,\n+                                        M extends VectorMask<?>> {\n+        long apply(V v, M m);\n+    }\n+\n@@ -219,4 +242,8 @@\n-    <V extends Vector<?>>\n-    long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int length,\n-                          V v,\n-                          Function<V,Long> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    long reductionCoerced(int oprId,\n+                          Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                          int length,\n+                          V v, M m,\n+                          ReductionOperation<V, M> defaultImpl) {\n@@ -224,1 +251,1 @@\n-        return defaultImpl.apply(v);\n+        return defaultImpl.apply(v, m);\n@@ -227,0 +254,1 @@\n+\n@@ -229,2 +257,2 @@\n-    public interface VecExtractOp<V> {\n-        long apply(V v1, int idx);\n+    public interface VecExtractOp<V extends Vector<?>> {\n+        long apply(V v, int i);\n@@ -235,3 +263,5 @@\n-    <V extends Vector<?>>\n-    long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n-                 V vec, int ix,\n+    <V extends Vector<E>,\n+     E>\n+    long extract(Class<? extends V> vClass, Class<E> eClass,\n+                 int length,\n+                 V v, int i,\n@@ -240,1 +270,1 @@\n-        return defaultImpl.apply(vec, ix);\n+        return defaultImpl.apply(v, i);\n@@ -245,2 +275,2 @@\n-    public interface VecInsertOp<V> {\n-        V apply(V v1, int idx, long val);\n+    public interface VecInsertOp<V extends Vector<?>> {\n+        V apply(V v, int i, long val);\n@@ -251,3 +281,5 @@\n-    <V extends Vector<?>>\n-    V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n-             V vec, int ix, long val,\n+    <V extends Vector<E>,\n+     E>\n+    V insert(Class<? extends V> vClass, Class<E> eClass,\n+             int length,\n+             V v, int i, long val,\n@@ -256,1 +288,1 @@\n-        return defaultImpl.apply(vec, ix, val);\n+        return defaultImpl.apply(v, i, val);\n@@ -261,0 +293,5 @@\n+    public interface UnaryOperation<V extends Vector<?>,\n+                                    M extends VectorMask<?>> {\n+        V apply(V v, M m);\n+    }\n+\n@@ -263,4 +300,8 @@\n-    <VM>\n-    VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-               VM vm,\n-               Function<VM, VM> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V unaryOp(int oprId,\n+              Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+              int length,\n+              V v, M m,\n+              UnaryOperation<V, M> defaultImpl) {\n@@ -268,1 +309,1 @@\n-        return defaultImpl.apply(vm);\n+        return defaultImpl.apply(v, m);\n@@ -273,0 +314,5 @@\n+    public interface BinaryOperation<VM extends VectorPayload,\n+                                     M extends VectorMask<?>> {\n+        VM apply(VM v1, VM v2, M m);\n+    }\n+\n@@ -275,4 +321,8 @@\n-    <VM>\n-    VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                VM vm1, VM vm2,\n-                BiFunction<VM, VM, VM> defaultImpl) {\n+    <VM extends VectorPayload,\n+     M extends VectorMask<E>,\n+     E>\n+    VM binaryOp(int oprId,\n+                Class<? extends VM> vmClass, Class<? extends M> mClass, Class<E> eClass,\n+                int length,\n+                VM v1, VM v2, M m,\n+                BinaryOperation<VM, M> defaultImpl) {\n@@ -280,1 +330,1 @@\n-        return defaultImpl.apply(vm1, vm2);\n+        return defaultImpl.apply(v1, v2, m);\n@@ -285,2 +335,3 @@\n-    public interface TernaryOperation<V> {\n-        V apply(V v1, V v2, V v3);\n+    public interface TernaryOperation<V extends Vector<?>,\n+                                      M extends VectorMask<?>> {\n+        V apply(V v1, V v2, V v3, M m);\n@@ -291,4 +342,8 @@\n-    <VM>\n-    VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                 VM vm1, VM vm2, VM vm3,\n-                 TernaryOperation<VM> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V ternaryOp(int oprId,\n+                Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                int length,\n+                V v1, V v2, V v3, M m,\n+                TernaryOperation<V, M> defaultImpl) {\n@@ -296,1 +351,1 @@\n-        return defaultImpl.apply(vm1, vm2, vm3);\n+        return defaultImpl.apply(v1, v2, v3, m);\n@@ -303,2 +358,4 @@\n-    public interface LoadOperation<C, V, E, S extends VectorSpecies<E>> {\n-        V load(C container, int index, S s);\n+    public interface LoadOperation<C,\n+                                   VM extends VectorPayload,\n+                                   S extends VectorSpecies<?>> {\n+        VM load(C container, int index, S s);\n@@ -309,5 +366,9 @@\n-    <C, VM, E, S extends VectorSpecies<E>>\n-    VM load(Class<? extends VM> vmClass, Class<E> E, int length,\n-           Object base, long offset,    \/\/ Unsafe addressing\n-           C container, int index, S s,     \/\/ Arguments for default implementation\n-           LoadOperation<C, VM, E, S> defaultImpl) {\n+    <C,\n+     VM extends VectorPayload,\n+     E,\n+     S extends VectorSpecies<E>>\n+    VM load(Class<? extends VM> vmClass, Class<E> eClass,\n+            int length,\n+            Object base, long offset,\n+            C container, int index, S s,\n+            LoadOperation<C, VM, S> defaultImpl) {\n@@ -320,2 +381,5 @@\n-    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>> {\n-        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s);\n+    public interface LoadVectorMaskedOperation<C,\n+                                               V extends Vector<?>,\n+                                               S extends VectorSpecies<?>,\n+                                               M extends VectorMask<?>> {\n+        V load(C container, int index, S s, M m);\n@@ -326,3 +390,35 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>, E, S extends VectorSpecies<E>>\n-    V loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n-                  Object base, long offset, \/\/ Unsafe addressing\n+    <C,\n+     V extends Vector<?>,\n+     E,\n+     S extends VectorSpecies<E>,\n+     M extends VectorMask<E>>\n+    V loadMasked(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                 int length,\n+                 Object base, long offset,\n+                 M m, C container, int index, S s,\n+                 LoadVectorMaskedOperation<C, V, S, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.load(container, index, s, m);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface LoadVectorOperationWithMap<C,\n+                                                V extends Vector<?>,\n+                                                S extends VectorSpecies<?>,\n+                                                M extends VectorMask<?>> {\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C,\n+     V extends Vector<?>,\n+     W extends Vector<Integer>,\n+     S extends VectorSpecies<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V loadWithMap(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                  int length,\n+                  Class<? extends Vector<Integer>> vectorIndexClass,\n+                  Object base, long offset,\n@@ -330,2 +426,2 @@\n-                  C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n-                  LoadVectorOperationWithMap<C, V, E, S> defaultImpl) {\n+                  M m, C container, int index, int[] indexMap, int indexM, S s,\n+                  LoadVectorOperationWithMap<C, V, S, M> defaultImpl) {\n@@ -333,1 +429,1 @@\n-        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s);\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s, m);\n@@ -338,1 +434,2 @@\n-    public interface StoreVectorOperation<C, V extends Vector<?>> {\n+    public interface StoreVectorOperation<C,\n+                                          V extends Vector<?>> {\n@@ -344,5 +441,6 @@\n-    <C, V extends Vector<?>>\n-    void store(Class<?> vectorClass, Class<?> elementType, int length,\n-               Object base, long offset,    \/\/ Unsafe addressing\n-               V v,\n-               C container, int index,      \/\/ Arguments for default implementation\n+    <C,\n+     V extends Vector<?>>\n+    void store(Class<?> vClass, Class<?> eClass,\n+               int length,\n+               Object base, long offset,\n+               V v, C container, int index,\n@@ -354,0 +452,21 @@\n+    public interface StoreVectorMaskedOperation<C,\n+                                                V extends Vector<?>,\n+                                                M extends VectorMask<?>> {\n+        void store(C container, int index, V v, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C,\n+     V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    void storeMasked(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                     int length,\n+                     Object base, long offset,\n+                     V v, M m, C container, int index,\n+                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v, m);\n+    }\n+\n@@ -356,2 +475,4 @@\n-    public interface StoreVectorOperationWithMap<C, V extends Vector<?>> {\n-        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM);\n+    public interface StoreVectorOperationWithMap<C,\n+                                                 V extends Vector<?>,\n+                                                 M extends VectorMask<?>> {\n+        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM, M m);\n@@ -362,6 +483,12 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>>\n-    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-                      Object base, long offset,    \/\/ Unsafe addressing\n-                      W index_vector, V v,\n-                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n-                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+    <C,\n+     V extends Vector<E>,\n+     W extends Vector<Integer>,\n+     M extends VectorMask<E>,\n+     E>\n+    void storeWithMap(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                      int length,\n+                      Class<? extends Vector<Integer>> vectorIndexClass,\n+                      Object base, long offset,\n+                      W index_vector,\n+                      V v, M m, C container, int index, int[] indexMap, int indexM,\n+                      StoreVectorOperationWithMap<C, V, M> defaultImpl) {\n@@ -369,1 +496,1 @@\n-        defaultImpl.storeWithMap(container, index, v, indexMap, indexM);\n+        defaultImpl.storeWithMap(container, index, v, indexMap, indexM, m);\n@@ -376,4 +503,7 @@\n-    <VM>\n-    boolean test(int cond, Class<?> vmClass, Class<?> elementType, int length,\n-                 VM vm1, VM vm2,\n-                 BiFunction<VM, VM, Boolean> defaultImpl) {\n+    <M extends VectorMask<E>,\n+     E>\n+    boolean test(int cond,\n+                 Class<?> mClass, Class<?> eClass,\n+                 int length,\n+                 M m1, M m2,\n+                 BiFunction<M, M, Boolean> defaultImpl) {\n@@ -381,1 +511,1 @@\n-        return defaultImpl.apply(vm1, vm2);\n+        return defaultImpl.apply(m1, m2);\n@@ -386,2 +516,3 @@\n-    public interface VectorCompareOp<V,M> {\n-        M apply(int cond, V v1, V v2);\n+    public interface VectorCompareOp<V extends Vector<?>,\n+                                     M extends VectorMask<?>> {\n+        M apply(int cond, V v1, V v2, M m);\n@@ -391,6 +522,9 @@\n-    public static <V extends Vector<E>,\n-                   M extends VectorMask<E>,\n-                   E>\n-    M compare(int cond, Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n-              V v1, V v2,\n-              VectorCompareOp<V,M> defaultImpl) {\n+    public static\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    M compare(int cond,\n+              Class<? extends V> vectorClass, Class<M> mClass, Class<E> eClass,\n+              int length,\n+              V v1, V v2, M m,\n+              VectorCompareOp<V, M> defaultImpl) {\n@@ -398,1 +532,1 @@\n-        return defaultImpl.apply(cond, v1, v2);\n+        return defaultImpl.apply(cond, v1, v2, m);\n@@ -402,5 +536,4 @@\n-\n-    public interface VectorRearrangeOp<V extends Vector<E>,\n-            Sh extends VectorShuffle<E>,\n-            E> {\n-        V apply(V v1, Sh shuffle);\n+    public interface VectorRearrangeOp<V extends Vector<?>,\n+                                       SH extends VectorShuffle<?>,\n+                                       M extends VectorMask<?>> {\n+        V apply(V v, SH sh, M m);\n@@ -412,5 +545,7 @@\n-            Sh extends VectorShuffle<E>,\n-            E>\n-    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<?> elementType, int vlen,\n-                  V v1, Sh sh,\n-                  VectorRearrangeOp<V,Sh, E> defaultImpl) {\n+     SH extends VectorShuffle<E>,\n+     M  extends VectorMask<E>,\n+     E>\n+    V rearrangeOp(Class<? extends V> vClass, Class<SH> shClass, Class<M> mClass, Class<E> eClass,\n+                  int length,\n+                  V v, SH sh, M m,\n+                  VectorRearrangeOp<V, SH, M> defaultImpl) {\n@@ -418,1 +553,1 @@\n-        return defaultImpl.apply(v1, sh);\n+        return defaultImpl.apply(v, sh, m);\n@@ -423,4 +558,3 @@\n-    public interface VectorBlendOp<V extends Vector<E>,\n-            M extends VectorMask<E>,\n-            E> {\n-        V apply(V v1, V v2, M mask);\n+    public interface VectorBlendOp<V extends Vector<?>,\n+                                   M extends VectorMask<?>> {\n+        V apply(V v1, V v2, M m);\n@@ -434,1 +568,2 @@\n-    V blend(Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n+    V blend(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+            int length,\n@@ -436,1 +571,1 @@\n-            VectorBlendOp<V,M, E> defaultImpl) {\n+            VectorBlendOp<V, M> defaultImpl) {\n@@ -443,2 +578,3 @@\n-    public interface VectorBroadcastIntOp<V extends Vector<?>> {\n-        V apply(V v, int n);\n+    public interface VectorBroadcastIntOp<V extends Vector<?>,\n+                                          M extends VectorMask<?>> {\n+        V apply(V v, int n, M m);\n@@ -449,4 +585,8 @@\n-    <V extends Vector<?>>\n-    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<?> elementType, int length,\n-                   V v, int n,\n-                   VectorBroadcastIntOp<V> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V broadcastInt(int opr,\n+                   Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                   int length,\n+                   V v, int n, M m,\n+                   VectorBroadcastIntOp<V, M> defaultImpl) {\n@@ -454,1 +594,1 @@\n-        return defaultImpl.apply(v, n);\n+        return defaultImpl.apply(v, n, m);\n@@ -459,2 +599,4 @@\n-    public interface VectorConvertOp<VOUT, VIN, S> {\n-        VOUT apply(VIN v, S species);\n+    public interface VectorConvertOp<VOUT extends VectorPayload,\n+                                     VIN extends VectorPayload,\n+                                     S extends VectorSpecies<?>> {\n+        VOUT apply(VIN v, S s);\n@@ -472,2 +614,2 @@\n-              Class<?> fromVectorClass, Class<?> fromElementType, int fromVLen,\n-              Class<?>   toVectorClass, Class<?>   toElementType, int   toVLen,\n+              Class<?> fromVectorClass, Class<?> fromeClass, int fromVLen,\n+              Class<?>   toVectorClass, Class<?>   toeClass, int   toVLen,\n@@ -483,1 +625,3 @@\n-    public static <V> V maybeRebox(V v) {\n+    public static\n+    <VP extends VectorPayload>\n+    VP maybeRebox(VP v) {\n@@ -491,2 +635,2 @@\n-    public interface VectorMaskOp<M> {\n-        int apply(M m);\n+    public interface VectorMaskOp<M extends VectorMask<?>> {\n+        long apply(M m);\n@@ -497,3 +641,7 @@\n-    <E, M>\n-    int maskReductionCoerced(int oper, Class<? extends M> maskClass, Class<?> elemClass, int length, M m,\n-               VectorMaskOp<M> defaultImpl) {\n+    <M extends VectorMask<E>,\n+     E>\n+    long maskReductionCoerced(int oper,\n+                              Class<? extends M> mClass, Class<?> eClass,\n+                              int length,\n+                              M m,\n+                              VectorMaskOp<M> defaultImpl) {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":282,"deletions":134,"binary":false,"changes":416,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import java.util.Objects;\n+\n@@ -65,0 +67,1 @@\n+    @ForceInline\n@@ -66,15 +69,6 @@\n-        return getBits()[i];\n-    }\n-\n-    @Override\n-    public long toLong() {\n-        \/\/ FIXME: This should be an intrinsic.\n-        if (length() > Long.SIZE) {\n-            throw new UnsupportedOperationException(\"too many lanes for one long\");\n-        }\n-        long res = 0;\n-        long set = 1;\n-        boolean[] bits = getBits();\n-        for (int i = 0; i < bits.length; i++) {\n-            res = bits[i] ? res | set : res;\n-            set = set << 1;\n+        int length = length();\n+        Objects.checkIndex(i, length);\n+        if (length <= Long.SIZE) {\n+            return ((toLong() >>> i) & 1L) == 1;\n+        } else {\n+            return getBits()[i];\n@@ -82,1 +76,0 @@\n-        return res;\n@@ -117,0 +110,17 @@\n+    @Override\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    <F> VectorMask<F> check(Class<? extends VectorMask<F>> maskClass, Vector<F> vector) {\n+        if (!sameSpecies(maskClass, vector)) {\n+            throw AbstractSpecies.checkFailed(this, vector);\n+        }\n+        return (VectorMask<F>) this;\n+    }\n+\n+    @ForceInline\n+    private <F> boolean sameSpecies(Class<? extends VectorMask<F>> maskClass, Vector<F> vector) {\n+        boolean same = getClass() == maskClass;\n+        assert (same == (vectorSpecies() == vector.species())) : same;\n+        return same;\n+    }\n+\n@@ -165,0 +175,11 @@\n+    \/*package-private*\/\n+    static long toLongHelper(boolean[] bits) {\n+        long res = 0;\n+        long set = 1;\n+        for (int i = 0; i < bits.length; i++) {\n+            res = bits[i] ? res | set : res;\n+            set = set << 1;\n+        }\n+        return res;\n+    }\n+\n@@ -218,1 +239,0 @@\n-            \/\/ This requires a split test.\n@@ -220,6 +240,3 @@\n-            int elemCount = Math.min(vlength, (alength - clipOffset) \/ esize);\n-            badMask = checkIndex0(0, elemCount, iota, vlength);\n-            clipOffset &= (esize - 1);  \/\/ power of two, so OK\n-            VectorMask<E> badMask2 = checkIndex0(clipOffset \/ esize, vlength,\n-                                                 iota, vlength);\n-            badMask = badMask.or(badMask2);\n+            badMask = checkIndex0(clipOffset, alength,\n+                                  iota.lanewise(VectorOperators.MUL, esize),\n+                                  vlength * esize);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractMask.java","additions":40,"deletions":23,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte128Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte128Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseShiftTemplate(op, Byte128Mask.class, e, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte128Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v1, v2, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte128Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte128Mask.class, op, v, (Byte128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte128Mask.class,\n@@ -615,10 +650,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -650,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte128Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +691,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte128Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +701,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte128Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -680,2 +711,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte128Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -687,2 +718,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -694,2 +725,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -806,0 +847,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -814,0 +863,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -821,0 +877,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -828,0 +891,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte128Mask.class, bb, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -835,0 +905,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n@@ -842,0 +927,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte128Mask.class, bb, offset, (Byte128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte256Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte256Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseShiftTemplate(op, Byte256Mask.class, e, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte256Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v1, v2, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte256Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte256Mask.class, op, v, (Byte256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte256Mask.class,\n@@ -647,10 +682,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -682,3 +713,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte256Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -692,3 +723,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte256Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -702,3 +733,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte256Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -712,2 +743,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte256Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -719,2 +750,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -726,2 +757,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -838,0 +879,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -846,0 +895,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -853,0 +909,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -860,0 +923,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte256Mask.class, bb, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -867,0 +937,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n@@ -874,0 +959,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte256Mask.class, bb, offset, (Byte256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte512Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte512Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseShiftTemplate(op, Byte512Mask.class, e, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte512Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v1, v2, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte512Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte512Mask.class, op, v, (Byte512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte512Mask.class,\n@@ -711,10 +746,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -746,3 +777,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte512Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -756,3 +787,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte512Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -766,3 +797,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte512Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -776,2 +807,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte512Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -783,2 +814,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -790,2 +821,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -902,0 +943,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -910,0 +959,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -917,0 +973,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -924,0 +987,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte512Mask.class, bb, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -931,0 +1001,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n@@ -938,0 +1023,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte512Mask.class, bb, offset, (Byte512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte64Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte64Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseShiftTemplate(op, Byte64Mask.class, e, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte64Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v1, v2, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte64Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte64Mask.class, op, v, (Byte64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte64Mask.class,\n@@ -599,10 +634,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte64Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte64Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte64Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -664,2 +695,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte64Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -671,2 +702,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -678,2 +709,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -790,0 +831,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -798,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -812,0 +875,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte64Mask.class, bb, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -819,0 +889,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n@@ -826,0 +911,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte64Mask.class, bb, offset, (Byte64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public ByteMaxVector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public ByteMaxVector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline ByteMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseShiftTemplate(op, ByteMaxMask.class, e, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    ByteMaxVector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v1, v2, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final ByteMaxMask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(ByteMaxMask.class, op, v, (ByteMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    ByteMaxMask.class,\n@@ -585,10 +620,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    ByteMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, ByteMaxMask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, ByteMaxMask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, ByteMaxMask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -650,2 +681,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ByteMaxMask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((ByteMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -657,2 +688,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((ByteMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -664,2 +695,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((ByteMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -776,0 +817,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -784,0 +833,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(ByteMaxMask.class, bb, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +875,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n@@ -812,0 +897,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(ByteMaxMask.class, bb, offset, (ByteMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    byte rOp(byte v, FBinOp f);\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    byte rOpTemplate(byte v, VectorMask<Byte> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        byte[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Byte>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (byte) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (byte) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ByteVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ByteVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ByteVector, VectorMask<Byte>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ByteVector.class);\n+\n+    private static UnaryOperation<ByteVector, VectorMask<Byte>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Byte> eqz = that.eq((byte)0);\n+                VectorMask<Byte> eqz = that.eq((byte) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ByteVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ByteVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Byte> m) {\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v, VectorMask<Byte> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Byte> eqz = that.eq((byte)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Byte> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (byte) 0);\n+                that = that.blend((byte) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Byte> eqz = that.eq((byte)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ByteVector, VectorMask<Byte>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ByteVector.class);\n+\n+    private static BinaryOperation<ByteVector, VectorMask<Byte>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (byte) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        byte e1 = (byte) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract ByteVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ByteVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Byte>> maskClass,\n+                          int e, VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ByteVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ByteVector, VectorMask<Byte>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ByteVector, VectorMask<Byte>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ByteVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ByteVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v1,\n+                                          Vector<Byte> v2,\n+                                          VectorMask<Byte> m) {\n+        ByteVector that = (ByteVector) v1;\n+        ByteVector tother = (ByteVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ByteVector, VectorMask<Byte>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ByteVector.class);\n+\n+    private static TernaryOperation<ByteVector, VectorMask<Byte>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ByteSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Byte> v, M m) {\n+        ByteVector that = (ByteVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, byte.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Byte> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Byte> compare(VectorOperators.Comparison op,\n-                                  Vector<Byte> v,\n-                                  VectorMask<Byte> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, byte.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, byte.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Byte>>\n+    <S extends VectorShuffle<Byte>, M extends VectorMask<Byte>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Byte> m) {\n-        ByteVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, byte.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((byte)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, byte.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Byte>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ByteVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ByteVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((byte)1, (i, a, b) -> (byte)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (byte) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (byte) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((byte)-1, (i, a, b) -> (byte)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ByteVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ByteVector.class);\n+    ImplCache<Associative, ReductionOperation<ByteVector, VectorMask<Byte>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ByteVector.class);\n+\n+    private static ReductionOperation<ByteVector, VectorMask<Byte>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((byte)1, m, (i, a, b) -> (byte)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (byte) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (byte) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((byte)-1, m, (i, a, b) -> (byte)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ByteVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2924,1 +3092,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3102,3 +3270,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3342,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3344,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3332,1 +3497,0 @@\n-            \/\/ FIXME: optimize\n@@ -3335,1 +3499,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -3454,1 +3618,0 @@\n-            \/\/ FIXME: optimize\n@@ -3457,3 +3620,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3471,1 +3632,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3490,1 +3651,0 @@\n-            \/\/ FIXME: optimize\n@@ -3496,3 +3656,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3536,0 +3694,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3552,0 +3728,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n+\n@@ -3570,0 +3763,19 @@\n+    abstract\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get(o + i * 1));\n+            });\n+    }\n+\n@@ -3586,0 +3798,18 @@\n+    abstract\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get(o + i * 1));\n+                });\n+    }\n+\n@@ -3605,0 +3835,36 @@\n+    abstract\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+\n@@ -3622,0 +3888,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put(o + i * 1, e));\n+            });\n+    }\n+\n@@ -3636,0 +3921,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+                });\n+    }\n+\n+\n@@ -3962,1 +4266,1 @@\n-                                      AbstractMask<Byte> m,\n+                                      VectorMask<Byte> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":495,"deletions":191,"binary":false,"changes":686,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double128Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double128Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double128Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v1, v2, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double128Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double128Mask.class, op, v, (Double128Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double128Mask.class,\n@@ -583,10 +611,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -618,3 +642,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -628,3 +652,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -638,3 +662,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double128Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -648,2 +672,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -655,2 +679,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -662,2 +686,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -774,0 +808,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, indexMap, mapOffset, (Double128Mask) m);\n+    }\n+\n@@ -783,0 +831,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -790,0 +845,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double128Mask.class, bb, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +859,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, indexMap, mapOffset, (Double128Mask) m);\n+    }\n+\n+\n@@ -804,0 +881,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double128Mask.class, bb, offset, (Double128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double128Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double256Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double256Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double256Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v1, v2, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double256Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double256Mask.class, op, v, (Double256Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double256Mask.class,\n@@ -587,10 +615,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -622,3 +646,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +656,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +666,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double256Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -659,2 +683,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -666,2 +690,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -778,0 +812,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, indexMap, mapOffset, (Double256Mask) m);\n+    }\n+\n@@ -787,0 +835,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +849,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double256Mask.class, bb, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +863,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, indexMap, mapOffset, (Double256Mask) m);\n+    }\n+\n+\n@@ -808,0 +885,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double256Mask.class, bb, offset, (Double256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double256Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double512Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double512Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double512Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v1, v2, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double512Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double512Mask.class, op, v, (Double512Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double512Mask.class,\n@@ -595,10 +623,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -630,3 +654,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -640,3 +664,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -650,3 +674,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double512Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -660,2 +684,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -667,2 +691,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -674,2 +698,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -786,0 +820,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, indexMap, mapOffset, (Double512Mask) m);\n+    }\n+\n@@ -795,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -802,0 +857,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double512Mask.class, bb, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -809,0 +871,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, indexMap, mapOffset, (Double512Mask) m);\n+    }\n+\n+\n@@ -816,0 +893,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double512Mask.class, bb, offset, (Double512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double512Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double64Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double64Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double64Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v1, v2, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double64Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double64Mask.class, op, v, (Double64Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double64Mask.class,\n@@ -581,10 +609,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -616,3 +640,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -626,3 +650,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -636,3 +660,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double64Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -646,2 +670,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -653,2 +677,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -660,2 +684,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -772,0 +806,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, indexMap, mapOffset, (Double64Mask) m);\n+    }\n+\n@@ -781,0 +829,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -788,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double64Mask.class, bb, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -795,0 +857,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, indexMap, mapOffset, (Double64Mask) m);\n+    }\n+\n+\n@@ -802,0 +879,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double64Mask.class, bb, offset, (Double64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double64Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public DoubleMaxVector lanewise(Unary op, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public DoubleMaxVector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    DoubleMaxVector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v1, v2, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final DoubleMaxMask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(DoubleMaxMask.class, op, v, (DoubleMaxMask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    DoubleMaxMask.class,\n@@ -580,10 +608,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    DoubleMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -615,3 +639,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, DoubleMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -625,3 +649,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, DoubleMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -635,3 +659,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, DoubleMaxMask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -645,2 +669,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, DoubleMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((DoubleMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((DoubleMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -659,2 +683,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((DoubleMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -771,0 +805,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, indexMap, mapOffset, (DoubleMaxMask) m);\n+    }\n+\n@@ -780,0 +828,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -787,0 +842,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(DoubleMaxMask.class, bb, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +856,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, indexMap, mapOffset, (DoubleMaxMask) m);\n+    }\n+\n+\n@@ -801,0 +878,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(DoubleMaxMask.class, bb, offset, (DoubleMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleMaxVector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    double rOp(double v, FBinOp f);\n+    double rOp(double v, VectorMask<Double> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    double rOpTemplate(double v, VectorMask<Double> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        double[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Double>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (double) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<DoubleVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, DoubleVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          VectorMask<Double> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<DoubleVector, VectorMask<Double>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, DoubleVector.class);\n+\n+    private static UnaryOperation<DoubleVector, VectorMask<Double>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<DoubleVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, DoubleVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v, VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<DoubleVector, VectorMask<Double>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, DoubleVector.class);\n+\n+    private static BinaryOperation<DoubleVector, VectorMask<Double>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        double e1 = (double) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<DoubleVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, DoubleVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v1,\n+                                          Vector<Double> v2,\n+                                          VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v1;\n+        DoubleVector tother = (DoubleVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<DoubleVector, VectorMask<Double>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, DoubleVector.class);\n+\n+    private static TernaryOperation<DoubleVector, VectorMask<Double>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1650,2 +1747,0 @@\n-        Objects.requireNonNull(v);\n-        DoubleSpecies vsp = vspecies();\n@@ -1657,2 +1752,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1668,0 +1763,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Double> v, M m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, double.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Double> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1681,12 +1798,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Double> compare(VectorOperators.Comparison op,\n-                                  Vector<Double> v,\n-                                  VectorMask<Double> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1751,1 +1856,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2002,3 +2107,3 @@\n-            getClass(), shuffletype, double.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, double.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2021,1 +2126,1 @@\n-    <S extends VectorShuffle<Double>>\n+    <S extends VectorShuffle<Double>, M extends VectorMask<Double>>\n@@ -2023,0 +2128,1 @@\n+                                           Class<M> masktype,\n@@ -2024,9 +2130,3 @@\n-                                           VectorMask<Double> m) {\n-        DoubleVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, double.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2038,1 +2138,7 @@\n-        return broadcast((double)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, double.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2061,3 +2167,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2069,3 +2175,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2312,0 +2418,1 @@\n+                               Class<? extends VectorMask<Double>> maskClass,\n@@ -2313,2 +2420,10 @@\n-        DoubleVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            DoubleVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2329,14 +2444,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((double)0, (i, a, b) -> (double)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((double)1, (i, a, b) -> (double)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (double) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (double) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2344,0 +2448,1 @@\n+\n@@ -2345,2 +2450,16 @@\n-    ImplCache<Associative,Function<DoubleVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, DoubleVector.class);\n+    ImplCache<Associative, ReductionOperation<DoubleVector, VectorMask<Double>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, DoubleVector.class);\n+\n+    private static ReductionOperation<DoubleVector, VectorMask<Double>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((double)0, m, (i, a, b) -> (double)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((double)1, m, (i, a, b) -> (double)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (double) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (double) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2552,3 +2671,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2616,2 +2733,1 @@\n-            DoubleVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2693,3 +2809,3 @@\n-            vectorType, double.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2697,1 +2813,1 @@\n-            (double[] c, int idx, int[] iMap, int idy, DoubleSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2699,1 +2815,1 @@\n-        }\n+    }\n@@ -2747,1 +2863,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2749,1 +2864,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2843,3 +2958,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2917,1 +3030,0 @@\n-            \/\/ FIXME: optimize\n@@ -2920,1 +3032,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2983,1 +3095,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2986,1 +3098,1 @@\n-            this,\n+            this, null,\n@@ -2988,1 +3100,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3035,6 +3147,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3070,1 +3177,0 @@\n-            \/\/ FIXME: optimize\n@@ -3073,3 +3179,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3087,1 +3191,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3106,1 +3210,0 @@\n-            \/\/ FIXME: optimize\n@@ -3112,3 +3215,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3152,0 +3253,69 @@\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends DoubleVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return DoubleVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Double species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3172,0 +3342,19 @@\n+    abstract\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+            });\n+    }\n+\n@@ -3188,0 +3377,18 @@\n+    abstract\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+                });\n+    }\n+\n@@ -3207,0 +3414,71 @@\n+    abstract\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(double[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Double species is S_MAX_BIT. and the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3224,0 +3502,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putDouble(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3238,0 +3535,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3555,1 +3871,1 @@\n-                                      AbstractMask<Double> m,\n+                                      VectorMask<Double> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":511,"deletions":195,"binary":false,"changes":706,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float128Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float128Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float128Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v1, v2, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float128Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float128Mask.class, op, v, (Float128Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float128Mask.class,\n@@ -587,10 +615,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -622,3 +646,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +656,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +666,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float128Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -659,2 +683,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -666,2 +690,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -778,0 +812,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, indexMap, mapOffset, (Float128Mask) m);\n+    }\n+\n@@ -787,0 +835,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +849,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float128Mask.class, bb, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +863,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, indexMap, mapOffset, (Float128Mask) m);\n+    }\n+\n+\n@@ -808,0 +885,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float128Mask.class, bb, offset, (Float128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float128Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float256Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float256Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float256Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v1, v2, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float256Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float256Mask.class, op, v, (Float256Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float256Mask.class,\n@@ -595,10 +623,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -630,3 +654,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -640,3 +664,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -650,3 +674,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float256Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -660,2 +684,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -667,2 +691,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -674,2 +698,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -786,0 +820,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, indexMap, mapOffset, (Float256Mask) m);\n+    }\n+\n@@ -795,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -802,0 +857,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float256Mask.class, bb, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -809,0 +871,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, indexMap, mapOffset, (Float256Mask) m);\n+    }\n+\n+\n@@ -816,0 +893,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float256Mask.class, bb, offset, (Float256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float256Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float512Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float512Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float512Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v1, v2, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float512Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float512Mask.class, op, v, (Float512Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float512Mask.class,\n@@ -611,10 +639,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -646,3 +670,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -656,3 +680,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -666,3 +690,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float512Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -676,2 +700,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -683,2 +707,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -690,2 +714,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -802,0 +836,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, indexMap, mapOffset, (Float512Mask) m);\n+    }\n+\n@@ -811,0 +859,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -818,0 +873,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float512Mask.class, bb, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -825,0 +887,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, indexMap, mapOffset, (Float512Mask) m);\n+    }\n+\n+\n@@ -832,0 +909,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float512Mask.class, bb, offset, (Float512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float512Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float64Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float64Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float64Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v1, v2, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float64Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float64Mask.class, op, v, (Float64Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float64Mask.class,\n@@ -583,10 +611,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -618,3 +642,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -628,3 +652,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -638,3 +662,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float64Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -648,2 +672,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -655,2 +679,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -662,2 +686,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -774,0 +808,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, indexMap, mapOffset, (Float64Mask) m);\n+    }\n+\n@@ -783,0 +831,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -790,0 +845,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float64Mask.class, bb, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +859,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, indexMap, mapOffset, (Float64Mask) m);\n+    }\n+\n+\n@@ -804,0 +881,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float64Mask.class, bb, offset, (Float64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float64Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public FloatMaxVector lanewise(Unary op, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public FloatMaxVector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    FloatMaxVector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v1, v2, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final FloatMaxMask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(FloatMaxMask.class, op, v, (FloatMaxMask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    FloatMaxMask.class,\n@@ -580,10 +608,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    FloatMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -615,3 +639,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, FloatMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -625,3 +649,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, FloatMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -635,3 +659,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, FloatMaxMask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -645,2 +669,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, FloatMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((FloatMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((FloatMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -659,2 +683,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((FloatMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -771,0 +805,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, indexMap, mapOffset, (FloatMaxMask) m);\n+    }\n+\n@@ -780,0 +828,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -787,0 +842,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(FloatMaxMask.class, bb, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +856,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, indexMap, mapOffset, (FloatMaxMask) m);\n+    }\n+\n+\n@@ -801,0 +878,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(FloatMaxMask.class, bb, offset, (FloatMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatMaxVector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    float rOp(float v, FBinOp f);\n+    float rOp(float v, VectorMask<Float> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    float rOpTemplate(float v, VectorMask<Float> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        float[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Float>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (float) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<FloatVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, FloatVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          VectorMask<Float> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<FloatVector, VectorMask<Float>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, FloatVector.class);\n+\n+    private static UnaryOperation<FloatVector, VectorMask<Float>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<FloatVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, FloatVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v, VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<FloatVector, VectorMask<Float>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, FloatVector.class);\n+\n+    private static BinaryOperation<FloatVector, VectorMask<Float>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        float e1 = (float) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<FloatVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, FloatVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v1,\n+                                          Vector<Float> v2,\n+                                          VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v1;\n+        FloatVector tother = (FloatVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<FloatVector, VectorMask<Float>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, FloatVector.class);\n+\n+    private static TernaryOperation<FloatVector, VectorMask<Float>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1662,2 +1759,0 @@\n-        Objects.requireNonNull(v);\n-        FloatSpecies vsp = vspecies();\n@@ -1669,2 +1764,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1680,0 +1775,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Float> v, M m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, float.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Float> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1693,12 +1810,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Float> compare(VectorOperators.Comparison op,\n-                                  Vector<Float> v,\n-                                  VectorMask<Float> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1763,1 +1868,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2014,3 +2119,3 @@\n-            getClass(), shuffletype, float.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, float.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2033,1 +2138,1 @@\n-    <S extends VectorShuffle<Float>>\n+    <S extends VectorShuffle<Float>, M extends VectorMask<Float>>\n@@ -2035,0 +2140,1 @@\n+                                           Class<M> masktype,\n@@ -2036,9 +2142,3 @@\n-                                           VectorMask<Float> m) {\n-        FloatVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, float.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2050,1 +2150,7 @@\n-        return broadcast((float)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, float.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2073,3 +2179,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2081,3 +2187,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2332,0 +2438,1 @@\n+                               Class<? extends VectorMask<Float>> maskClass,\n@@ -2333,2 +2440,10 @@\n-        FloatVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            FloatVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2349,14 +2464,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((float)0, (i, a, b) -> (float)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((float)1, (i, a, b) -> (float)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (float) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (float) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2364,0 +2468,1 @@\n+\n@@ -2365,2 +2470,16 @@\n-    ImplCache<Associative,Function<FloatVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, FloatVector.class);\n+    ImplCache<Associative, ReductionOperation<FloatVector, VectorMask<Float>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, FloatVector.class);\n+\n+    private static ReductionOperation<FloatVector, VectorMask<Float>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((float)0, m, (i, a, b) -> (float)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((float)1, m, (i, a, b) -> (float)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (float) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (float) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2576,3 +2695,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2640,2 +2757,1 @@\n-            FloatVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2699,3 +2815,3 @@\n-            vectorType, float.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2703,1 +2819,1 @@\n-            (float[] c, int idx, int[] iMap, int idy, FloatSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2705,1 +2821,1 @@\n-        }\n+    }\n@@ -2753,1 +2869,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2755,1 +2870,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2849,3 +2964,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2923,1 +3036,0 @@\n-            \/\/ FIXME: optimize\n@@ -2926,1 +3038,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2970,1 +3082,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2973,1 +3085,1 @@\n-            this,\n+            this, null,\n@@ -2975,1 +3087,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3022,6 +3134,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3057,1 +3164,0 @@\n-            \/\/ FIXME: optimize\n@@ -3060,3 +3166,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3074,1 +3178,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3093,1 +3197,0 @@\n-            \/\/ FIXME: optimize\n@@ -3099,3 +3202,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3139,0 +3240,51 @@\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends FloatVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3159,0 +3311,19 @@\n+    abstract\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+            });\n+    }\n+\n@@ -3175,0 +3346,18 @@\n+    abstract\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+                });\n+    }\n+\n@@ -3194,0 +3383,52 @@\n+    abstract\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(float[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3211,0 +3452,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putFloat(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3225,0 +3485,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3542,1 +3821,1 @@\n-                                      AbstractMask<Float> m,\n+                                      VectorMask<Float> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":474,"deletions":195,"binary":false,"changes":669,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int128Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int128Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseShiftTemplate(op, Int128Mask.class, e, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int128Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v1, v2, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int128Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int128Mask.class, op, v, (Int128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int128Mask.class,\n@@ -591,10 +626,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -626,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -636,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -646,3 +677,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int128Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -656,2 +687,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -663,2 +694,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -670,2 +701,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -782,0 +823,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, indexMap, mapOffset, (Int128Mask) m);\n+    }\n+\n@@ -791,0 +846,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +860,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int128Mask.class, bb, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +874,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, indexMap, mapOffset, (Int128Mask) m);\n+    }\n+\n+\n@@ -812,0 +896,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int128Mask.class, bb, offset, (Int128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int128Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int256Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int256Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseShiftTemplate(op, Int256Mask.class, e, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int256Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v1, v2, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int256Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int256Mask.class, op, v, (Int256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int256Mask.class,\n@@ -599,10 +634,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int256Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -664,2 +695,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -671,2 +702,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -678,2 +709,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -790,0 +831,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, indexMap, mapOffset, (Int256Mask) m);\n+    }\n+\n@@ -799,0 +854,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -806,0 +868,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int256Mask.class, bb, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -813,0 +882,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, indexMap, mapOffset, (Int256Mask) m);\n+    }\n+\n+\n@@ -820,0 +904,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int256Mask.class, bb, offset, (Int256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int256Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int512Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int512Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseShiftTemplate(op, Int512Mask.class, e, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int512Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v1, v2, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int512Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int512Mask.class, op, v, (Int512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int512Mask.class,\n@@ -615,10 +650,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -650,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +691,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +701,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int512Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -680,2 +711,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -687,2 +718,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -694,2 +725,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -806,0 +847,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, indexMap, mapOffset, (Int512Mask) m);\n+    }\n+\n@@ -815,0 +870,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -822,0 +884,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int512Mask.class, bb, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -829,0 +898,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, indexMap, mapOffset, (Int512Mask) m);\n+    }\n+\n+\n@@ -836,0 +920,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int512Mask.class, bb, offset, (Int512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int512Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int64Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int64Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseShiftTemplate(op, Int64Mask.class, e, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int64Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v1, v2, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int64Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int64Mask.class, op, v, (Int64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int64Mask.class,\n@@ -587,10 +622,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -622,3 +653,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +663,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +673,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int64Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -652,2 +683,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -659,2 +690,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -666,2 +697,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -778,0 +819,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, indexMap, mapOffset, (Int64Mask) m);\n+    }\n+\n@@ -787,0 +842,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +856,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int64Mask.class, bb, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +870,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, indexMap, mapOffset, (Int64Mask) m);\n+    }\n+\n+\n@@ -808,0 +892,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int64Mask.class, bb, offset, (Int64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int64Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public IntMaxVector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public IntMaxVector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline IntMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseShiftTemplate(op, IntMaxMask.class, e, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    IntMaxVector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v1, v2, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final IntMaxMask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(IntMaxMask.class, op, v, (IntMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    IntMaxMask.class,\n@@ -585,10 +620,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    IntMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, IntMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, IntMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, IntMaxMask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -650,2 +681,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, IntMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((IntMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -657,2 +688,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((IntMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -664,2 +695,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((IntMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -787,0 +828,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, indexMap, mapOffset, (IntMaxMask) m);\n+    }\n+\n@@ -796,0 +851,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -803,0 +865,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(IntMaxMask.class, bb, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -810,0 +879,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, indexMap, mapOffset, (IntMaxMask) m);\n+    }\n+\n+\n@@ -817,0 +901,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(IntMaxMask.class, bb, offset, (IntMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntMaxVector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    int rOp(int v, FBinOp f);\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    int rOpTemplate(int v, VectorMask<Integer> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        int[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Integer>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (int) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (int) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<IntVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, IntVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<IntVector, VectorMask<Integer>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, IntVector.class);\n+\n+    private static UnaryOperation<IntVector, VectorMask<Integer>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Integer> eqz = that.eq((int)0);\n+                VectorMask<Integer> eqz = that.eq((int) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<IntVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, IntVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Integer> m) {\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v, VectorMask<Integer> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Integer> eqz = that.eq((int)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Integer> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (int) 0);\n+                that = that.blend((int) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Integer> eqz = that.eq((int)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<IntVector, VectorMask<Integer>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, IntVector.class);\n+\n+    private static BinaryOperation<IntVector, VectorMask<Integer>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (int) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        int e1 = (int) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract IntVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final IntVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Integer>> maskClass,\n+                          int e, VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<IntVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<IntVector, VectorMask<Integer>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<IntVector, VectorMask<Integer>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -880,6 +1000,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n@@ -887,3 +1004,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<IntVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, IntVector.class);\n@@ -897,2 +1011,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -902,2 +1016,37 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v1,\n+                                          Vector<Integer> v2,\n+                                          VectorMask<Integer> m) {\n+        IntVector that = (IntVector) v1;\n+        IntVector tother = (IntVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<IntVector, VectorMask<Integer>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, IntVector.class);\n+\n+    private static TernaryOperation<IntVector, VectorMask<Integer>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -960,1 +1109,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1018,1 +1167,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1075,1 +1224,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1747,2 +1896,0 @@\n-        Objects.requireNonNull(v);\n-        IntSpecies vsp = vspecies();\n@@ -1754,2 +1901,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1765,0 +1912,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Integer> v, M m) {\n+        IntVector that = (IntVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, int.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Integer> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1782,12 +1951,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Integer> compare(VectorOperators.Comparison op,\n-                                  Vector<Integer> v,\n-                                  VectorMask<Integer> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1852,1 +2009,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2103,3 +2260,3 @@\n-            getClass(), shuffletype, int.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, int.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2122,1 +2279,1 @@\n-    <S extends VectorShuffle<Integer>>\n+    <S extends VectorShuffle<Integer>, M extends VectorMask<Integer>>\n@@ -2124,0 +2281,1 @@\n+                                           Class<M> masktype,\n@@ -2125,9 +2283,3 @@\n-                                           VectorMask<Integer> m) {\n-        IntVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, int.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2139,1 +2291,7 @@\n-        return broadcast((int)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, int.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2162,3 +2320,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2170,3 +2328,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2435,0 +2593,1 @@\n+                               Class<? extends VectorMask<Integer>> maskClass,\n@@ -2436,2 +2595,10 @@\n-        IntVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            IntVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2452,20 +2619,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((int)1, (i, a, b) -> (int)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (int) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (int) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((int)-1, (i, a, b) -> (int)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2473,0 +2623,1 @@\n+\n@@ -2474,2 +2625,22 @@\n-    ImplCache<Associative,Function<IntVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, IntVector.class);\n+    ImplCache<Associative, ReductionOperation<IntVector, VectorMask<Integer>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, IntVector.class);\n+\n+    private static ReductionOperation<IntVector, VectorMask<Integer>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((int)1, m, (i, a, b) -> (int)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (int) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (int) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((int)-1, m, (i, a, b) -> (int)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2694,3 +2865,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2758,2 +2927,1 @@\n-            IntVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2817,3 +2985,3 @@\n-            vectorType, int.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2821,1 +2989,1 @@\n-            (int[] c, int idx, int[] iMap, int idy, IntSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2823,1 +2991,1 @@\n-        }\n+    }\n@@ -2871,1 +3039,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2873,1 +3040,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2967,3 +3134,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3041,1 +3206,0 @@\n-            \/\/ FIXME: optimize\n@@ -3044,1 +3208,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3088,1 +3252,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3091,1 +3255,1 @@\n-            this,\n+            this, null,\n@@ -3093,1 +3257,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3140,6 +3304,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3175,1 +3334,0 @@\n-            \/\/ FIXME: optimize\n@@ -3178,3 +3336,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3192,1 +3348,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3211,1 +3367,0 @@\n-            \/\/ FIXME: optimize\n@@ -3217,3 +3372,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3257,0 +3410,51 @@\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends IntVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3277,0 +3481,19 @@\n+    abstract\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getInt(o + i * 4));\n+            });\n+    }\n+\n@@ -3293,0 +3516,18 @@\n+    abstract\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getInt(o + i * 4));\n+                });\n+    }\n+\n@@ -3312,0 +3553,52 @@\n+    abstract\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(int[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3329,0 +3622,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putInt(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3343,0 +3655,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3660,1 +3991,1 @@\n-                                      AbstractMask<Integer> m,\n+                                      VectorMask<Integer> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":535,"deletions":204,"binary":false,"changes":739,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long128Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long128Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseShiftTemplate(op, Long128Mask.class, e, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long128Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v1, v2, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long128Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long128Mask.class, op, v, (Long128Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long128Mask.class,\n@@ -577,10 +612,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -612,3 +643,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -622,3 +653,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -632,3 +663,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long128Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -642,2 +673,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -649,2 +680,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -656,2 +687,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -768,0 +809,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, indexMap, mapOffset, (Long128Mask) m);\n+    }\n+\n@@ -777,0 +832,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -784,0 +846,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long128Mask.class, bb, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +860,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, indexMap, mapOffset, (Long128Mask) m);\n+    }\n+\n+\n@@ -798,0 +882,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long128Mask.class, bb, offset, (Long128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long128Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long256Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long256Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseShiftTemplate(op, Long256Mask.class, e, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long256Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v1, v2, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long256Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long256Mask.class, op, v, (Long256Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long256Mask.class,\n@@ -581,10 +616,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -616,3 +647,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -626,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -636,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long256Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -646,2 +677,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -653,2 +684,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -660,2 +691,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -772,0 +813,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, indexMap, mapOffset, (Long256Mask) m);\n+    }\n+\n@@ -781,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -788,0 +850,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long256Mask.class, bb, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -795,0 +864,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, indexMap, mapOffset, (Long256Mask) m);\n+    }\n+\n+\n@@ -802,0 +886,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long256Mask.class, bb, offset, (Long256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long256Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long512Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long512Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseShiftTemplate(op, Long512Mask.class, e, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long512Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v1, v2, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long512Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long512Mask.class, op, v, (Long512Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long512Mask.class,\n@@ -589,10 +624,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -624,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long512Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -654,2 +685,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -661,2 +692,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -668,2 +699,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -780,0 +821,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, indexMap, mapOffset, (Long512Mask) m);\n+    }\n+\n@@ -789,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -796,0 +858,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long512Mask.class, bb, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -803,0 +872,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, indexMap, mapOffset, (Long512Mask) m);\n+    }\n+\n+\n@@ -810,0 +894,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long512Mask.class, bb, offset, (Long512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long512Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long64Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long64Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseShiftTemplate(op, Long64Mask.class, e, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long64Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v1, v2, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long64Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long64Mask.class, op, v, (Long64Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long64Mask.class,\n@@ -575,10 +610,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -610,3 +641,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long64Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -640,2 +671,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -647,2 +678,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -654,2 +685,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -766,0 +807,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, indexMap, mapOffset, (Long64Mask) m);\n+    }\n+\n@@ -775,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -782,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long64Mask.class, bb, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -789,0 +858,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, indexMap, mapOffset, (Long64Mask) m);\n+    }\n+\n+\n@@ -796,0 +880,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long64Mask.class, bb, offset, (Long64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long64Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public LongMaxVector lanewise(Unary op, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public LongMaxVector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline LongMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseShiftTemplate(op, LongMaxMask.class, e, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    LongMaxVector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v1, v2, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final LongMaxMask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(LongMaxMask.class, op, v, (LongMaxMask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    LongMaxMask.class,\n@@ -575,10 +610,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    LongMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -610,3 +641,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, LongMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, LongMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, LongMaxMask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -640,2 +671,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, LongMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((LongMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -647,2 +678,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((LongMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -654,2 +685,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((LongMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -766,0 +807,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, indexMap, mapOffset, (LongMaxMask) m);\n+    }\n+\n@@ -775,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -782,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(LongMaxMask.class, bb, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -789,0 +858,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, indexMap, mapOffset, (LongMaxMask) m);\n+    }\n+\n+\n@@ -796,0 +880,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(LongMaxMask.class, bb, offset, (LongMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongMaxVector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    long rOp(long v, FBinOp f);\n+    long rOp(long v, VectorMask<Long> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    long rOpTemplate(long v, VectorMask<Long> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        long[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Long>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -510,1 +533,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -513,1 +536,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -518,10 +541,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (long) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (long) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n@@ -529,3 +545,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<LongVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, LongVector.class);\n@@ -536,2 +549,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -539,2 +552,36 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<LongVector, VectorMask<Long>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, LongVector.class);\n+\n+    private static UnaryOperation<LongVector, VectorMask<Long>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) Math.abs(a));\n+            default: return null;\n+        }\n@@ -560,0 +607,1 @@\n+\n@@ -578,1 +626,1 @@\n-                VectorMask<Long> eqz = that.eq((long)0);\n+                VectorMask<Long> eqz = that.eq((long) 0);\n@@ -584,0 +632,1 @@\n+\n@@ -586,34 +635,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n@@ -621,3 +639,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<LongVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, LongVector.class);\n@@ -629,2 +644,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -633,1 +648,6 @@\n-                                  VectorMask<Long> m) {\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v, VectorMask<Long> m) {\n@@ -635,4 +655,10 @@\n-        if (op == DIV) {\n-            VectorMask<Long> eqz = that.eq((long)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Long> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (long) 0);\n+                that = that.blend((long) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n@@ -640,3 +666,61 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Long> eqz = that.eq((long)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<LongVector, VectorMask<Long>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, LongVector.class);\n+\n+    private static BinaryOperation<LongVector, VectorMask<Long>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n@@ -644,1 +728,0 @@\n-        return blend(lanewise(op, v), m);\n@@ -646,0 +729,1 @@\n+\n@@ -708,1 +792,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (long) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -726,16 +816,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n@@ -743,0 +820,22 @@\n+\n+    \/*package-private*\/\n+    abstract LongVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final LongVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Long>> maskClass,\n+                          int e, VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n+    }\n+\n@@ -744,1 +843,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<LongVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<LongVector, VectorMask<Long>>> BIN_INT_IMPL\n@@ -747,0 +846,16 @@\n+    private static VectorBroadcastIntOp<LongVector, VectorMask<Long>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -798,6 +913,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n@@ -805,3 +917,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<LongVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, LongVector.class);\n@@ -815,2 +924,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -820,2 +929,37 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v1,\n+                                          Vector<Long> v2,\n+                                          VectorMask<Long> m) {\n+        LongVector that = (LongVector) v1;\n+        LongVector tother = (LongVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<LongVector, VectorMask<Long>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, LongVector.class);\n+\n+    private static TernaryOperation<LongVector, VectorMask<Long>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -878,1 +1022,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -936,1 +1080,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -993,1 +1137,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1665,2 +1809,0 @@\n-        Objects.requireNonNull(v);\n-        LongSpecies vsp = vspecies();\n@@ -1672,2 +1814,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1683,0 +1825,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Long> v, M m) {\n+        LongVector that = (LongVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, long.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Long> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1700,12 +1864,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Long> compare(VectorOperators.Comparison op,\n-                                  Vector<Long> v,\n-                                  VectorMask<Long> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1770,1 +1922,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -1974,3 +2126,3 @@\n-            getClass(), shuffletype, long.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, long.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -1993,1 +2145,1 @@\n-    <S extends VectorShuffle<Long>>\n+    <S extends VectorShuffle<Long>, M extends VectorMask<Long>>\n@@ -1995,0 +2147,1 @@\n+                                           Class<M> masktype,\n@@ -1996,9 +2149,3 @@\n-                                           VectorMask<Long> m) {\n-        LongVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, long.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2010,1 +2157,7 @@\n-        return broadcast((long)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, long.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2033,3 +2186,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2041,3 +2194,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2306,0 +2459,1 @@\n+                               Class<? extends VectorMask<Long>> maskClass,\n@@ -2307,2 +2461,10 @@\n-        LongVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            LongVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2323,20 +2485,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((long)1, (i, a, b) -> (long)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (long) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (long) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((long)-1, (i, a, b) -> (long)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2344,0 +2489,1 @@\n+\n@@ -2345,2 +2491,22 @@\n-    ImplCache<Associative,Function<LongVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, LongVector.class);\n+    ImplCache<Associative, ReductionOperation<LongVector, VectorMask<Long>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, LongVector.class);\n+\n+    private static ReductionOperation<LongVector, VectorMask<Long>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((long)1, m, (i, a, b) -> (long)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (long) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (long) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((long)-1, m, (i, a, b) -> (long)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2560,3 +2726,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2624,2 +2788,1 @@\n-            LongVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2701,3 +2864,3 @@\n-            vectorType, long.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2705,1 +2868,1 @@\n-            (long[] c, int idx, int[] iMap, int idy, LongSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2707,1 +2870,1 @@\n-        }\n+    }\n@@ -2755,1 +2918,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2757,1 +2919,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2851,3 +3013,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2925,1 +3085,0 @@\n-            \/\/ FIXME: optimize\n@@ -2928,1 +3087,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2991,1 +3150,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2994,1 +3153,1 @@\n-            this,\n+            this, null,\n@@ -2996,1 +3155,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3043,6 +3202,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3078,1 +3232,0 @@\n-            \/\/ FIXME: optimize\n@@ -3081,3 +3234,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3095,1 +3246,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3114,1 +3265,0 @@\n-            \/\/ FIXME: optimize\n@@ -3120,3 +3270,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3160,0 +3308,69 @@\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends LongVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return LongVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Long species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3180,0 +3397,19 @@\n+    abstract\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getLong(o + i * 8));\n+            });\n+    }\n+\n@@ -3196,0 +3432,18 @@\n+    abstract\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getLong(o + i * 8));\n+                });\n+    }\n+\n@@ -3215,0 +3469,71 @@\n+    abstract\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(long[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Long species is S_MAX_BIT. and the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3232,0 +3557,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putLong(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3246,0 +3590,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3554,1 +3917,1 @@\n-                                      AbstractMask<Long> m,\n+                                      VectorMask<Long> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":564,"deletions":201,"binary":false,"changes":765,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short128Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short128Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseShiftTemplate(op, Short128Mask.class, e, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short128Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v1, v2, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short128Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short128Mask.class, op, v, (Short128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short128Mask.class,\n@@ -599,10 +634,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short128Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short128Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short128Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -664,2 +695,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short128Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -671,2 +702,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -678,2 +709,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -790,0 +831,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -797,0 +846,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -812,0 +875,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short128Mask.class, bb, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -819,0 +889,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n+\n+\n@@ -826,0 +905,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short128Mask.class, bb, offset, (Short128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short256Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short256Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseShiftTemplate(op, Short256Mask.class, e, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short256Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v1, v2, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short256Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short256Mask.class, op, v, (Short256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short256Mask.class,\n@@ -615,10 +650,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -650,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short256Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +691,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short256Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +701,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short256Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -680,2 +711,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short256Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -687,2 +718,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -694,2 +725,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -806,0 +847,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -813,0 +862,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -821,0 +877,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -828,0 +891,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short256Mask.class, bb, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -835,0 +905,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n+\n+\n@@ -842,0 +921,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short256Mask.class, bb, offset, (Short256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short512Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short512Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseShiftTemplate(op, Short512Mask.class, e, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short512Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v1, v2, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short512Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short512Mask.class, op, v, (Short512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short512Mask.class,\n@@ -647,10 +682,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -682,3 +713,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short512Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -692,3 +723,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short512Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -702,3 +733,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short512Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -712,2 +743,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short512Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -719,2 +750,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -726,2 +757,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -838,0 +879,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -845,0 +894,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -853,0 +909,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -860,0 +923,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short512Mask.class, bb, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -867,0 +937,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n+\n+\n@@ -874,0 +953,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short512Mask.class, bb, offset, (Short512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short64Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short64Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseShiftTemplate(op, Short64Mask.class, e, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short64Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v1, v2, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short64Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short64Mask.class, op, v, (Short64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short64Mask.class,\n@@ -591,10 +626,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -626,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short64Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -636,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short64Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -646,3 +677,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short64Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -656,2 +687,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short64Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -663,2 +694,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -670,2 +701,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -782,0 +823,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -789,0 +838,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +853,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -804,0 +867,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short64Mask.class, bb, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -811,0 +881,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n+\n+\n@@ -818,0 +897,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short64Mask.class, bb, offset, (Short64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public ShortMaxVector lanewise(Unary op, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public ShortMaxVector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline ShortMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseShiftTemplate(op, ShortMaxMask.class, e, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    ShortMaxVector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v1, v2, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final ShortMaxMask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(ShortMaxMask.class, op, v, (ShortMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    ShortMaxMask.class,\n@@ -585,10 +620,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    ShortMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, ShortMaxMask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, ShortMaxMask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, ShortMaxMask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -650,2 +681,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ShortMaxMask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((ShortMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -657,2 +688,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((ShortMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -664,2 +695,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((ShortMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -776,0 +817,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -783,0 +832,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(ShortMaxMask.class, bb, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +875,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n+\n+\n@@ -812,0 +891,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(ShortMaxMask.class, bb, offset, (ShortMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    short rOp(short v, FBinOp f);\n+    short rOp(short v, VectorMask<Short> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    short rOpTemplate(short v, VectorMask<Short> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        short[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Short>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (short) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (short) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ShortVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ShortVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ShortVector, VectorMask<Short>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ShortVector.class);\n+\n+    private static UnaryOperation<ShortVector, VectorMask<Short>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Short> eqz = that.eq((short)0);\n+                VectorMask<Short> eqz = that.eq((short) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ShortVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ShortVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Short> m) {\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v, VectorMask<Short> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Short> eqz = that.eq((short)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Short> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n+                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Short> eqz = that.eq((short)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ShortVector, VectorMask<Short>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ShortVector.class);\n+\n+    private static BinaryOperation<ShortVector, VectorMask<Short>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (short) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        short e1 = (short) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,24 @@\n-            opc, getClass(), short.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract ShortVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ShortVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Short>> maskClass,\n+                          int e, VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n@@ -825,0 +928,1 @@\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ShortVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ShortVector, VectorMask<Short>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ShortVector, VectorMask<Short>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ShortVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ShortVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v1,\n+                                          Vector<Short> v2,\n+                                          VectorMask<Short> m) {\n+        ShortVector that = (ShortVector) v1;\n+        ShortVector tother = (ShortVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ShortVector, VectorMask<Short>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ShortVector.class);\n+\n+    private static TernaryOperation<ShortVector, VectorMask<Short>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ShortSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Short> v, M m) {\n+        ShortVector that = (ShortVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, short.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Short> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Short> compare(VectorOperators.Comparison op,\n-                                  Vector<Short> v,\n-                                  VectorMask<Short> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, short.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, short.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Short>>\n+    <S extends VectorShuffle<Short>, M extends VectorMask<Short>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Short> m) {\n-        ShortVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, short.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((short)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, short.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Short>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ShortVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ShortVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((short)1, (i, a, b) -> (short)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (short) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (short) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((short)-1, (i, a, b) -> (short)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ShortVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ShortVector.class);\n+    ImplCache<Associative, ReductionOperation<ShortVector, VectorMask<Short>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ShortVector.class);\n+\n+    private static ReductionOperation<ShortVector, VectorMask<Short>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((short)1, m, (i, a, b) -> (short)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (short) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (short) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((short)-1, m, (i, a, b) -> (short)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2916,2 +3084,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3102,3 +3269,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3341,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3343,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3324,1 +3488,0 @@\n-            \/\/ FIXME: optimize\n@@ -3327,1 +3490,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -3441,1 +3604,0 @@\n-            \/\/ FIXME: optimize\n@@ -3444,3 +3606,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3458,1 +3618,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3477,1 +3637,0 @@\n-            \/\/ FIXME: optimize\n@@ -3483,3 +3642,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3523,0 +3680,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3538,0 +3713,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n+\n@@ -3557,0 +3749,19 @@\n+    abstract\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getShort(o + i * 2));\n+            });\n+    }\n+\n@@ -3573,0 +3784,18 @@\n+    abstract\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getShort(o + i * 2));\n+                });\n+    }\n+\n@@ -3592,0 +3821,19 @@\n+    abstract\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+\n@@ -3609,0 +3857,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putShort(o + i * 2, e));\n+            });\n+    }\n+\n@@ -3623,0 +3890,36 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+                });\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+\n@@ -3957,1 +4260,1 @@\n-                                      AbstractMask<Short> m,\n+                                      VectorMask<Short> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":495,"deletions":192,"binary":false,"changes":687,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -519,0 +519,2 @@\n+     * @throws IndexOutOfBoundsException if the index is out of range\n+     * ({@code < 0 || >= length()})\n@@ -556,0 +558,18 @@\n+    \/**\n+     * Checks that this mask has the same class with the given mask class,\n+     * and it has the same species with given vector's species,\n+     * and returns this mask unchanged.\n+     * The effect is similar to this pseudocode:\n+     * {@code getClass() == maskClass &&\n+     *        vectorSpecies() == vector.species()\n+     *        ? this\n+     *        : throw new ClassCastException()}.\n+     *\n+     * @param maskClass the class required for this mask\n+     * @param vector its species required for this mask\n+     * @param <F> the boxed element type of the required species\n+     * @return the same mask\n+     * @throws ClassCastException if the species is wrong\n+     *\/\n+    abstract <F> VectorMask<F> check(Class<? extends VectorMask<F>> maskClass, Vector<F> vector);\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorMask.java","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -180,0 +179,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -223,0 +225,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -272,0 +277,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -287,1 +295,16 @@\n-    $type$ rOp($type$ v, FBinOp f);\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    $type$ rOpTemplate($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        $type$[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<$Boxtype$>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -575,1 +598,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -578,1 +601,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -584,44 +607,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.abs(a));\n-#if[FP]\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log1p(a));\n-#end[FP]\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n@@ -629,3 +611,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<$abstractvectortype$>> UN_IMPL\n-        = new ImplCache<>(Unary.class, $Type$Vector.class);\n@@ -636,2 +615,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -639,2 +618,72 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+#if[BITWISE]\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+#end[BITWISE]\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, $Type$Vector.class);\n+\n+    private static UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.abs(a));\n+#if[FP]\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log1p(a));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -660,0 +709,1 @@\n+\n@@ -687,1 +737,1 @@\n-                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$) 0);\n@@ -694,0 +744,1 @@\n+\n@@ -696,44 +747,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.min(a, b));\n-#if[BITWISE]\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-#end[BITWISE]\n-#if[FP]\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n@@ -741,3 +751,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<$abstractvectortype$>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, $Type$Vector.class);\n@@ -749,2 +756,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -753,2 +760,6 @@\n-                                  VectorMask<$Boxtype$> m) {\n-#if[BITWISE]\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n@@ -756,4 +767,34 @@\n-        if (op == DIV) {\n-            VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL {#if[!FP]? | VO_SHIFT})) {\n+            if (op == FIRST_NONZERO) {\n+#if[FP]\n+                return blend(lanewise(op, v), m);\n+#else[FP]\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<$Boxbitstype$> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n+                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+#end[FP]\n+            }\n+#if[BITWISE]\n+#if[!FP]\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+#end[!FP]\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -761,3 +802,1 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+#end[BITWISE]\n@@ -765,0 +804,43 @@\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, $Type$Vector.class);\n+\n+    private static BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.min(a, b));\n+#if[BITWISE]\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n@@ -766,1 +848,12 @@\n-        return blend(lanewise(op, v), m);\n+#if[FP]\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.hypot(a, b));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -768,0 +861,1 @@\n+\n@@ -832,1 +926,9 @@\n-        return blend(lanewise(op, e), m);\n+#if[BITWISE]\n+        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = ($type$) ~e;\n+        }\n+#end[BITWISE]\n+        return lanewise(op, broadcast(e), m);\n@@ -851,1 +953,0 @@\n-        if ((long)e1 != e\n@@ -853,0 +954,1 @@\n+        if ((long)e1 != e\n@@ -854,1 +956,3 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n@@ -856,1 +960,0 @@\n-            ) {\n@@ -876,1 +979,11 @@\n-        return blend(lanewise(op, e), m);\n+        $type$ e1 = ($type$) e;\n+#if[BITWISE]\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n+#end[BITWISE]\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -895,16 +1008,24 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract $abstractvectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final $abstractvectortype$\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                          int e, VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n@@ -912,0 +1033,1 @@\n+\n@@ -913,1 +1035,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>>> BIN_INT_IMPL\n@@ -916,0 +1038,16 @@\n+    private static VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -975,10 +1113,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-#if[FP]\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n@@ -986,3 +1117,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<$abstractvectortype$>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, $Type$Vector.class);\n@@ -996,2 +1124,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -1001,2 +1129,43 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v1,\n+                                          Vector<$Boxtype$> v2,\n+                                          VectorMask<$Boxtype$> m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v1;\n+        $abstractvectortype$ tother = ($abstractvectortype$) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+#if[BITWISE]\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+#end[BITWISE]\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, $Type$Vector.class);\n+\n+    private static TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+#if[FP]\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -1059,1 +1228,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1117,1 +1286,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1174,1 +1343,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -2019,2 +2188,0 @@\n-        Objects.requireNonNull(v);\n-        $Type$Species vsp = vspecies();\n@@ -2026,2 +2193,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -2037,0 +2204,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<$Boxtype$> v, M m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, $type$.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<$Boxtype$> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -2056,12 +2245,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<$Boxtype$> compare(VectorOperators.Comparison op,\n-                                  Vector<$Boxtype$> v,\n-                                  VectorMask<$Boxtype$> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -2126,1 +2303,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2381,3 +2558,3 @@\n-            getClass(), shuffletype, $type$.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, $type$.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2400,1 +2577,1 @@\n-    <S extends VectorShuffle<$Boxtype$>>\n+    <S extends VectorShuffle<$Boxtype$>, M extends VectorMask<$Boxtype$>>\n@@ -2402,0 +2579,1 @@\n+                                           Class<M> masktype,\n@@ -2403,9 +2581,3 @@\n-                                           VectorMask<$Boxtype$> m) {\n-        $abstractvectortype$ unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2417,1 +2589,7 @@\n-        return broadcast(($type$)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, $type$.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2440,3 +2618,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2448,3 +2626,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2842,0 +3020,1 @@\n+                               Class<? extends VectorMask<$Boxtype$>> maskClass,\n@@ -2843,2 +3022,10 @@\n-        $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n@@ -2859,12 +3046,19 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp(($type$)1, (i, a, b) -> ($type$)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> ($type$) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> ($type$) Math.max(a, b)));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n+    }\n+\n+    private static final\n+    ImplCache<Associative, ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, $Type$Vector.class);\n+\n+    private static ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp(($type$)1, m, (i, a, b) -> ($type$)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> ($type$) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> ($type$) Math.max(a, b)));\n@@ -2872,6 +3066,6 @@\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp(($type$)-1, (i, a, b) -> ($type$)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a ^ b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp(($type$)-1, m, (i, a, b) -> ($type$)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a ^ b)));\n@@ -2879,2 +3073,2 @@\n-              default: return null;\n-              }})));\n+            default: return null;\n+        }\n@@ -2882,3 +3076,0 @@\n-    private static final\n-    ImplCache<Associative,Function<$abstractvectortype$,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, $Type$Vector.class);\n@@ -3178,3 +3369,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -3242,2 +3431,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -3336,3 +3524,3 @@\n-            vectorType, $type$.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -3340,1 +3528,1 @@\n-            ($type$[] c, int idx, int[] iMap, int idy, $Type$Species s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -3342,1 +3530,1 @@\n-        }\n+    }\n@@ -3402,1 +3590,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -3404,1 +3591,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -3465,2 +3652,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3626,1 +3812,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3817,3 +4003,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3891,1 +4075,0 @@\n-            \/\/ FIXME: optimize\n@@ -3894,1 +4077,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3976,1 +4159,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3979,1 +4162,1 @@\n-            this,\n+            this, null,\n@@ -3981,1 +4164,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -4042,6 +4225,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -4115,1 +4293,0 @@\n-            \/\/ FIXME: optimize\n@@ -4118,1 +4295,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -4278,1 +4455,0 @@\n-            \/\/ FIXME: optimize\n@@ -4281,1 +4457,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -4401,1 +4577,0 @@\n-            \/\/ FIXME: optimize\n@@ -4404,3 +4579,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -4418,1 +4591,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -4437,1 +4610,0 @@\n-            \/\/ FIXME: optimize\n@@ -4443,3 +4615,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -4483,0 +4653,78 @@\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+#if[!byteOrShort]\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+          return $abstractvectortype$.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of $Type$ species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#end[!byteOrShort]\n+\n@@ -4498,0 +4746,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n@@ -4515,0 +4780,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n@@ -4535,0 +4817,19 @@\n+    abstract\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+            });\n+    }\n+\n@@ -4551,0 +4852,18 @@\n+    abstract\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+                });\n+    }\n+\n@@ -4570,0 +4889,99 @@\n+    abstract\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+#if[!byteOrShort]\n+    abstract\n+    void intoArray0($type$[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of $Type$ species is S_MAX_BIT. and the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+#end[byte]\n+\n@@ -4587,0 +5005,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            });\n+    }\n+\n@@ -4601,0 +5038,38 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+                });\n+    }\n+\n+#if[short]\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+#end[short]\n+\n@@ -4976,1 +5451,1 @@\n-                                      AbstractMask<$Boxtype$> m,\n+                                      VectorMask<$Boxtype$> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":737,"deletions":262,"binary":false,"changes":999,"status":"modified"},{"patch":"@@ -241,2 +241,2 @@\n-    $type$ rOp($type$ v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -278,0 +278,6 @@\n+    @Override\n+    @ForceInline\n+    public $vectortype$ lanewise(Unary op, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +290,6 @@\n+    @Override\n+    @ForceInline\n+    public $vectortype$ lanewise(Binary op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -291,0 +303,7 @@\n+\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline $vectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseShiftTemplate(op, $masktype$.class, e, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -298,1 +317,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2) {\n+    lanewise(Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2) {\n@@ -302,0 +321,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    $vectortype$\n+    lanewise(Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v1, v2, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -321,1 +348,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialized\n@@ -334,1 +361,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialized\n@@ -372,0 +399,7 @@\n+    @Override\n+    @ForceInline\n+    public final $masktype$ compare(Comparison op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return super.compareTemplate($masktype$.class, op, v, ($masktype$) m);\n+    }\n+\n+\n@@ -428,0 +462,1 @@\n+                                    $masktype$.class,\n@@ -858,10 +893,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    $Type$$bits$Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -893,3 +924,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -903,3 +934,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -913,3 +944,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -923,2 +954,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, $masktype$.class, $bitstype$.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper((($masktype$)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -930,2 +961,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper((($masktype$)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -937,2 +968,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper((($masktype$)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -1064,0 +1105,16 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n+#if[!byteOrShort]\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, int[] indexMap, int mapOffset, VectorMask<$Boxtype$> m) {\n+        return super.fromArray0Template($masktype$.class, a, offset, indexMap, mapOffset, ($masktype$) m);\n+    }\n+#end[!byteOrShort]\n+\n@@ -1071,0 +1128,7 @@\n+\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromCharArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -1080,0 +1144,7 @@\n+\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -1089,0 +1160,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromByteArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -1096,0 +1174,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromByteBuffer0Template($masktype$.class, bb, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -1103,0 +1188,25 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+\n+#if[!byteOrShort]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, int[] indexMap, int mapOffset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, indexMap, mapOffset, ($masktype$) m);\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+#end[byte]\n+\n@@ -1110,0 +1220,23 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoByteArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoByteBuffer0Template($masktype$.class, bb, offset, ($masktype$) m);\n+    }\n+\n+#if[short]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoCharArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+#end[short]\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":163,"deletions":30,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -22,3 +22,0 @@\n-; This file contains duplicate entries as globalDefinitions_vecApi.hpp\n-; It is intended for inclusion in .s files compiled with masm\n-\n","filename":"src\/jdk.incubator.vector\/windows\/native\/libsvml\/globals_vectorApiSupport_windows.S.inc","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1558,0 +1558,17 @@\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, p0, 0);\",                       \"lsl\\tz0.b, p0\/m, z0.b, #0\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, p0, 5);\",                       \"lsl\\tz0.b, p0\/m, z0.b, #5\"],\n+                        [\"lsl\",     \"__ sve_lsl(z1, __ H, p1, 15);\",                      \"lsl\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"lsl\",     \"__ sve_lsl(z2, __ S, p2, 31);\",                      \"lsl\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"lsl\",     \"__ sve_lsl(z3, __ D, p3, 63);\",                      \"lsl\\tz3.d, p3\/m, z3.d, #63\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, p0, 1);\",                       \"lsr\\tz0.b, p0\/m, z0.b, #1\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, p0, 8);\",                       \"lsr\\tz0.b, p0\/m, z0.b, #8\"],\n+                        [\"lsr\",     \"__ sve_lsr(z1, __ H, p1, 15);\",                      \"lsr\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"lsr\",     \"__ sve_lsr(z2, __ S, p2, 7);\",                       \"lsr\\tz2.s, p2\/m, z2.s, #7\"],\n+                        [\"lsr\",     \"__ sve_lsr(z2, __ S, p2, 31);\",                      \"lsr\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"lsr\",     \"__ sve_lsr(z3, __ D, p3, 63);\",                      \"lsr\\tz3.d, p3\/m, z3.d, #63\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ B, p0, 1);\",                       \"asr\\tz0.b, p0\/m, z0.b, #1\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ B, p0, 7);\",                       \"asr\\tz0.b, p0\/m, z0.b, #7\"],\n+                        [\"asr\",     \"__ sve_asr(z1, __ H, p1, 5);\",                       \"asr\\tz1.h, p1\/m, z1.h, #5\"],\n+                        [\"asr\",     \"__ sve_asr(z1, __ H, p1, 15);\",                      \"asr\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"asr\",     \"__ sve_asr(z2, __ S, p2, 31);\",                      \"asr\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"asr\",     \"__ sve_asr(z3, __ D, p3, 63);\",                      \"asr\\tz3.d, p3\/m, z3.d, #63\"],\n@@ -1652,0 +1669,23 @@\n+                        [\"and\",     \"__ sve_and(p0, p1, p2, p3);\",                        \"and\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"ands\",    \"__ sve_ands(p4, p5, p6, p0);\",                       \"ands\\tp4.b, p5\/z, p6.b, p0.b\"],\n+                        [\"eor\",     \"__ sve_eor(p0, p1, p2, p3);\",                        \"eor\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"eors\",    \"__ sve_eors(p5, p6, p0, p1);\",                       \"eors\\tp5.b, p6\/z, p0.b, p1.b\"],\n+                        [\"orr\",     \"__ sve_orr(p0, p1, p2, p3);\",                        \"orr\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"orrs\",    \"__ sve_orrs(p9, p1, p4, p5);\",                       \"orrs\\tp9.b, p1\/z, p4.b, p5.b\"],\n+                        [\"bic\",     \"__ sve_bic(p10, p7, p9, p11);\",                      \"bic\\tp10.b, p7\/z, p9.b, p11.b\"],\n+                        [\"ptest\",   \"__ sve_ptest(p7, p1);\",                              \"ptest\\tp7, p1.b\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p1, __ B);\",                            \"ptrue\\tp1.b\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p2, __ H);\",                            \"ptrue\\tp2.h\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p3, __ S);\",                            \"ptrue\\tp3.s\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p4, __ D);\",                            \"ptrue\\tp4.d\"],\n+                        [\"pfalse\",  \"__ sve_pfalse(p7);\",                                 \"pfalse\\tp7.b\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ B, p0, p1);\",                     \"uzp1\\tp0.b, p0.b, p1.b\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ H, p0, p1);\",                     \"uzp1\\tp0.h, p0.h, p1.h\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ S, p0, p1);\",                     \"uzp1\\tp0.s, p0.s, p1.s\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ D, p0, p1);\",                     \"uzp1\\tp0.d, p0.d, p1.d\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ B, p0, p1);\",                     \"uzp2\\tp0.b, p0.b, p1.b\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ H, p0, p1);\",                     \"uzp2\\tp0.h, p0.h, p1.h\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ S, p0, p1);\",                     \"uzp2\\tp0.s, p0.s, p1.s\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ D, p0, p1);\",                     \"uzp2\\tp0.d, p0.d, p1.d\"],\n+                        [\"punpklo\", \"__ sve_punpklo(p1, p0);\",                            \"punpklo\\tp1.h, p0.b\"],\n+                        [\"punpkhi\", \"__ sve_punpkhi(p1, p0);\",                            \"punpkhi\\tp1.h, p0.b\"],\n@@ -1689,0 +1729,1 @@\n+                       [\"and\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1691,0 +1732,1 @@\n+                       [\"eor\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1696,0 +1738,1 @@\n+                       [\"orr\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1711,0 +1754,1 @@\n+                       [\"fmad\", \"ZPZZ\", \"m\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":44,"deletions":0,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -747,0 +747,17 @@\n+    __ sve_lsl(z0, __ B, p0, 0);                       \/\/       lsl     z0.b, p0\/m, z0.b, #0\n+    __ sve_lsl(z0, __ B, p0, 5);                       \/\/       lsl     z0.b, p0\/m, z0.b, #5\n+    __ sve_lsl(z1, __ H, p1, 15);                      \/\/       lsl     z1.h, p1\/m, z1.h, #15\n+    __ sve_lsl(z2, __ S, p2, 31);                      \/\/       lsl     z2.s, p2\/m, z2.s, #31\n+    __ sve_lsl(z3, __ D, p3, 63);                      \/\/       lsl     z3.d, p3\/m, z3.d, #63\n+    __ sve_lsr(z0, __ B, p0, 1);                       \/\/       lsr     z0.b, p0\/m, z0.b, #1\n+    __ sve_lsr(z0, __ B, p0, 8);                       \/\/       lsr     z0.b, p0\/m, z0.b, #8\n+    __ sve_lsr(z1, __ H, p1, 15);                      \/\/       lsr     z1.h, p1\/m, z1.h, #15\n+    __ sve_lsr(z2, __ S, p2, 7);                       \/\/       lsr     z2.s, p2\/m, z2.s, #7\n+    __ sve_lsr(z2, __ S, p2, 31);                      \/\/       lsr     z2.s, p2\/m, z2.s, #31\n+    __ sve_lsr(z3, __ D, p3, 63);                      \/\/       lsr     z3.d, p3\/m, z3.d, #63\n+    __ sve_asr(z0, __ B, p0, 1);                       \/\/       asr     z0.b, p0\/m, z0.b, #1\n+    __ sve_asr(z0, __ B, p0, 7);                       \/\/       asr     z0.b, p0\/m, z0.b, #7\n+    __ sve_asr(z1, __ H, p1, 5);                       \/\/       asr     z1.h, p1\/m, z1.h, #5\n+    __ sve_asr(z1, __ H, p1, 15);                      \/\/       asr     z1.h, p1\/m, z1.h, #15\n+    __ sve_asr(z2, __ S, p2, 31);                      \/\/       asr     z2.s, p2\/m, z2.s, #31\n+    __ sve_asr(z3, __ D, p3, 63);                      \/\/       asr     z3.d, p3\/m, z3.d, #63\n@@ -841,0 +858,23 @@\n+    __ sve_and(p0, p1, p2, p3);                        \/\/       and     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_ands(p4, p5, p6, p0);                       \/\/       ands    p4.b, p5\/z, p6.b, p0.b\n+    __ sve_eor(p0, p1, p2, p3);                        \/\/       eor     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_eors(p5, p6, p0, p1);                       \/\/       eors    p5.b, p6\/z, p0.b, p1.b\n+    __ sve_orr(p0, p1, p2, p3);                        \/\/       orr     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_orrs(p9, p1, p4, p5);                       \/\/       orrs    p9.b, p1\/z, p4.b, p5.b\n+    __ sve_bic(p10, p7, p9, p11);                      \/\/       bic     p10.b, p7\/z, p9.b, p11.b\n+    __ sve_ptest(p7, p1);                              \/\/       ptest   p7, p1.b\n+    __ sve_ptrue(p1, __ B);                            \/\/       ptrue   p1.b\n+    __ sve_ptrue(p2, __ H);                            \/\/       ptrue   p2.h\n+    __ sve_ptrue(p3, __ S);                            \/\/       ptrue   p3.s\n+    __ sve_ptrue(p4, __ D);                            \/\/       ptrue   p4.d\n+    __ sve_pfalse(p7);                                 \/\/       pfalse  p7.b\n+    __ sve_uzp1(p0, __ B, p0, p1);                     \/\/       uzp1    p0.b, p0.b, p1.b\n+    __ sve_uzp1(p0, __ H, p0, p1);                     \/\/       uzp1    p0.h, p0.h, p1.h\n+    __ sve_uzp1(p0, __ S, p0, p1);                     \/\/       uzp1    p0.s, p0.s, p1.s\n+    __ sve_uzp1(p0, __ D, p0, p1);                     \/\/       uzp1    p0.d, p0.d, p1.d\n+    __ sve_uzp2(p0, __ B, p0, p1);                     \/\/       uzp2    p0.b, p0.b, p1.b\n+    __ sve_uzp2(p0, __ H, p0, p1);                     \/\/       uzp2    p0.h, p0.h, p1.h\n+    __ sve_uzp2(p0, __ S, p0, p1);                     \/\/       uzp2    p0.s, p0.s, p1.s\n+    __ sve_uzp2(p0, __ D, p0, p1);                     \/\/       uzp2    p0.d, p0.d, p1.d\n+    __ sve_punpklo(p1, p0);                            \/\/       punpklo p1.h, p0.b\n+    __ sve_punpkhi(p1, p0);                            \/\/       punpkhi p1.h, p0.b\n@@ -984,34 +1024,38 @@\n-    __ sve_asr(z26, __ H, p5, z28);                    \/\/       asr     z26.h, p5\/m, z26.h, z28.h\n-    __ sve_cnt(z13, __ D, p7, z16);                    \/\/       cnt     z13.d, p7\/m, z16.d\n-    __ sve_lsl(z5, __ H, p0, z13);                     \/\/       lsl     z5.h, p0\/m, z5.h, z13.h\n-    __ sve_lsr(z15, __ S, p2, z26);                    \/\/       lsr     z15.s, p2\/m, z15.s, z26.s\n-    __ sve_mul(z11, __ S, p1, z22);                    \/\/       mul     z11.s, p1\/m, z11.s, z22.s\n-    __ sve_neg(z4, __ S, p0, z19);                     \/\/       neg     z4.s, p0\/m, z19.s\n-    __ sve_not(z17, __ H, p3, z14);                    \/\/       not     z17.h, p3\/m, z14.h\n-    __ sve_smax(z2, __ S, p4, z3);                     \/\/       smax    z2.s, p4\/m, z2.s, z3.s\n-    __ sve_smin(z23, __ B, p1, z6);                    \/\/       smin    z23.b, p1\/m, z23.b, z6.b\n-    __ sve_sub(z17, __ S, p3, z27);                    \/\/       sub     z17.s, p3\/m, z17.s, z27.s\n-    __ sve_fabs(z16, __ D, p1, z2);                    \/\/       fabs    z16.d, p1\/m, z2.d\n-    __ sve_fadd(z3, __ D, p1, z6);                     \/\/       fadd    z3.d, p1\/m, z3.d, z6.d\n-    __ sve_fdiv(z19, __ D, p3, z12);                   \/\/       fdiv    z19.d, p3\/m, z19.d, z12.d\n-    __ sve_fmax(z8, __ D, p6, z19);                    \/\/       fmax    z8.d, p6\/m, z8.d, z19.d\n-    __ sve_fmin(z0, __ S, p2, z23);                    \/\/       fmin    z0.s, p2\/m, z0.s, z23.s\n-    __ sve_fmul(z19, __ D, p7, z13);                   \/\/       fmul    z19.d, p7\/m, z19.d, z13.d\n-    __ sve_fneg(z6, __ S, p0, z7);                     \/\/       fneg    z6.s, p0\/m, z7.s\n-    __ sve_frintm(z17, __ S, p6, z8);                  \/\/       frintm  z17.s, p6\/m, z8.s\n-    __ sve_frintn(z22, __ D, p5, z22);                 \/\/       frintn  z22.d, p5\/m, z22.d\n-    __ sve_frintp(z2, __ D, p0, z15);                  \/\/       frintp  z2.d, p0\/m, z15.d\n-    __ sve_fsqrt(z20, __ D, p1, z4);                   \/\/       fsqrt   z20.d, p1\/m, z4.d\n-    __ sve_fsub(z7, __ D, p0, z8);                     \/\/       fsub    z7.d, p0\/m, z7.d, z8.d\n-    __ sve_fmla(z19, __ S, p5, z4, z15);               \/\/       fmla    z19.s, p5\/m, z4.s, z15.s\n-    __ sve_fmls(z22, __ D, p2, z25, z5);               \/\/       fmls    z22.d, p2\/m, z25.d, z5.d\n-    __ sve_fnmla(z16, __ S, p3, z22, z11);             \/\/       fnmla   z16.s, p3\/m, z22.s, z11.s\n-    __ sve_fnmls(z13, __ D, p2, z20, z16);             \/\/       fnmls   z13.d, p2\/m, z20.d, z16.d\n-    __ sve_mla(z15, __ H, p1, z4, z17);                \/\/       mla     z15.h, p1\/m, z4.h, z17.h\n-    __ sve_mls(z6, __ S, p7, z4, z28);                 \/\/       mls     z6.s, p7\/m, z4.s, z28.s\n-    __ sve_and(z29, z26, z9);                          \/\/       and     z29.d, z26.d, z9.d\n-    __ sve_eor(z2, z11, z28);                          \/\/       eor     z2.d, z11.d, z28.d\n-    __ sve_orr(z7, z1, z26);                           \/\/       orr     z7.d, z1.d, z26.d\n-    __ sve_bic(z17, z14, z8);                          \/\/       bic     z17.d, z14.d, z8.d\n-    __ sve_uzp1(z21, __ S, z24, z5);                   \/\/       uzp1    z21.s, z24.s, z5.s\n-    __ sve_uzp2(z21, __ S, z17, z22);                  \/\/       uzp2    z21.s, z17.s, z22.s\n+    __ sve_and(z26, __ H, p5, z28);                    \/\/       and     z26.h, p5\/m, z26.h, z28.h\n+    __ sve_asr(z13, __ D, p7, z16);                    \/\/       asr     z13.d, p7\/m, z13.d, z16.d\n+    __ sve_cnt(z5, __ H, p0, z13);                     \/\/       cnt     z5.h, p0\/m, z13.h\n+    __ sve_eor(z15, __ S, p2, z26);                    \/\/       eor     z15.s, p2\/m, z15.s, z26.s\n+    __ sve_lsl(z11, __ S, p1, z22);                    \/\/       lsl     z11.s, p1\/m, z11.s, z22.s\n+    __ sve_lsr(z4, __ S, p0, z19);                     \/\/       lsr     z4.s, p0\/m, z4.s, z19.s\n+    __ sve_mul(z17, __ H, p3, z14);                    \/\/       mul     z17.h, p3\/m, z17.h, z14.h\n+    __ sve_neg(z2, __ S, p4, z3);                      \/\/       neg     z2.s, p4\/m, z3.s\n+    __ sve_not(z23, __ B, p1, z6);                     \/\/       not     z23.b, p1\/m, z6.b\n+    __ sve_orr(z17, __ S, p3, z27);                    \/\/       orr     z17.s, p3\/m, z17.s, z27.s\n+    __ sve_smax(z16, __ D, p1, z2);                    \/\/       smax    z16.d, p1\/m, z16.d, z2.d\n+    __ sve_smin(z3, __ S, p1, z6);                     \/\/       smin    z3.s, p1\/m, z3.s, z6.s\n+    __ sve_sub(z19, __ S, p3, z12);                    \/\/       sub     z19.s, p3\/m, z19.s, z12.s\n+    __ sve_fabs(z8, __ D, p6, z19);                    \/\/       fabs    z8.d, p6\/m, z19.d\n+    __ sve_fadd(z0, __ S, p2, z23);                    \/\/       fadd    z0.s, p2\/m, z0.s, z23.s\n+    __ sve_fdiv(z19, __ D, p7, z13);                   \/\/       fdiv    z19.d, p7\/m, z19.d, z13.d\n+    __ sve_fmax(z6, __ S, p0, z7);                     \/\/       fmax    z6.s, p0\/m, z6.s, z7.s\n+    __ sve_fmin(z17, __ S, p6, z8);                    \/\/       fmin    z17.s, p6\/m, z17.s, z8.s\n+    __ sve_fmul(z22, __ D, p5, z22);                   \/\/       fmul    z22.d, p5\/m, z22.d, z22.d\n+    __ sve_fneg(z2, __ D, p0, z15);                    \/\/       fneg    z2.d, p0\/m, z15.d\n+    __ sve_frintm(z20, __ D, p1, z4);                  \/\/       frintm  z20.d, p1\/m, z4.d\n+    __ sve_frintn(z7, __ D, p0, z8);                   \/\/       frintn  z7.d, p0\/m, z8.d\n+    __ sve_frintp(z19, __ D, p5, z4);                  \/\/       frintp  z19.d, p5\/m, z4.d\n+    __ sve_fsqrt(z9, __ D, p5, z11);                   \/\/       fsqrt   z9.d, p5\/m, z11.d\n+    __ sve_fsub(z5, __ S, p7, z16);                    \/\/       fsub    z5.s, p7\/m, z5.s, z16.s\n+    __ sve_fmad(z22, __ S, p3, z1, z13);               \/\/       fmad    z22.s, p3\/m, z1.s, z13.s\n+    __ sve_fmla(z20, __ S, p4, z25, z15);              \/\/       fmla    z20.s, p4\/m, z25.s, z15.s\n+    __ sve_fmls(z4, __ D, p4, z8, z6);                 \/\/       fmls    z4.d, p4\/m, z8.d, z6.d\n+    __ sve_fnmla(z4, __ D, p7, z16, z29);              \/\/       fnmla   z4.d, p7\/m, z16.d, z29.d\n+    __ sve_fnmls(z9, __ D, p3, z2, z11);               \/\/       fnmls   z9.d, p3\/m, z2.d, z11.d\n+    __ sve_mla(z3, __ S, p1, z1, z26);                 \/\/       mla     z3.s, p1\/m, z1.s, z26.s\n+    __ sve_mls(z17, __ S, p3, z8, z17);                \/\/       mls     z17.s, p3\/m, z8.s, z17.s\n+    __ sve_and(z24, z5, z19);                          \/\/       and     z24.d, z5.d, z19.d\n+    __ sve_eor(z17, z22, z16);                         \/\/       eor     z17.d, z22.d, z16.d\n+    __ sve_orr(z20, z19, z0);                          \/\/       orr     z20.d, z19.d, z0.d\n+    __ sve_bic(z17, z23, z4);                          \/\/       bic     z17.d, z23.d, z4.d\n+    __ sve_uzp1(z4, __ S, z23, z25);                   \/\/       uzp1    z4.s, z23.s, z25.s\n+    __ sve_uzp2(z2, __ H, z8, z8);                     \/\/       uzp2    z2.h, z8.h, z8.h\n@@ -1020,9 +1064,9 @@\n-    __ sve_andv(v29, __ B, p5, z19);                   \/\/       andv b29, p5, z19.b\n-    __ sve_orv(v4, __ B, p4, z23);                     \/\/       orv b4, p4, z23.b\n-    __ sve_eorv(v19, __ D, p1, z23);                   \/\/       eorv d19, p1, z23.d\n-    __ sve_smaxv(v19, __ H, p0, z8);                   \/\/       smaxv h19, p0, z8.h\n-    __ sve_sminv(v14, __ D, p6, z17);                  \/\/       sminv d14, p6, z17.d\n-    __ sve_fminv(v21, __ S, p1, z30);                  \/\/       fminv s21, p1, z30.s\n-    __ sve_fmaxv(v10, __ S, p5, z12);                  \/\/       fmaxv s10, p5, z12.s\n-    __ sve_fadda(v9, __ D, p1, z24);                   \/\/       fadda d9, p1, d9, z24.d\n-    __ sve_uaddv(v4, __ H, p6, z6);                    \/\/       uaddv d4, p6, z6.h\n+    __ sve_andv(v24, __ S, p4, z30);                   \/\/       andv s24, p4, z30.s\n+    __ sve_orv(v4, __ H, p7, z1);                      \/\/       orv h4, p7, z1.h\n+    __ sve_eorv(v19, __ H, p3, z0);                    \/\/       eorv h19, p3, z0.h\n+    __ sve_smaxv(v7, __ B, p6, z17);                   \/\/       smaxv b7, p6, z17.b\n+    __ sve_sminv(v27, __ D, p1, z9);                   \/\/       sminv d27, p1, z9.d\n+    __ sve_fminv(v23, __ D, p3, z16);                  \/\/       fminv d23, p3, z16.d\n+    __ sve_fmaxv(v22, __ D, p5, z20);                  \/\/       fmaxv d22, p5, z20.d\n+    __ sve_fadda(v28, __ D, p2, z13);                  \/\/       fadda d28, p2, d28, z13.d\n+    __ sve_uaddv(v7, __ H, p5, z28);                   \/\/       uaddv d7, p5, z28.h\n@@ -1047,7 +1091,7 @@\n-    0x14000000,     0x17ffffd7,     0x1400034e,     0x94000000,\n-    0x97ffffd4,     0x9400034b,     0x3400000a,     0x34fffa2a,\n-    0x3400690a,     0x35000008,     0x35fff9c8,     0x350068a8,\n-    0xb400000b,     0xb4fff96b,     0xb400684b,     0xb500001d,\n-    0xb5fff91d,     0xb50067fd,     0x10000013,     0x10fff8b3,\n-    0x10006793,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36306716,     0x3758000c,     0x375ff7cc,     0x375866ac,\n+    0x14000000,     0x17ffffd7,     0x1400037a,     0x94000000,\n+    0x97ffffd4,     0x94000377,     0x3400000a,     0x34fffa2a,\n+    0x34006e8a,     0x35000008,     0x35fff9c8,     0x35006e28,\n+    0xb400000b,     0xb4fff96b,     0xb4006dcb,     0xb500001d,\n+    0xb5fff91d,     0xb5006d7d,     0x10000013,     0x10fff8b3,\n+    0x10006d13,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36306c96,     0x3758000c,     0x375ff7cc,     0x37586c2c,\n@@ -1058,13 +1102,13 @@\n-    0x54006480,     0x54000001,     0x54fff541,     0x54006421,\n-    0x54000002,     0x54fff4e2,     0x540063c2,     0x54000002,\n-    0x54fff482,     0x54006362,     0x54000003,     0x54fff423,\n-    0x54006303,     0x54000003,     0x54fff3c3,     0x540062a3,\n-    0x54000004,     0x54fff364,     0x54006244,     0x54000005,\n-    0x54fff305,     0x540061e5,     0x54000006,     0x54fff2a6,\n-    0x54006186,     0x54000007,     0x54fff247,     0x54006127,\n-    0x54000008,     0x54fff1e8,     0x540060c8,     0x54000009,\n-    0x54fff189,     0x54006069,     0x5400000a,     0x54fff12a,\n-    0x5400600a,     0x5400000b,     0x54fff0cb,     0x54005fab,\n-    0x5400000c,     0x54fff06c,     0x54005f4c,     0x5400000d,\n-    0x54fff00d,     0x54005eed,     0x5400000e,     0x54ffefae,\n-    0x54005e8e,     0x5400000f,     0x54ffef4f,     0x54005e2f,\n+    0x54006a00,     0x54000001,     0x54fff541,     0x540069a1,\n+    0x54000002,     0x54fff4e2,     0x54006942,     0x54000002,\n+    0x54fff482,     0x540068e2,     0x54000003,     0x54fff423,\n+    0x54006883,     0x54000003,     0x54fff3c3,     0x54006823,\n+    0x54000004,     0x54fff364,     0x540067c4,     0x54000005,\n+    0x54fff305,     0x54006765,     0x54000006,     0x54fff2a6,\n+    0x54006706,     0x54000007,     0x54fff247,     0x540066a7,\n+    0x54000008,     0x54fff1e8,     0x54006648,     0x54000009,\n+    0x54fff189,     0x540065e9,     0x5400000a,     0x54fff12a,\n+    0x5400658a,     0x5400000b,     0x54fff0cb,     0x5400652b,\n+    0x5400000c,     0x54fff06c,     0x540064cc,     0x5400000d,\n+    0x54fff00d,     0x5400646d,     0x5400000e,     0x54ffefae,\n+    0x5400640e,     0x5400000f,     0x54ffef4f,     0x540063af,\n@@ -1102,1 +1146,1 @@\n-    0xbd1b1869,     0x58004e7b,     0x1800000b,     0xf8945060,\n+    0xbd1b1869,     0x580053fb,     0x1800000b,     0xf8945060,\n@@ -1195,24 +1239,34 @@\n-    0x042053ff,     0x047f5401,     0x25208028,     0x2538cfe0,\n-    0x2578d001,     0x25b8efe2,     0x25f8f007,     0x2538dfea,\n-    0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,     0xa4484be0,\n-    0xa467afe0,     0xa4a8a7ea,     0xa547a814,     0xa4084ffe,\n-    0xa55c53e0,     0xa5e1540b,     0xe400fbf6,     0xe408ffff,\n-    0xe420e7e0,     0xe4484be0,     0xe460efe0,     0xe547e400,\n-    0xe4014be0,     0xe4a84fe0,     0xe5f15000,     0x858043e0,\n-    0x85a043ff,     0xe59f5d08,     0x0420e3e9,     0x0460e3ea,\n-    0x04a0e3eb,     0x04e0e3ec,     0x25104042,     0x25104871,\n-    0x25904861,     0x25904c92,     0x05344020,     0x05744041,\n-    0x05b44062,     0x05f44083,     0x252c8840,     0x253c1420,\n-    0x25681572,     0x25a21ce3,     0x25ea1e34,     0x0522c020,\n-    0x05e6c0a4,     0x2401a001,     0x2443a051,     0x24858881,\n-    0x24c78cd1,     0x24850891,     0x24c70cc1,     0x250f9001,\n-    0x25508051,     0x25802491,     0x25df28c1,     0x25850c81,\n-    0x251e10d1,     0x65816001,     0x65c36051,     0x65854891,\n-    0x65c74cc1,     0x05733820,     0x05b238a4,     0x05f138e6,\n-    0x0570396a,     0x65d0a001,     0x65d6a443,     0x65d4a826,\n-    0x6594ac26,     0x6554ac26,     0x6556ac26,     0x6552ac26,\n-    0x65cbac85,     0x65caac01,     0x65dea833,     0x659ca509,\n-    0x65d8a801,     0x65dcac01,     0x655cb241,     0x0520a1e0,\n-    0x0521a601,     0x052281e0,     0x05238601,     0x04a14026,\n-    0x0568aca7,     0x05b23230,     0x853040af,     0xc5b040af,\n-    0xe57080af,     0xe5b080af,     0x1e601000,     0x1e603000,\n+    0x04038100,     0x040381a0,     0x040387e1,     0x04438be2,\n+    0x04c38fe3,     0x040181e0,     0x04018100,     0x04018621,\n+    0x04418b22,     0x04418822,     0x04818c23,     0x040081e0,\n+    0x04008120,     0x04008761,     0x04008621,     0x04408822,\n+    0x04808c23,     0x042053ff,     0x047f5401,     0x25208028,\n+    0x2538cfe0,     0x2578d001,     0x25b8efe2,     0x25f8f007,\n+    0x2538dfea,     0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,\n+    0xa4484be0,     0xa467afe0,     0xa4a8a7ea,     0xa547a814,\n+    0xa4084ffe,     0xa55c53e0,     0xa5e1540b,     0xe400fbf6,\n+    0xe408ffff,     0xe420e7e0,     0xe4484be0,     0xe460efe0,\n+    0xe547e400,     0xe4014be0,     0xe4a84fe0,     0xe5f15000,\n+    0x858043e0,     0x85a043ff,     0xe59f5d08,     0x0420e3e9,\n+    0x0460e3ea,     0x04a0e3eb,     0x04e0e3ec,     0x25104042,\n+    0x25104871,     0x25904861,     0x25904c92,     0x05344020,\n+    0x05744041,     0x05b44062,     0x05f44083,     0x252c8840,\n+    0x253c1420,     0x25681572,     0x25a21ce3,     0x25ea1e34,\n+    0x0522c020,     0x05e6c0a4,     0x2401a001,     0x2443a051,\n+    0x24858881,     0x24c78cd1,     0x24850891,     0x24c70cc1,\n+    0x250f9001,     0x25508051,     0x25802491,     0x25df28c1,\n+    0x25850c81,     0x251e10d1,     0x65816001,     0x65c36051,\n+    0x65854891,     0x65c74cc1,     0x05733820,     0x05b238a4,\n+    0x05f138e6,     0x0570396a,     0x65d0a001,     0x65d6a443,\n+    0x65d4a826,     0x6594ac26,     0x6554ac26,     0x6556ac26,\n+    0x6552ac26,     0x65cbac85,     0x65caac01,     0x65dea833,\n+    0x659ca509,     0x65d8a801,     0x65dcac01,     0x655cb241,\n+    0x0520a1e0,     0x0521a601,     0x052281e0,     0x05238601,\n+    0x04a14026,     0x0568aca7,     0x05b23230,     0x853040af,\n+    0xc5b040af,     0xe57080af,     0xe5b080af,     0x25034440,\n+    0x254054c4,     0x25034640,     0x25415a05,     0x25834440,\n+    0x25c54489,     0x250b5d3a,     0x2550dc20,     0x2518e3e1,\n+    0x2558e3e2,     0x2598e3e3,     0x25d8e3e4,     0x2518e407,\n+    0x05214800,     0x05614800,     0x05a14800,     0x05e14800,\n+    0x05214c00,     0x05614c00,     0x05a14c00,     0x05e14c00,\n+    0x05304001,     0x05314001,     0x1e601000,     0x1e603000,\n@@ -1248,11 +1302,12 @@\n-    0x04001f8b,     0x0450979a,     0x04dabe0d,     0x045381a5,\n-    0x04918b4f,     0x049006cb,     0x0497a264,     0x045eadd1,\n-    0x04881062,     0x040a04d7,     0x04810f71,     0x04dca450,\n-    0x65c084c3,     0x65cd8d93,     0x65c69a68,     0x65878ae0,\n-    0x65c29db3,     0x049da0e6,     0x6582b911,     0x65c0b6d6,\n-    0x65c1a1e2,     0x65cda494,     0x65c18107,     0x65af1493,\n-    0x65e52b36,     0x65ab4ed0,     0x65f06a8d,     0x0451448f,\n-    0x049c7c86,     0x0429335d,     0x04bc3162,     0x047a3027,\n-    0x04e831d1,     0x05a56b15,     0x05b66e35,     0x041a367d,\n-    0x041832e4,     0x04d926f3,     0x04482113,     0x04ca3a2e,\n-    0x658727d5,     0x6586358a,     0x65d82709,     0x044138c4,\n+    0x04001f8b,     0x045a179a,     0x04d09e0d,     0x045aa1a5,\n+    0x04990b4f,     0x049386cb,     0x04918264,     0x04500dd1,\n+    0x0497b062,     0x041ea4d7,     0x04980f71,     0x04c80450,\n+    0x048a04c3,     0x04810d93,     0x04dcba68,     0x65808ae0,\n+    0x65cd9db3,     0x658680e6,     0x65879911,     0x65c296d6,\n+    0x04dda1e2,     0x65c2a494,     0x65c0a107,     0x65c1b493,\n+    0x65cdb569,     0x65819e05,     0x65ad8c36,     0x65af1334,\n+    0x65e63104,     0x65fd5e04,     0x65eb6c49,     0x049a4423,\n+    0x04916d11,     0x043330b8,     0x04b032d1,     0x04603274,\n+    0x04e432f1,     0x05b96ae4,     0x05686d02,     0x049a33d8,\n+    0x04583c24,     0x04592c13,     0x04083a27,     0x04ca253b,\n+    0x65c72e17,     0x65c63696,     0x65d829bc,     0x04413787,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":154,"deletions":99,"binary":false,"changes":253,"status":"modified"},{"patch":"@@ -0,0 +1,468 @@\n+\/*\n+ * Copyright (c) 2021, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.util.Random;\n+\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.DoubleVector;\n+import jdk.incubator.vector.FloatVector;\n+import jdk.incubator.vector.IntVector;\n+import jdk.incubator.vector.LongVector;\n+import jdk.incubator.vector.ShortVector;\n+import jdk.incubator.vector.VectorMask;\n+import jdk.test.lib.Utils;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+\/**\n+ * @test\n+ * @bug 8273264\n+ * @key randomness\n+ * @library \/test\/lib\n+ * @summary AArch64: [vector] Add missing rules for VectorMaskCast\n+ * @modules jdk.incubator.vector\n+ *\n+ * @run testng\/othervm -XX:-TieredCompilation -XX:CompileThreshold=100 compiler.vectorapi.VectorMaskCastTest\n+ *\/\n+\n+\n+\/\/ Current vector mask cast test cases at test\/jdk\/jdk\/incubator\/vector\/*ConversionTests.java\n+\/\/ could not be intrinsfied, hence not able to verify compiler codegen, see [1]. As a\n+\/\/ supplement, we add more tests for vector mask cast operations, which could be intrinsified\n+\/\/ by c2 compiler to generate vector\/mask instructions on supported targets.\n+\/\/\n+\/\/ [1] https:\/\/bugs.openjdk.java.net\/browse\/JDK-8259610\n+\n+public class VectorMaskCastTest{\n+\n+    private static final int NUM_ITER = 5000;\n+    private static final Random rd = Utils.getRandomInstance();\n+\n+    public static boolean[] genMask() {\n+        boolean[] mask = new boolean[64];\n+        for (int i = 0; i < 64; i ++) {\n+            mask[i] = rd.nextBoolean();\n+        }\n+        return mask;\n+    }\n+\n+    \/\/ Byte\n+    private static void testByte64ToShort128(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(ShortVector.SPECIES_128).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToInt256(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(IntVector.SPECIES_256).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(FloatVector.SPECIES_256).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToLong512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(LongVector.SPECIES_512).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(DoubleVector.SPECIES_512).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte128ToShort256(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte128 = VectorMask.fromArray(ByteVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mByte128.cast(ShortVector.SPECIES_256).toString(), mByte128.toString());\n+    }\n+\n+    private static void testByte128ToInt512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte128 = VectorMask.fromArray(ByteVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mByte128.cast(IntVector.SPECIES_512).toString(), mByte128.toString());\n+    }\n+\n+    private static void testByte128ToFloat512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte128 = VectorMask.fromArray(ByteVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mByte128.cast(FloatVector.SPECIES_512).toString(), mByte128.toString());\n+    }\n+\n+    private static void testByte256ToShort512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte256 = VectorMask.fromArray(ByteVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mByte256.cast(ShortVector.SPECIES_512).toString(), mByte256.toString());\n+    }\n+\n+    \/\/ Short\n+    private static void testShort64ToInt128(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(IntVector.SPECIES_128).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort64ToFloat128(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(FloatVector.SPECIES_128).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort64ToLong256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(LongVector.SPECIES_256).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort64ToDouble256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(DoubleVector.SPECIES_256).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort128ToByte64(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(ByteVector.SPECIES_64).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToInt256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(IntVector.SPECIES_256).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(FloatVector.SPECIES_256).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToLong512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(LongVector.SPECIES_512).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(DoubleVector.SPECIES_512).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort256ToByte128(boolean[] mask_arr) {\n+        VectorMask<Short> mShort256 = VectorMask.fromArray(ShortVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mShort256.cast(ByteVector.SPECIES_128).toString(), mShort256.toString());\n+    }\n+\n+    private static void testShort256ToInt512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort256 = VectorMask.fromArray(ShortVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mShort256.cast(IntVector.SPECIES_512).toString(), mShort256.toString());\n+    }\n+\n+    private static void testShort256ToFloat512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort256 = VectorMask.fromArray(ShortVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mShort256.cast(FloatVector.SPECIES_512).toString(), mShort256.toString());\n+    }\n+\n+    private static void testShort512ToByte256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort512 = VectorMask.fromArray(ShortVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mShort512.cast(ByteVector.SPECIES_256).toString(), mShort512.toString());\n+    }\n+\n+    \/\/ Int\n+    private static void testInt64ToLong128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt64 = VectorMask.fromArray(IntVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mInt64.cast(LongVector.SPECIES_128).toString(), mInt64.toString());\n+    }\n+\n+    private static void testInt64ToDouble128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt64 = VectorMask.fromArray(IntVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mInt64.cast(DoubleVector.SPECIES_128).toString(), mInt64.toString());\n+    }\n+\n+    private static void testInt128ToShort64(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt128 = VectorMask.fromArray(IntVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mInt128.cast(ShortVector.SPECIES_64).toString(), mInt128.toString());\n+    }\n+\n+    private static void testInt128ToLong256(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt128 = VectorMask.fromArray(IntVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mInt128.cast(LongVector.SPECIES_256).toString(), mInt128.toString());\n+    }\n+\n+    private static void testInt128ToDouble256(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt128 = VectorMask.fromArray(IntVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mInt128.cast(DoubleVector.SPECIES_256).toString(), mInt128.toString());\n+    }\n+\n+    private static void testInt256ToShort128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(ShortVector.SPECIES_128).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt256ToByte64(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(ByteVector.SPECIES_64).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt256ToLong512(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(LongVector.SPECIES_512).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt256ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(DoubleVector.SPECIES_512).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt512ToShort256(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt512 = VectorMask.fromArray(IntVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mInt512.cast(ShortVector.SPECIES_256).toString(), mInt512.toString());\n+    }\n+\n+    private static void testInt512ToByte128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt512 = VectorMask.fromArray(IntVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mInt512.cast(ByteVector.SPECIES_128).toString(), mInt512.toString());\n+    }\n+\n+    \/\/ Float\n+    private static void testFloat64ToLong128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat64 = VectorMask.fromArray(FloatVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mFloat64.cast(LongVector.SPECIES_128).toString(), mFloat64.toString());\n+    }\n+\n+    private static void testFloat64ToDouble128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat64 = VectorMask.fromArray(FloatVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mFloat64.cast(DoubleVector.SPECIES_128).toString(), mFloat64.toString());\n+    }\n+\n+    private static void testFloat128ToShort64(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat128 = VectorMask.fromArray(FloatVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mFloat128.cast(ShortVector.SPECIES_64).toString(), mFloat128.toString());\n+    }\n+\n+    private static void testFloat128ToLong256(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat128 = VectorMask.fromArray(FloatVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mFloat128.cast(LongVector.SPECIES_256).toString(), mFloat128.toString());\n+    }\n+\n+    private static void testFloat128ToDouble256(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat128 = VectorMask.fromArray(FloatVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mFloat128.cast(DoubleVector.SPECIES_256).toString(), mFloat128.toString());\n+    }\n+\n+    private static void testFloat256ToShort128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(ShortVector.SPECIES_128).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat256ToByte64(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(ByteVector.SPECIES_64).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat256ToLong512(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(LongVector.SPECIES_512).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat256ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(DoubleVector.SPECIES_512).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat512ToShort256(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat512 = VectorMask.fromArray(FloatVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mFloat512.cast(ShortVector.SPECIES_256).toString(), mFloat512.toString());\n+    }\n+\n+    private static void testFloat512ToByte128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat512 = VectorMask.fromArray(FloatVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mFloat512.cast(ByteVector.SPECIES_128).toString(), mFloat512.toString());\n+    }\n+\n+    \/\/ Long\n+    private static void testLong128ToInt64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong128 = VectorMask.fromArray(LongVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mLong128.cast(IntVector.SPECIES_64).toString(), mLong128.toString());\n+    }\n+\n+    private static void testLong128ToFloat64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong128 = VectorMask.fromArray(LongVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mLong128.cast(FloatVector.SPECIES_64).toString(), mLong128.toString());\n+    }\n+\n+    private static void testLong256ToInt128(boolean[] mask_arr) {\n+        VectorMask<Long> mLong256 = VectorMask.fromArray(LongVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mLong256.cast(IntVector.SPECIES_128).toString(), mLong256.toString());\n+    }\n+\n+    private static void testLong256ToFloat128(boolean[] mask_arr) {\n+        VectorMask<Long> mLong256 = VectorMask.fromArray(LongVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mLong256.cast(FloatVector.SPECIES_128).toString(), mLong256.toString());\n+    }\n+\n+    private static void testLong256ToShort64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong256 = VectorMask.fromArray(LongVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mLong256.cast(ShortVector.SPECIES_64).toString(), mLong256.toString());\n+    }\n+\n+    private static void testLong512ToInt256(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(IntVector.SPECIES_256).toString(), mLong512.toString());\n+    }\n+\n+    private static void testLong512ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(FloatVector.SPECIES_256).toString(), mLong512.toString());\n+    }\n+\n+    private static void testLong512ToShort128(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(ShortVector.SPECIES_128).toString(), mLong512.toString());\n+    }\n+\n+    private static void testLong512ToByte64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(ByteVector.SPECIES_64).toString(), mLong512.toString());\n+    }\n+\n+    \/\/ Double\n+    private static void testDouble128ToInt64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble128 = VectorMask.fromArray(DoubleVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mDouble128.cast(IntVector.SPECIES_64).toString(), mDouble128.toString());\n+    }\n+\n+    private static void testDouble128ToFloat64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble128 = VectorMask.fromArray(DoubleVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mDouble128.cast(FloatVector.SPECIES_64).toString(), mDouble128.toString());\n+    }\n+\n+    private static void testDouble256ToInt128(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble256 = VectorMask.fromArray(DoubleVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mDouble256.cast(IntVector.SPECIES_128).toString(), mDouble256.toString());\n+    }\n+\n+    private static void testDouble256ToFloat128(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble256 = VectorMask.fromArray(DoubleVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mDouble256.cast(FloatVector.SPECIES_128).toString(), mDouble256.toString());\n+    }\n+\n+    private static void testDouble256ToShort64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble256 = VectorMask.fromArray(DoubleVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mDouble256.cast(ShortVector.SPECIES_64).toString(), mDouble256.toString());\n+    };\n+\n+    private static void testDouble512ToInt256(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(IntVector.SPECIES_256).toString(), mDouble512.toString());\n+    }\n+\n+    private static void testDouble512ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(FloatVector.SPECIES_256).toString(), mDouble512.toString());\n+    }\n+\n+    private static void testDouble512ToShort128(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(ShortVector.SPECIES_128).toString(), mDouble512.toString());\n+    }\n+\n+    private static void testDouble512ToByte64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(ByteVector.SPECIES_64).toString(), mDouble512.toString());\n+    }\n+\n+\n+    @Test\n+    public static void testMaskCast() {\n+        for (int i = 0; i < NUM_ITER; i++) {\n+            boolean[] mask = genMask();\n+            \/\/ Byte\n+            testByte64ToShort128(mask);\n+            testByte64ToInt256(mask);\n+            testByte64ToFloat256(mask);\n+            testByte64ToLong512(mask);\n+            testByte64ToDouble512(mask);\n+            testByte128ToShort256(mask);\n+            testByte128ToInt512(mask);\n+            testByte128ToFloat512(mask);\n+            testByte256ToShort512(mask);\n+\n+            \/\/ Short\n+            testShort64ToInt128(mask);\n+            testShort64ToFloat128(mask);\n+            testShort64ToLong256(mask);\n+            testShort64ToDouble256(mask);\n+            testShort128ToByte64(mask);\n+            testShort128ToInt256(mask);\n+            testShort128ToFloat256(mask);\n+            testShort128ToLong512(mask);\n+            testShort128ToDouble512(mask);\n+            testShort256ToByte128(mask);\n+            testShort256ToInt512(mask);\n+            testShort256ToFloat512(mask);\n+            testShort512ToByte256(mask);\n+\n+            \/\/ Int\n+            testInt64ToLong128(mask);\n+            testInt64ToDouble128(mask);\n+            testInt128ToShort64(mask);\n+            testInt128ToLong256(mask);\n+            testInt128ToDouble256(mask);\n+            testInt256ToShort128(mask);\n+            testInt256ToByte64(mask);\n+            testInt256ToLong512(mask);\n+            testInt256ToDouble512(mask);\n+            testInt512ToShort256(mask);\n+            testInt512ToByte128(mask);\n+\n+            \/\/ Float\n+            testFloat64ToLong128(mask);\n+            testFloat64ToDouble128(mask);\n+            testFloat128ToShort64(mask);\n+            testFloat128ToLong256(mask);\n+            testFloat128ToDouble256(mask);\n+            testFloat256ToShort128(mask);\n+            testFloat256ToByte64(mask);\n+            testFloat256ToLong512(mask);\n+            testFloat256ToDouble512(mask);\n+            testFloat512ToShort256(mask);\n+            testFloat512ToByte128(mask);\n+\n+            \/\/ Long\n+            testLong128ToInt64(mask);\n+            testLong128ToFloat64(mask);\n+            testLong256ToInt128(mask);\n+            testLong256ToFloat128(mask);\n+            testLong256ToShort64(mask);\n+            testLong512ToInt256(mask);\n+            testLong512ToFloat256(mask);\n+            testLong512ToShort128(mask);\n+            testLong512ToByte64(mask);\n+\n+            \/\/ Double\n+            testDouble128ToInt64(mask);\n+            testDouble128ToFloat64(mask);\n+            testDouble256ToInt128(mask);\n+            testDouble256ToFloat128(mask);\n+            testDouble256ToShort64(mask);\n+            testDouble512ToInt256(mask);\n+            testDouble512ToFloat256(mask);\n+            testDouble512ToShort128(mask);\n+            testDouble512ToByte64(mask);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskCastTest.java","additions":468,"deletions":0,"binary":false,"changes":468,"status":"added"},{"patch":"@@ -0,0 +1,235 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.util.Random;\n+\n+import jdk.incubator.vector.VectorSpecies;\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.DoubleVector;\n+import jdk.incubator.vector.FloatVector;\n+import jdk.incubator.vector.IntVector;\n+import jdk.incubator.vector.LongVector;\n+import jdk.incubator.vector.ShortVector;\n+import jdk.incubator.vector.VectorMask;\n+import jdk.test.lib.Utils;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+\/**\n+ * @test\n+ * @bug 8274569\n+ * @key randomness\n+ * @library \/test\/lib\n+ * @summary Tests X86 backend related incorrectness issues in legacy storemask patterns\n+ * @modules jdk.incubator.vector\n+ *\n+ * @run testng\/othervm -XX:-TieredCompilation -XX:CompileThreshold=100 compiler.vectorapi.VectorMaskLoadStoreTest\n+ *\/\n+\n+\n+public class VectorMaskLoadStoreTest{\n+\n+    private static final int NUM_ITER = 5000;\n+    private static final Random rd = Utils.getRandomInstance();\n+\n+    public static void testByte64(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_64;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testByte128(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_128;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testByte256(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_256;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFFFFFL);\n+    }\n+\n+    public static void testByte512(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_512;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & -1L);\n+    }\n+\n+    public static void testShort64(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_64;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testShort128(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_128;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testShort256(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_256;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testShort512(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_512;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFFFFFL);\n+    }\n+\n+    public static void testInteger64(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_64;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testInteger128(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_128;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testInteger256(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_256;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testInteger512(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_512;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testLong64(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_64;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x1L);\n+    }\n+\n+    public static void testLong128(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_128;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testLong256(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_256;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testLong512(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_512;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testFloat64(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_64;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testFloat128(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_128;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testFloat256(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_256;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testFloat512(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_512;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testDouble64(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_64;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x1L);\n+    }\n+\n+    public static void testDouble128(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_128;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testDouble256(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_256;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testDouble512(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_512;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    @Test\n+    public static void testMaskCast() {\n+        long [] vals = {-1L, 0, rd.nextLong(), rd.nextLong()};\n+        for(int i = 0; i < vals.length; i++) {\n+            long val = vals[i];\n+            for (int ctr = 0; ctr < NUM_ITER; ctr++) {\n+                testByte64(val);\n+                testByte128(val);\n+                testByte256(val);\n+                testByte512(val);\n+                testShort64(val);\n+                testShort128(val);\n+                testShort256(val);\n+                testShort512(val);\n+                testInteger64(val);\n+                testInteger128(val);\n+                testInteger256(val);\n+                testInteger512(val);\n+                testLong64(val);\n+                testLong128(val);\n+                testLong256(val);\n+                testLong512(val);\n+                testFloat64(val);\n+                testFloat128(val);\n+                testFloat256(val);\n+                testFloat512(val);\n+                testDouble64(val);\n+                testDouble128(val);\n+                testDouble256(val);\n+                testDouble512(val);\n+            }\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskLoadStoreTest.java","additions":235,"deletions":0,"binary":false,"changes":235,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ *  Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *  Copyright (c) 2021, Rado Smogura. All rights reserved.\n+ *\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary Test if memory ordering is preserved\n+ *\n+ * @run main\/othervm -XX:-TieredCompilation -XX:+UnlockDiagnosticVMOptions -XX:+AbortVMOnCompilationFailure\n+ *      -XX:CompileThreshold=100 -XX:CompileCommand=dontinline,compiler.vectorapi.VectorMemoryAlias::test\n+ *      compiler.vectorapi.VectorMemoryAlias\n+ * @modules jdk.incubator.vector\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.VectorSpecies;\n+\n+public class VectorMemoryAlias {\n+  public static final VectorSpecies<Byte> SPECIES = VectorSpecies.ofLargestShape(byte.class);\n+  public static void main(String[] args) {\n+    for (int i=0; i < 30000; i++) {\n+      if (test() != 1) {\n+        throw new AssertionError();\n+      }\n+    }\n+  }\n+\n+  public static int test() {\n+    byte arr[] = new byte[256];\n+    final var bb = ByteBuffer.wrap(arr);\n+    final var ones = ByteVector.broadcast(SPECIES, 1);\n+    var res = ByteVector.zero(SPECIES);\n+\n+    int result = 0;\n+    result += arr[2];\n+    res.add(ones).intoByteBuffer(bb, 0, ByteOrder.nativeOrder());\n+    result += arr[2];\n+\n+    return result;\n+  }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMemoryAlias.java","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"}]}