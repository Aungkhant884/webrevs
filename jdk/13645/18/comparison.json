{"files":[{"patch":"@@ -367,0 +367,16 @@\n+  \/\/ handle unaligned access\n+  static inline uint16_t ld_c_instr(address addr) {\n+    return Bytes::get_native_u2(addr);\n+  }\n+  static inline void sd_c_instr(address addr, uint16_t c_instr) {\n+    Bytes::put_native_u2(addr, c_instr);\n+  }\n+\n+  \/\/ handle unaligned access\n+  static inline uint32_t ld_instr(address addr) {\n+    return Bytes::get_native_u4(addr);\n+  }\n+  static inline void sd_instr(address addr, uint32_t instr) {\n+    Bytes::put_native_u4(addr, instr);\n+  }\n+\n@@ -391,1 +407,1 @@\n-    unsigned target = *(unsigned *)a;\n+    unsigned target = ld_instr(a);\n@@ -394,1 +410,1 @@\n-    *(unsigned *)a = target;\n+    sd_instr(a, target);\n@@ -1969,1 +1985,1 @@\n-    uint16_t target = *(uint16_t *)a;\n+    uint16_t target = ld_c_instr(a);\n@@ -1972,1 +1988,1 @@\n-    *(uint16_t *)a = target;\n+    sd_c_instr(a, target);\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1114,2 +1114,4 @@\n-\/\/ comparisons (including the final one, which may overlap) are\n-\/\/ performed 8 bytes at a time.  For strings < 8 bytes, we compare a\n+\/\/ comparisons (for hw supporting unaligned access: including the final one,\n+\/\/ which may overlap) are performed 8 bytes at a time.\n+\/\/ For strings < 8 bytes (and for tails of long strings when\n+\/\/ AvoidUnalignedAccesses is true), we compare a\n@@ -1126,1 +1128,1 @@\n-  assert_different_registers(a1, a2, result, cnt1, t0, t1);\n+  assert_different_registers(a1, a2, result, cnt1, tmp1, tmp2);\n@@ -1130,0 +1132,1 @@\n+  beqz(cnt1, SAME);\n@@ -1144,13 +1147,18 @@\n-  } bgtz(cnt1, NEXT_WORD);\n-\n-  \/\/ Last longword.  In the case where length == 4 we compare the\n-  \/\/ same longword twice, but that's still faster than another\n-  \/\/ conditional branch.\n-  \/\/ cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when\n-  \/\/ length == 4.\n-  add(tmp1, a1, cnt1);\n-  ld(tmp1, Address(tmp1, 0));\n-  add(tmp2, a2, cnt1);\n-  ld(tmp2, Address(tmp2, 0));\n-  bne(tmp1, tmp2, DONE);\n-  j(SAME);\n+  } bgez(cnt1, NEXT_WORD);\n+\n+  if (!AvoidUnalignedAccesses) {\n+    \/\/ Last longword.  In the case where length == 4 we compare the\n+    \/\/ same longword twice, but that's still faster than another\n+    \/\/ conditional branch.\n+    \/\/ cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when\n+    \/\/ length == 4.\n+    add(tmp1, a1, cnt1);\n+    ld(tmp1, Address(tmp1, 0));\n+    add(tmp2, a2, cnt1);\n+    ld(tmp2, Address(tmp2, 0));\n+    bne(tmp1, tmp2, DONE);\n+    j(SAME);\n+  } else {\n+    add(tmp1, cnt1, wordSize);\n+    beqz(tmp1, SAME);\n+  }\n@@ -1162,2 +1170,2 @@\n-  test_bit(t0, cnt1, 2);\n-  beqz(t0, TAIL03);\n+  test_bit(tmp1, cnt1, 2);\n+  beqz(tmp1, TAIL03);\n@@ -1174,2 +1182,2 @@\n-  test_bit(t0, cnt1, 1);\n-  beqz(t0, TAIL01);\n+  test_bit(tmp1, cnt1, 1);\n+  beqz(tmp1, TAIL01);\n@@ -1187,2 +1195,2 @@\n-    test_bit(t0, cnt1, 0);\n-    beqz(t0, SAME);\n+    test_bit(tmp1, cnt1, 0);\n+    beqz(tmp1, SAME);\n@@ -1869,1 +1877,1 @@\n-#undef VFCVT_SAFE\n\\ No newline at end of file\n+#undef VFCVT_SAFE\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":31,"deletions":23,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -146,1 +146,1 @@\n-  intptr_t addr = (intptr_t) instruction_address();\n+  address addr = instruction_address();\n@@ -148,1 +148,1 @@\n-    uint32_t inst = *((uint32_t*) addr);\n+    uint32_t inst = Assembler::ld_instr(addr);\n@@ -150,1 +150,1 @@\n-      msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x not an %s instruction\", addr, inst, barrierInsn[i].name);\n+      msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x not an %s instruction\", p2i(addr), inst, barrierInsn[i].name);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetNMethod_riscv.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -180,2 +180,9 @@\n-  lhu(reg, Address(xbcp, bcp_offset));\n-  revb_h(reg, reg);\n+  if (AvoidUnalignedAccesses && (bcp_offset % 2)) {\n+    lbu(t1, Address(xbcp, bcp_offset));\n+    lbu(reg, Address(xbcp, bcp_offset + 1));\n+    slli(t1, t1, 8);\n+    add(reg, reg, t1);\n+  } else {\n+    lhu(reg, Address(xbcp, bcp_offset));\n+    revb_h_h_u(reg, reg);\n+  }\n@@ -194,0 +201,1 @@\n+                                                       Register tmp,\n@@ -198,1 +206,9 @@\n-    load_unsigned_short(index, Address(xbcp, bcp_offset));\n+    if (AvoidUnalignedAccesses) {\n+      assert_different_registers(index, tmp);\n+      load_unsigned_byte(index, Address(xbcp, bcp_offset));\n+      load_unsigned_byte(tmp, Address(xbcp, bcp_offset + 1));\n+      slli(tmp, tmp, 8);\n+      add(index, index, tmp);\n+    } else {\n+      load_unsigned_short(index, Address(xbcp, bcp_offset));\n+    }\n@@ -200,1 +216,2 @@\n-    lwu(index, Address(xbcp, bcp_offset));\n+    load_int_misaligned(index, Address(xbcp, bcp_offset), tmp, false);\n+\n@@ -227,1 +244,2 @@\n-  get_cache_index_at_bcp(index, bcp_offset, index_size);\n+  \/\/ register \"cache\" is trashed in next shadd, so lets use it as a temporary register\n+  get_cache_index_at_bcp(index, cache, bcp_offset, index_size);\n@@ -264,1 +282,2 @@\n-  get_cache_index_at_bcp(tmp, bcp_offset, index_size);\n+  \/\/ register \"cache\" is trashed in next ld, so lets use it as a temporary register\n+  get_cache_index_at_bcp(tmp, cache, bcp_offset, index_size);\n@@ -1960,1 +1979,2 @@\n-  get_cache_index_at_bcp(index, 1, sizeof(u4));\n+  \/\/ register \"cache\" is trashed in next ld, so lets use it as a temporary register\n+  get_cache_index_at_bcp(index, cache, 1, sizeof(u4));\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":27,"deletions":7,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -140,1 +140,1 @@\n-  void get_cache_index_at_bcp(Register index, int bcp_offset, size_t index_size = sizeof(u2));\n+  void get_cache_index_at_bcp(Register index, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1421,1 +1421,1 @@\n-  unsigned insn = *(unsigned*)insn_addr;\n+  unsigned insn = Assembler::ld_instr(insn_addr);\n@@ -1434,1 +1434,1 @@\n-  unsigned insn = *(unsigned*)insn_addr;\n+  unsigned insn = Assembler::ld_instr(insn_addr);\n@@ -1446,2 +1446,2 @@\n-  offset = ((long)(Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12))) << 12;                                  \/\/ Auipc.\n-  offset += ((long)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20));                                         \/\/ Addi\/Jalr\/Load.\n+  offset = ((long)(Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12))) << 12;                               \/\/ Auipc.\n+  offset += ((long)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20));                                  \/\/ Addi\/Jalr\/Load.\n@@ -1454,4 +1454,4 @@\n-  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 29;    \/\/ Lui.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 17;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 6;                         \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20));                              \/\/ Addi\/Jalr\/Load.\n+  intptr_t target_address = (((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12)) & 0xfffff) << 29; \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20)) << 17;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 12), 31, 20)) << 6;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 20), 31, 20));                      \/\/ Addi\/Jalr\/Load.\n@@ -1463,5 +1463,5 @@\n-  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 44;    \/\/ Lui.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 32;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 20;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20)) << 8;                         \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[7], 31, 20));                              \/\/ Addi.\n+  intptr_t target_address = (((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12)) & 0xfffff) << 44; \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20)) << 32;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 12), 31, 20)) << 20;                \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 20), 31, 20)) << 8;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 28), 31, 20));                      \/\/ Addi.\n@@ -1473,2 +1473,2 @@\n-  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 12;    \/\/ Lui.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20));                              \/\/ Addiw.\n+  intptr_t target_address = (((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12)) & 0xfffff) << 12; \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20));                       \/\/ Addiw.\n@@ -1499,1 +1499,1 @@\n-                  *(unsigned*)branch, p2i(branch));\n+                  Assembler::ld_instr(branch), p2i(branch));\n@@ -1693,0 +1693,87 @@\n+\/\/ granularity is 1, 2 bytes per load\n+void MacroAssembler::load_int_misaligned(Register dst, Address src, Register tmp, bool is_signed, int granularity) {\n+  if (AvoidUnalignedAccesses && (granularity != 4)) {\n+    assert_different_registers(dst, tmp, src.base());\n+    switch(granularity) {\n+      case 1:\n+        lbu(dst, src);\n+        lbu(tmp, Address(src.base(), src.offset() + 1));\n+        slli(tmp, tmp, 8);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        is_signed ? lb(tmp, Address(src.base(), src.offset() + 3)) : lbu(tmp, Address(src.base(), src.offset() + 3));\n+        slli(tmp, tmp, 24);\n+        add(dst, dst, tmp);\n+        break;\n+      case 2:\n+        lhu(dst, src);\n+        is_signed ? lh(tmp, Address(src.base(), src.offset() + 2)) : lhu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    is_signed ? lw(dst, src) : lwu(dst, src);\n+  }\n+}\n+\n+\/\/ granularity is 1, 2 or 4 bytes per load\n+void MacroAssembler::load_long_misaligned(Register dst, Address src, Register tmp, int granularity) {\n+  if (AvoidUnalignedAccesses && (granularity != 8)) {\n+    assert_different_registers(dst, tmp, src.base());\n+    switch(granularity){\n+      case 1:\n+        lbu(dst, src);\n+        lbu(tmp, Address(src.base(), src.offset() + 1));\n+        slli(tmp, tmp, 8);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 3));\n+        slli(tmp, tmp, 24);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 4));\n+        slli(tmp, tmp, 32);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 5));\n+        slli(tmp, tmp, 40);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 6));\n+        slli(tmp, tmp, 48);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 7));\n+        slli(tmp, tmp, 56);\n+        add(dst, dst, tmp);\n+        break;\n+      case 2:\n+        lhu(dst, src);\n+        lhu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        lhu(tmp, Address(src.base(), src.offset() + 4));\n+        slli(tmp, tmp, 32);\n+        add(dst, dst, tmp);\n+        lhu(tmp, Address(src.base(), src.offset() + 6));\n+        slli(tmp, tmp, 48);\n+        add(dst, dst, tmp);\n+        break;\n+      case 4:\n+        lwu(dst, src);\n+        lwu(tmp, Address(src.base(), src.offset() + 4));\n+        slli(tmp, tmp, 32);\n+        add(dst, dst, tmp);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    ld(dst, src);\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":103,"deletions":16,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -433,0 +433,4 @@\n+  \/\/ Misaligned loads, will use the best way, according to the AvoidUnalignedAccess flag\n+  void load_int_misaligned(Register dst, Address src, Register tmp, bool is_signed, int granularity = 1);\n+  void load_long_misaligned(Register dst, Address src, Register tmp, int granularity = 1);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-  return as_Register(Assembler::extract(((unsigned*)instr)[0], 19, 15));\n+  return as_Register(Assembler::extract(Assembler::ld_instr(instr), 19, 15));\n@@ -50,1 +50,1 @@\n-  return as_Register(Assembler::extract(((unsigned*)instr)[0], 24, 20));\n+  return as_Register(Assembler::extract(Assembler::ld_instr(instr), 24, 20));\n@@ -55,1 +55,1 @@\n-  return as_Register(Assembler::extract(((unsigned*)instr)[0], 11, 7));\n+  return as_Register(Assembler::extract(Assembler::ld_instr(instr), 11, 7));\n@@ -60,1 +60,1 @@\n-  return Assembler::extract(((unsigned*)instr)[0], 6, 0);\n+  return Assembler::extract(Assembler::ld_instr(instr), 6, 0);\n@@ -65,1 +65,1 @@\n-  return Assembler::extract(((unsigned*)instr)[0], 14, 12);\n+  return Assembler::extract(Assembler::ld_instr(instr), 14, 12);\n@@ -209,1 +209,1 @@\n-    return *(intptr_t*)addr;\n+    return Bytes::get_native_u8(addr);\n@@ -218,1 +218,1 @@\n-    *(intptr_t*)addr = x;\n+    Bytes::put_native_u8(addr, x);\n@@ -234,1 +234,1 @@\n-        *oop_addr = cast_to_oop(x);\n+        Bytes::put_native_u8((address)oop_addr, x);\n@@ -238,1 +238,1 @@\n-        *metadata_addr = (Metadata*)x;\n+        Bytes::put_native_u8((address)metadata_addr, x);\n@@ -346,1 +346,1 @@\n-  *(juint*)code_pos = 0xffffffff; \/\/ all bits ones is permanently reserved as an illegal instruction\n+  Assembler::sd_instr(code_pos, 0xffffffff);   \/\/ all bits ones is permanently reserved as an illegal instruction\n@@ -382,1 +382,1 @@\n-    *(unsigned int*)verified_entry = insn;\n+    Assembler::sd_instr(verified_entry, insn);\n@@ -440,1 +440,1 @@\n-  *(unsigned int*) membar = insn;\n+  Assembler::sd_instr(membar, insn);\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-            Assembler::extract(((unsigned*)instr)[0], 25, 20) == shift);    \/\/ shamt field\n+            Assembler::extract(Assembler::ld_instr(instr), 25, 20) == shift);    \/\/ shamt field\n@@ -211,2 +211,2 @@\n-  jint int_at(int offset) const        { return *(jint*) addr_at(offset); }\n-  juint uint_at(int offset) const      { return *(juint*) addr_at(offset); }\n+  jint int_at(int offset) const        { return (jint)Bytes::get_native_u4(addr_at(offset)); }\n+  juint uint_at(int offset) const      { return Bytes::get_native_u4(addr_at(offset)); }\n@@ -214,1 +214,1 @@\n-  address ptr_at(int offset) const     { return *(address*) addr_at(offset); }\n+  address ptr_at(int offset) const     { return (address)Bytes::get_native_u8(addr_at(offset)); }\n@@ -216,1 +216,1 @@\n-  oop  oop_at (int offset) const       { return *(oop*) addr_at(offset); }\n+  oop  oop_at (int offset) const       { return cast_to_oop(Bytes::get_native_u8(addr_at(offset))); }\n@@ -219,4 +219,4 @@\n-  void set_int_at(int offset, jint  i)        { *(jint*)addr_at(offset) = i; }\n-  void set_uint_at(int offset, jint  i)       { *(juint*)addr_at(offset) = i; }\n-  void set_ptr_at (int offset, address  ptr)  { *(address*) addr_at(offset) = ptr; }\n-  void set_oop_at (int offset, oop  o)        { *(oop*) addr_at(offset) = o; }\n+  void set_int_at(int offset, jint i)        { Bytes::put_native_u4(addr_at(offset), i); }\n+  void set_uint_at(int offset, jint i)       { Bytes::put_native_u4(addr_at(offset), i); }\n+  void set_ptr_at (int offset, address ptr)  { Bytes::put_native_u8(addr_at(offset), (u8)ptr); }\n+  void set_oop_at (int offset, oop o)        { Bytes::put_native_u8(addr_at(offset), cast_from_oop<u8>(o)); }\n@@ -493,1 +493,1 @@\n-  uint32_t insn = *(uint32_t*)addr_at(0);\n+  uint32_t insn = Assembler::ld_instr(addr_at(0));\n@@ -535,1 +535,1 @@\n-      (Assembler::extract(((unsigned*)addr)[1], 31, 20) == NativeCallTrampolineStub::data_offset)) {\n+      (Assembler::extract(Assembler::ld_instr(addr + 4), 31, 20) == NativeCallTrampolineStub::data_offset)) {\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-        assert(*(address*)constptr == x, \"error in oop relocation\");\n+        assert((address)Bytes::get_native_u8(constptr) == x, \"error in oop relocation\");\n","filename":"src\/hotspot\/cpu\/riscv\/relocInfo_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1100,2 +1100,6 @@\n-    __ mv(t1, unsatisfied);\n-    __ ld(t1, Address(t1, 0));\n+    __ mv(t, unsatisfied);\n+    if (AvoidUnalignedAccesses) {\n+      __ load_long_misaligned(t1, Address(t, 0), t0, 2); \/\/ 2 bytes aligned, but not 4 or 8\n+    } else {\n+      __ ld(t1, Address(t, 0));\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/templateInterpreterGenerator_riscv.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -283,3 +283,9 @@\n-  __ load_unsigned_short(x10, at_bcp(1));\n-  __ revb_w_w(x10, x10);\n-  __ sraiw(x10, x10, 16);\n+  if (AvoidUnalignedAccesses) {\n+    __ load_signed_byte(x10, at_bcp(1));\n+    __ load_unsigned_byte(t1, at_bcp(2));\n+    __ slli(x10, x10, 8);\n+    __ add(x10, x10, t1);\n+  } else {\n+    __ load_unsigned_short(x10, at_bcp(1));\n+    __ revb_h_h(x10, x10); \/\/ reverse bytes in half-word and sign-extend\n+  }\n@@ -371,1 +377,2 @@\n-  __ get_cache_index_at_bcp(tmp, 1, index_size);\n+  \/\/ register result is trashed by next load, let's use it as temporary register\n+  __ get_cache_index_at_bcp(tmp, result, 1, index_size);\n@@ -1611,2 +1618,9 @@\n-    __ lhu(x12, at_bcp(1));\n-    __ revb_h_h(x12, x12); \/\/ reverse bytes in half-word and sign-extend\n+    if (AvoidUnalignedAccesses) {\n+      __ lb(x12, at_bcp(1));\n+      __ lbu(t1, at_bcp(2));\n+      __ slli(x12, x12, 8);\n+      __ add(x12, x12, t1);\n+    } else {\n+      __ lhu(x12, at_bcp(1));\n+      __ revb_h_h(x12, x12); \/\/ reverse bytes in half-word and sign-extend\n+    }\n@@ -2011,1 +2025,1 @@\n-    __ ld(temp, Address(temp, 0));\n+    __ lwu(temp, Address(temp, 0));\n@@ -2034,1 +2048,1 @@\n-  __ ld(temp, Address(temp, 0));\n+  __ lwu(temp, Address(temp, 0));\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":22,"deletions":8,"binary":false,"changes":30,"status":"modified"}]}