{"files":[{"patch":"@@ -4067,1 +4067,1 @@\n- *    values(NaN,-Inf,Int,Max,-Min).\n+ *    values(NaN,-Inf,Inf,Max,-Min).\n@@ -4087,1 +4087,1 @@\n-  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_US, vec_enc);\n+  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n@@ -4098,1 +4098,1 @@\n-  vmovdqu(xtmp1, float_sign_flip, scratch);\n+  vmovdqu(xtmp1, float_sign_flip, scratch, vec_enc);\n@@ -4103,0 +4103,3 @@\n+  vpcmpeqd(xtmp4, xtmp4, xtmp4, vec_enc);\n+  vpxor(xtmp1, xtmp1, xtmp4, vec_enc);\n+\n@@ -4107,0 +4110,1 @@\n+  \/\/ Recompute the mask for remaining special value.\n@@ -4108,0 +4112,1 @@\n+  \/\/ Extract SRC values corresponding to TRUE mask lanes.\n@@ -4109,0 +4114,2 @@\n+  \/\/ Flip mask bits so that MSB bit of MASK lanes corresponding to +ve special\n+  \/\/ values are set.\n@@ -4111,3 +4118,0 @@\n-  vpcmpeqd(xtmp4, xtmp4, xtmp4, vec_enc);\n-  vpxor(xtmp1, xtmp1, xtmp4, vec_enc);\n-\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2549,0 +2549,9 @@\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len) {\n+  assert(vector_len <= AVX_256bit, \"AVX2 vector length\");\n+  if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src, scratch_reg);\n+  } else {\n+    movdqu(dst, src, scratch_reg);\n+  }\n+}\n+\n@@ -9038,0 +9047,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1120,0 +1120,2 @@\n+  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}