{"files":[{"patch":"@@ -2063,0 +2063,7 @@\n+void Assembler::vcvttps2dq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx() : VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5B, (0xC0 | encode));\n+}\n+\n@@ -2071,0 +2078,8 @@\n+void Assembler::evcvttpd2qq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 2 && VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x7A, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1171,0 +1171,3 @@\n+  \/\/ Convert vector float and int\n+  void vcvttps2dq(XMMRegister dst, XMMRegister src, int vector_len);\n+\n@@ -1175,0 +1178,3 @@\n+  \/\/ Convert vector double to long\n+  void evcvttpd2qq(XMMRegister dst, XMMRegister src, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4062,0 +4062,81 @@\n+\/*\n+ * Algorithm for vector D2L and F2I conversions:-\n+ * a) Perform vector D2L\/F2I cast.\n+ * b) Choose fast path if none of the result vector lane contains 0x80000000 value.\n+ *    It signifies that source value could be any of the special floating point\n+ *    values(NaN,-Inf,Inf,Max,-Min).\n+ * c) Set destination to zero if source is NaN value.\n+ * d) Replace 0x80000000 with MaxInt if source lane contains a +ve value.\n+ *\/\n+\n+void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  Label done;\n+  evcvttpd2qq(dst, src, vec_enc);\n+  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n+  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmppd(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                           AddressLiteral float_sign_flip, Register scratch, int vec_enc) {\n+  Label done;\n+  vcvttps2dq(dst, src, vec_enc);\n+  vmovdqu(xtmp1, float_sign_flip, scratch, vec_enc);\n+  vpcmpeqd(xtmp2, dst, xtmp1, vec_enc);\n+  vptest(xtmp2, xtmp2, vec_enc);\n+  jccb(Assembler::equal, done);\n+\n+  vpcmpeqd(xtmp4, xtmp4, xtmp4, vec_enc);\n+  vpxor(xtmp1, xtmp1, xtmp4, vec_enc);\n+\n+  vpxor(xtmp4, xtmp4, xtmp4, vec_enc);\n+  vcmpps(xtmp3, src, src, Assembler::UNORD_Q, vec_enc);\n+  vblendvps(dst, dst, xtmp4, xtmp3, vec_enc);\n+\n+  \/\/ Recompute the mask for remaining special value.\n+  vpxor(xtmp2, xtmp2, xtmp3, vec_enc);\n+  \/\/ Extract SRC values corresponding to TRUE mask lanes.\n+  vpand(xtmp4, xtmp2, src, vec_enc);\n+  \/\/ Flip mask bits so that MSB bit of MASK lanes corresponding to +ve special\n+  \/\/ values are set.\n+  vpxor(xtmp3, xtmp2, xtmp4, vec_enc);\n+\n+  vblendvps(dst, dst, xtmp1, xtmp3, vec_enc);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  Label done;\n+  vcvttps2dq(dst, src, vec_enc);\n+  evmovdqul(xtmp1, k0, float_sign_flip, false, vec_enc, scratch);\n+  Assembler::evpcmpeqd(ktmp1, k0, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmpps(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdqul(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmpps(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogd(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdqul(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -291,0 +291,12 @@\n+\n+  void vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                          XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                          AddressLiteral float_sign_flip, Register scratch, int vec_enc);\n+\n+  void vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                           KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                           Register scratch, int vec_enc);\n+\n+  void vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                           KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                           Register scratch, int vec_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2549,0 +2549,9 @@\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len) {\n+  assert(vector_len <= AVX_256bit, \"AVX2 vector length\");\n+  if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src, scratch_reg);\n+  } else {\n+    movdqu(dst, src, scratch_reg);\n+  }\n+}\n+\n@@ -9038,0 +9047,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1120,0 +1120,2 @@\n+  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1385,0 +1385,2 @@\n+  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n+  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n@@ -1795,1 +1797,0 @@\n-    case Op_VectorCastF2X:\n@@ -1797,4 +1798,10 @@\n-      if (is_integral_type(bt)) {\n-        \/\/ Casts from FP to integral types require special fixup logic not easily\n-        \/\/ implementable with vectors.\n-        return false; \/\/ Implementation limitation\n+      if (is_subword_type(bt) || bt == T_INT) {\n+        return false;\n+      }\n+      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2X:\n+      if (is_subword_type(bt) || bt == T_LONG) {\n+        return false;\n@@ -1802,0 +1809,1 @@\n+      break;\n@@ -7160,1 +7168,1 @@\n-  format %{ \"vector_cast_f2x  $dst,$src\\t!\" %}\n+  format %{ \"vector_cast_f2d  $dst,$src\\t!\" %}\n@@ -7168,0 +7176,32 @@\n+instruct vcastFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n) < 64 &&\n+            Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3 and $xtmp4 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vector_castF2I_avx($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                          $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,\n+                          ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcastFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate((VM_Version::supports_avx512vl() ||\n+             Matcher::vector_length_in_bytes(n) == 64) &&\n+             Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vector_castF2I_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                           ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7179,0 +7219,14 @@\n+instruct vcastDtoL_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_cast_d2l $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vector_castD2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                           $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                           ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":60,"deletions":6,"binary":false,"changes":66,"status":"modified"}]}