{"files":[{"patch":"@@ -2711,1 +2711,1 @@\n-  assert(type == T_BYTE || type == T_SHORT || type == T_CHAR || type == T_INT || type == T_LONG, \"\");\n+  assert(is_java_primitive((BasicType)type), \"\");\n@@ -2714,1 +2714,1 @@\n-  int prefix = (type == T_BYTE ||  type == T_SHORT || type == T_CHAR) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  int prefix = is_subword_type((BasicType)type) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n@@ -2727,1 +2727,1 @@\n-  assert(type == T_BYTE || type == T_SHORT || type == T_CHAR || type == T_INT || type == T_LONG, \"\");\n+  assert(is_java_primitive((BasicType)type), \"\");\n@@ -2730,1 +2730,1 @@\n-  int prefix = (type == T_BYTE ||  type == T_SHORT || type == T_CHAR) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  int prefix = is_subword_type((BasicType)type) ? VEX_SIMD_F2 : VEX_SIMD_F3;\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1887,0 +1887,14 @@\n+\n+void C2_MacroAssembler::genmask(Register dst, Register len, Register temp) {\n+  if (ArrayCopyPartialInlineSize <= 32) {\n+    mov64(dst, 1);\n+    shlxq(dst, dst, len);\n+    decq(dst);\n+  } else {\n+    mov64(dst, -1);\n+    movq(temp, len);\n+    negptr(temp);\n+    addptr(temp, 64);\n+    shrxq(dst, dst, temp);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -142,0 +142,1 @@\n+  void genmask(Register dst, Register len, Register temp);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1360,0 +1360,1 @@\n+\n@@ -1397,0 +1398,23 @@\n+#ifdef COMPILER2\n+    if (FLAG_IS_DEFAULT(ArrayCopyPartialInlineSize) ||\n+        (!FLAG_IS_DEFAULT(ArrayCopyPartialInlineSize) &&\n+         ArrayCopyPartialInlineSize != 0 &&\n+         ArrayCopyPartialInlineSize != 32 &&\n+         ArrayCopyPartialInlineSize != 64)) {\n+      int pi_size = 0;\n+      if (MaxVectorSize > 32 && AVX3Threshold == 0) {\n+        pi_size = 64;\n+      } else if (MaxVectorSize >= 32) {\n+        pi_size = 32;\n+      }\n+      if(!FLAG_IS_DEFAULT(ArrayCopyPartialInlineSize)) {\n+        warning(\"Setting ArrayCopyPartialInlineSize as %d\", pi_size);\n+      }\n+      ArrayCopyPartialInlineSize = pi_size;\n+    }\n+\n+    if (ArrayCopyPartialInlineSize > MaxVectorSize) {\n+      ArrayCopyPartialInlineSize = MaxVectorSize;\n+      warning(\"Setting ArrayCopyPartialInlineSize as MaxVectorSize\");\n+    }\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1524,0 +1524,7 @@\n+    case Op_VectorMaskGen:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (UseAVX < 3) {\n+        return false;\n+      }\n+      break;\n@@ -1597,0 +1604,10 @@\n+    case Op_VectorMaskGen:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -7921,0 +7938,47 @@\n+#ifdef _LP64\n+\/\/ ---------------------------------- Masked Block Copy ------------------------------------\n+\n+instruct vmasked_load64(vec dst, memory mem, rRegL mask) %{\n+  match(Set dst (LoadVectorMasked mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(this);\n+    __ kmovql(k2, $mask$$Register);\n+    __ evmovdqu($dst$$XMMRegister, k2, $mem$$Address, vector_len, elmType);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_gen(rRegL dst, rRegL len, rRegL tempLen) %{\n+  match(Set dst (VectorMaskGen len));\n+  effect(TEMP_DEF dst, TEMP tempLen);\n+  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  ins_encode %{\n+    __ genmask($dst$$Register, $len$$Register, $tempLen$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_gen_imm(rRegL dst, immL len) %{\n+  match(Set dst (VectorMaskGen len));\n+  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, (1L << ($len$$constant & 63)) -1);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmasked_store64(memory mem, vec src, rRegL mask) %{\n+  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(src_node);\n+    __ kmovql(k2, $mask$$Register);\n+    __ evmovdqu($mem$$Address, k2, $src$$XMMRegister, vector_len, elmType);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":64,"deletions":0,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -272,0 +272,1 @@\n+  if( strcmp(opType,\"LoadVectorMasked\")==0 )  return Form::idealV;\n@@ -289,0 +290,1 @@\n+  if( strcmp(opType,\"StoreVectorMasked\")==0 )  return Form::idealV;\n","filename":"src\/hotspot\/share\/adlc\/forms.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -782,0 +782,1 @@\n+       !strcmp(_matrule->_rChild->_opType,\"VectorMaskGen\")||\n@@ -3487,1 +3488,1 @@\n-    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\",\n+    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n@@ -4179,1 +4180,1 @@\n-    \"VectorMaskWrapper\", \"VectorMaskCmp\", \"VectorReinterpret\",\n+    \"VectorMaskWrapper\", \"VectorMaskCmp\", \"VectorReinterpret\",\"LoadVectorMasked\",\"StoreVectorMasked\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -691,0 +691,2 @@\n+  } else if (mb->trailing_partial_array_copy()) {\n+    return true;\n@@ -737,0 +739,13 @@\n+\n+\/\/ As an optimization, choose optimum vector size for copy length known at compile time.\n+int ArrayCopyNode::get_partial_inline_vector_lane_count(BasicType type, int con_len) {\n+  int lane_count = ArrayCopyPartialInlineSize\/type2aelembytes(type);\n+  if (con_len > 0) {\n+    int size_in_bytes = con_len * type2aelembytes(type);\n+    if (size_in_bytes <= 16)\n+      lane_count = 16\/type2aelembytes(type);\n+    else if (size_in_bytes > 16 && size_in_bytes <= 32)\n+      lane_count = 32\/type2aelembytes(type);\n+  }\n+  return lane_count;\n+}\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -183,0 +183,3 @@\n+\n+  static int get_partial_inline_vector_lane_count(BasicType type, int con_len);\n+\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -83,0 +83,4 @@\n+  product(intx, ArrayCopyPartialInlineSize, -1, DIAGNOSTIC,                 \\\n+          \"Partial inline size used for array copy acceleration.\")          \\\n+          range(-1, 64)                                                     \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -397,0 +397,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -104,1 +104,3 @@\n-  bool try_clean_mem_phi(PhaseGVN* phase);\n+  bool try_clean_mem_phi(PhaseGVN *phase);\n+  bool is_self_loop(Node* n, PhaseGVN *phase);\n+  bool try_phi_disintegration(PhaseGVN *phase);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -408,0 +408,3 @@\n+macro(LoadVectorMasked)\n+macro(StoreVectorMasked)\n+macro(VectorMaskGen)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3400,0 +3400,3 @@\n+  case Op_VectorMaskGen:\n+  case Op_LoadVectorMasked:\n+  case Op_StoreVectorMasked:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -690,0 +690,1 @@\n+        case Op_StoreVectorMasked:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -245,2 +245,4 @@\n-          assert(ac != NULL && ac->is_clonebasic(), \"Only basic clone is a non escaping clone\");\n-          return ac;\n+          if (ac != NULL) {\n+            assert(ac->is_clonebasic(), \"Only basic clone is a non escaping clone\");\n+            return ac;\n+          }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -129,0 +129,5 @@\n+\n+  bool generate_partial_inlining_block(Node** ctrl, MergeMemNode** mem, const TypePtr* adr_type,\n+                                       RegionNode** exit_block, Node** result_memory, Node* length,\n+                                       Node* src_start, Node* dst_start, BasicType type);\n+\n@@ -177,1 +182,1 @@\n-  void generate_unchecked_arraycopy(Node** ctrl, MergeMemNode** mem,\n+  bool generate_unchecked_arraycopy(Node** ctrl, MergeMemNode** mem,\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"opto\/vectornode.hpp\"\n@@ -172,0 +173,124 @@\n+\/\/\n+\/\/ Partial in-lining handling for smaller conjoint\/disjoint array copies having\n+\/\/ length(in bytes) less than ArrayCopyPartialInlineSize.\n+\/\/  if (length <= ArrayCopyPartialInlineSize) {\n+\/\/    partial_inlining_block:\n+\/\/      mask = Mask_Gen\n+\/\/      vload = LoadVectorMasked src , mask\n+\/\/      StoreVectorMasked dst, mask, vload\n+\/\/  } else {\n+\/\/    stub_block:\n+\/\/      callstub array_copy\n+\/\/  }\n+\/\/  exit_block:\n+\/\/    Phi = label partial_inlining_block:mem , label stub_block:mem (filled by caller)\n+\/\/    mem = MergeMem (Phi)\n+\/\/    control = stub_block\n+\/\/\n+\/\/  Exit_block and associated phi(memory) are partially initialized for partial_in-lining_block\n+\/\/  edges. Remaining edges for exit_block coming from stub_block are connected by the caller\n+\/\/  post stub nodes creation.\n+\/\/\n+\/\/  Constant copy length operation having size less than ArrayCopyPartialInlineSize prevents\n+\/\/  creation of additional control flow for stub_block and exit_block.\n+\/\/\n+\/\/  Return true to emit subsequent stub calling code.\n+\/\/\n+\n+bool PhaseMacroExpand::generate_partial_inlining_block(Node** ctrl, MergeMemNode** mem, const TypePtr* adr_type,\n+                                                       RegionNode** exit_block, Node** result_memory, Node* length,\n+                                                       Node* src_start, Node* dst_start, BasicType type) {\n+  const TypePtr *src_adr_type = _igvn.type(src_start)->isa_ptr();\n+\n+  Node* orig_mem = *mem;\n+  Node* is_lt64bytes_tp = NULL;\n+  Node* is_lt64bytes_fp = NULL;\n+\n+  int con_len = -1;\n+  const TypeInt* lty = NULL;\n+  uint shift  = exact_log2(type2aelembytes(type));\n+  if (length->Opcode() == Op_ConvI2L && (lty = _igvn.type(length->in(1))->isa_int()) && lty->is_con()) {\n+    con_len = lty->get_con() << shift;\n+  } else if ((lty = _igvn.type(length)->isa_int()) && lty->is_con()) {\n+    con_len = lty->get_con() << shift;\n+  }\n+\n+  \/\/ Return if copy length is greater than partial inline size limit or\n+  \/\/ target does not supports masked load\/stores.\n+  int lane_count = ArrayCopyNode::get_partial_inline_vector_lane_count(type, con_len);\n+  if ( con_len > ArrayCopyPartialInlineSize ||\n+      !Matcher::match_rule_supported_vector(Op_LoadVectorMasked, lane_count, type)  ||\n+      !Matcher::match_rule_supported_vector(Op_StoreVectorMasked, lane_count, type) ||\n+      !Matcher::match_rule_supported_vector(Op_VectorMaskGen, lane_count, type)) {\n+    return true;\n+  }\n+\n+  Node* inline_block = NULL;\n+  \/\/ Emit length comparison check for non-constant length.\n+  if (con_len < 0) {\n+    Node* copy_bytes = new LShiftXNode(length, intcon(shift));\n+    transform_later(copy_bytes);\n+\n+    Node* cmp_le = new CmpULNode(copy_bytes, longcon(ArrayCopyPartialInlineSize));\n+    transform_later(cmp_le);\n+    Node* bol_le = new BoolNode(cmp_le, BoolTest::le);\n+    transform_later(bol_le);\n+    is_lt64bytes_tp  = generate_guard(ctrl, bol_le, NULL, PROB_FAIR);\n+    is_lt64bytes_fp = *ctrl;\n+\n+    inline_block = is_lt64bytes_tp;\n+  } else {\n+    inline_block = *ctrl;\n+  }\n+\n+  Node* mask_gen =  new VectorMaskGenNode(length, TypeLong::LONG, Type::get_const_basic_type(type));\n+  transform_later(mask_gen);\n+\n+  unsigned vec_size = lane_count *  type2aelembytes(type);\n+  if (C->max_vector_size() < vec_size) {\n+    C->set_max_vector_size(vec_size);\n+  }\n+\n+  int alias_idx = C->get_alias_index(src_adr_type);\n+  Node* mm = (*mem)->memory_at(alias_idx);\n+  const TypeVect * vt = TypeVect::make(type, lane_count);\n+  Node* masked_load = new LoadVectorMaskedNode(inline_block, mm, src_start,\n+                                               src_adr_type, vt, mask_gen);\n+  transform_later(masked_load);\n+\n+  mm = (*mem)->memory_at(C->get_alias_index(adr_type));\n+  Node* masked_store = new StoreVectorMaskedNode(inline_block, mm, dst_start,\n+                                                 masked_load, adr_type, mask_gen);\n+  transform_later(masked_store);\n+\n+  \/\/ Stub region is created for non-constant copy length.\n+  if (con_len < 0) {\n+    \/\/ Region containing stub calling node.\n+    Node* stub_block = is_lt64bytes_fp;\n+\n+    \/\/ Convergence region for inline_block and stub_block.\n+    *exit_block = new RegionNode(3);\n+    transform_later(*exit_block);\n+    (*exit_block)->init_req(1, is_lt64bytes_tp);\n+    *result_memory = new PhiNode(*exit_block, Type::MEMORY, adr_type);\n+    transform_later(*result_memory);\n+    (*result_memory)->init_req(1, masked_store);\n+\n+    *ctrl = stub_block;\n+    return true;\n+  } else {\n+    \/\/ Prevent stub call generation for constant length less\n+    \/\/ than partial inline size.\n+    uint alias_idx = C->get_alias_index(adr_type);\n+    if (alias_idx != Compile::AliasIdxBot) {\n+      *mem = MergeMemNode::make(*mem);\n+      (*mem)->set_memory_at(alias_idx, masked_store);\n+    } else {\n+      *mem = MergeMemNode::make(masked_store);\n+    }\n+    transform_later(*mem);\n+    return false;\n+  }\n+}\n+\n+\n@@ -562,0 +687,1 @@\n+  bool is_partial_array_copy = false;\n@@ -568,4 +694,4 @@\n-    generate_unchecked_arraycopy(&local_ctrl, &local_mem,\n-                                 adr_type, copy_type, disjoint_bases,\n-                                 src, src_offset, dest, dest_offset,\n-                                 ConvI2X(copy_length), dest_uninitialized);\n+    is_partial_array_copy = generate_unchecked_arraycopy(&local_ctrl, &local_mem,\n+                                                         adr_type, copy_type, disjoint_bases,\n+                                                         src, src_offset, dest, dest_offset,\n+                                                         ConvI2X(copy_length), dest_uninitialized);\n@@ -718,0 +844,6 @@\n+  if (is_partial_array_copy) {\n+    assert((*ctrl)->is_Proj(), \"MemBar control projection\");\n+    assert((*ctrl)->in(0)->isa_MemBar(), \"MemBar node\");\n+    (*ctrl)->in(0)->isa_MemBar()->set_trailing_partial_array_copy();\n+  }\n+\n@@ -724,1 +856,1 @@\n-  if (dest_t->is_known_instance()) {\n+  if (dest_t->is_known_instance() && !is_partial_array_copy) {\n@@ -1056,1 +1188,1 @@\n-void PhaseMacroExpand::generate_unchecked_arraycopy(Node** ctrl, MergeMemNode** mem,\n+bool PhaseMacroExpand::generate_unchecked_arraycopy(Node** ctrl, MergeMemNode** mem,\n@@ -1063,1 +1195,1 @@\n-  if ((*ctrl)->is_top()) return;\n+  if ((*ctrl)->is_top()) return false;\n@@ -1078,3 +1210,8 @@\n-  const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n-  Node* call = make_leaf_call(*ctrl, *mem, call_type, copyfunc_addr, copyfunc_name, adr_type,\n-                              src_start, dest_start, copy_length XTOP);\n+  Node* result_memory = NULL;\n+  RegionNode* exit_block = NULL;\n+  bool gen_stub_call = true;\n+  if (ArrayCopyPartialInlineSize > 0 && is_subword_type(basic_elem_type) &&\n+    Matcher::vector_width_in_bytes(basic_elem_type) >= 32) {\n+    gen_stub_call = generate_partial_inlining_block(ctrl, mem, adr_type, &exit_block, &result_memory,\n+                                                    copy_length, src_start, dest_start, basic_elem_type);\n+  }\n@@ -1082,1 +1219,27 @@\n-  finish_arraycopy_call(call, ctrl, mem, adr_type);\n+  if (gen_stub_call) {\n+    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+    Node* call = make_leaf_call(*ctrl, *mem, call_type, copyfunc_addr, copyfunc_name, adr_type,\n+                                src_start, dest_start, copy_length XTOP);\n+\n+    finish_arraycopy_call(call, ctrl, mem, adr_type);\n+  }\n+\n+  \/\/ Connecting remaining edges for exit_block coming from stub_block.\n+  if (exit_block) {\n+    exit_block->init_req(2, *ctrl);\n+\n+    \/\/ Memory edge corresponding to stub_region.\n+    result_memory->init_req(2, *mem);\n+\n+    uint alias_idx = C->get_alias_index(adr_type);\n+    if (alias_idx != Compile::AliasIdxBot) {\n+      *mem = MergeMemNode::make(*mem);\n+      (*mem)->set_memory_at(alias_idx, result_memory);\n+    } else {\n+      *mem = MergeMemNode::make(result_memory);\n+    }\n+    transform_later(*mem);\n+    *ctrl = exit_block;\n+    return true;\n+  }\n+  return false;\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":174,"deletions":11,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -2204,0 +2204,1 @@\n+    case Op_LoadVectorMasked:\n@@ -2306,0 +2307,6 @@\n+    case Op_StoreVectorMasked: {\n+      Node* pair = new BinaryNode(n->in(3), n->in(4));\n+      n->set_req(3, pair);\n+      n->del_req(4);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1187,1 +1187,2 @@\n-    LeadingLoadStore\n+    LeadingLoadStore,\n+    TrailingPartialArrayCopy\n@@ -1224,0 +1225,2 @@\n+  void set_trailing_partial_array_copy() { _kind = TrailingPartialArrayCopy; }\n+  bool trailing_partial_array_copy() const { return _kind == TrailingPartialArrayCopy; }\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -155,0 +155,2 @@\n+class LoadVectorMaskedNode;\n+class StoreVectorMaskedNode;\n@@ -159,0 +161,1 @@\n+\n@@ -691,2 +694,2 @@\n-    DEFINE_CLASS_ID(Mem,   Node, 4)\n-      DEFINE_CLASS_ID(Load,  Mem, 0)\n+    DEFINE_CLASS_ID(Mem, Node, 4)\n+      DEFINE_CLASS_ID(Load, Mem, 0)\n@@ -695,0 +698,1 @@\n+          DEFINE_CLASS_ID(LoadVectorMasked, LoadVector, 1)\n@@ -698,0 +702,1 @@\n+          DEFINE_CLASS_ID(StoreVectorMasked, StoreVector, 1)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -762,0 +762,50 @@\n+LoadVectorMaskedNode* make(int opc, Node* ctl, Node* mem, Node* src,\n+                       const TypePtr* atype, const TypeVect* vt,\n+                       Node* mask) {\n+  return new LoadVectorMaskedNode(ctl, mem, src, atype, vt, mask);\n+}\n+\n+StoreVectorMaskedNode* make(int opc, Node* ctl, Node* mem, Node* dst,\n+                            Node* src, const TypePtr* atype, Node* mask) {\n+  return new StoreVectorMaskedNode(ctl, mem, dst, src, atype, mask);\n+}\n+\n+VectorMaskGenNode* make(int opc, Node* src, const Type* ty, const Type* ety) {\n+  return new VectorMaskGenNode(src, ty, ety);\n+}\n+\n+Node* LoadVectorMaskedNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* mask_len = in(3)->in(1);\n+  const TypeLong* ty = phase->type(mask_len)->isa_long();\n+  if (ty && ty->is_con()) {\n+    BasicType mask_bt = ((VectorMaskGenNode*)in(3))->get_elem_type()->array_element_basic_type();\n+    uint load_sz      = type2aelembytes(mask_bt) * ty->get_con();\n+    if ( load_sz == 32 || load_sz == 64) {\n+      assert(load_sz == 32 || MaxVectorSize > 32, \"Unexpected load size\");\n+      Node* ctr = in(MemNode::Control);\n+      Node* mem = in(MemNode::Memory);\n+      Node* adr = in(MemNode::Address);\n+      return phase->transform(new LoadVectorNode(ctr, mem, adr, adr_type(), vect_type()));\n+    }\n+  }\n+  return NULL;\n+}\n+\n+Node* StoreVectorMaskedNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* mask_len = in(4)->in(1);\n+  const TypeLong* ty = phase->type(mask_len)->isa_long();\n+  if (ty && ty->is_con()) {\n+    BasicType mask_bt = ((VectorMaskGenNode*)in(4))->get_elem_type()->array_element_basic_type();\n+    uint load_sz      = type2aelembytes(mask_bt) * ty->get_con();\n+    if ( load_sz == 32 || load_sz == 64) {\n+      assert(load_sz == 32 || MaxVectorSize > 32, \"Unexpected store size\");\n+      Node* ctr = in(MemNode::Control);\n+      Node* mem = in(MemNode::Memory);\n+      Node* adr = in(MemNode::Address);\n+      Node* val = in(MemNode::ValueIn);\n+      return phase->transform(new StoreVectorNode(ctr, mem, adr, adr_type(), val));\n+    }\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -781,0 +781,57 @@\n+class StoreVectorMaskedNode : public StoreVectorNode {\n+ public:\n+  StoreVectorMaskedNode(Node* c, Node* mem, Node* dst, Node* src, const TypePtr* at, Node* mask)\n+   : StoreVectorNode(c, mem, dst, at, src) {\n+    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    init_class_id(Class_StoreVector);\n+    set_mismatched_access();\n+    add_req(mask);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  virtual uint match_edge(uint idx) const {\n+    return idx > 1;\n+  }\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n+  static StoreVectorMaskedNode* make(int opc, Node* ctl, Node* mem, Node* dst, Node* src,\n+                                    const TypePtr* atype, Node* mask);\n+};\n+\n+class LoadVectorMaskedNode : public LoadVectorNode {\n+ public:\n+  LoadVectorMaskedNode(Node* c, Node* mem, Node* src, const TypePtr* at, const TypeVect* vt, Node* mask)\n+   : LoadVectorNode(c, mem, src, at, vt) {\n+    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    init_class_id(Class_LoadVector);\n+    set_mismatched_access();\n+    add_req(mask);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  virtual uint match_edge(uint idx) const {\n+    return idx > 1;\n+  }\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n+  static LoadVectorMaskedNode* make(int opc, Node* ctl, Node* mem, Node* src,\n+                                const TypePtr* atype, const TypeVect* vt,\n+                                Node* mask);\n+};\n+\n+class VectorMaskGenNode : public TypeNode {\n+ public:\n+  VectorMaskGenNode(Node* src, const Type* ty, const Type* ety): TypeNode(ty, 2), _elemType(ety) {\n+    init_req(1, src);\n+  }\n+\n+  virtual int Opcode() const;\n+  const Type* get_elem_type()  { return _elemType;}\n+\n+  static VectorMaskGenNode* make(int opc, Node* src, const Type* ty, const Type* ety);\n+  private:\n+   const Type* _elemType;\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"}]}