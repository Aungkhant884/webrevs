{"files":[{"patch":"@@ -42,0 +42,12 @@\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_or_4_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_or_8_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_or_4_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_or_8_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_and_4_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_and_8_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_and_4_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_and_8_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_xor_4_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_xor_8_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_xor_4_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_xor_8_relaxed_impl;\n","filename":"src\/hotspot\/cpu\/aarch64\/atomic_aarch64.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -8003,0 +8003,12 @@\n+DEFAULT_ATOMIC_OP(fetch_or, 4, )\n+DEFAULT_ATOMIC_OP(fetch_or, 8, )\n+DEFAULT_ATOMIC_OP(fetch_or, 4, _relaxed)\n+DEFAULT_ATOMIC_OP(fetch_or, 8, _relaxed)\n+DEFAULT_ATOMIC_OP(fetch_and, 4, )\n+DEFAULT_ATOMIC_OP(fetch_and, 8, )\n+DEFAULT_ATOMIC_OP(fetch_and, 4, _relaxed)\n+DEFAULT_ATOMIC_OP(fetch_and, 8, _relaxed)\n+DEFAULT_ATOMIC_OP(fetch_xor, 4, )\n+DEFAULT_ATOMIC_OP(fetch_xor, 8, )\n+DEFAULT_ATOMIC_OP(fetch_xor, 4, _relaxed)\n+DEFAULT_ATOMIC_OP(fetch_xor, 8, _relaxed)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -206,0 +206,52 @@\n+template<size_t byte_size>\n+struct Atomic::PlatformBitOp {\n+  template<typename D>\n+  D fetch_and_or(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D old_val = Atomic::load(dest, order);\n+\n+    do {\n+      const D new_val = old_val | set_value;\n+      const D cur_val = Atomic::cmpxchg(dest, old_val, new_val, order);\n+      if (cur_val == old_val) {\n+        \/\/ Success\n+        return old_val;\n+      }\n+\n+      \/\/ The value changed, retry\n+      old_val = cur_val;\n+    } while (true);\n+  }\n+  template<typename D>\n+  D fetch_and_and(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D old_val = Atomic::load(dest, order);\n+\n+    do {\n+      const D new_val = old_val & set_value;\n+      const D cur_val = Atomic::cmpxchg(dest, old_val, new_val, order);\n+      if (cur_val == old_val) {\n+        \/\/ Success\n+        return old_val;\n+      }\n+\n+      \/\/ The value changed, retry\n+      old_val = cur_val;\n+    } while (true);\n+  }\n+  template<typename D>\n+  D fetch_and_xor(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D old_val = Atomic::load(dest, order);\n+\n+    do {\n+      const D new_val = old_val ^ set_value;\n+      const D cur_val = Atomic::cmpxchg(dest, old_val, new_val, order);\n+      if (cur_val == old_val) {\n+        \/\/ Success\n+        return old_val;\n+      }\n+\n+      \/\/ The value changed, retry\n+      old_val = cur_val;\n+    } while (true);\n+  }\n+};\n+\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/atomic_bsd_zero.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -88,0 +88,186 @@\n+        .globl aarch64_atomic_fetch_or_8_default_impl\n+        .align 5\n+aarch64_atomic_fetch_or_8_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldsetal x1, x2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr   x2, [x0]\n+        orr     x8, x2, x1\n+        stlxr   w9, x8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        dmb     ish\n+        mov     x0, x2\n+        ret\n+\n+        .globl aarch64_atomic_fetch_or_4_default_impl\n+        .align 5\n+aarch64_atomic_fetch_or_4_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldsetal w1, w2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr   w2, [x0]\n+        orr     w8, w2, w1\n+        stlxr   w9, w8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        dmb     ish\n+        mov     w0, w2\n+        ret\n+\n+        .global aarch64_atomic_fetch_or_8_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_or_8_relaxed_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldset x1, x2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    x2, [x0]\n+        orr     x8, x2, x1\n+        stxr    w9, x8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        mov     x0, x2\n+        ret\n+\n+        .global aarch64_atomic_fetch_or_4_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_or_4_relaxed_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldset w1, w2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    w2, [x0]\n+        orr     w8, w2, w1\n+        stxr    w9, w8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        mov     w0, w2\n+        ret\n+\n+        .globl aarch64_atomic_fetch_and_8_default_impl\n+        .align 5\n+aarch64_atomic_fetch_and_8_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldsetal x1, x2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr   x2, [x0]\n+        and     x8, x2, x1\n+        stlxr   w9, x8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        dmb     ish\n+        mov     x0, x2\n+        ret\n+\n+        .globl aarch64_atomic_fetch_and_4_default_impl\n+        .align 5\n+aarch64_atomic_fetch_and_4_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldsetal w1, w2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr   w2, [x0]\n+        and     w8, w2, w1\n+        stlxr   w9, w8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        dmb     ish\n+        mov     w0, w2\n+        ret\n+\n+        .global aarch64_atomic_fetch_and_8_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_and_8_relaxed_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldset x1, x2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    x2, [x0]\n+        and     x8, x2, x1\n+        stxr    w9, x8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        mov     x0, x2\n+        ret\n+\n+        .global aarch64_atomic_fetch_and_4_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_and_4_relaxed_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldset w1, w2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    w2, [x0]\n+        and     w8, w2, w1\n+        stxr    w9, w8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        mov     w0, w2\n+        ret\n+\n+        .globl aarch64_atomic_fetch_xor_8_default_impl\n+        .align 5\n+aarch64_atomic_fetch_xor_8_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldsetal x1, x2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr   x2, [x0]\n+        eor     x8, x2, x1\n+        stlxr   w9, x8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        dmb     ish\n+        mov     x0, x2\n+        ret\n+\n+        .globl aarch64_atomic_fetch_xor_4_default_impl\n+        .align 5\n+aarch64_atomic_fetch_xor_4_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldsetal w1, w2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr   w2, [x0]\n+        eor     w8, w2, w1\n+        stlxr   w9, w8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        dmb     ish\n+        mov     w0, w2\n+        ret\n+\n+        .global aarch64_atomic_fetch_xor_8_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_xor_8_relaxed_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldset x1, x2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    x2, [x0]\n+        eor     x8, x2, x1\n+        stxr    w9, x8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        mov     x0, x2\n+        ret\n+\n+        .global aarch64_atomic_fetch_xor_4_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_xor_4_relaxed_default_impl:\n+#ifdef __ARM_FEATURE_ATOMICS\n+        ldset w1, w2, [x0]\n+#else\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    w2, [x0]\n+        eor     w8, w2, w1\n+        stxr    w9, w8, [x0]\n+        cbnz    w9, 0b\n+#endif\n+        mov     w0, w2\n+        ret\n+\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.S","additions":186,"deletions":0,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -116,0 +116,100 @@\n+template<size_t byte_size>\n+struct Atomic::PlatformBitOp {\n+  template<typename D>\n+  D fetch_and_or(D volatile* dest, D set_value, atomic_memory_order order) const;\n+  template<typename D>\n+  D fetch_and_and(D volatile* dest, D set_value, atomic_memory_order order) const;\n+  template<typename D>\n+  D fetch_and_xor(D volatile* dest, D set_value, atomic_memory_order order) const;\n+};\n+\n+template<>\n+template<typename D>\n+inline D Atomic::PlatformBitOp<4>::fetch_and_or(D volatile* dest, D set_value,\n+                                                atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(D));\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_or_4_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_or_4_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, set_value);\n+}\n+\n+template<>\n+template<typename D>\n+inline D Atomic::PlatformBitOp<8>::fetch_and_or(D volatile* dest, D set_value,\n+                                                atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(D));\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_or_8_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_or_8_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, set_value);\n+}\n+\n+template<>\n+template<typename D>\n+inline D Atomic::PlatformBitOp<4>::fetch_and_and(D volatile* dest, D set_value,\n+                                                 atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(D));\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_and_4_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_and_4_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, set_value);\n+}\n+\n+template<>\n+template<typename D>\n+inline D Atomic::PlatformBitOp<8>::fetch_and_and(D volatile* dest, D set_value,\n+                                                 atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(D));\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_and_8_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_and_8_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, set_value);\n+}\n+\n+template<>\n+template<typename D>\n+inline D Atomic::PlatformBitOp<4>::fetch_and_xor(D volatile* dest, D set_value,\n+                                                 atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(D));\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_xor_4_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_xor_4_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, set_value);\n+}\n+\n+template<>\n+template<typename D>\n+inline D Atomic::PlatformBitOp<8>::fetch_and_xor(D volatile* dest, D set_value,\n+                                                 atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(D));\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_xor_8_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_xor_8_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, set_value);\n+}\n+\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -55,0 +55,52 @@\n+template<size_t byte_size>\n+struct Atomic::PlatformBitOp {\n+  template<typename D>\n+  D fetch_and_or(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D old_val = Atomic::load(dest, order);\n+\n+    do {\n+      const D new_val = old_val | set_value;\n+      const D cur_val = Atomic::cmpxchg(dest, old_val, new_val, order);\n+      if (cur_val == old_val) {\n+        \/\/ Success\n+        return old_val;\n+      }\n+\n+      \/\/ The value changed, retry\n+      old_val = cur_val;\n+    } while (true);\n+  }\n+  template<typename D>\n+  D fetch_and_and(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D old_val = Atomic::load(dest, order);\n+\n+    do {\n+      const D new_val = old_val & set_value;\n+      const D cur_val = Atomic::cmpxchg(dest, old_val, new_val, order);\n+      if (cur_val == old_val) {\n+        \/\/ Success\n+        return old_val;\n+      }\n+\n+      \/\/ The value changed, retry\n+      old_val = cur_val;\n+    } while (true);\n+  }\n+  template<typename D>\n+  D fetch_and_xor(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D old_val = Atomic::load(dest, order);\n+\n+    do {\n+      const D new_val = old_val ^ set_value;\n+      const D cur_val = Atomic::cmpxchg(dest, old_val, new_val, order);\n+      if (cur_val == old_val) {\n+        \/\/ Success\n+        return old_val;\n+      }\n+\n+      \/\/ The value changed, retry\n+      old_val = cur_val;\n+    } while (true);\n+  }\n+};\n+\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/atomic_linux_x86.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -68,0 +68,22 @@\n+template<size_t byte_size>\n+struct Atomic::PlatformBitOp {\n+  template<typename D>\n+  D fetch_and_or(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D res = __atomic_fetch_or(dest, set_value, __ATOMIC_RELEASE);\n+    FULL_MEM_BARRIER;\n+    return res;\n+  }\n+  template<typename D>\n+  D fetch_and_and(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D res = __atomic_fetch_and(dest, set_value, __ATOMIC_RELEASE);\n+    FULL_MEM_BARRIER;\n+    return res;\n+  }\n+  template<typename D>\n+  D fetch_and_xor(D volatile* dest, D set_value, atomic_memory_order order) const {\n+    D res = __atomic_fetch_xor(dest, set_value, __ATOMIC_RELEASE);\n+    FULL_MEM_BARRIER;\n+    return res;\n+  }\n+};\n+\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/atomic_linux_zero.hpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  bm_word_t old_val = *addr;\n+  bm_word_t cur_val = *addr;\n@@ -52,14 +52,16 @@\n-  do {\n-    const bm_word_t new_val = old_val | pair_mask;\n-    if (new_val == old_val) {\n-      \/\/ Someone else beat us to it\n-      inc_live = false;\n-      return false;\n-    }\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val);\n-    if (cur_val == old_val) {\n-      \/\/ Success\n-      const bm_word_t marked_mask = bit_mask(bit);\n-      inc_live = !(old_val & marked_mask);\n-      return true;\n-    }\n+  \/\/ Fast path\n+  bm_word_t new_val = cur_val | pair_mask;\n+  if (new_val == cur_val) {\n+    \/\/ Someone else beat us to it\n+    inc_live = false;\n+    return false;\n+  }\n+\n+  \/\/ Slow path\n+  cur_val = Atomic::fetch_and_or(addr, pair_mask);\n+  new_val = cur_val | pair_mask;\n+  if (new_val == cur_val) {\n+    \/\/ Someone else beat us to it\n+    inc_live = false;\n+    return false;\n+  }\n@@ -67,3 +69,4 @@\n-    \/\/ The value changed, retry\n-    old_val = cur_val;\n-  } while (true);\n+  \/\/ Success\n+  const bm_word_t marked_mask = bit_mask(bit);\n+  inc_live = !(cur_val & marked_mask);\n+  return true;\n","filename":"src\/hotspot\/share\/gc\/z\/zBitMap.inline.hpp","additions":21,"deletions":18,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -94,0 +94,3 @@\n+  template<typename D, typename T>\n+  inline static void store(volatile D* dest, T store_value, atomic_memory_order order);\n+\n@@ -103,0 +106,3 @@\n+  template<typename T>\n+  inline static T load(const volatile T* dest, atomic_memory_order order);\n+\n@@ -138,0 +144,24 @@\n+  \/\/ Atomically bit-or to a location. *or*() provide:\n+  \/\/ <fence> bit-or-value-to-dest <membar StoreLoad|StoreStore>\n+\n+  \/\/ Returns previous value.\n+  template<typename D>\n+  inline static D fetch_and_or(D volatile* dest, D set_value,\n+                               atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Atomically bit-and to a location. *or*() provide:\n+  \/\/ <fence> bit-and-value-to-dest <membar StoreLoad|StoreStore>\n+\n+  \/\/ Returns previous value.\n+  template<typename D>\n+  inline static D fetch_and_and(D volatile* dest, D set_value,\n+                                atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Atomically bit-xor to a location. *or*() provide:\n+  \/\/ <fence> bit-xor-value-to-dest <membar StoreLoad|StoreStore>\n+\n+  \/\/ Returns previous value.\n+  template<typename D>\n+  inline static D fetch_and_xor(D volatile* dest, D set_value,\n+                                atomic_memory_order order = memory_order_conservative);\n+\n@@ -280,0 +310,34 @@\n+  \/\/ Dispatch handler for bitop.  Provides type-based validity checking\n+  \/\/ and limited conversions around calls to the platform-specific\n+  \/\/ implementation layer provided by PlatformBitOp.\n+  template<typename D, typename Enable = void>\n+  struct BitOpImpl;\n+\n+  \/\/ Platform-specific implementation of bitop.  Support for sizes of 4\n+  \/\/ bytes and (if different) pointer size bytes are required.  The\n+  \/\/ class must be default constructable, with these requirements:\n+  \/\/\n+  \/\/ - dest is of type D*, an integral or pointer type.\n+  \/\/ - set_value is of type D, an integral type.\n+  \/\/ - order is of type atomic_memory_order.\n+  \/\/ - platform_bitop is an object of type PlatformBitOp<sizeof(D)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/   platform_bitop.fetch_and_or(dest, set_value, order)\n+  \/\/   platform_bitop.fetch_and_and(dest, set_value, order)\n+  \/\/   platform_bitop.fetch_and_xor(dest, set_value, order)\n+  \/\/ must be valid expressions returning a result convertible to D.\n+  \/\/\n+  \/\/ fetch_and_or atomically bit-or set_value to the value of dest,\n+  \/\/ returning the old value.\n+  \/\/\n+  \/\/ fetch_and_and atomically bit-and set_value to the value of dest,\n+  \/\/ returning the old value.\n+  \/\/\n+  \/\/ fetch_and_xor atomically bit-xor set_value to the value of dest,\n+  \/\/ returning the old value.\n+  \/\/\n+  \/\/ No definition is provided; all platforms must explicitly define\n+  \/\/ this class and any needed specializations.\n+  template<size_t byte_size> struct PlatformBitOp;\n+\n@@ -631,0 +695,11 @@\n+template<typename T>\n+inline T Atomic::load(const volatile T* dest, atomic_memory_order order) {\n+  switch (order) {\n+  case memory_order_relaxed:\n+  case memory_order_release:\n+    return load(dest);\n+  default:\n+    return load_acquire(dest);\n+  }\n+}\n+\n@@ -655,0 +730,18 @@\n+template<typename D, typename T>\n+inline void Atomic::store(volatile D* dest, T store_value, atomic_memory_order order) {\n+  switch (order) {\n+  case memory_order_relaxed:\n+  case memory_order_acquire:\n+    store(dest, store_value);\n+    break;\n+  case memory_order_acq_rel:\n+  case memory_order_release:\n+    release_store(dest, store_value);\n+    break;\n+  case memory_order_seq_cst:\n+  case memory_order_conservative:\n+    release_store_fence(dest, store_value);\n+    break;\n+  }\n+}\n+\n@@ -717,0 +810,34 @@\n+template<typename D>\n+inline D Atomic::fetch_and_or(D volatile* dest, D set_value,\n+                              atomic_memory_order order) {\n+  return BitOpImpl<D>::fetch_and_or(dest, set_value, order);\n+}\n+\n+template<typename D>\n+inline D Atomic::fetch_and_and(D volatile* dest, D set_value,\n+                               atomic_memory_order order) {\n+  return BitOpImpl<D>::fetch_and_and(dest, set_value, order);\n+}\n+\n+template<typename D>\n+inline D Atomic::fetch_and_xor(D volatile* dest, D set_value,\n+                               atomic_memory_order order) {\n+  return BitOpImpl<D>::fetch_and_xor(dest, set_value, order);\n+}\n+\n+template<typename D>\n+struct Atomic::BitOpImpl<\n+  D,\n+  typename EnableIf<IsIntegral<D>::value || IsPointer<D>::value>::type>\n+{\n+  static D fetch_and_or(D volatile* dest, D set_value, atomic_memory_order order) {\n+    return PlatformBitOp<sizeof(D)>().fetch_and_or(dest, set_value, order);\n+  }\n+  static D fetch_and_and(D volatile* dest, D set_value, atomic_memory_order order) {\n+    return PlatformBitOp<sizeof(D)>().fetch_and_and(dest, set_value, order);\n+  }\n+  static D fetch_and_xor(D volatile* dest, D set_value, atomic_memory_order order) {\n+    return PlatformBitOp<sizeof(D)>().fetch_and_xor(dest, set_value, order);\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":127,"deletions":0,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -236,1 +236,0 @@\n-    bm_word_t w = Atomic::load(pw);\n@@ -238,6 +237,4 @@\n-    bm_word_t nw = value ? (w | ~mr) : (w & mr);\n-    while (true) {\n-      bm_word_t res = Atomic::cmpxchg(pw, w, nw);\n-      if (res == w) break;\n-      w  = res;\n-      nw = value ? (w | ~mr) : (w & mr);\n+    if (value) {\n+      Atomic::fetch_and_or(pw, ~mr);\n+    } else {\n+      Atomic::fetch_and_and(pw, mr);\n","filename":"src\/hotspot\/share\/utilities\/bitMap.cpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  bm_word_t old_val = load_word_ordered(addr, memory_order);\n+  bm_word_t cur_val = load_word_ordered(addr, memory_order);\n@@ -71,11 +71,17 @@\n-  do {\n-    const bm_word_t new_val = old_val | mask;\n-    if (new_val == old_val) {\n-      return false;     \/\/ Someone else beat us to it.\n-    }\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order);\n-    if (cur_val == old_val) {\n-      return true;      \/\/ Success.\n-    }\n-    old_val = cur_val;  \/\/ The value changed, try again.\n-  } while (true);\n+  \/\/ Fast path\n+  bm_word_t new_val = cur_val | mask;\n+  if (new_val == cur_val) {\n+    \/\/ Someone else beat us to it.\n+    return false;\n+  }\n+\n+  \/\/ Slow path\n+  cur_val = Atomic::fetch_and_or(addr, mask, memory_order);\n+  new_val = cur_val | mask;\n+  if (new_val == cur_val) {\n+    \/\/ Someone else beat us to it.\n+    return false;\n+  }\n+\n+  \/\/ Success\n+  return true;\n@@ -88,1 +94,1 @@\n-  bm_word_t old_val = load_word_ordered(addr, memory_order);\n+  bm_word_t cur_val = load_word_ordered(addr, memory_order);\n@@ -90,11 +96,17 @@\n-  do {\n-    const bm_word_t new_val = old_val & mask;\n-    if (new_val == old_val) {\n-      return false;     \/\/ Someone else beat us to it.\n-    }\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order);\n-    if (cur_val == old_val) {\n-      return true;      \/\/ Success.\n-    }\n-    old_val = cur_val;  \/\/ The value changed, try again.\n-  } while (true);\n+  \/\/ Fast path\n+  bm_word_t new_val = cur_val & mask;\n+  if (new_val == cur_val) {\n+    \/\/ Someone else beat us to it.\n+    return false;\n+  }\n+\n+  \/\/ Slow path\n+  cur_val = Atomic::fetch_and_and(addr, mask, memory_order);\n+  new_val = cur_val & mask;\n+  if (new_val == cur_val) {\n+    \/\/ Someone else beat us to it.\n+    return false;\n+  }\n+\n+  \/\/ Success\n+  return true;\n","filename":"src\/hotspot\/share\/utilities\/bitMap.inline.hpp","additions":36,"deletions":24,"binary":false,"changes":60,"status":"modified"}]}