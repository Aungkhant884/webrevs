{"files":[{"patch":"@@ -1013,1 +1013,1 @@\n-void G1CollectedHeap::prepare_heap_for_mutators() {\n+void G1CollectedHeap::prepare_for_mutator_after_full_collection() {\n@@ -1028,1 +1028,0 @@\n-  \/\/ Start a new incremental collection set for the next pause\n@@ -1030,1 +1029,0 @@\n-\n@@ -1869,26 +1867,0 @@\n-#ifndef PRODUCT\n-void G1CollectedHeap::allocate_dummy_regions() {\n-  \/\/ Let's fill up most of the region\n-  size_t word_size = HeapRegion::GrainWords - 1024;\n-  \/\/ And as a result the region we'll allocate will be humongous.\n-  guarantee(is_humongous(word_size), \"sanity\");\n-\n-  \/\/ _filler_array_max_size is set to humongous object threshold\n-  \/\/ but temporarily change it to use CollectedHeap::fill_with_object().\n-  AutoModifyRestore<size_t> temporarily(_filler_array_max_size, word_size);\n-\n-  for (uintx i = 0; i < G1DummyRegionsPerGC; ++i) {\n-    \/\/ Let's use the existing mechanism for the allocation\n-    HeapWord* dummy_obj = humongous_obj_allocate(word_size);\n-    if (dummy_obj != NULL) {\n-      MemRegion mr(dummy_obj, word_size);\n-      CollectedHeap::fill_with_object(mr);\n-    } else {\n-      \/\/ If we can't allocate once, we probably cannot allocate\n-      \/\/ again. Let's get out of the loop.\n-      break;\n-    }\n-  }\n-}\n-#endif \/\/ !PRODUCT\n-\n@@ -2645,2 +2617,0 @@\n-  double start = os::elapsedTime();\n-\n@@ -2657,2 +2627,0 @@\n-\n-  phase_times()->record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);\n@@ -2768,1 +2736,1 @@\n-void G1CollectedHeap::prepare_tlabs_for_mutator() {\n+void G1CollectedHeap::prepare_for_mutator_after_young_collection() {\n@@ -2774,2 +2742,2 @@\n-  allocate_dummy_regions();\n-\n+  \/\/ Start a new incremental collection set for the mutator phase.\n+  start_new_collection_set();\n@@ -2778,3 +2746,1 @@\n-  resize_all_tlabs();\n-\n-  phase_times()->record_resize_tlab_time_ms((Ticks::now() - start).seconds() * 1000.0);\n+  phase_times()->record_prepare_for_mutator_time_ms((Ticks::now() - start).seconds() * 1000.0);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":5,"deletions":39,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"runtime\/threadSMR.hpp\"\n@@ -123,0 +124,26 @@\n+\/\/ Helper to claim contiguous sets of JavaThread for processing by multiple threads.\n+class G1JavaThreadsListClaimer : public StackObj {\n+  ThreadsListHandle _list;\n+  uint _claim_step;\n+\n+  volatile uint _cur_claim;\n+\n+  \/\/ Attempts to claim _claim_step JavaThreads, returning an array of claimed\n+  \/\/ JavaThread* with count elements. Returns null (and a zero count) if there\n+  \/\/ are no more threads to claim.\n+  JavaThread* const* claim(uint& count);\n+\n+public:\n+  G1JavaThreadsListClaimer(uint claim_step) : _list(), _claim_step(claim_step), _cur_claim(0) {\n+    assert(claim_step > 0, \"must be\");\n+  }\n+\n+  \/\/ Executes the given closure on the elements of the JavaThread list, chunking the\n+  \/\/ JavaThread set in claim_step chunks for each caller to reduce parallelization\n+  \/\/ overhead.\n+  void apply(ThreadClosure* cl);\n+\n+  \/\/ Total number of JavaThreads that can be claimed.\n+  uint length() const { return _list.length(); }\n+};\n+\n@@ -276,8 +303,0 @@\n-  \/\/ This is a non-product method that is helpful for testing. It is\n-  \/\/ called at the end of a GC and artificially expands the heap by\n-  \/\/ allocating a number of dead regions. This way we can induce very\n-  \/\/ frequent marking cycles and stress the cleanup \/ concurrent\n-  \/\/ cleanup code more (as all the regions that will be allocated by\n-  \/\/ this method will be found dead by the marking cycle).\n-  void allocate_dummy_regions() PRODUCT_RETURN;\n-\n@@ -494,1 +513,1 @@\n-  void prepare_heap_for_mutators();\n+  void prepare_for_mutator_after_full_collection();\n@@ -774,1 +793,1 @@\n-  void prepare_tlabs_for_mutator();\n+  void prepare_for_mutator_after_young_collection();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":29,"deletions":10,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"runtime\/threadSMR.inline.hpp\"\n@@ -52,0 +53,24 @@\n+inline JavaThread* const* G1JavaThreadsListClaimer::claim(uint& count) {\n+  count = 0;\n+  if (Atomic::load(&_cur_claim) >= _list.length()) {\n+    return nullptr;\n+  }\n+  uint claim = Atomic::fetch_and_add(&_cur_claim, _claim_step);\n+  if (claim >= _list.length()) {\n+    return nullptr;\n+  }\n+  count = MIN2(_list.length() - claim, _claim_step);\n+  return _list.list()->threads() + claim;\n+}\n+\n+inline void G1JavaThreadsListClaimer::apply(ThreadClosure* cl) {\n+  JavaThread* const* list;\n+  uint count;\n+\n+  while ((list = claim(count)) != nullptr) {\n+    for (uint i = 0; i < count; i++) {\n+      cl->do_thread(list[i]);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-  _heap->prepare_heap_for_mutators();\n+  _heap->prepare_for_mutator_after_full_collection();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -156,0 +156,2 @@\n+  _gc_par_phases[ResizeThreadLABs] = new WorkerDataArray<double>(\"ResizeTLABs\", \"Resize TLABs (ms):\", max_gc_threads);\n+\n@@ -176,1 +178,0 @@\n-  _cur_resize_tlab_time_ms = 0.0;\n@@ -187,1 +188,1 @@\n-  _recorded_start_new_cset_time_ms = 0.0;\n+  _recorded_prepare_for_mutator_time_ms = 0.0;\n@@ -492,1 +493,1 @@\n-                        _recorded_start_new_cset_time_ms +\n+                        _recorded_prepare_for_mutator_time_ms +\n@@ -530,0 +531,3 @@\n+  if (UseTLAB && ResizeTLAB) {\n+    debug_phase(_gc_par_phases[ResizeThreadLABs], 1);\n+  }\n@@ -540,4 +544,1 @@\n-  debug_time(\"Start New Collection Set\", _recorded_start_new_cset_time_ms);\n-  if (UseTLAB && ResizeTLAB) {\n-    debug_time(\"Resize TLABs\", _cur_resize_tlab_time_ms);\n-  }\n+  debug_time(\"Prepare For Mutator\", _recorded_prepare_for_mutator_time_ms);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+    ResizeThreadLABs,\n@@ -182,1 +183,0 @@\n-  double _cur_resize_tlab_time_ms;\n@@ -202,1 +202,1 @@\n-  double _recorded_start_new_cset_time_ms;\n+  double _recorded_prepare_for_mutator_time_ms;\n@@ -279,4 +279,0 @@\n-  void record_resize_tlab_time_ms(double ms) {\n-    _cur_resize_tlab_time_ms = ms;\n-  }\n-\n@@ -359,2 +355,2 @@\n-  void record_start_new_cset_time_ms(double time_ms) {\n-    _recorded_start_new_cset_time_ms = time_ms;\n+  void record_prepare_for_mutator_time_ms(double time_ms) {\n+    _recorded_prepare_for_mutator_time_ms = time_ms;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.hpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1025,3 +1025,1 @@\n-  _g1h->start_new_collection_set();\n-\n-  _g1h->prepare_tlabs_for_mutator();\n+  _g1h->prepare_for_mutator_after_young_collection();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,0 +40,2 @@\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/threadSMR.hpp\"\n@@ -704,0 +706,25 @@\n+class G1PostEvacuateCollectionSetCleanupTask2::ResizeTLABsTask : public G1AbstractSubTask {\n+  G1JavaThreadsListClaimer _claimer;\n+\n+  \/\/ There is not much work per thread so the number of threads per worker is high.\n+  static const uint ThreadsPerWorker = 250;\n+\n+public:\n+  ResizeTLABsTask() : G1AbstractSubTask(G1GCPhaseTimes::ResizeThreadLABs), _claimer(ThreadsPerWorker) { }\n+\n+  void do_work(uint worker_id) override {\n+    class ResizeClosure : public ThreadClosure {\n+    public:\n+\n+      void do_thread(Thread* thread) {\n+        static_cast<JavaThread*>(thread)->tlab().resize();\n+      }\n+    } cl;\n+    _claimer.apply(&cl);\n+  }\n+\n+  double worker_cost() const override {\n+    return (double)_claimer.length() \/ ThreadsPerWorker;\n+  }\n+};\n+\n@@ -725,0 +752,3 @@\n+  if (UseTLAB && ResizeTLAB) {\n+    add_parallel_task(new ResizeTLABsTask());\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+\/\/ - Resize TLABs\n@@ -73,0 +74,1 @@\n+  class ResizeTLABsTask;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -247,5 +247,0 @@\n-  develop(uintx, G1DummyRegionsPerGC, 0,                                    \\\n-          \"The number of dummy regions G1 will allocate at the end of \"     \\\n-          \"each evacuation pause in order to artificially fill up the \"     \\\n-          \"heap and stress the marking implementation.\")                    \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -186,0 +186,1 @@\n+        new LogMessageWithLevel(\"Resize TLABs\", Level.DEBUG),\n@@ -195,2 +196,1 @@\n-        new LogMessageWithLevel(\"Start New Collection Set\", Level.DEBUG),\n-        new LogMessageWithLevel(\"Resize TLABs\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Prepare For Mutator\", Level.DEBUG),\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestGCLogMessages.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -110,0 +110,1 @@\n+            \"ResizeTLABs\",\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestG1ParallelPhases.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}