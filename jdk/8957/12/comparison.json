{"files":[{"patch":"@@ -82,13 +82,0 @@\n-void G1BlockOffsetTablePart::update() {\n-  HeapWord* next_addr = _hr->bottom();\n-  HeapWord* const limit = _hr->top();\n-\n-  HeapWord* prev_addr;\n-  while (next_addr < limit) {\n-    prev_addr = next_addr;\n-    next_addr  = prev_addr + block_size(prev_addr);\n-    update_for_block(prev_addr, next_addr);\n-  }\n-  assert(next_addr == limit, \"Should stop the scan at the limit.\");\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -125,0 +125,1 @@\n+  inline size_t block_size(const HeapWord* p, HeapWord* pb) const;\n@@ -132,0 +133,1 @@\n+  \/\/ \"pb\" is the current value of the region's parsable_bottom.\n@@ -133,1 +135,2 @@\n-                                                    const void* addr) const;\n+                                                    const void* addr,\n+                                                    HeapWord* pb) const;\n@@ -155,2 +158,0 @@\n-  void update();\n-\n@@ -164,1 +165,2 @@\n-  inline HeapWord* block_start(const void* addr);\n+  \/\/ \"pb\" is the current value of the region's parsable_bottom.\n+  inline HeapWord* block_start(const void* addr, HeapWord* pb);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-inline HeapWord* G1BlockOffsetTablePart::block_start(const void* addr) {\n+inline HeapWord* G1BlockOffsetTablePart::block_start(const void* addr, HeapWord* const pb) {\n@@ -38,2 +38,2 @@\n-  HeapWord* n = q + block_size(q);\n-  return forward_to_block_containing_addr(q, n, addr);\n+  HeapWord* n = q + block_size(q, pb);\n+  return forward_to_block_containing_addr(q, n, addr, pb);\n@@ -102,0 +102,4 @@\n+inline size_t G1BlockOffsetTablePart::block_size(const HeapWord* p, HeapWord* const pb) const {\n+  return _hr->block_size(p, pb);\n+}\n+\n@@ -129,1 +133,2 @@\n-                                                                          const void* addr) const {\n+                                                                          const void* addr,\n+                                                                          HeapWord* const pb) const {\n@@ -141,1 +146,1 @@\n-    n += block_size(q);\n+    n += block_size(q, pb);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.inline.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-    _cm->mark_in_next_bitmap(_worker_id, o);\n+    _cm->mark_in_bitmap(_worker_id, o);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CodeBlobClosure.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -288,2 +288,0 @@\n-  _verifier->check_bitmaps(\"Humongous Region Allocation\", first_hr);\n-\n@@ -439,1 +437,1 @@\n-                                                              : GCCause::_g1_inc_collection_pause;\n+                                                               : GCCause::_g1_inc_collection_pause;\n@@ -988,1 +986,1 @@\n-void G1CollectedHeap::abort_concurrent_cycle() {\n+bool G1CollectedHeap::abort_concurrent_cycle() {\n@@ -1005,1 +1003,1 @@\n-  concurrent_mark()->concurrent_cycle_abort();\n+  return concurrent_mark()->concurrent_cycle_abort();\n@@ -1030,1 +1028,1 @@\n-  _verifier->check_bitmaps(\"Full GC Start\");\n+  _verifier->verify_bitmap_clear(false \/* above_tams_only *\/);\n@@ -1079,3 +1077,1 @@\n-\n-  \/\/ This call implicitly verifies that the next bitmap is clear after Full GC.\n-  _verifier->check_bitmaps(\"Full GC End\");\n+  _verifier->verify_bitmap_clear(false \/* above_tams_only *\/);\n@@ -1631,1 +1627,1 @@\n-  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.\n+  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmap.\n@@ -1648,4 +1644,2 @@\n-  G1RegionToSpaceMapper* prev_bitmap_storage =\n-    create_aux_memory_mapper(\"Prev Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n-  G1RegionToSpaceMapper* next_bitmap_storage =\n-    create_aux_memory_mapper(\"Next Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n+  G1RegionToSpaceMapper* bitmap_storage =\n+    create_aux_memory_mapper(\"Mark Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n@@ -1653,1 +1647,1 @@\n-  _hrm.initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n+  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n@@ -1700,1 +1694,1 @@\n-  _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);\n+  _cm = new G1ConcurrentMark(this, bitmap_storage);\n@@ -2357,1 +2351,1 @@\n-  return hr->block_start(addr);\n+  return hr->block_start(addr, hr->parsable_bottom_acquire());\n@@ -2362,1 +2356,1 @@\n-  return hr->block_is_obj(addr);\n+  return hr->block_is_obj(addr, hr->parsable_bottom_acquire());\n@@ -2417,1 +2411,1 @@\n-    case VerifyOption::G1UsePrevMarking: return is_obj_dead(obj, hr);\n+    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj, hr);\n@@ -2419,1 +2413,1 @@\n-    default:                            ShouldNotReachHere();\n+    default:                             ShouldNotReachHere();\n@@ -2427,1 +2421,1 @@\n-    case VerifyOption::G1UsePrevMarking: return is_obj_dead(obj);\n+    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj);\n@@ -2429,1 +2423,1 @@\n-    default:                            ShouldNotReachHere();\n+    default:                             ShouldNotReachHere();\n@@ -2477,1 +2471,2 @@\n-               \"TAMS=top-at-mark-start (previous, next)\");\n+               \"TAMS=top-at-mark-start, \"\n+               \"PB=parsable bottom\");\n@@ -2763,1 +2758,0 @@\n-  _verifier->check_bitmaps(\"GC Start\");\n@@ -2779,1 +2773,0 @@\n-  _verifier->check_bitmaps(\"GC End\");\n@@ -2892,0 +2885,1 @@\n+    verifier()->verify_bitmap_clear(true \/* above_tams_only *\/);\n@@ -2923,1 +2917,1 @@\n-      _cm->mark_in_next_bitmap(0 \/* worker_id *\/, pll_head);\n+      _cm->mark_in_bitmap(0 \/* worker_id *\/, pll_head);\n@@ -2966,1 +2960,1 @@\n-void G1CollectedHeap::clear_prev_bitmap_for_region(HeapRegion* hr) {\n+void G1CollectedHeap::clear_bitmap_for_region(HeapRegion* hr) {\n@@ -2968,1 +2962,1 @@\n-  concurrent_mark()->clear_range_in_prev_bitmap(mr);\n+  concurrent_mark()->clear_range_in_bitmap(mr);\n@@ -2976,4 +2970,0 @@\n-  if (G1VerifyBitmaps) {\n-    clear_prev_bitmap_for_region(hr);\n-  }\n-\n@@ -3227,1 +3217,0 @@\n-      _verifier->check_bitmaps(\"Mutator Region Allocation\", new_alloc_region);\n@@ -3284,1 +3273,0 @@\n-      _verifier->check_bitmaps(\"Survivor Region Allocation\", new_alloc_region);\n@@ -3288,1 +3276,0 @@\n-      _verifier->check_bitmaps(\"Old Region Allocation\", new_alloc_region);\n@@ -3311,1 +3298,1 @@\n-    _cm->root_regions()->add(alloc_region->next_top_at_mark_start(), alloc_region->top());\n+    _cm->root_regions()->add(alloc_region->top_at_mark_start(), alloc_region->top());\n@@ -3332,3 +3319,3 @@\n-  \/\/ that we'll update the prev marking info so that they are\n-  \/\/ all under PTAMS and explicitly marked.\n-  _cm->par_mark_in_prev_bitmap(obj);\n+  \/\/ that we'll update the marking info so that they are\n+  \/\/ all below TAMS and explicitly marked.\n+  _cm->raw_mark_in_bitmap(obj);\n@@ -3338,1 +3325,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":26,"deletions":40,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -512,1 +512,1 @@\n-  void abort_concurrent_cycle();\n+  bool abort_concurrent_cycle();\n@@ -627,1 +627,1 @@\n-  void clear_prev_bitmap_for_region(HeapRegion* hr);\n+  void clear_bitmap_for_region(HeapRegion* hr);\n@@ -1238,1 +1238,1 @@\n-  bool is_marked_next(oop obj) const;\n+  bool is_marked(oop obj) const;\n@@ -1247,3 +1247,1 @@\n-\n-  \/\/ Added if it is NULL it isn't dead.\n-\n+  \/\/ If obj is NULL it is not dead.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/g1\/g1ConcurrentMark.inline.hpp\"\n@@ -162,2 +163,2 @@\n-inline bool G1CollectedHeap::is_marked_next(oop obj) const {\n-  return _cm->next_mark_bitmap()->is_marked(obj);\n+inline bool G1CollectedHeap::is_marked(oop obj) const {\n+  return _cm->mark_bitmap()->is_marked(obj);\n@@ -224,1 +225,1 @@\n-  return hr->is_obj_dead(obj, _cm->prev_mark_bitmap());\n+  return hr->is_obj_dead(obj, hr->parsable_bottom());\n@@ -235,1 +236,1 @@\n-   return !is_marked_next(obj) && !hr->is_closed_archive();\n+   return !is_marked(obj) && !hr->is_closed_archive();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -378,1 +378,1 @@\n-    _st->print_cr(\"  \" HR_FORMAT \", P: \" PTR_FORMAT \"N: \" PTR_FORMAT \", age: %4d\",\n+    _st->print_cr(\"  \" HR_FORMAT \", TAMS: \" PTR_FORMAT \" PB: \" PTR_FORMAT \", age: %4d\",\n@@ -380,2 +380,2 @@\n-                  p2i(r->prev_top_at_mark_start()),\n-                  p2i(r->next_top_at_mark_start()),\n+                  p2i(r->top_at_mark_start()),\n+                  p2i(r->parsable_bottom()),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectionSet.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,2 +45,2 @@\n-  \/\/ previous marking cycle (e.g., clearing the next marking\n-  \/\/ bitmap). If that is the case we cannot start a new cycle and\n+  \/\/ previous marking cycle (e.g., clearing the marking bitmap).\n+  \/\/ If that is the case we cannot start a new cycle and\n@@ -67,3 +67,2 @@\n-  \/\/ The next bitmap is currently being cleared or about to be cleared. TAMS and bitmap\n-  \/\/ may be out of sync.\n-  bool _clearing_next_bitmap;\n+  \/\/ The marking bitmap is currently being cleared or about to be cleared.\n+  bool _clearing_bitmap;\n@@ -83,1 +82,1 @@\n-    _clearing_next_bitmap(false),\n+    _clearing_bitmap(false),\n@@ -97,1 +96,1 @@\n-  void set_clearing_next_bitmap(bool v) { _clearing_next_bitmap = v; }\n+  void set_clearing_bitmap(bool v) { _clearing_bitmap = v; }\n@@ -111,1 +110,1 @@\n-  bool clearing_next_bitmap() const { return _clearing_next_bitmap; }\n+  bool clearing_bitmap() const { return _clearing_bitmap; }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectorState.hpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/g1\/g1ConcurrentRebuildAndScrub.hpp\"\n@@ -365,2 +366,1 @@\n-                                   G1RegionToSpaceMapper* prev_bitmap_storage,\n-                                   G1RegionToSpaceMapper* next_bitmap_storage) :\n+                                   G1RegionToSpaceMapper* bitmap_storage) :\n@@ -370,4 +370,1 @@\n-  _mark_bitmap_1(),\n-  _mark_bitmap_2(),\n-  _prev_mark_bitmap(&_mark_bitmap_1),\n-  _next_mark_bitmap(&_mark_bitmap_2),\n+  _mark_bitmap(),\n@@ -422,2 +419,1 @@\n-  _mark_bitmap_1.initialize(g1h->reserved(), prev_bitmap_storage);\n-  _mark_bitmap_2.initialize(g1h->reserved(), next_bitmap_storage);\n+  _mark_bitmap.initialize(g1h->reserved(), bitmap_storage);\n@@ -470,1 +466,1 @@\n-    _tasks[i]->reset(_next_mark_bitmap);\n+    _tasks[i]->reset(mark_bitmap());\n@@ -503,6 +499,0 @@\n-static void clear_mark_if_set(G1CMBitMap* bitmap, HeapWord* addr) {\n-  if (bitmap->is_marked(addr)) {\n-    bitmap->clear(addr);\n-  }\n-}\n-\n@@ -512,3 +502,2 @@\n-  \/\/ Need to clear all mark bits of the humongous object.\n-  clear_mark_if_set(_prev_mark_bitmap, r->bottom());\n-  clear_mark_if_set(_next_mark_bitmap, r->bottom());\n+  \/\/ Need to clear mark bit of the humongous object. Doing this unconditionally is fine.\n+  mark_bitmap()->clear(r->bottom());\n@@ -590,1 +579,1 @@\n-  \/\/ Heap region closure used for clearing the _next_mark_bitmap.\n+  \/\/ Heap region closure used for clearing the _mark_bitmap.\n@@ -614,5 +603,3 @@\n-      \/\/ During a Concurrent Undo Mark cycle, the _next_mark_bitmap is  cleared\n-      \/\/ without swapping with the _prev_mark_bitmap. Therefore, the per region\n-      \/\/ next_top_at_mark_start and live_words data are current wrt\n-      \/\/ _next_mark_bitmap. We use this information to only clear ranges of the\n-      \/\/ bitmap that require clearing.\n+      \/\/ During a Concurrent Undo Mark cycle, the per region top_at_mark_start and\n+      \/\/ live_words data are current wrt to the _mark_bitmap. We use this information\n+      \/\/ to only clear ranges of the bitmap that require clearing.\n@@ -625,1 +612,1 @@\n-        assert(_bitmap->get_next_marked_addr(r->next_top_at_mark_start(), r->end()) == r->end(), \"Should not have marked bits above ntams\");\n+        assert(_bitmap->get_next_marked_addr(r->top_at_mark_start(), r->end()) == r->end(), \"Should not have marked bits above tams\");\n@@ -634,1 +621,1 @@\n-      _bitmap(cm->next_mark_bitmap()),\n+      _bitmap(cm->mark_bitmap()),\n@@ -643,0 +630,7 @@\n+      \/\/ When calling this concurrent to the application, pb must already be reset, so\n+      \/\/ resetting it again does not hurt. At a safepoint, when we clear the bitmap,\n+      \/\/ we must reset pb to be consistent with the (then cleared) marks.\n+      assert(!_suspendible || r->parsable_bottom_acquire() == r->bottom(),\n+             \"While concurrently clearing the bitmap, parsable bottom must already be reset.\");\n+      r->reset_parsable_bottom();\n+\n@@ -695,1 +689,1 @@\n-void G1ConcurrentMark::clear_next_bitmap(WorkerThreads* workers, bool may_yield) {\n+void G1ConcurrentMark::clear_bitmap(WorkerThreads* workers, bool may_yield) {\n@@ -721,1 +715,1 @@\n-  clear_next_bitmap(_concurrent_workers, true);\n+  clear_bitmap(_concurrent_workers, true);\n@@ -728,1 +722,1 @@\n-void G1ConcurrentMark::clear_next_bitmap(WorkerThreads* workers) {\n+void G1ConcurrentMark::clear_bitmap(WorkerThreads* workers) {\n@@ -735,1 +729,1 @@\n-  clear_next_bitmap(workers, false);\n+  clear_bitmap(workers, false);\n@@ -958,1 +952,1 @@\n-  assert(hr->is_old() || hr->next_top_at_mark_start() == hr->bottom(),\n+  assert(hr->is_old() || hr->top_at_mark_start() == hr->bottom(),\n@@ -960,2 +954,2 @@\n-  assert(hr->next_top_at_mark_start() == region->start(),\n-         \"MemRegion start should be equal to nTAMS\");\n+  assert(hr->top_at_mark_start() == region->start(),\n+         \"MemRegion start should be equal to TAMS\");\n@@ -1029,1 +1023,1 @@\n-  _g1h->collector_state()->set_clearing_next_bitmap(false);\n+  _g1h->collector_state()->set_clearing_bitmap(false);\n@@ -1064,1 +1058,11 @@\n-void G1ConcurrentMark::verify_during_pause(G1HeapVerifier::G1VerifyType type, VerifyOption vo, const char* caller) {\n+const char* G1ConcurrentMark::verify_location_string(VerifyLocation location) {\n+  static const char* location_strings[] = { \"Remark Before\",\n+                                            \"Remark After\",\n+                                            \"Remark Overflow\",\n+                                            \"Cleanup Before\",\n+                                            \"Cleanup After\" };\n+  return location_strings[static_cast<std::underlying_type_t<VerifyLocation>>(location)];\n+}\n+\n+void G1ConcurrentMark::verify_during_pause(G1HeapVerifier::G1VerifyType type,\n+                                           VerifyLocation location) {\n@@ -1069,0 +1073,2 @@\n+  const char* caller = verify_location_string(location);\n+\n@@ -1076,2 +1082,1 @@\n-    verifier->verify(type, vo, buffer);\n-  }\n+    verifier->verify(type, VerifyOption::G1UseConcMarking, buffer);\n@@ -1079,1 +1084,6 @@\n-  verifier->check_bitmaps(caller);\n+    \/\/ Only check bitmap in Remark, and not at After-Verification because the regions\n+    \/\/ already have their TAMS'es reset.\n+    if (location != VerifyLocation::RemarkAfter) {\n+      verifier->verify_bitmap_clear(true \/* above_tams_only *\/);\n+    }\n+  }\n@@ -1119,1 +1129,1 @@\n-      size_t const obj_size_in_words = (size_t)cast_to_oop(hr->bottom())->size();\n+      size_t const obj_size_in_words = cast_to_oop(hr->bottom())->size();\n@@ -1162,1 +1172,1 @@\n-      hr->add_to_marked_bytes(marked_bytes);\n+      hr->note_end_of_marking(marked_bytes);\n@@ -1164,1 +1174,0 @@\n-      hr->note_end_of_marking();\n@@ -1198,1 +1207,1 @@\n-class G1UpdateRemSetTrackingAfterRebuild : public HeapRegionClosure {\n+class G1UpdateRegionsAfterRebuild : public HeapRegionClosure {\n@@ -1200,0 +1209,1 @@\n+\n@@ -1201,1 +1211,3 @@\n-  G1UpdateRemSetTrackingAfterRebuild(G1CollectedHeap* g1h) : _g1h(g1h) { }\n+  G1UpdateRegionsAfterRebuild(G1CollectedHeap* g1h) :\n+    _g1h(g1h) {\n+  }\n@@ -1204,0 +1216,2 @@\n+    \/\/ Update the remset tracking state from updating to complete\n+    \/\/ if remembered sets have been rebuilt.\n@@ -1223,1 +1237,1 @@\n-  verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UsePrevMarking, \"Remark before\");\n+  verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyLocation::RemarkBefore);\n@@ -1248,2 +1262,3 @@\n-    \/\/ Install newly created mark bitmap as \"prev\".\n-    swap_mark_bitmaps();\n+    \/\/ All marking completed. Check bitmap now as we will start to reset TAMSes\n+    \/\/ in parallel below so that we can not do this in the After-Remark verification.\n+    _g1h->verifier()->verify_bitmap_clear(true \/* above_tams_only *\/);\n@@ -1251,1 +1266,0 @@\n-    _g1h->collector_state()->set_clearing_next_bitmap(true);\n@@ -1284,1 +1298,1 @@\n-    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UsePrevMarking, \"Remark after\");\n+    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyLocation::RemarkAfter);\n@@ -1287,1 +1301,1 @@\n-    \/\/ Completely reset the marking state since marking completed\n+    \/\/ Completely reset the marking state (except bitmaps) since marking completed.\n@@ -1293,1 +1307,1 @@\n-    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UsePrevMarking, \"Remark overflow\");\n+    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyLocation::RemarkOverflow);\n@@ -1343,1 +1357,1 @@\n-      if (hr->used() > 0 && hr->max_live_bytes() == 0 && !hr->is_young() && !hr->is_closed_archive()) {\n+      if (hr->used() > 0 && hr->live_bytes() == 0 && !hr->is_young() && !hr->is_closed_archive()) {\n@@ -1439,1 +1453,1 @@\n-  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption::G1UsePrevMarking, \"Cleanup before\");\n+  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyLocation::CleanupBefore);\n@@ -1442,0 +1456,2 @@\n+    \/\/ Update the remset tracking information as well as marking all regions\n+    \/\/ as fully parsable.\n@@ -1443,1 +1459,1 @@\n-    G1UpdateRemSetTrackingAfterRebuild cl(_g1h);\n+    G1UpdateRegionsAfterRebuild cl(_g1h);\n@@ -1449,1 +1465,1 @@\n-  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption::G1UsePrevMarking, \"Cleanup after\");\n+  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyLocation::CleanupAfter);\n@@ -1714,2 +1730,0 @@\n-\/\/ When sampling object counts, we already swapped the mark bitmaps, so we need to use\n-\/\/ the prev bitmap determining liveness.\n@@ -1729,1 +1743,1 @@\n-  \/\/ using either the next or prev bitmap.\n+  \/\/ using either the bitmap or after the cycle using the scrubbing information.\n@@ -1739,7 +1753,0 @@\n-\n-void G1ConcurrentMark::swap_mark_bitmaps() {\n-  G1CMBitMap* temp = _prev_mark_bitmap;\n-  _prev_mark_bitmap = _next_mark_bitmap;\n-  _next_mark_bitmap = temp;\n-}\n-\n@@ -1877,2 +1884,3 @@\n-void G1ConcurrentMark::clear_range_in_prev_bitmap(MemRegion mr) {\n-  _prev_mark_bitmap->clear_range(mr);\n+void G1ConcurrentMark::clear_range_in_bitmap(MemRegion mr) {\n+  assert_at_safepoint();\n+  _mark_bitmap.clear_range(mr);\n@@ -1900,2 +1908,2 @@\n-      HeapWord*   bottom        = curr_region->bottom();\n-      HeapWord*   limit         = curr_region->next_top_at_mark_start();\n+      HeapWord* bottom = curr_region->bottom();\n+      HeapWord* limit = curr_region->top_at_mark_start();\n@@ -1998,3 +2006,1 @@\n-void G1ConcurrentMark::rebuild_rem_set_concurrently() {\n-  \/\/ If Remark did not select any regions for RemSet rebuild,\n-  \/\/ skip the rebuild remembered set phase\n+void G1ConcurrentMark::rebuild_and_scrub() {\n@@ -2002,2 +2008,1 @@\n-    log_debug(gc, marking)(\"Skipping Remembered Set Rebuild. No regions selected for rebuild\");\n-    return;\n+    log_debug(gc, marking)(\"Skipping Remembered Set Rebuild. No regions selected for rebuild, will only scrub\");\n@@ -2005,1 +2010,2 @@\n-  _g1h->rem_set()->rebuild_rem_set(this, _concurrent_workers, _worker_id_offset);\n+\n+  G1ConcurrentRebuildAndScrub::rebuild_and_scrub(this, needs_remembered_set_rebuild(), _concurrent_workers);\n@@ -2019,1 +2025,1 @@\n-void G1ConcurrentMark::concurrent_cycle_abort() {\n+bool G1ConcurrentMark::concurrent_cycle_abort() {\n@@ -2030,8 +2036,1 @@\n-    return;\n-  }\n-\n-  \/\/ Clear all marks in the next bitmap for this full gc as it has been used by the\n-  \/\/ marking that is interrupted by this full gc.\n-  {\n-    GCTraceTime(Debug, gc) debug(\"Clear Next Bitmap\");\n-    clear_next_bitmap(_g1h->workers());\n+    return false;\n@@ -2039,3 +2038,0 @@\n-  \/\/ Note we cannot clear the previous marking bitmap here\n-  \/\/ since VerifyDuringGC verifies the objects marked during\n-  \/\/ a full GC against the previous bitmap.\n@@ -2055,3 +2051,3 @@\n-  satb_mq_set.set_active_all_threads(\n-                                 false, \/* new active value *\/\n-                                 satb_mq_set.is_active() \/* expected_active *\/);\n+  satb_mq_set.set_active_all_threads(false, \/* new active value *\/\n+                                     satb_mq_set.is_active() \/* expected_active *\/);\n+  return true;\n@@ -2105,4 +2101,2 @@\n-  st->print_cr(\"Marking Bits (Prev, Next): (CMBitMap*) \" PTR_FORMAT \", (CMBitMap*) \" PTR_FORMAT,\n-               p2i(_prev_mark_bitmap), p2i(_next_mark_bitmap));\n-  _prev_mark_bitmap->print_on_error(st, \" Prev Bits: \");\n-  _next_mark_bitmap->print_on_error(st, \" Next Bits: \");\n+  st->print_cr(\"Marking Bits: (CMBitMap*) \" PTR_FORMAT, p2i(mark_bitmap()));\n+  _mark_bitmap.print_on_error(st, \" Bits: \");\n@@ -2132,3 +2126,3 @@\n-  HeapRegion* hr            = _curr_region;\n-  HeapWord* bottom          = hr->bottom();\n-  HeapWord* limit           = hr->next_top_at_mark_start();\n+  HeapRegion* hr = _curr_region;\n+  HeapWord* bottom = hr->bottom();\n+  HeapWord* limit = hr->top_at_mark_start();\n@@ -2148,2 +2142,2 @@\n-    \/\/ evacuation pause empties the region underneath our feet (NTAMS\n-    \/\/ at bottom). We then do some allocation in the region (NTAMS\n+    \/\/ evacuation pause empties the region underneath our feet (TAMS\n+    \/\/ at bottom). We then do some allocation in the region (TAMS\n@@ -2151,1 +2145,1 @@\n-    \/\/ alloc region (NTAMS will move to top() and the objects\n+    \/\/ alloc region (TAMS will move to top() and the objects\n@@ -2185,3 +2179,3 @@\n-void G1CMTask::reset(G1CMBitMap* next_mark_bitmap) {\n-  guarantee(next_mark_bitmap != NULL, \"invariant\");\n-  _next_mark_bitmap              = next_mark_bitmap;\n+void G1CMTask::reset(G1CMBitMap* mark_bitmap) {\n+  guarantee(mark_bitmap != NULL, \"invariant\");\n+  _mark_bitmap              = mark_bitmap;\n@@ -2671,1 +2665,1 @@\n-        if (_next_mark_bitmap->is_marked(mr.start())) {\n+        if (_mark_bitmap->is_marked(mr.start())) {\n@@ -2679,1 +2673,1 @@\n-      } else if (_next_mark_bitmap->iterate(&bitmap_closure, mr)) {\n+      } else if (_mark_bitmap->iterate(&bitmap_closure, mr)) {\n@@ -2897,1 +2891,1 @@\n-  _next_mark_bitmap(NULL),\n+  _mark_bitmap(NULL),\n@@ -2963,3 +2957,5 @@\n-  _total_used_bytes(0), _total_capacity_bytes(0),\n-  _total_prev_live_bytes(0), _total_next_live_bytes(0),\n-  _total_remset_bytes(0), _total_code_roots_bytes(0)\n+  _total_used_bytes(0),\n+  _total_capacity_bytes(0),\n+  _total_live_bytes(0),\n+  _total_remset_bytes(0),\n+  _total_code_roots_bytes(0)\n@@ -2988,1 +2984,0 @@\n-                          G1PPRL_BYTE_H_FORMAT\n@@ -2994,1 +2989,1 @@\n-                          \"used\", \"prev-live\", \"next-live\", \"gc-eff\",\n+                          \"used\", \"live\", \"gc-eff\",\n@@ -3001,1 +2996,0 @@\n-                          G1PPRL_BYTE_H_FORMAT\n@@ -3007,1 +3001,1 @@\n-                          \"(bytes)\", \"(bytes)\", \"(bytes)\", \"(bytes\/ms)\",\n+                          \"(bytes)\", \"(bytes)\", \"(bytes\/ms)\",\n@@ -3021,2 +3015,1 @@\n-  size_t prev_live_bytes = r->live_bytes();\n-  size_t next_live_bytes = r->next_live_bytes();\n+  size_t live_bytes      = r->live_bytes();\n@@ -3031,2 +3024,1 @@\n-  _total_prev_live_bytes += prev_live_bytes;\n-  _total_next_live_bytes += next_live_bytes;\n+  _total_live_bytes      += live_bytes;\n@@ -3048,1 +3040,0 @@\n-                        G1PPRL_BYTE_FORMAT\n@@ -3054,1 +3045,1 @@\n-                        used_bytes, prev_live_bytes, next_live_bytes, gc_efficiency.buffer(),\n+                        used_bytes, live_bytes, gc_efficiency.buffer(),\n@@ -3073,2 +3064,1 @@\n-                         G1PPRL_SUM_MB_PERC_FORMAT(\"prev-live\")\n-                         G1PPRL_SUM_MB_PERC_FORMAT(\"next-live\")\n+                         G1PPRL_SUM_MB_PERC_FORMAT(\"live\")\n@@ -3080,4 +3070,2 @@\n-                         bytes_to_mb(_total_prev_live_bytes),\n-                         percent_of(_total_prev_live_bytes, _total_capacity_bytes),\n-                         bytes_to_mb(_total_next_live_bytes),\n-                         percent_of(_total_next_live_bytes, _total_capacity_bytes),\n+                         bytes_to_mb(_total_live_bytes),\n+                         percent_of(_total_live_bytes, _total_capacity_bytes),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":112,"deletions":124,"binary":false,"changes":236,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -219,1 +219,1 @@\n-\/\/ Typically they contain the areas from nTAMS to top of the regions.\n+\/\/ Typically they contain the areas from TAMS to top of the regions.\n@@ -295,4 +295,1 @@\n-  G1CMBitMap              _mark_bitmap_1;\n-  G1CMBitMap              _mark_bitmap_2;\n-  G1CMBitMap*             _prev_mark_bitmap; \/\/ Completed mark bitmap\n-  G1CMBitMap*             _next_mark_bitmap; \/\/ Under-construction mark bitmap\n+  G1CMBitMap              _mark_bitmap;\n@@ -362,1 +359,10 @@\n-  void verify_during_pause(G1HeapVerifier::G1VerifyType type, VerifyOption vo, const char* caller);\n+  enum class VerifyLocation {\n+    RemarkBefore,\n+    RemarkAfter,\n+    RemarkOverflow,\n+    CleanupBefore,\n+    CleanupAfter\n+  };\n+  static const char* verify_location_string(VerifyLocation location);\n+  void verify_during_pause(G1HeapVerifier::G1VerifyType type,\n+                           VerifyLocation location);\n@@ -446,1 +452,1 @@\n-  void clear_next_bitmap(WorkerThreads* workers, bool may_yield);\n+  void clear_bitmap(WorkerThreads* workers, bool may_yield);\n@@ -458,0 +464,2 @@\n+  \/\/ To be called when an object is marked the first time, e.g. after a successful\n+  \/\/ mark_in_bitmap call. Updates various statistics data.\n@@ -460,1 +468,1 @@\n-  \/\/ live words between bottom and nTAMS.\n+  \/\/ live words between bottom and TAMS.\n@@ -496,1 +504,1 @@\n-  void concurrent_cycle_abort();\n+  bool concurrent_cycle_abort();\n@@ -519,2 +527,1 @@\n-                   G1RegionToSpaceMapper* prev_bitmap_storage,\n-                   G1RegionToSpaceMapper* next_bitmap_storage);\n+                   G1RegionToSpaceMapper* bitmap_storage);\n@@ -525,2 +532,1 @@\n-  const G1CMBitMap* const prev_mark_bitmap() const { return _prev_mark_bitmap; }\n-  G1CMBitMap* next_mark_bitmap() const { return _next_mark_bitmap; }\n+  G1CMBitMap* mark_bitmap() const { return (G1CMBitMap*)&_mark_bitmap; }\n@@ -543,1 +549,1 @@\n-  void clear_next_bitmap(WorkerThreads* workers);\n+  void clear_bitmap(WorkerThreads* workers);\n@@ -566,2 +572,0 @@\n-  void swap_mark_bitmaps();\n-\n@@ -569,3 +573,0 @@\n-  \/\/ Mark in the previous bitmap. Caution: the prev bitmap is usually read-only, so use\n-  \/\/ this carefully.\n-  inline void par_mark_in_prev_bitmap(oop p);\n@@ -573,4 +574,3 @@\n-  \/\/ Clears marks for all objects in the given range, for the prev or\n-  \/\/ next bitmaps.  Caution: the previous bitmap is usually\n-  \/\/ read-only, so use this carefully!\n-  void clear_range_in_prev_bitmap(MemRegion mr);\n+  \/\/ Mark in the marking bitmap. Used during evacuation failure to\n+  \/\/ remember what objects need handling. Not for use during marking.\n+  inline void raw_mark_in_bitmap(oop p);\n@@ -578,1 +578,4 @@\n-  inline bool is_marked_in_prev_bitmap(oop p) const;\n+  \/\/ Clears marks for all objects in the given range in the marking\n+  \/\/ bitmap. This should only be used clean the bitmap during a\n+  \/\/ safepoint.\n+  void clear_range_in_bitmap(MemRegion mr);\n@@ -595,3 +598,3 @@\n-  \/\/ Mark the given object on the next bitmap if it is below nTAMS.\n-  inline bool mark_in_next_bitmap(uint worker_id, HeapRegion* const hr, oop const obj);\n-  inline bool mark_in_next_bitmap(uint worker_id, oop const obj);\n+  \/\/ Mark the given object on the marking bitmap if it is below TAMS.\n+  inline bool mark_in_bitmap(uint worker_id, HeapRegion* const hr, oop const obj);\n+  inline bool mark_in_bitmap(uint worker_id, oop const obj);\n@@ -599,1 +602,1 @@\n-  inline bool is_marked_in_next_bitmap(oop p) const;\n+  inline bool is_marked_in_bitmap(oop p) const;\n@@ -604,2 +607,3 @@\n-  \/\/ Rebuilds the remembered sets for chosen regions in parallel and concurrently to the application.\n-  void rebuild_rem_set_concurrently();\n+  \/\/ Rebuilds the remembered sets for chosen regions in parallel and concurrently\n+  \/\/ to the application. Also scrubs dead objects to ensure region is parsable.\n+  void rebuild_and_scrub();\n@@ -630,1 +634,1 @@\n-  G1CMBitMap*                 _next_mark_bitmap;\n+  G1CMBitMap*                 _mark_bitmap;\n@@ -736,1 +740,1 @@\n-  void reset(G1CMBitMap* next_mark_bitmap);\n+  void reset(G1CMBitMap* mark_bitmap);\n@@ -780,1 +784,1 @@\n-  \/\/ the local queue if below the finger. obj is required to be below its region's NTAMS.\n+  \/\/ the local queue if below the finger. obj is required to be below its region's TAMS.\n@@ -785,1 +789,1 @@\n-  \/\/ e.g. obj is below its containing region's NTAMS.\n+  \/\/ e.g. obj is below its containing region's TAMS.\n@@ -787,1 +791,1 @@\n-  \/\/ Returns true if the reference caused a mark to be set in the next bitmap.\n+  \/\/ Returns true if the reference caused a mark to be set in the marking bitmap.\n@@ -844,2 +848,1 @@\n-  size_t _total_prev_live_bytes;\n-  size_t _total_next_live_bytes;\n+  size_t _total_live_bytes;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.hpp","additions":41,"deletions":38,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,1 +53,1 @@\n-  if (hr->obj_allocated_since_next_marking(obj)) {\n+  if (hr->obj_allocated_since_marking_start(obj)) {\n@@ -63,1 +63,1 @@\n-  return _g1h->is_marked_next(obj);\n+  return _g1h->is_marked(obj);\n@@ -78,1 +78,1 @@\n-inline bool G1ConcurrentMark::mark_in_next_bitmap(uint const worker_id, oop const obj) {\n+inline bool G1ConcurrentMark::mark_in_bitmap(uint const worker_id, oop const obj) {\n@@ -80,1 +80,1 @@\n-  return mark_in_next_bitmap(worker_id, hr, obj);\n+  return mark_in_bitmap(worker_id, hr, obj);\n@@ -83,1 +83,1 @@\n-inline bool G1ConcurrentMark::mark_in_next_bitmap(uint const worker_id, HeapRegion* const hr, oop const obj) {\n+inline bool G1ConcurrentMark::mark_in_bitmap(uint const worker_id, HeapRegion* const hr, oop const obj) {\n@@ -87,1 +87,1 @@\n-  if (hr->obj_allocated_since_next_marking(obj)) {\n+  if (hr->obj_allocated_since_marking_start(obj)) {\n@@ -91,1 +91,1 @@\n-  \/\/ Some callers may have stale objects to mark above nTAMS after humongous reclaim.\n+  \/\/ Some callers may have stale objects to mark above TAMS after humongous reclaim.\n@@ -93,1 +93,1 @@\n-  assert(!hr->is_continues_humongous(), \"Should not try to mark object \" PTR_FORMAT \" in Humongous continues region %u above nTAMS \" PTR_FORMAT, p2i(obj), hr->hrm_index(), p2i(hr->next_top_at_mark_start()));\n+  assert(!hr->is_continues_humongous(), \"Should not try to mark object \" PTR_FORMAT \" in Humongous continues region %u above TAMS \" PTR_FORMAT, p2i(obj), hr->hrm_index(), p2i(hr->top_at_mark_start()));\n@@ -95,1 +95,1 @@\n-  bool success = _next_mark_bitmap->par_mark(obj);\n+  bool success = _mark_bitmap.par_mark(obj);\n@@ -132,1 +132,1 @@\n-  assert(task_entry.is_array_slice() || _next_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())), \"invariant\");\n+  assert(task_entry.is_array_slice() || _mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())), \"invariant\");\n@@ -180,1 +180,1 @@\n-  assert(task_entry.is_array_slice() || _next_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())),\n+  assert(task_entry.is_array_slice() || _mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())),\n@@ -237,1 +237,1 @@\n-  if (!_cm->mark_in_next_bitmap(_worker_id, obj)) {\n+  if (!_cm->mark_in_bitmap(_worker_id, obj)) {\n@@ -289,2 +289,2 @@\n-inline void G1ConcurrentMark::par_mark_in_prev_bitmap(oop p) {\n-  _prev_mark_bitmap->par_mark(p);\n+inline void G1ConcurrentMark::raw_mark_in_bitmap(oop p) {\n+  _mark_bitmap.par_mark(p);\n@@ -293,1 +293,1 @@\n-bool G1ConcurrentMark::is_marked_in_prev_bitmap(oop p) const {\n+bool G1ConcurrentMark::is_marked_in_bitmap(oop p) const {\n@@ -295,6 +295,1 @@\n-  return _prev_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(p));\n-}\n-\n-bool G1ConcurrentMark::is_marked_in_next_bitmap(oop p) const {\n-  assert(p != NULL && oopDesc::is_oop(p), \"expected an oop\");\n-  return _next_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(p));\n+  return _mark_bitmap.is_marked(cast_from_oop<HeapWord*>(p));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.inline.hpp","additions":17,"deletions":22,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,4 @@\n+G1CMBitMap::G1CMBitMap() : MarkBitMap(), _listener() {\n+  _listener.set_bitmap(this);\n+}\n+\n@@ -44,15 +48,0 @@\n-\n-void G1CMBitMap::clear_region(HeapRegion* region) {\n- if (!region->is_empty()) {\n-   MemRegion mr(region->bottom(), region->top());\n-   clear_range(mr);\n- }\n-}\n-\n-#ifdef ASSERT\n-void G1CMBitMap::check_mark(HeapWord* addr) {\n-  assert(G1CollectedHeap::heap()->is_in(addr),\n-         \"Trying to access bitmap \" PTR_FORMAT \" for address \" PTR_FORMAT \" not in the heap.\",\n-         p2i(this), p2i(addr));\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.cpp","additions":5,"deletions":16,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,0 @@\n-\n@@ -67,4 +66,0 @@\n-protected:\n-\n-  virtual void check_mark(HeapWord* addr) NOT_DEBUG_RETURN;\n-\n@@ -72,2 +67,1 @@\n-\n-  G1CMBitMap() : MarkBitMap(), _listener() { _listener.set_bitmap(this); }\n+  G1CMBitMap();\n@@ -80,2 +74,0 @@\n-\n-  void clear_region(HeapRegion* hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.hpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -236,3 +236,3 @@\n-bool G1ConcurrentMarkThread::phase_rebuild_remembered_sets() {\n-  G1ConcPhaseTimer p(_cm, \"Concurrent Rebuild Remembered Sets\");\n-  _cm->rebuild_rem_set_concurrently();\n+bool G1ConcurrentMarkThread::phase_rebuild_and_scrub() {\n+  G1ConcPhaseTimer p(_cm, \"Concurrent Rebuild Remembered Sets and Scrub Regions\");\n+  _cm->rebuild_and_scrub();\n@@ -293,2 +293,2 @@\n-  \/\/ Phase 4: Rebuild remembered sets.\n-  if (phase_rebuild_remembered_sets()) return;\n+  \/\/ Phase 4: Rebuild remembered sets and scrub dead objects.\n+  if (phase_rebuild_and_scrub()) return;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkThread.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -70,1 +70,1 @@\n-  bool phase_rebuild_remembered_sets();\n+  bool phase_rebuild_and_scrub();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkThread.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,358 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/g1\/g1ConcurrentRebuildAndScrub.hpp\"\n+\n+#include \"gc\/g1\/g1ConcurrentMark.inline.hpp\"\n+#include \"gc\/g1\/g1ConcurrentMarkBitMap.inline.hpp\"\n+#include \"gc\/g1\/g1_globals.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+#include \"gc\/g1\/heapRegionManager.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Worker task that scans the objects in the old generation to rebuild the remembered\n+\/\/ set and at the same time scrubs dead objects by replacing them with filler objects\n+\/\/ to make them completely parseable.\n+\/\/\n+\/\/ The remark pause recorded two pointers within the regions:\n+\/\/\n+\/\/ parsable_bottom (pb): this is the TAMS of the recent marking for that region. Objects\n+\/\/                       below that may or may not be dead (as per mark bitmap).\n+\/\/                       This task needs to remove the dead objects, replacing them\n+\/\/                       with filler objects so that they can be walked through later.\n+\/\/\n+\/\/ top_at_rebuild_start (tars): at rebuild phase start we record the current top: up to\n+\/\/                              this address (live) objects need to be scanned for references\n+\/\/                              that might need to be added to the remembered sets.\n+\/\/\n+\/\/ Note that bottom <= parsable_bottom <= tars; if there is no tars (i.e. NULL),\n+\/\/ obviously there can not be a parsable_bottom.\n+\/\/\n+\/\/ We need to scrub and scan objects to rebuild remembered sets until parsable_bottom;\n+\/\/ we need to scan objects to rebuild remembered sets until tars.\n+class G1RebuildRSAndScrubTask : public WorkerTask {\n+  G1ConcurrentMark* _cm;\n+  HeapRegionClaimer _hr_claimer;\n+\n+  const bool _should_rebuild_remset;\n+\n+  class G1RebuildRSAndScrubRegionClosure : public HeapRegionClosure {\n+    G1ConcurrentMark* _cm;\n+    const G1CMBitMap* _bitmap;\n+\n+    G1RebuildRemSetClosure _rebuild_closure;\n+\n+    const bool _should_rebuild_remset;\n+\n+    size_t _marked_words;\n+    size_t _processed_words;\n+\n+    const size_t ProcessingYieldLimitInWords = G1RebuildRemSetChunkSize \/ HeapWordSize;\n+\n+    void reset_marked_words() {\n+      _marked_words = 0;\n+    }\n+\n+    void reset_processed_words() {\n+      _processed_words = 0;\n+    }\n+\n+    void assert_marked_words(HeapRegion* hr) {\n+      assert((_marked_words * HeapWordSize) == hr->marked_bytes(),\n+             \"Mismatch between marking and re-calculation for region %u, %zu != %zu\",\n+             hr->hrm_index(), (_marked_words * HeapWordSize), hr->marked_bytes());\n+    }\n+\n+    void add_processed_words(size_t processed) {\n+      _processed_words += processed;\n+      _marked_words += processed;\n+    }\n+\n+    \/\/ Yield if enough has been processed; returns if the concurrent marking cycle\n+    \/\/ has been aborted for any reason.\n+    bool yield_if_necessary() {\n+      if (_processed_words >= ProcessingYieldLimitInWords) {\n+        reset_processed_words();\n+        _cm->do_yield_check();\n+      }\n+      return _cm->has_aborted();\n+    }\n+\n+    \/\/ Returns whether the top at rebuild start value for the given region indicates\n+    \/\/ that there is some rebuild or scrubbing work.\n+    \/\/\n+    \/\/ Based on the results of G1RemSetTrackingPolicy::needs_scan_for_rebuild(),\n+    \/\/ the value may be changed to nullptr during rebuilding if the region has either:\n+    \/\/  - been allocated after rebuild start, or\n+    \/\/  - been eagerly reclaimed by a young collection (only humongous)\n+    bool should_rebuild_or_scrub(HeapRegion* hr) const {\n+      return _cm->top_at_rebuild_start(hr->hrm_index()) != nullptr;\n+    }\n+\n+    \/\/ Helper used by both humongous objects and when chunking an object larger than the\n+    \/\/ G1RebuildRemSetChunkSize. The heap region is needed to ensure a humongous object\n+    \/\/ is not eagerly reclaimed during yielding.\n+    \/\/ Returns whether marking has been aborted.\n+    bool scan_large_object(HeapRegion* hr, const oop obj, MemRegion scan_range) {\n+      HeapWord* start = scan_range.start();\n+      HeapWord* limit = scan_range.end();\n+      do {\n+        MemRegion mr(start, MIN2(start + ProcessingYieldLimitInWords, limit));\n+        obj->oop_iterate(&_rebuild_closure, mr);\n+\n+        \/\/ Update processed words and yield, for humongous objects we will yield\n+        \/\/ after each chunk.\n+        add_processed_words(mr.word_size());\n+        bool mark_aborted = yield_if_necessary();\n+        if (mark_aborted) {\n+          return true;\n+        } else if (!should_rebuild_or_scrub(hr)) {\n+          \/\/ We need to check should_rebuild_or_scrub() again (for humongous objects)\n+          \/\/ because the region might have been eagerly reclaimed during the yield.\n+          log_trace(gc, marking)(\"Rebuild aborted for eagerly reclaimed humongous region: %u\", hr->hrm_index());\n+          return false;\n+        }\n+\n+        \/\/ Step to next chunk of the humongous object\n+        start = mr.end();\n+      } while (start < limit);\n+      return false;\n+    }\n+\n+    \/\/ Scan for references into regions that need remembered set update for the given\n+    \/\/ live object. Returns the offset to the next object.\n+    size_t scan_object(HeapRegion* hr, HeapWord* current) {\n+      oop obj = cast_to_oop(current);\n+      size_t obj_size = obj->size();\n+\n+      if (!_should_rebuild_remset) {\n+        \/\/ Not rebuilding, just step to next object.\n+        add_processed_words(obj_size);\n+      } else if (obj_size > ProcessingYieldLimitInWords) {\n+        \/\/ Large object, needs to be chunked to avoid stalling safepoints.\n+        MemRegion mr(current, obj_size);\n+        scan_large_object(hr, obj, mr);\n+        \/\/ No need to add to _processed_words, this is all handled by the above call;\n+        \/\/ we also ignore the marking abort result of scan_large_object - we will check\n+        \/\/ again right afterwards.\n+      } else {\n+        \/\/ Object smaller than yield limit, process it fully.\n+        obj->oop_iterate(&_rebuild_closure);\n+        \/\/ Update how much we have processed. Yield check in main loop\n+        \/\/ will handle this case.\n+        add_processed_words(obj_size);\n+      }\n+\n+      return obj_size;\n+    }\n+\n+    \/\/ Scrub a range of dead objects starting at scrub_start. Will never scrub past limit.\n+    HeapWord* scrub_to_next_live(HeapRegion* hr, HeapWord* scrub_start, HeapWord* limit) {\n+      assert(!_bitmap->is_marked(scrub_start), \"Should not scrub live object\");\n+\n+      HeapWord* scrub_end = _bitmap->get_next_marked_addr(scrub_start, limit);\n+      hr->fill_range_with_dead_objects(scrub_start, scrub_end);\n+\n+      \/\/ Return the next object to handle.\n+      return scrub_end;\n+    }\n+\n+    \/\/ Scan the given region from bottom to parsable_bottom. Returns whether marking has\n+    \/\/ been aborted.\n+    bool scan_and_scrub_to_pb(HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n+\n+      while (start < limit) {\n+        if (_bitmap->is_marked(start)) {\n+          \/\/  Live object, need to scan to rebuild remembered sets for this object.\n+          start += scan_object(hr, start);\n+        } else {\n+          \/\/ Found dead object (which klass has potentially been unloaded). Scrub to next\n+          \/\/ marked object and continue.\n+          start = scrub_to_next_live(hr, start, limit);\n+        }\n+\n+        bool mark_aborted = yield_if_necessary();\n+        if (mark_aborted) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Scan the given region from parsable_bottom to tars. Returns whether marking has\n+    \/\/ been aborted.\n+    bool scan_from_pb_to_tars(HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n+\n+      while (start < limit) {\n+        start += scan_object(hr, start);\n+        \/\/ Avoid stalling safepoints and stop iteration if mark cycle has been aborted.\n+        bool mark_aborted = yield_if_necessary();\n+        if (mark_aborted) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Scan and scrub the given region to tars. Returns whether marking has\n+    \/\/ been aborted.\n+    bool scan_and_scrub_region(HeapRegion* hr, HeapWord* const pb) {\n+      assert(should_rebuild_or_scrub(hr), \"must be\");\n+\n+      reset_marked_words();\n+      log_trace(gc, marking)(\"Scrub and rebuild region: \" HR_FORMAT \" pb: \" PTR_FORMAT \" TARS: \" PTR_FORMAT,\n+                             HR_FORMAT_PARAMS(hr), p2i(pb), p2i(_cm->top_at_rebuild_start(hr->hrm_index())));\n+\n+      if (scan_and_scrub_to_pb(hr, hr->bottom(), pb)) {\n+        log_trace(gc, marking)(\"Scan and scrub aborted for region: %u\", hr->hrm_index());\n+        return true;\n+      }\n+\n+      \/\/ Scrubbing completed for this region - notify that we are done with it, resetting\n+      \/\/ pb to bottom.\n+      hr->note_end_of_scrubbing();\n+      \/\/ Assert that the size of marked objects from the marking matches\n+      \/\/ the size of the objects which we scanned to rebuild remembered sets.\n+      assert_marked_words(hr);\n+\n+      \/\/ Rebuild from TAMS (= parsable_bottom) to TARS.\n+      if (scan_from_pb_to_tars(hr, pb, _cm->top_at_rebuild_start(hr->hrm_index()))) {\n+        log_trace(gc, marking)(\"Rebuild aborted for region: %u (%s)\", hr->hrm_index(), hr->get_short_type_str());\n+        return true;\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Scan a humongous region for remembered set updates. Scans in chunks to avoid\n+    \/\/ stalling safepoints. Returns whether the concurrent marking phase has been aborted.\n+    bool scan_humongous_region(HeapRegion* hr, HeapWord* const pb) {\n+      assert(should_rebuild_or_scrub(hr), \"must be\");\n+\n+      if (!_should_rebuild_remset) {\n+        \/\/ When not rebuilding there is nothing to do for humongous objects.\n+        return false;\n+      }\n+\n+      \/\/ At this point we should only have live humongous objects, that\n+      \/\/ means it must either be:\n+      \/\/ - marked\n+      \/\/ - or seen as fully parsable, i.e. allocated after the marking started\n+      oop humongous = cast_to_oop(hr->humongous_start_region()->bottom());\n+      assert(_bitmap->is_marked(humongous) || pb == hr->bottom(),\n+             \"Humongous object not live\");\n+\n+      reset_marked_words();\n+      log_trace(gc, marking)(\"Rebuild for humongous region: \" HR_FORMAT \" pb: \" PTR_FORMAT \" TARS: \" PTR_FORMAT,\n+                              HR_FORMAT_PARAMS(hr), p2i(pb), p2i(_cm->top_at_rebuild_start(hr->hrm_index())));\n+\n+      \/\/ Scan the humongous object in chunks from bottom to top to rebuild remembered sets.\n+      HeapWord* humongous_end = hr->humongous_start_region()->bottom() + humongous->size();\n+      MemRegion mr(hr->bottom(), MIN2(hr->top(), humongous_end));\n+\n+      bool mark_aborted = scan_large_object(hr, humongous, mr);\n+      if (mark_aborted) {\n+        log_trace(gc, marking)(\"Rebuild aborted for region: %u (%s)\", hr->hrm_index(), hr->get_short_type_str());\n+        return true;\n+      } else if (_bitmap->is_marked(humongous) && should_rebuild_or_scrub(hr)) {\n+        \/\/ Only verify that the marked size matches the rebuilt size if this object was marked\n+        \/\/ and the object should still be handled. The should_rebuild_or_scrub() state can\n+        \/\/ change during rebuild for humongous objects that are eagerly reclaimed so we need to\n+        \/\/ check this.\n+        \/\/ If the object has not been marked the size from marking will be 0.\n+        assert_marked_words(hr);\n+      }\n+      return false;\n+    }\n+\n+  public:\n+    G1RebuildRSAndScrubRegionClosure(G1ConcurrentMark* cm, bool should_rebuild_remset, uint worker_id) :\n+      _cm(cm),\n+      _bitmap(_cm->mark_bitmap()),\n+      _rebuild_closure(G1CollectedHeap::heap(), worker_id),\n+      _should_rebuild_remset(should_rebuild_remset),\n+      _marked_words(0),\n+      _processed_words(0) { }\n+\n+    bool do_heap_region(HeapRegion* hr) {\n+      \/\/ Avoid stalling safepoints and stop iteration if mark cycle has been aborted.\n+      _cm->do_yield_check();\n+      if (_cm->has_aborted()) {\n+        return true;\n+      }\n+\n+      HeapWord* const pb = hr->parsable_bottom_acquire();\n+\n+      if (!should_rebuild_or_scrub(hr)) {\n+        \/\/ Region has been allocated during this phase, no need to either scrub or\n+        \/\/ scan to rebuild remembered sets.\n+        log_trace(gc, marking)(\"Scrub and rebuild region skipped for \" HR_FORMAT \" pb: \" PTR_FORMAT,\n+                               HR_FORMAT_PARAMS(hr), p2i(pb));\n+        assert(hr->bottom() == pb, \"Region must be fully parsable\");\n+        return false;\n+      }\n+\n+      bool mark_aborted;\n+      if (hr->needs_scrubbing()) {\n+        \/\/ This is a region with potentially unparsable (dead) objects.\n+        mark_aborted = scan_and_scrub_region(hr, pb);\n+      } else {\n+        assert(hr->is_humongous(), \"must be, but %u is %s\", hr->hrm_index(), hr->get_short_type_str());\n+        \/\/ No need to scrub humongous, but we should scan it to rebuild remsets.\n+        mark_aborted = scan_humongous_region(hr, pb);\n+      }\n+\n+      return mark_aborted;\n+    }\n+  };\n+\n+public:\n+  G1RebuildRSAndScrubTask(G1ConcurrentMark* cm, bool should_rebuild_remset, uint num_workers) :\n+    WorkerTask(\"Scrub dead objects\"),\n+    _cm(cm),\n+    _hr_claimer(num_workers),\n+    _should_rebuild_remset(should_rebuild_remset) { }\n+\n+  void work(uint worker_id) {\n+    SuspendibleThreadSetJoiner sts_join;\n+\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+    G1RebuildRSAndScrubRegionClosure cl(_cm, _should_rebuild_remset, worker_id);\n+    g1h->heap_region_par_iterate_from_worker_offset(&cl, &_hr_claimer, worker_id);\n+  }\n+};\n+\n+void G1ConcurrentRebuildAndScrub::rebuild_and_scrub(G1ConcurrentMark* cm, bool should_rebuild_remset, WorkerThreads* workers) {\n+  uint num_workers = workers->active_workers();\n+\n+  G1RebuildRSAndScrubTask task(cm, should_rebuild_remset, num_workers);\n+  workers->run_task(&task, num_workers);\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRebuildAndScrub.cpp","additions":358,"deletions":0,"binary":false,"changes":358,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1CONCURRENTREBUILDANDSCRUB_HPP\n+#define SHARE_GC_G1_G1CONCURRENTREBUILDANDSCRUB_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class G1ConcurrentMark;\n+class WorkerThreads;\n+\n+\/\/ Rebuild and scrubbing helper class.\n+class G1ConcurrentRebuildAndScrub : AllStatic {\n+public:\n+\n+  static void rebuild_and_scrub(G1ConcurrentMark* cm, bool should_rebuild_remset, WorkerThreads* workers);\n+};\n+\n+\n+#endif \/* SHARE_GC_G1_G1CONCURRENTREBUILDANDSCRUB_HPP *\/\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRebuildAndScrub.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -71,0 +71,1 @@\n+    size_t obj_size = obj->size();\n@@ -79,1 +80,1 @@\n-    assert(_cm->is_marked_in_prev_bitmap(obj), \"should be correctly marked\");\n+    assert(_cm->is_marked_in_bitmap(obj), \"should be correctly marked\");\n@@ -81,11 +82,3 @@\n-      \/\/ For the next marking info we'll only mark the\n-      \/\/ self-forwarded objects explicitly if we are during\n-      \/\/ concurrent start (since, normally, we only mark objects pointed\n-      \/\/ to by roots if we succeed in copying them). By marking all\n-      \/\/ self-forwarded objects we ensure that we mark any that are\n-      \/\/ still pointed to be roots. During concurrent marking, and\n-      \/\/ after concurrent start, we don't need to mark any objects\n-      \/\/ explicitly and all objects in the CSet are considered\n-      \/\/ (implicitly) live. So, we won't mark them explicitly and\n-      \/\/ we'll leave them over NTAMS.\n-      _cm->mark_in_next_bitmap(_worker_id, obj);\n+      \/\/ If the evacuation failure occurs during concurrent start we should do\n+      \/\/ any additional necessary per-object actions.\n+      _cm->add_to_liveness(_worker_id, obj, obj_size);\n@@ -93,1 +86,0 @@\n-    size_t obj_size = obj->size();\n@@ -106,2 +98,1 @@\n-  \/\/ accordingly. Since we clear and use the prev bitmap for marking objects that\n-  \/\/ failed evacuation, there is no work to be done there.\n+  \/\/ accordingly.\n@@ -113,24 +104,1 @@\n-    size_t gap_size = pointer_delta(end, start);\n-    MemRegion mr(start, gap_size);\n-    if (gap_size >= CollectedHeap::min_fill_size()) {\n-      CollectedHeap::fill_with_objects(start, gap_size);\n-\n-      HeapWord* end_first_obj = start + cast_to_oop(start)->size();\n-      _hr->update_bot_for_block(start, end_first_obj);\n-      \/\/ Fill_with_objects() may have created multiple (i.e. two)\n-      \/\/ objects, as the max_fill_size() is half a region.\n-      \/\/ After updating the BOT for the first object, also update the\n-      \/\/ BOT for the second object to make the BOT complete.\n-      if (end_first_obj != end) {\n-        _hr->update_bot_for_block(end_first_obj, end);\n-#ifdef ASSERT\n-        size_t size_second_obj = cast_to_oop(end_first_obj)->size();\n-        HeapWord* end_of_second_obj = end_first_obj + size_second_obj;\n-        assert(end == end_of_second_obj,\n-               \"More than two objects were used to fill the area from \" PTR_FORMAT \" to \" PTR_FORMAT \", \"\n-               \"second objects size \" SIZE_FORMAT \" ends at \" PTR_FORMAT,\n-               p2i(start), p2i(end), size_second_obj, p2i(end_of_second_obj));\n-#endif\n-      }\n-    }\n-    assert(!_cm->is_marked_in_prev_bitmap(cast_to_oop(start)), \"should not be marked in prev bitmap\");\n+    _hr->fill_range_with_dead_objects(start, end);\n@@ -167,1 +135,1 @@\n-    \/\/ All objects that failed evacuation has been marked in the prev bitmap.\n+    \/\/ All objects that failed evacuation has been marked in the bitmap.\n@@ -169,1 +137,1 @@\n-    G1CMBitMap* bitmap = const_cast<G1CMBitMap*>(_g1h->concurrent_mark()->prev_mark_bitmap());\n+    G1CMBitMap* bitmap = _g1h->concurrent_mark()->mark_bitmap();\n@@ -173,0 +141,12 @@\n+    \/\/ Now clear all the marks to be ready for a new marking cyle.\n+    if (!during_concurrent_start) {\n+      assert(hr->top_at_mark_start() == hr->bottom(), \"TAMS must be bottom to make all objects look live\");\n+      _g1h->clear_bitmap_for_region(hr);\n+    } else {\n+      assert(hr->top_at_mark_start() == hr->top(), \"TAMS must be top for bitmap to have any value\");\n+      \/\/ Keep the bits.\n+    }\n+    \/\/ We never evacuate Old (non-humongous, non-archive) regions during scrubbing\n+    \/\/ (only afterwards); other regions (young, humongous, archive) never need\n+    \/\/ scrubbing, so the following must hold.\n+    assert(hr->parsable_bottom() == hr->bottom(), \"PB must be bottom to make the whole area parsable\");\n@@ -201,1 +181,0 @@\n-    _g1h->verifier()->check_bitmaps(\"Self-Forwarding Ptr Removal\", hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.cpp","additions":21,"deletions":42,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  return _heap->concurrent_mark()->next_mark_bitmap();\n+  return _heap->concurrent_mark()->mark_bitmap();\n@@ -123,1 +123,1 @@\n-    _is_alive(this, heap->concurrent_mark()->next_mark_bitmap()),\n+    _is_alive(this, heap->concurrent_mark()->mark_bitmap()),\n@@ -174,1 +174,2 @@\n-  _heap->abort_concurrent_cycle();\n+  \/\/ Verification needs the bitmap, so we should clear the bitmap only later.\n+  bool in_concurrent_cycle = _heap->abort_concurrent_cycle();\n@@ -176,0 +177,4 @@\n+  if (in_concurrent_cycle) {\n+    GCTraceTime(Debug, gc) debug(\"Clear Bitmap\");\n+    _heap->concurrent_mark()->clear_bitmap(_heap->workers());\n+  }\n@@ -217,1 +222,0 @@\n-  _heap->concurrent_mark()->swap_mark_bitmaps();\n@@ -219,1 +223,1 @@\n-  _heap->concurrent_mark()->clear_next_bitmap(_heap->workers());\n+  _heap->concurrent_mark()->clear_bitmap(_heap->workers());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_prev_bitmap(oop obj) {\n+void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_bitmap(oop obj) {\n@@ -78,7 +78,2 @@\n-  if (!obj->is_forwarded()) {\n-    \/\/ Object not moving, but clear the mark to allow reuse of the bitmap.\n-    clear_in_prev_bitmap(obj);\n-    return size;\n-  }\n-\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  if (obj->is_forwarded()) {\n+    HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n@@ -86,4 +81,4 @@\n-  \/\/ copy object and reinit its mark\n-  HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n-  assert(obj_addr != destination, \"everything in this pass should be moving\");\n-  Copy::aligned_conjoint_words(obj_addr, destination, size);\n+    \/\/ copy object and reinit its mark\n+    HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n+    assert(obj_addr != destination, \"everything in this pass should be moving\");\n+    Copy::aligned_conjoint_words(obj_addr, destination, size);\n@@ -91,3 +86,4 @@\n-  \/\/ There is no need to transform stack chunks - marking already did that.\n-  cast_to_oop(destination)->init_mark();\n-  assert(cast_to_oop(destination)->klass() != NULL, \"should have a class\");\n+    \/\/ There is no need to transform stack chunks - marking already did that.\n+    cast_to_oop(destination)->init_mark();\n+    assert(cast_to_oop(destination)->klass() != NULL, \"should have a class\");\n+  }\n@@ -97,1 +93,1 @@\n-  clear_in_prev_bitmap(obj);\n+  clear_in_bitmap(obj);\n@@ -108,1 +104,1 @@\n-    \/\/ for bitmap verification and to be able to use the prev_bitmap\n+    \/\/ for bitmap verification and to be able to use the bitmap\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":13,"deletions":17,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,1 +53,1 @@\n-    void clear_in_prev_bitmap(oop object);\n+    void clear_in_bitmap(oop object);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -134,6 +134,2 @@\n-    if (hr->is_young()) {\n-      \/\/ G1 updates the BOT for old region contents incrementally, but young regions\n-      \/\/ lack BOT information for performance reasons.\n-      \/\/ Recreate BOT information of high live ratio young regions here to keep expected\n-      \/\/ performance during scanning their card tables in the collection pauses later.\n-      hr->update_bot();\n+    if (hr->needs_scrubbing_during_full_gc()) {\n+      scrub_skip_compacting_region(hr, hr->is_young());\n@@ -164,0 +160,28 @@\n+\n+void G1FullGCPrepareTask::G1ResetMetadataClosure::scrub_skip_compacting_region(HeapRegion* hr, bool update_bot_for_live) {\n+  assert(hr->needs_scrubbing_during_full_gc(), \"must be\");\n+\n+  HeapWord* limit = hr->top();\n+  HeapWord* current_obj = hr->bottom();\n+  G1CMBitMap* bitmap = _collector->mark_bitmap();\n+\n+  while (current_obj < limit) {\n+    if (bitmap->is_marked(current_obj)) {\n+      oop current = cast_to_oop(current_obj);\n+      size_t size = current->size();\n+      if (update_bot_for_live) {\n+        hr->update_bot_for_block(current_obj, current_obj + size);\n+      }\n+      current_obj += size;\n+      continue;\n+    }\n+    \/\/ Found dead object, which is potentially unloaded, scrub to next\n+    \/\/ marked object.\n+    HeapWord* scrub_start = current_obj;\n+    HeapWord* scrub_end = bitmap->get_next_marked_addr(scrub_start, limit);\n+    assert(scrub_start != scrub_end, \"must advance\");\n+    hr->fill_range_with_dead_objects(scrub_start, scrub_end);\n+\n+    current_obj = scrub_end;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":30,"deletions":6,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -98,0 +98,4 @@\n+    \/\/ Scrub all runs of dead objects within the given region by putting filler\n+    \/\/ objects and updating the corresponding BOT. If update_bot_for_live is true,\n+    \/\/ also update the BOT for live objects.\n+    void scrub_skip_compacting_region(HeapRegion* hr, bool update_bot_for_live);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -227,2 +227,2 @@\n-      if (!_hr->obj_allocated_since_prev_marking(o)) {\n-        size_t obj_size = o->size();    \/\/ Make sure we don't overflow\n+      if (_hr->obj_in_unparsable_area(o, _hr->parsable_bottom())) {\n+        size_t obj_size = o->size();\n@@ -402,3 +402,3 @@\n-        if (r->max_live_bytes() < not_dead_yet_cl.live_bytes()) {\n-          log_error(gc, verify)(\"[\" PTR_FORMAT \",\" PTR_FORMAT \"] max_live_bytes \" SIZE_FORMAT \" < calculated \" SIZE_FORMAT,\n-                                  p2i(r->bottom()), p2i(r->end()), r->max_live_bytes(), not_dead_yet_cl.live_bytes());\n+        if (r->live_bytes() < not_dead_yet_cl.live_bytes()) {\n+          log_error(gc, verify)(HR_FORMAT \" max_live_bytes %zu < calculated %zu\",\n+                                HR_FORMAT_PARAMS(r), r->live_bytes(), not_dead_yet_cl.live_bytes());\n@@ -587,1 +587,1 @@\n-  verify(type, VerifyOption::G1UsePrevMarking, \"Before GC\");\n+  verify(type, VerifyOption::G1UseConcMarking, \"Before GC\");\n@@ -591,1 +591,1 @@\n-  verify(type, VerifyOption::G1UsePrevMarking, \"After GC\");\n+  verify(type, VerifyOption::G1UseConcMarking, \"After GC\");\n@@ -594,0 +594,24 @@\n+void G1HeapVerifier::verify_bitmap_clear(bool from_tams) {\n+  if (!G1VerifyBitmaps) {\n+    return;\n+  }\n+\n+  class G1VerifyBitmapClear : public HeapRegionClosure {\n+    bool _from_tams;\n+\n+  public:\n+    G1VerifyBitmapClear(bool from_tams) : _from_tams(from_tams) { }\n+\n+    virtual bool do_heap_region(HeapRegion* r) {\n+      G1CMBitMap* bitmap = G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap();\n+\n+      HeapWord* start = _from_tams ? r->top_at_mark_start() : r->bottom();\n+\n+      HeapWord* mark = bitmap->get_next_marked_addr(start, r->end());\n+      guarantee(mark == r->end(), \"Found mark at \" PTR_FORMAT \" in region %u from start \" PTR_FORMAT, p2i(mark), r->hrm_index(), p2i(start));\n+      return false;\n+    }\n+  } cl(from_tams);\n+\n+  G1CollectedHeap::heap()->heap_region_iterate(&cl);\n+}\n@@ -658,75 +682,0 @@\n-bool G1HeapVerifier::verify_no_bits_over_tams(const char* bitmap_name, const G1CMBitMap* const bitmap,\n-                                               HeapWord* tams, HeapWord* end) {\n-  guarantee(tams <= end,\n-            \"tams: \" PTR_FORMAT \" end: \" PTR_FORMAT, p2i(tams), p2i(end));\n-  HeapWord* result = bitmap->get_next_marked_addr(tams, end);\n-  if (result < end) {\n-    log_error(gc, verify)(\"## wrong marked address on %s bitmap: \" PTR_FORMAT, bitmap_name, p2i(result));\n-    log_error(gc, verify)(\"## %s tams: \" PTR_FORMAT \" end: \" PTR_FORMAT, bitmap_name, p2i(tams), p2i(end));\n-    return false;\n-  }\n-  return true;\n-}\n-\n-bool G1HeapVerifier::verify_bitmaps(const char* caller, HeapRegion* hr) {\n-  const G1CMBitMap* const prev_bitmap = _g1h->concurrent_mark()->prev_mark_bitmap();\n-  const G1CMBitMap* const next_bitmap = _g1h->concurrent_mark()->next_mark_bitmap();\n-\n-  HeapWord* ptams  = hr->prev_top_at_mark_start();\n-  HeapWord* ntams  = hr->next_top_at_mark_start();\n-  HeapWord* end    = hr->end();\n-\n-  bool res_p = verify_no_bits_over_tams(\"prev\", prev_bitmap, ptams, end);\n-\n-  bool res_n = true;\n-  \/\/ We cannot verify the next bitmap while we are about to clear it.\n-  if (!_g1h->collector_state()->clearing_next_bitmap()) {\n-    res_n = verify_no_bits_over_tams(\"next\", next_bitmap, ntams, end);\n-  }\n-  if (!res_p || !res_n) {\n-    log_error(gc, verify)(\"#### Bitmap verification failed for \" HR_FORMAT, HR_FORMAT_PARAMS(hr));\n-    log_error(gc, verify)(\"#### Caller: %s\", caller);\n-    return false;\n-  }\n-  return true;\n-}\n-\n-void G1HeapVerifier::check_bitmaps(const char* caller, HeapRegion* hr) {\n-  if (!G1VerifyBitmaps) {\n-    return;\n-  }\n-\n-  guarantee(verify_bitmaps(caller, hr), \"bitmap verification\");\n-}\n-\n-class G1VerifyBitmapClosure : public HeapRegionClosure {\n-private:\n-  const char* _caller;\n-  G1HeapVerifier* _verifier;\n-  bool _failures;\n-\n-public:\n-  G1VerifyBitmapClosure(const char* caller, G1HeapVerifier* verifier) :\n-    _caller(caller), _verifier(verifier), _failures(false) { }\n-\n-  bool failures() { return _failures; }\n-\n-  virtual bool do_heap_region(HeapRegion* hr) {\n-    bool result = _verifier->verify_bitmaps(_caller, hr);\n-    if (!result) {\n-      _failures = true;\n-    }\n-    return false;\n-  }\n-};\n-\n-void G1HeapVerifier::check_bitmaps(const char* caller) {\n-  if (!G1VerifyBitmaps) {\n-    return;\n-  }\n-\n-  G1VerifyBitmapClosure cl(caller, this);\n-  _g1h->heap_region_iterate(&cl);\n-  guarantee(!cl.failures(), \"bitmap verification\");\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.cpp","additions":32,"deletions":83,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,23 +73,1 @@\n-#ifndef PRODUCT\n-  \/\/ Make sure that the given bitmap has no marked objects in the\n-  \/\/ range [from,limit). If it does, print an error message and return\n-  \/\/ false. Otherwise, just return true. bitmap_name should be \"prev\"\n-  \/\/ or \"next\".\n-  bool verify_no_bits_over_tams(const char* bitmap_name, const G1CMBitMap* const bitmap,\n-                                HeapWord* from, HeapWord* limit);\n-\n-  \/\/ Verify that the prev \/ next bitmap range [tams,end) for the given\n-  \/\/ region has no marks. Return true if all is well, false if errors\n-  \/\/ are detected.\n-  bool verify_bitmaps(const char* caller, HeapRegion* hr);\n-#endif \/\/ PRODUCT\n-\n-  \/\/ If G1VerifyBitmaps is set, verify that the marking bitmaps for\n-  \/\/ the given region do not have any spurious marks. If errors are\n-  \/\/ detected, print appropriate error messages and crash.\n-  void check_bitmaps(const char* caller, HeapRegion* hr) PRODUCT_RETURN;\n-\n-  \/\/ If G1VerifyBitmaps is set, verify that the marking bitmaps do not\n-  \/\/ have any spurious marks. If errors are detected, print\n-  \/\/ appropriate error messages and crash.\n-  void check_bitmaps(const char* caller) PRODUCT_RETURN;\n+  void verify_bitmap_clear(bool above_tams_only);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.hpp","additions":2,"deletions":24,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -111,1 +111,1 @@\n-  _cm->mark_in_next_bitmap(_worker_id, obj);\n+  _cm->mark_in_bitmap(_worker_id, obj);\n@@ -213,1 +213,1 @@\n-  _cm->mark_in_next_bitmap(_worker_id, obj);\n+  _cm->mark_in_bitmap(_worker_id, obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -627,2 +627,2 @@\n-    \/\/ are relabeled as such. We mark the failing objects in the prev bitmap and\n-    \/\/ later use it to handle all failed objects.\n+    \/\/ are relabeled as such. We mark the failing objects in the marking bitmap\n+    \/\/ and later use it to handle all failed objects.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -451,1 +451,1 @@\n-  collector_state()->set_clearing_next_bitmap(false);\n+  collector_state()->set_clearing_bitmap(false);\n@@ -918,1 +918,1 @@\n-    bytes_to_copy = hr->max_live_bytes();\n+    bytes_to_copy = hr->live_bytes();\n@@ -1141,0 +1141,1 @@\n+  collector_state()->set_clearing_bitmap(true);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,2 +38,2 @@\n-\/\/ to ntams. This is an exact measure.\n-\/\/ The code corrects later for the live data between ntams and top.\n+\/\/ to tams. This is an exact measure.\n+\/\/ The code corrects later for the live data between tams and top.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RegionMarkStatsCache.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1263,1 +1263,2 @@\n-  \/\/ Closure to clear the prev bitmap for any old region in the collection set.\n+  \/\/ Closure to make sure that the marking bitmap is clear for any old region in\n+  \/\/ the collection set.\n@@ -1267,0 +1268,1 @@\n+\n@@ -1269,1 +1271,1 @@\n-             \"Bitmap should have no mark for young regions\");\n+             \"Bitmap should have no mark for region %u\", hr->hrm_index());\n@@ -1271,0 +1273,1 @@\n+\n@@ -1276,3 +1279,11 @@\n-      \/\/ Young regions should always have cleared bitmaps, so only clear old.\n-      if (hr->is_old()) {\n-        _g1h->clear_prev_bitmap_for_region(hr);\n+\n+      \/\/ Evacuation failure uses the bitmap to record evacuation failed objects,\n+      \/\/ so the bitmap for the regions in the collection set must be cleared if not already.\n+      \/\/\n+      \/\/ A clear bitmap is obvious for young regions as we never mark through them;\n+      \/\/ old regions are only in the collection set after the concurrent cycle completed,\n+      \/\/ so their bitmaps must also be clear except when the pause occurs during the\n+      \/\/ concurrent bitmap clear. At that point the region's bitmap may contain marks\n+      \/\/ while being in the collection set at the same time.\n+      if (_g1h->collector_state()->clearing_bitmap() && hr->is_old()) {\n+        _g1h->clear_bitmap_for_region(hr);\n@@ -1280,2 +1291,1 @@\n-        assert(hr->is_young(), \"Should only be young and old regions in collection set\");\n-        assert_bitmap_clear(hr, _g1h->concurrent_mark()->prev_mark_bitmap());\n+        assert_bitmap_clear(hr, _g1h->concurrent_mark()->mark_bitmap());\n@@ -1793,263 +1803,0 @@\n-\n-class G1RebuildRemSetTask: public WorkerTask {\n-  \/\/ Aggregate the counting data that was constructed concurrently\n-  \/\/ with marking.\n-  class G1RebuildRemSetHeapRegionClosure : public HeapRegionClosure {\n-    G1ConcurrentMark* _cm;\n-    G1RebuildRemSetClosure _update_cl;\n-\n-    \/\/ Applies _update_cl to the references of the given object, limiting objArrays\n-    \/\/ to the given MemRegion. Returns the amount of words actually scanned.\n-    size_t scan_for_references(oop const obj, MemRegion mr) {\n-      size_t const obj_size = obj->size();\n-      \/\/ All non-objArrays and objArrays completely within the mr\n-      \/\/ can be scanned without passing the mr.\n-      if (!obj->is_objArray() || mr.contains(MemRegion(cast_from_oop<HeapWord*>(obj), obj_size))) {\n-        obj->oop_iterate(&_update_cl);\n-        return obj_size;\n-      }\n-      \/\/ This path is for objArrays crossing the given MemRegion. Only scan the\n-      \/\/ area within the MemRegion.\n-      obj->oop_iterate(&_update_cl, mr);\n-      return mr.intersection(MemRegion(cast_from_oop<HeapWord*>(obj), obj_size)).word_size();\n-    }\n-\n-    \/\/ A humongous object is live (with respect to the scanning) either\n-    \/\/ a) it is marked on the bitmap as such\n-    \/\/ b) its TARS is larger than TAMS, i.e. has been allocated during marking.\n-    bool is_humongous_live(oop const humongous_obj, const G1CMBitMap* const bitmap, HeapWord* tams, HeapWord* tars) const {\n-      return bitmap->is_marked(humongous_obj) || (tars > tams);\n-    }\n-\n-    \/\/ Iterator over the live objects within the given MemRegion.\n-    class LiveObjIterator : public StackObj {\n-      const G1CMBitMap* const _bitmap;\n-      const HeapWord* _tams;\n-      const MemRegion _mr;\n-      HeapWord* _current;\n-\n-      bool is_below_tams() const {\n-        return _current < _tams;\n-      }\n-\n-      bool is_live(HeapWord* obj) const {\n-        return !is_below_tams() || _bitmap->is_marked(obj);\n-      }\n-\n-      HeapWord* bitmap_limit() const {\n-        return MIN2(const_cast<HeapWord*>(_tams), _mr.end());\n-      }\n-\n-      void move_if_below_tams() {\n-        if (is_below_tams() && has_next()) {\n-          _current = _bitmap->get_next_marked_addr(_current, bitmap_limit());\n-        }\n-      }\n-    public:\n-      LiveObjIterator(const G1CMBitMap* const bitmap, const HeapWord* tams, const MemRegion mr, HeapWord* first_oop_into_mr) :\n-          _bitmap(bitmap),\n-          _tams(tams),\n-          _mr(mr),\n-          _current(first_oop_into_mr) {\n-\n-        assert(_current <= _mr.start(),\n-               \"First oop \" PTR_FORMAT \" should extend into mr [\" PTR_FORMAT \", \" PTR_FORMAT \")\",\n-               p2i(first_oop_into_mr), p2i(mr.start()), p2i(mr.end()));\n-\n-        \/\/ Step to the next live object within the MemRegion if needed.\n-        if (is_live(_current)) {\n-          \/\/ Non-objArrays were scanned by the previous part of that region.\n-          if (_current < mr.start() && !cast_to_oop(_current)->is_objArray()) {\n-            _current += cast_to_oop(_current)->size();\n-            \/\/ We might have positioned _current on a non-live object. Reposition to the next\n-            \/\/ live one if needed.\n-            move_if_below_tams();\n-          }\n-        } else {\n-          \/\/ The object at _current can only be dead if below TAMS, so we can use the bitmap.\n-          \/\/ immediately.\n-          _current = _bitmap->get_next_marked_addr(_current, bitmap_limit());\n-          assert(_current == _mr.end() || is_live(_current),\n-                 \"Current \" PTR_FORMAT \" should be live (%s) or beyond the end of the MemRegion (\" PTR_FORMAT \")\",\n-                 p2i(_current), BOOL_TO_STR(is_live(_current)), p2i(_mr.end()));\n-        }\n-      }\n-\n-      void move_to_next() {\n-        _current += next()->size();\n-        move_if_below_tams();\n-      }\n-\n-      oop next() const {\n-        oop result = cast_to_oop(_current);\n-        assert(is_live(_current),\n-               \"Object \" PTR_FORMAT \" must be live TAMS \" PTR_FORMAT \" below %d mr \" PTR_FORMAT \" \" PTR_FORMAT \" outside %d\",\n-               p2i(_current), p2i(_tams), _tams > _current, p2i(_mr.start()), p2i(_mr.end()), _mr.contains(result));\n-        return result;\n-      }\n-\n-      bool has_next() const {\n-        return _current < _mr.end();\n-      }\n-    };\n-\n-    \/\/ Rebuild remembered sets in the part of the region specified by mr and hr.\n-    \/\/ Objects between the bottom of the region and the TAMS are checked for liveness\n-    \/\/ using the given bitmap. Objects between TAMS and TARS are assumed to be live.\n-    \/\/ Returns the number of live words between bottom and TAMS.\n-    size_t rebuild_rem_set_in_region(const G1CMBitMap* const bitmap,\n-                                     HeapWord* const top_at_mark_start,\n-                                     HeapWord* const top_at_rebuild_start,\n-                                     HeapRegion* hr,\n-                                     MemRegion mr) {\n-      size_t marked_words = 0;\n-\n-      if (hr->is_humongous()) {\n-        oop const humongous_obj = cast_to_oop(hr->humongous_start_region()->bottom());\n-        if (is_humongous_live(humongous_obj, bitmap, top_at_mark_start, top_at_rebuild_start)) {\n-          \/\/ We need to scan both [bottom, TAMS) and [TAMS, top_at_rebuild_start);\n-          \/\/ however in case of humongous objects it is sufficient to scan the encompassing\n-          \/\/ area (top_at_rebuild_start is always larger or equal to TAMS) as one of the\n-          \/\/ two areas will be zero sized. I.e. TAMS is either\n-          \/\/ the same as bottom or top(_at_rebuild_start). There is no way TAMS has a different\n-          \/\/ value: this would mean that TAMS points somewhere into the object.\n-          assert(hr->top() == top_at_mark_start || hr->top() == top_at_rebuild_start,\n-                 \"More than one object in the humongous region?\");\n-          humongous_obj->oop_iterate(&_update_cl, mr);\n-          return top_at_mark_start != hr->bottom() ? mr.intersection(MemRegion(cast_from_oop<HeapWord*>(humongous_obj), humongous_obj->size())).byte_size() : 0;\n-        } else {\n-          return 0;\n-        }\n-      }\n-\n-      for (LiveObjIterator it(bitmap, top_at_mark_start, mr, hr->block_start(mr.start())); it.has_next(); it.move_to_next()) {\n-        oop obj = it.next();\n-        size_t scanned_size = scan_for_references(obj, mr);\n-        if (cast_from_oop<HeapWord*>(obj) < top_at_mark_start) {\n-          marked_words += scanned_size;\n-        }\n-      }\n-\n-      return marked_words * HeapWordSize;\n-    }\n-public:\n-  G1RebuildRemSetHeapRegionClosure(G1CollectedHeap* g1h,\n-                                   G1ConcurrentMark* cm,\n-                                   uint worker_id) :\n-    HeapRegionClosure(),\n-    _cm(cm),\n-    _update_cl(g1h, worker_id) { }\n-\n-    bool do_heap_region(HeapRegion* hr) {\n-      if (_cm->has_aborted()) {\n-        return true;\n-      }\n-\n-      uint const region_idx = hr->hrm_index();\n-      DEBUG_ONLY(HeapWord* const top_at_rebuild_start_check = _cm->top_at_rebuild_start(region_idx);)\n-      assert(top_at_rebuild_start_check == NULL ||\n-             top_at_rebuild_start_check > hr->bottom(),\n-             \"A TARS (\" PTR_FORMAT \") == bottom() (\" PTR_FORMAT \") indicates the old region %u is empty (%s)\",\n-             p2i(top_at_rebuild_start_check), p2i(hr->bottom()),  region_idx, hr->get_type_str());\n-\n-      size_t total_marked_bytes = 0;\n-      size_t const chunk_size_in_words = G1RebuildRemSetChunkSize \/ HeapWordSize;\n-\n-      HeapWord* const top_at_mark_start = hr->prev_top_at_mark_start();\n-\n-      HeapWord* cur = hr->bottom();\n-      while (true) {\n-        \/\/ After every iteration (yield point) we need to check whether the region's\n-        \/\/ TARS changed due to e.g. eager reclaim.\n-        HeapWord* const top_at_rebuild_start = _cm->top_at_rebuild_start(region_idx);\n-        if (top_at_rebuild_start == NULL) {\n-          return false;\n-        }\n-\n-        MemRegion next_chunk = MemRegion(hr->bottom(), top_at_rebuild_start).intersection(MemRegion(cur, chunk_size_in_words));\n-        if (next_chunk.is_empty()) {\n-          break;\n-        }\n-\n-        const Ticks start = Ticks::now();\n-        size_t marked_bytes = rebuild_rem_set_in_region(_cm->prev_mark_bitmap(),\n-                                                        top_at_mark_start,\n-                                                        top_at_rebuild_start,\n-                                                        hr,\n-                                                        next_chunk);\n-        Tickspan time = Ticks::now() - start;\n-\n-        log_trace(gc, remset, tracking)(\"Rebuilt region %u \"\n-                                        \"live \" SIZE_FORMAT \" \"\n-                                        \"time %.3fms \"\n-                                        \"marked bytes \" SIZE_FORMAT \" \"\n-                                        \"bot \" PTR_FORMAT \" \"\n-                                        \"TAMS \" PTR_FORMAT \" \"\n-                                        \"TARS \" PTR_FORMAT,\n-                                        region_idx,\n-                                        _cm->live_bytes(region_idx),\n-                                        time.seconds() * 1000.0,\n-                                        marked_bytes,\n-                                        p2i(hr->bottom()),\n-                                        p2i(top_at_mark_start),\n-                                        p2i(top_at_rebuild_start));\n-\n-        if (marked_bytes > 0) {\n-          total_marked_bytes += marked_bytes;\n-        }\n-        cur += chunk_size_in_words;\n-\n-        _cm->do_yield_check();\n-        if (_cm->has_aborted()) {\n-          return true;\n-        }\n-      }\n-      \/\/ In the final iteration of the loop the region might have been eagerly reclaimed.\n-      \/\/ Simply filter out those regions. We can not just use region type because there\n-      \/\/ might have already been new allocations into these regions.\n-      DEBUG_ONLY(HeapWord* const top_at_rebuild_start = _cm->top_at_rebuild_start(region_idx);)\n-      assert(top_at_rebuild_start == NULL ||\n-             total_marked_bytes == hr->marked_bytes(),\n-             \"Marked bytes \" SIZE_FORMAT \" for region %u (%s) in [bottom, TAMS) do not match calculated marked bytes \" SIZE_FORMAT \" \"\n-             \"(\" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \")\",\n-             total_marked_bytes, hr->hrm_index(), hr->get_type_str(), hr->marked_bytes(),\n-             p2i(hr->bottom()), p2i(top_at_mark_start), p2i(top_at_rebuild_start));\n-       \/\/ Abort state may have changed after the yield check.\n-      return _cm->has_aborted();\n-    }\n-  };\n-\n-  HeapRegionClaimer _hr_claimer;\n-  G1ConcurrentMark* _cm;\n-\n-  uint _worker_id_offset;\n-public:\n-  G1RebuildRemSetTask(G1ConcurrentMark* cm,\n-                      uint n_workers,\n-                      uint worker_id_offset) :\n-      WorkerTask(\"G1 Rebuild Remembered Set\"),\n-      _hr_claimer(n_workers),\n-      _cm(cm),\n-      _worker_id_offset(worker_id_offset) {\n-  }\n-\n-  void work(uint worker_id) {\n-    SuspendibleThreadSetJoiner sts_join;\n-\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-\n-    G1RebuildRemSetHeapRegionClosure cl(g1h, _cm, _worker_id_offset + worker_id);\n-    g1h->heap_region_par_iterate_from_worker_offset(&cl, &_hr_claimer, worker_id);\n-  }\n-};\n-\n-void G1RemSet::rebuild_rem_set(G1ConcurrentMark* cm,\n-                               WorkerThreads* workers,\n-                               uint worker_id_offset) {\n-  uint num_workers = workers->active_workers();\n-\n-  G1RebuildRemSetTask cl(cm,\n-                         num_workers,\n-                         worker_id_offset);\n-  workers->run_task(&cl, num_workers);\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":17,"deletions":270,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -150,4 +150,0 @@\n-\n-  \/\/ Rebuilds the remembered set by scanning from bottom to TARS for all regions\n-  \/\/ using the given workers.\n-  void rebuild_rem_set(G1ConcurrentMark* cm, WorkerThreads* workers, uint worker_id_offset);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,2 +65,2 @@\n-                                  \"(ntams: \" PTR_FORMAT \") \"\n-                                  \"total_live_bytes \" SIZE_FORMAT \" \"\n+                                  \"(tams: \" PTR_FORMAT \") \"\n+                                  \"total_live_bytes %zu \"\n@@ -68,3 +68,2 @@\n-                                  \"(live_bytes \" SIZE_FORMAT \" \"\n-                                  \"next_marked \" SIZE_FORMAT \" \"\n-                                  \"marked \" SIZE_FORMAT \" \"\n+                                  \"(live_bytes %zu \"\n+                                  \"marked %zu \"\n@@ -73,1 +72,1 @@\n-                                  p2i(r->next_top_at_mark_start()),\n+                                  p2i(r->top_at_mark_start()),\n@@ -77,1 +76,0 @@\n-                                  r->next_marked_bytes(),\n@@ -119,2 +117,2 @@\n-  size_t between_ntams_and_top = (r->top() - r->next_top_at_mark_start()) * HeapWordSize;\n-  size_t total_live_bytes = live_bytes + between_ntams_and_top;\n+  size_t between_tams_and_top = (r->top() - r->top_at_mark_start()) * HeapWordSize;\n+  size_t total_live_bytes = live_bytes + between_tams_and_top;\n@@ -166,5 +164,4 @@\n-                                    \"(ntams \" PTR_FORMAT \" \"\n-                                    \"liveness \" SIZE_FORMAT \" \"\n-                                    \"next_marked_bytes \" SIZE_FORMAT \" \"\n-                                    \"remset occ \" SIZE_FORMAT \" \"\n-                                    \"size \" SIZE_FORMAT \")\",\n+                                    \"(tams \" PTR_FORMAT \" \"\n+                                    \"liveness %zu \"\n+                                    \"remset occ %zu \"\n+                                    \"size %zu)\",\n@@ -172,1 +169,1 @@\n-                                    p2i(r->next_top_at_mark_start()),\n+                                    p2i(r->top_at_mark_start()),\n@@ -174,1 +171,0 @@\n-                                    r->next_marked_bytes(),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSetTrackingPolicy.cpp","additions":13,"deletions":17,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,1 +56,1 @@\n-\/\/ An entry that is below the NTAMS pointer for the containing heap\n+\/\/ An entry that is below the TAMS pointer for the containing heap\n@@ -59,1 +59,1 @@\n-\/\/ An entry that is at least the NTAMS pointer for the containing heap\n+\/\/ An entry that is at least the TAMS pointer for the containing heap\n@@ -78,1 +78,1 @@\n-\/\/ The stale reference cases are implicitly handled by the NTAMS\n+\/\/ The stale reference cases are implicitly handled by the TAMS\n@@ -90,1 +90,1 @@\n-  if (entry >= region->next_top_at_mark_start()) {\n+  if (entry >= region->top_at_mark_start()) {\n@@ -101,1 +101,1 @@\n-  return !requires_marking(entry, g1h) || g1h->is_marked_next(cast_to_oop(entry));\n+  return !requires_marking(entry, g1h) || g1h->is_marked(cast_to_oop(entry));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SATBMarkQueueSet.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -394,1 +394,1 @@\n-      log_debug(gc, humongous)(\"Humongous region %u (object size \" SIZE_FORMAT \" @ \" PTR_FORMAT \") remset \" SIZE_FORMAT \" code roots \" SIZE_FORMAT \" marked %d reclaim candidate %d type array %d\",\n+      log_debug(gc, humongous)(\"Humongous region %u (object size %zu @ \" PTR_FORMAT \") remset %zu code roots %zu marked %d reclaim candidate %d type array %d\",\n@@ -400,1 +400,1 @@\n-                               _g1h->concurrent_mark()->next_mark_bitmap()->is_marked(hr->bottom()),\n+                               _g1h->concurrent_mark()->mark_bitmap()->is_marked(hr->bottom()),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -208,2 +208,2 @@\n-    assert(!cm->is_marked_in_prev_bitmap(obj) && !cm->is_marked_in_next_bitmap(obj),\n-           \"Eagerly reclaimed humongous region %u should not be marked at all but is in prev %s next %s\",\n+    assert(!cm->is_marked_in_bitmap(obj),\n+           \"Eagerly reclaimed humongous region %u should not be marked at all but is in bitmap %s\",\n@@ -211,2 +211,1 @@\n-           BOOL_TO_STR(cm->is_marked_in_prev_bitmap(obj)),\n-           BOOL_TO_STR(cm->is_marked_in_next_bitmap(obj)));\n+           BOOL_TO_STR(cm->is_marked_in_bitmap(obj)));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -107,1 +107,0 @@\n-  _next_marked_bytes = 0;\n@@ -128,2 +127,0 @@\n-  zero_marked_bytes();\n-\n@@ -241,2 +238,4 @@\n-  _prev_top_at_mark_start(NULL), _next_top_at_mark_start(NULL),\n-  _prev_marked_bytes(0), _next_marked_bytes(0),\n+  _top_at_mark_start(NULL),\n+  _parsable_bottom(NULL),\n+  _garbage_bytes(0),\n+  _marked_bytes(0),\n@@ -277,4 +276,5 @@\n-  \/\/ We always recreate the prev marking info and we'll explicitly\n-  \/\/ mark all objects we find to be self-forwarded on the prev\n-  \/\/ bitmap. So all objects need to be below PTAMS.\n-  _prev_marked_bytes = 0;\n+  \/\/ We always scrub the region to make sure the entire region is\n+  \/\/ parsable after the self-forwarding point removal, and update _marked_bytes\n+  \/\/ at the end.\n+  _marked_bytes = 0;\n+  _garbage_bytes = 0;\n@@ -283,11 +283,8 @@\n-    \/\/ During concurrent start, we'll also explicitly mark all objects\n-    \/\/ we find to be self-forwarded on the next bitmap. So all\n-    \/\/ objects need to be below NTAMS.\n-    _next_top_at_mark_start = top();\n-    _next_marked_bytes = 0;\n-  } else if (during_conc_mark) {\n-    \/\/ During concurrent mark, all objects in the CSet (including\n-    \/\/ the ones we find to be self-forwarded) are implicitly live.\n-    \/\/ So all objects need to be above NTAMS.\n-    _next_top_at_mark_start = bottom();\n-    _next_marked_bytes = 0;\n+    \/\/ Self-forwarding marks all objects. Adjust TAMS so that these marks are\n+    \/\/ below it.\n+    _top_at_mark_start = top();\n+  } else {\n+    \/\/ Outside of the mixed phase all regions that had an evacuation failure must\n+    \/\/ be young regions, and their TAMS is always bottom. Similarly, before the\n+    \/\/ start of the mixed phase, we scrubbed and reset TAMS to bottom.\n+    assert(_top_at_mark_start == bottom(), \"must be\");\n@@ -300,2 +297,2 @@\n-  _prev_top_at_mark_start = top();\n-  _prev_marked_bytes = marked_bytes;\n+  _marked_bytes = marked_bytes;\n+  _garbage_bytes = used() - marked_bytes;\n@@ -459,2 +456,2 @@\n-  st->print(\"|TAMS \" PTR_FORMAT \", \" PTR_FORMAT \"| %s \",\n-               p2i(prev_top_at_mark_start()), p2i(next_top_at_mark_start()), rem_set()->get_state_str());\n+  st->print(\"|TAMS \" PTR_FORMAT \"| PB \" PTR_FORMAT \"| %s \",\n+               p2i(top_at_mark_start()), p2i(parsable_bottom_acquire()), rem_set()->get_state_str());\n@@ -482,0 +479,1 @@\n+\n@@ -526,1 +524,2 @@\n-      if (!_g1h->is_in(obj) || _g1h->is_obj_dead_cond(obj, _vo)) {\n+      bool is_in_heap = _g1h->is_in(obj);\n+      if (!is_in_heap || _g1h->is_obj_dead_cond(obj, _vo)) {\n@@ -533,1 +532,1 @@\n-        if (!_g1h->is_in(obj)) {\n+        if (!is_in_heap) {\n@@ -767,1 +766,1 @@\n-  verify_rem_set(VerifyOption::G1UsePrevMarking, &failures);\n+  verify_rem_set(VerifyOption::G1UseConcMarking, &failures);\n@@ -793,1 +792,1 @@\n-    if (block_is_obj(p)) {\n+    if (block_is_obj(p, parsable_bottom())) {\n@@ -808,0 +807,18 @@\n+\n+void HeapRegion::fill_range_with_dead_objects(HeapWord* start, HeapWord* end) {\n+  size_t range_size = pointer_delta(end, start);\n+\n+  \/\/ Fill the dead range with objects. G1 might need to create two objects if\n+  \/\/ the range is larger than half a region, which is the max_fill_size().\n+  CollectedHeap::fill_with_objects(start, range_size);\n+  HeapWord* current = start;\n+  do {\n+    \/\/ Update the BOT if the a threshold is crossed.\n+    size_t obj_size = cast_to_oop(current)->size();\n+    update_bot_for_block(current, current + obj_size);\n+\n+    \/\/ Advance to the next object.\n+    current += obj_size;\n+    guarantee(current <= end, \"Should never go past end\");\n+  } while (current != end);\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":46,"deletions":29,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -104,1 +104,1 @@\n-  HeapWord* pre_dummy_top() { return (_pre_dummy_top == NULL) ? top() : _pre_dummy_top; }\n+  HeapWord* pre_dummy_top() const { return (_pre_dummy_top == NULL) ? top() : _pre_dummy_top; }\n@@ -147,0 +147,2 @@\n+  static bool obj_is_filler(oop obj);\n+\n@@ -148,1 +150,1 @@\n-  HeapWord* block_start(const void* p);\n+  HeapWord* block_start(const void* addr, HeapWord* const pb);\n@@ -156,0 +158,5 @@\n+  \/\/ Create objects in the given range. The BOT will be updated if needed and\n+  \/\/ the created objects will have their header marked to show that they are\n+  \/\/ dead.\n+  void fill_range_with_dead_objects(HeapWord* start, HeapWord* end);\n+\n@@ -175,2 +182,2 @@\n-  \/\/ All allocated blocks are occupied by objects in a HeapRegion\n-  bool block_is_obj(const HeapWord* p) const;\n+  \/\/ All allocated blocks are occupied by objects in a HeapRegion.\n+  bool block_is_obj(const HeapWord* p, HeapWord* pb) const;\n@@ -178,4 +185,3 @@\n-  \/\/ Returns whether the given object is dead based on TAMS and bitmap.\n-  \/\/ An object is dead iff a) it was not allocated since the last mark (>TAMS), b) it\n-  \/\/ is not marked (bitmap).\n-  bool is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const;\n+  \/\/ Returns whether the given object is dead based on the given parsable_bottom (pb).\n+  \/\/ For an object to be considered dead it must be below pb and scrubbed.\n+  bool is_obj_dead(oop obj, HeapWord* pb) const;\n@@ -183,2 +189,3 @@\n-  \/\/ Returns the object size for all valid block starts\n-  \/\/ and the amount of unallocated words if called on top()\n+  \/\/ Returns the object size for all valid block starts. If parsable_bottom (pb)\n+  \/\/ is given, calculates the block size based on that parsable_bottom, not the\n+  \/\/ current value of this HeapRegion.\n@@ -186,0 +193,1 @@\n+  size_t block_size(const HeapWord* p, HeapWord* pb) const;\n@@ -192,3 +200,3 @@\n-  void update_bot() {\n-    _bot_part.update();\n-  }\n+  \/\/ Update the BOT for the entire region - assumes that all objects are parsable\n+  \/\/ and contiguous for this region.\n+  void update_bot();\n@@ -225,5 +233,12 @@\n-  \/\/ \"prev\" is the top at the start of the last completed marking.\n-  \/\/ \"next\" is the top at the start of the in-progress marking (if any.)\n-  HeapWord* _prev_top_at_mark_start;\n-  HeapWord* _next_top_at_mark_start;\n-\n+  HeapWord* _top_at_mark_start;\n+\n+  \/\/ The area above this limit is fully parsable. This limit\n+  \/\/ is equal to bottom except from Remark and until the region has been\n+  \/\/ scrubbed concurrently. The scrubbing ensures that all dead objects (with\n+  \/\/ possibly unloaded classes) have beenreplaced with filler objects that\n+  \/\/ are parsable. Below this limit the marking bitmap must be used to\n+  \/\/ determine size and liveness.\n+  HeapWord* volatile _parsable_bottom;\n+\n+  \/\/ Amount of dead data in the region.\n+  size_t _garbage_bytes;\n@@ -232,2 +247,1 @@\n-  size_t _prev_marked_bytes;    \/\/ Bytes known to be live via last completed marking.\n-  size_t _next_marked_bytes;    \/\/ Bytes known to be live via in-progress marking.\n+  size_t _marked_bytes;    \/\/ Bytes known to be live via last completed marking.\n@@ -236,4 +250,4 @@\n-    assert(_prev_marked_bytes == 0 &&\n-           _next_marked_bytes == 0,\n-           \"Must be called after zero_marked_bytes.\");\n-    _prev_top_at_mark_start = _next_top_at_mark_start = bottom();\n+    _top_at_mark_start = bottom();\n+    _parsable_bottom = bottom();\n+    _garbage_bytes = 0;\n+    _marked_bytes = 0;\n@@ -256,6 +270,5 @@\n-  \/\/ Returns whether the given object address refers to a dead object, and either the\n-  \/\/ size of the object (if live) or the size of the block (if dead) in size.\n-  \/\/ May\n-  \/\/ - only called with obj < top()\n-  \/\/ - not called on humongous objects or archive regions\n-  inline bool is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const;\n+  template <class Closure, bool in_gc_pause>\n+  inline HeapWord* oops_on_memregion_iterate(MemRegion mr, Closure* cl);\n+\n+  template <class Closure>\n+  inline HeapWord* oops_on_memregion_iterate_in_unparsable(MemRegion mr, HeapWord* pb, Closure* cl);\n@@ -270,1 +283,1 @@\n-  template <class Closure, bool is_gc_active>\n+  template <class Closure, bool in_gc_pause>\n@@ -272,2 +285,6 @@\n-                                                     Closure* cl,\n-                                                     G1CollectedHeap* g1h);\n+                                                     Closure* cl);\n+\n+  inline bool is_marked_in_bitmap(oop obj) const;\n+\n+  inline HeapWord* next_live_in_unparsable(G1CMBitMap* bitmap, const HeapWord* p, HeapWord* limit) const;\n+  inline HeapWord* next_live_in_unparsable(const HeapWord* p, HeapWord* limit) const;\n@@ -275,3 +292,0 @@\n-  \/\/ Returns the block size of the given (dead, potentially having its class unloaded) object\n-  \/\/ starting at p extending to at most the prev TAMS using the given mark bitmap.\n-  inline size_t block_size_using_bitmap(const HeapWord* p, const G1CMBitMap* const prev_bitmap) const;\n@@ -325,11 +339,4 @@\n-  size_t marked_bytes()    { return _prev_marked_bytes; }\n-  size_t live_bytes() {\n-    return (top() - prev_top_at_mark_start()) * HeapWordSize + marked_bytes();\n-  }\n-\n-  \/\/ The number of bytes counted in the next marking.\n-  size_t next_marked_bytes() { return _next_marked_bytes; }\n-  \/\/ The number of bytes live wrt the next marking.\n-  size_t next_live_bytes() {\n-    return\n-      (top() - next_top_at_mark_start()) * HeapWordSize + next_marked_bytes();\n+  size_t marked_bytes() const { return _marked_bytes; }\n+  \/\/ An upper bound on the number of live bytes in the region.\n+  size_t live_bytes() const {\n+    return used() - garbage_bytes();\n@@ -339,5 +346,1 @@\n-  size_t garbage_bytes() {\n-    size_t used_at_mark_start_bytes =\n-      (prev_top_at_mark_start() - bottom()) * HeapWordSize;\n-    return used_at_mark_start_bytes - marked_bytes();\n-  }\n+  size_t garbage_bytes() const { return _garbage_bytes; }\n@@ -355,10 +358,0 @@\n-  \/\/ An upper bound on the number of live bytes in the region.\n-  size_t max_live_bytes() { return used() - garbage_bytes(); }\n-\n-  void add_to_marked_bytes(size_t incr_bytes) {\n-    _next_marked_bytes = _next_marked_bytes + incr_bytes;\n-  }\n-\n-  void zero_marked_bytes()      {\n-    _prev_marked_bytes = _next_marked_bytes = 0;\n-  }\n@@ -366,2 +359,7 @@\n-  HeapWord* prev_top_at_mark_start() const { return _prev_top_at_mark_start; }\n-  HeapWord* next_top_at_mark_start() const { return _next_top_at_mark_start; }\n+  HeapWord* top_at_mark_start() const { return _top_at_mark_start; }\n+\n+  \/\/ Retrieve parsable bottom; since it may be modified concurrently, outside a\n+  \/\/ safepoint the _acquire method must be used.\n+  HeapWord* parsable_bottom() const;\n+  HeapWord* parsable_bottom_acquire() const;\n+  void reset_parsable_bottom();\n@@ -377,4 +375,18 @@\n-  \/\/ Notify the region that concurrent marking has finished. Copy the\n-  \/\/ (now finalized) next marking info fields into the prev marking\n-  \/\/ info fields.\n-  inline void note_end_of_marking();\n+  \/\/ Notify the region that concurrent marking has finished. Passes the number of\n+  \/\/ bytes between bottom and TAMS.\n+  inline void note_end_of_marking(size_t marked_bytes);\n+\n+  \/\/ Notify the region that scrubbing has completed.\n+  inline void note_end_of_scrubbing();\n+\n+  \/\/ During the concurrent scrubbing phase, can there be any areas with unloaded\n+  \/\/ classes or dead objects in this region?\n+  \/\/ This set only includes old and open archive regions - humongous regions only\n+  \/\/ contain a single object which is either dead or live, contents of closed archive\n+  \/\/ regions never die (so is always contiguous), and young regions are never even\n+  \/\/ considered during concurrent scrub.\n+  bool needs_scrubbing() const { return is_old() || is_open_archive(); }\n+  \/\/ Same question as above, during full gc. Full gc needs to scrub any region that\n+  \/\/ might be skipped for compaction. This includes young generation regions as the\n+  \/\/ region relabeling to old happens later than scrubbing.\n+  bool needs_scrubbing_during_full_gc() const { return is_young() || needs_scrubbing(); }\n@@ -538,8 +550,6 @@\n-  \/\/ Determine if an object has been allocated since the last\n-  \/\/ mark performed by the collector. This returns true iff the object\n-  \/\/ is within the unmarked area of the region.\n-  bool obj_allocated_since_prev_marking(oop obj) const {\n-    return cast_from_oop<HeapWord*>(obj) >= prev_top_at_mark_start();\n-  }\n-  bool obj_allocated_since_next_marking(oop obj) const {\n-    return cast_from_oop<HeapWord*>(obj) >= next_top_at_mark_start();\n+  \/\/ Determine if an object is in the parsable or the to-be-scrubbed area.\n+  inline static bool obj_in_parsable_area(const HeapWord* addr, HeapWord* pb);\n+  inline static bool obj_in_unparsable_area(oop obj, HeapWord* pb);\n+\n+  bool obj_allocated_since_marking_start(oop obj) const {\n+    return cast_from_oop<HeapWord*>(obj) >= top_at_mark_start();\n@@ -559,1 +569,1 @@\n-  template <bool is_gc_active, class Closure>\n+  template <bool in_gc_pause, class Closure>\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":88,"deletions":78,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -82,2 +85,2 @@\n-inline HeapWord* HeapRegion::block_start(const void* p) {\n-  return _bot_part.block_start(p);\n+inline HeapWord* HeapRegion::block_start(const void* addr, HeapWord* const pb) {\n+  return _bot_part.block_start(addr, pb);\n@@ -86,2 +89,3 @@\n-inline bool HeapRegion::is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const {\n-  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+inline bool HeapRegion::obj_in_unparsable_area(oop obj, HeapWord* const pb) {\n+  return !HeapRegion::obj_in_parsable_area(cast_from_oop<HeapWord*>(obj), pb);\n+}\n@@ -89,5 +93,3 @@\n-  assert(addr < top(), \"must be\");\n-  assert(!is_closed_archive(),\n-         \"Closed archive regions should not have references into other regions\");\n-  assert(!is_humongous(), \"Humongous objects not handled here\");\n-  bool obj_is_dead = is_obj_dead(obj, prev_bitmap);\n+inline bool HeapRegion::obj_in_parsable_area(const HeapWord* addr, HeapWord* const pb) {\n+  return addr >= pb;\n+}\n@@ -95,8 +97,2 @@\n-  if (ClassUnloading && obj_is_dead) {\n-    assert(!block_is_obj(addr), \"must be\");\n-    *size = block_size_using_bitmap(addr, prev_bitmap);\n-  } else {\n-    assert(block_is_obj(addr), \"must be\");\n-    *size = obj->size();\n-  }\n-  return obj_is_dead;\n+inline bool HeapRegion::is_marked_in_bitmap(oop obj) const {\n+  return G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap()->is_marked(obj);\n@@ -105,1 +101,1 @@\n-inline bool HeapRegion::block_is_obj(const HeapWord* p) const {\n+inline bool HeapRegion::block_is_obj(const HeapWord* const p, HeapWord* const pb) const {\n@@ -108,0 +104,5 @@\n+\n+  if (obj_in_parsable_area(p, pb)) {\n+    return true;\n+  }\n+\n@@ -111,8 +112,6 @@\n-  \/\/ During a Full GC regions can be excluded from compaction due to high live ratio, and\n-  \/\/ because of this there can be stale objects for unloaded classes left in these regions.\n-  \/\/ During a concurrent cycle class unloading is done after marking is complete and objects\n-  \/\/ for the unloaded classes will be stale until the regions are collected.\n-  if (ClassUnloading) {\n-    return !G1CollectedHeap::heap()->is_obj_dead(cast_to_oop(p), this);\n-  }\n-  return true;\n+  \/\/ To make sure dead objects can be handled without always keeping an additional bitmap, we\n+  \/\/ scrub dead objects and create filler objects that are considered dead. We do this even if\n+  \/\/ class unloading is disabled to avoid special code.\n+  \/\/ From Remark until the region has been completely scrubbed obj_is_parsable will return false\n+  \/\/ and we have to use the bitmap to know if a block is a valid object.\n+  return is_marked_in_bitmap(cast_to_oop(p));\n@@ -121,6 +120,4 @@\n-inline size_t HeapRegion::block_size_using_bitmap(const HeapWord* addr, const G1CMBitMap* const prev_bitmap) const {\n-  assert(ClassUnloading,\n-         \"All blocks should be objects if class unloading isn't used, so this method should not be called. \"\n-         \"HR: [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \") \"\n-         \"addr: \" PTR_FORMAT,\n-         p2i(bottom()), p2i(top()), p2i(end()), p2i(addr));\n+inline bool HeapRegion::obj_is_filler(const oop obj) {\n+  Klass* k = obj->klass();\n+  return k == Universe::fillerArrayKlassObj() || k == vmClasses::FillerObject_klass();\n+}\n@@ -128,3 +125,2 @@\n-  \/\/ Old regions' dead objects may have dead classes\n-  \/\/ We need to find the next live object using the bitmap\n-  HeapWord* next = prev_bitmap->get_next_marked_addr(addr, prev_top_at_mark_start());\n+inline bool HeapRegion::is_obj_dead(const oop obj, HeapWord* const pb) const {\n+  assert(is_in_reserved(obj), \"Object \" PTR_FORMAT \" must be in region\", p2i(obj));\n@@ -132,2 +128,13 @@\n-  assert(next > addr, \"must get the next live object\");\n-  return pointer_delta(next, addr);\n+  \/\/ Objects in closed archive regions are always live.\n+  if (is_closed_archive()) {\n+    return false;\n+  }\n+\n+  \/\/ From Remark until a region has been concurrently scrubbed, parts of the\n+  \/\/ region is not guaranteed to be parsable. Use the bitmap for liveness.\n+  if (obj_in_unparsable_area(obj, pb)) {\n+    return !is_marked_in_bitmap(obj);\n+  }\n+\n+  \/\/ This object is in the parsable part of the heap, live unless scrubbed.\n+  return obj_is_filler(obj);\n@@ -136,5 +143,11 @@\n-inline bool HeapRegion::is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const {\n-  assert(is_in_reserved(obj), \"Object \" PTR_FORMAT \" must be in region\", p2i(obj));\n-  return !obj_allocated_since_prev_marking(obj) &&\n-         !prev_bitmap->is_marked(obj) &&\n-         !is_closed_archive();\n+inline HeapWord* HeapRegion::next_live_in_unparsable(G1CMBitMap* const bitmap, const HeapWord* p, HeapWord* const limit) const {\n+  return bitmap->get_next_marked_addr(p, limit);\n+}\n+\n+inline HeapWord* HeapRegion::next_live_in_unparsable(const HeapWord* p, HeapWord* const limit) const {\n+  G1CMBitMap* bitmap = G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap();\n+  return next_live_in_unparsable(bitmap, p, limit);\n+}\n+\n+inline size_t HeapRegion::block_size(const HeapWord* p) const {\n+  return block_size(p, parsable_bottom());\n@@ -143,2 +156,2 @@\n-inline size_t HeapRegion::block_size(const HeapWord* addr) const {\n-  assert(addr < top(), \"precondition\");\n+inline size_t HeapRegion::block_size(const HeapWord* p, HeapWord* const pb) const {\n+  assert(p < top(), \"precondition\");\n@@ -146,2 +159,2 @@\n-  if (block_is_obj(addr)) {\n-    return cast_to_oop(addr)->size();\n+  if (!block_is_obj(p, pb)) {\n+    return pointer_delta(next_live_in_unparsable(p, pb), p);\n@@ -150,1 +163,1 @@\n-  return block_size_using_bitmap(addr, G1CollectedHeap::heap()->concurrent_mark()->prev_mark_bitmap());\n+  return cast_to_oop(p)->size();\n@@ -163,2 +176,1 @@\n-  \/\/ We treat all objects as being above PTAMS.\n-  zero_marked_bytes();\n+  \/\/ But all objects are live, we get this by setting TAMS to bottom.\n@@ -177,4 +189,4 @@\n-  _prev_top_at_mark_start = top(); \/\/ Keep existing top and usage.\n-  _prev_marked_bytes = used();\n-  _next_top_at_mark_start = bottom();\n-  _next_marked_bytes = 0;\n+  _marked_bytes = used();\n+  _garbage_bytes = 0;\n+\n+  _top_at_mark_start = bottom();\n@@ -186,0 +198,3 @@\n+  \/\/ Everything above bottom() is parsable and live.\n+  _parsable_bottom = bottom();\n+\n@@ -230,0 +245,12 @@\n+inline void HeapRegion::update_bot() {\n+  HeapWord* next_addr = bottom();\n+\n+  HeapWord* prev_addr;\n+  while (next_addr < top()) {\n+    prev_addr = next_addr;\n+    next_addr  = prev_addr + cast_to_oop(prev_addr)->size();\n+    update_bot_for_block(prev_addr, next_addr);\n+  }\n+  assert(next_addr == top(), \"Should stop the scan at the limit.\");\n+}\n+\n@@ -243,0 +270,13 @@\n+inline HeapWord* HeapRegion::parsable_bottom() const {\n+  assert(!is_init_completed() || SafepointSynchronize::is_at_safepoint(), \"only during initialization or safepoint\");\n+  return _parsable_bottom;\n+}\n+\n+inline HeapWord* HeapRegion::parsable_bottom_acquire() const {\n+  return Atomic::load_acquire(&_parsable_bottom);\n+}\n+\n+inline void HeapRegion::reset_parsable_bottom() {\n+  Atomic::release_store(&_parsable_bottom, bottom());\n+}\n+\n@@ -244,1 +284,1 @@\n-  _next_marked_bytes = 0;\n+  assert(!is_closed_archive() || top_at_mark_start() == bottom(), \"CA region's TAMS must always be at bottom\");\n@@ -246,1 +286,1 @@\n-    _next_top_at_mark_start = top();\n+    _top_at_mark_start = top();\n@@ -248,1 +288,0 @@\n-  assert(!is_closed_archive() || next_top_at_mark_start() == bottom(), \"CA region's nTAMS must always be at bottom\");\n@@ -252,5 +291,14 @@\n-inline void HeapRegion::note_end_of_marking() {\n-  _prev_top_at_mark_start = _next_top_at_mark_start;\n-  _next_top_at_mark_start = bottom();\n-  _prev_marked_bytes = _next_marked_bytes;\n-  _next_marked_bytes = 0;\n+inline void HeapRegion::note_end_of_marking(size_t marked_bytes) {\n+  assert_at_safepoint();\n+\n+  _marked_bytes = marked_bytes;\n+  _garbage_bytes = byte_size(bottom(), _top_at_mark_start) - _marked_bytes;\n+\n+  if (needs_scrubbing()) {\n+    _parsable_bottom = _top_at_mark_start;\n+  }\n+  _top_at_mark_start = bottom();\n+}\n+\n+inline void HeapRegion::note_end_of_scrubbing() {\n+  reset_parsable_bottom();\n@@ -263,1 +311,1 @@\n-template <class Closure, bool is_gc_active>\n+template <class Closure, bool in_gc_pause>\n@@ -265,2 +313,1 @@\n-                                                        Closure* cl,\n-                                                        G1CollectedHeap* g1h) {\n+                                                        Closure* cl) {\n@@ -277,1 +324,1 @@\n-  if (!is_gc_active && (obj->klass_or_null_acquire() == NULL)) {\n+  if (!in_gc_pause && (obj->klass_or_null_acquire() == NULL)) {\n@@ -285,1 +332,2 @@\n-  if (g1h->is_obj_dead(obj, sr)) {\n+  HeapWord* const pb = in_gc_pause ? sr->parsable_bottom() : sr->parsable_bottom_acquire();\n+  if (sr->is_obj_dead(obj, pb)) {\n@@ -311,5 +359,6 @@\n-template <bool is_gc_active, class Closure>\n-HeapWord* HeapRegion::oops_on_memregion_seq_iterate_careful(MemRegion mr,\n-                                                            Closure* cl) {\n-  assert(MemRegion(bottom(), end()).contains(mr), \"Card region not in heap region\");\n-  G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+template <class Closure>\n+inline HeapWord* HeapRegion::oops_on_memregion_iterate_in_unparsable(MemRegion mr, HeapWord* const pb, Closure* cl) {\n+  \/\/ Cache the boundaries of the area to scan in some locals.\n+  HeapWord* const start = mr.start();\n+  \/\/ Only scan until parsable_bottom.\n+  HeapWord* const end = MIN2(mr.end(), pb);\n@@ -317,3 +366,21 @@\n-  \/\/ Special handling for humongous regions.\n-  if (is_humongous()) {\n-    return do_oops_on_memregion_in_humongous<Closure, is_gc_active>(mr, cl, g1h);\n+  G1CMBitMap* bitmap = G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap();\n+  \/\/ Find the obj that extends onto mr.start().\n+  \/\/\n+  \/\/ The BOT itself is stable enough to be read at any time as\n+  \/\/\n+  \/\/ * during refinement the individual elements of the BOT are read and written\n+  \/\/   atomically and any visible mix of new and old BOT entries will eventually lead\n+  \/\/   to some (possibly outdated) object start.\n+  \/\/   The result of block_start() during concurrent refinement may be outdated - the\n+  \/\/   scrubbing may have written a (partial) filler object header exactly crossing\n+  \/\/   that perceived object start. So we have to advance to the next live object\n+  \/\/   (using the bitmap) to be able to start the following iteration.\n+  \/\/\n+  \/\/ * during GC the BOT does not change while reading, and the objects corresponding\n+  \/\/   to these block starts are valid as \"holes\" are filled atomically wrt to\n+  \/\/   safepoints.\n+  \/\/\n+  HeapWord* cur = block_start(start, pb);\n+\n+  if (!bitmap->is_marked(cur)) {\n+    cur = bitmap->get_next_marked_addr(cur, end);\n@@ -321,1 +388,0 @@\n-  assert(is_old() || is_archive(), \"Wrongly trying to iterate over region %u type %s\", _hrm_index, get_type_str());\n@@ -323,4 +389,2 @@\n-  \/\/ Because mr has been trimmed to what's been allocated in this\n-  \/\/ region, the parts of the heap that are examined here are always\n-  \/\/ parsable; there's no need to use klass_or_null to detect\n-  \/\/ in-progress allocation.\n+  while (cur != end) {\n+    assert(bitmap->is_marked(cur), \"must be\");\n@@ -328,0 +392,32 @@\n+    oop obj = cast_to_oop(cur);\n+    assert(oopDesc::is_oop(obj, true), \"Not an oop at \" PTR_FORMAT, p2i(cur));\n+\n+    cur += obj->size();\n+    bool is_precise = false;\n+\n+    if (!obj->is_objArray() || (cast_from_oop<HeapWord*>(obj) >= start && cur <= end)) {\n+      obj->oop_iterate(cl);\n+    } else {\n+      obj->oop_iterate(cl, mr);\n+      is_precise = true;\n+    }\n+\n+    if (cur >= end) {\n+      return is_precise ? end : cur;\n+    }\n+\n+    cur = bitmap->get_next_marked_addr(cur, end);\n+  }\n+  return end;\n+}\n+\n+\/\/ Applies cl to all reference fields of live objects in mr in non-humongous regions.\n+\/\/\n+\/\/ For performance, the strategy here is to divide the work into two parts: areas\n+\/\/ below parsable_bottom (unparsable) and above parsable_bottom. The unparsable parts\n+\/\/ use the bitmap to locate live objects.\n+\/\/ Otherwise we would need to check for every object what the current location is;\n+\/\/ we expect that the amount of GCs executed during scrubbing is very low so such\n+\/\/ tests would be unnecessary almost all the time.\n+template <class Closure, bool in_gc_pause>\n+inline HeapWord* HeapRegion::oops_on_memregion_iterate(MemRegion mr, Closure* cl) {\n@@ -332,2 +428,2 @@\n-  \/\/ Find the obj that extends onto mr.start().\n-  HeapWord* cur = block_start(start);\n+  \/\/ Snapshot the region's parsable_bottom.\n+  HeapWord* const pb = in_gc_pause ? parsable_bottom() : parsable_bottom_acquire();\n@@ -335,1 +431,18 @@\n-  const G1CMBitMap* const bitmap = g1h->concurrent_mark()->prev_mark_bitmap();\n+  \/\/ Find the obj that extends onto mr.start()\n+  HeapWord* cur;\n+  if (obj_in_parsable_area(start, pb)) {\n+    cur = block_start(start, pb);\n+  } else {\n+    cur = oops_on_memregion_iterate_in_unparsable<Closure>(mr, pb, cl);\n+    \/\/ We might have scanned beyond end at this point because of imprecise iteration.\n+    if (cur >= end) {\n+      return cur;\n+    }\n+    \/\/ Parsable_bottom is always the start of a valid parsable object, so we must either\n+    \/\/ have stopped at parsable_bottom, or already iterated beyond end. The\n+    \/\/ latter case is handled above.\n+    assert(cur == pb, \"must be cur \" PTR_FORMAT \" pb \" PTR_FORMAT, p2i(cur), p2i(pb));\n+  }\n+  assert(cur < top(), \"must be cur \" PTR_FORMAT \" top \" PTR_FORMAT, p2i(cur), p2i(top()));\n+\n+  \/\/ All objects >= pb are parsable. So we can just take object sizes directly.\n@@ -339,2 +452,0 @@\n-    assert(obj->klass_or_null() != NULL,\n-           \"Unparsable heap at \" PTR_FORMAT, p2i(cur));\n@@ -342,2 +453,0 @@\n-    size_t size;\n-    bool is_dead = is_obj_dead_with_size(obj, bitmap, &size);\n@@ -346,14 +455,12 @@\n-    cur += size;\n-    if (!is_dead) {\n-      \/\/ Process live object's references.\n-\n-      \/\/ Non-objArrays are usually marked imprecise at the object\n-      \/\/ start, in which case we need to iterate over them in full.\n-      \/\/ objArrays are precisely marked, but can still be iterated\n-      \/\/ over in full if completely covered.\n-      if (!obj->is_objArray() || (cast_from_oop<HeapWord*>(obj) >= start && cur <= end)) {\n-        obj->oop_iterate(cl);\n-      } else {\n-        obj->oop_iterate(cl, mr);\n-        is_precise = true;\n-      }\n+    cur += obj->size();\n+    \/\/ Process live object's references.\n+\n+    \/\/ Non-objArrays are usually marked imprecise at the object\n+    \/\/ start, in which case we need to iterate over them in full.\n+    \/\/ objArrays are precisely marked, but can still be iterated\n+    \/\/ over in full if completely covered.\n+    if (!obj->is_objArray() || (cast_from_oop<HeapWord*>(obj) >= start && cur <= end)) {\n+      obj->oop_iterate(cl);\n+    } else {\n+      obj->oop_iterate(cl, mr);\n+      is_precise = true;\n@@ -367,0 +474,22 @@\n+template <bool in_gc_pause, class Closure>\n+HeapWord* HeapRegion::oops_on_memregion_seq_iterate_careful(MemRegion mr,\n+                                                            Closure* cl) {\n+  assert(MemRegion(bottom(), top()).contains(mr), \"Card region not in heap region\");\n+\n+  \/\/ Special handling for humongous regions.\n+  if (is_humongous()) {\n+    return do_oops_on_memregion_in_humongous<Closure, in_gc_pause>(mr, cl);\n+  }\n+  assert(is_old() || is_archive(), \"Wrongly trying to iterate over region %u type %s\", _hrm_index, get_type_str());\n+\n+  \/\/ Because mr has been trimmed to what's been allocated in this\n+  \/\/ region, the objects in these parts of the heap have non-NULL\n+  \/\/ klass pointers. There's no need to use klass_or_null to detect\n+  \/\/ in-progress allocation.\n+  \/\/ We might be in the progress of scrubbing this region and in this\n+  \/\/ case there might be objects that have their classes unloaded and\n+  \/\/ therefore needs to be scanned using the bitmap.\n+\n+  return oops_on_memregion_iterate<Closure, in_gc_pause>(mr, cl);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":230,"deletions":101,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -71,2 +71,1 @@\n-  _prev_bitmap_mapper(NULL),\n-  _next_bitmap_mapper(NULL),\n+  _bitmap_mapper(NULL),\n@@ -77,2 +76,1 @@\n-                                   G1RegionToSpaceMapper* prev_bitmap,\n-                                   G1RegionToSpaceMapper* next_bitmap,\n+                                   G1RegionToSpaceMapper* bitmap,\n@@ -86,2 +84,1 @@\n-  _prev_bitmap_mapper = prev_bitmap;\n-  _next_bitmap_mapper = next_bitmap;\n+  _bitmap_mapper = bitmap;\n@@ -193,2 +190,1 @@\n-  _prev_bitmap_mapper->commit_regions(index, num_regions, pretouch_workers);\n-  _next_bitmap_mapper->commit_regions(index, num_regions, pretouch_workers);\n+  _bitmap_mapper->commit_regions(index, num_regions, pretouch_workers);\n@@ -220,2 +216,1 @@\n-  _prev_bitmap_mapper->uncommit_regions(start, num_regions);\n-  _next_bitmap_mapper->uncommit_regions(start, num_regions);\n+  _bitmap_mapper->uncommit_regions(start, num_regions);\n@@ -274,2 +269,1 @@\n-  _prev_bitmap_mapper->signal_mapping_changed(start, num_regions);\n-  _next_bitmap_mapper->signal_mapping_changed(start, num_regions);\n+  _bitmap_mapper->signal_mapping_changed(start, num_regions);\n@@ -286,2 +280,1 @@\n-    _prev_bitmap_mapper->committed_size() +\n-    _next_bitmap_mapper->committed_size() +\n+    _bitmap_mapper->committed_size() +\n@@ -293,2 +286,1 @@\n-    _prev_bitmap_mapper->reserved_size() +\n-    _next_bitmap_mapper->reserved_size() +\n+    _bitmap_mapper->reserved_size() +\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionManager.cpp","additions":8,"deletions":16,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -126,2 +126,1 @@\n-  G1RegionToSpaceMapper* _prev_bitmap_mapper;\n-  G1RegionToSpaceMapper* _next_bitmap_mapper;\n+  G1RegionToSpaceMapper* _bitmap_mapper;\n@@ -165,2 +164,1 @@\n-                  G1RegionToSpaceMapper* prev_bitmap,\n-                  G1RegionToSpaceMapper* next_bitmap,\n+                  G1RegionToSpaceMapper* bitmap,\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionManager.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,2 +56,0 @@\n-public:\n-  static size_t compute_size(size_t heap_size);\n@@ -60,0 +58,3 @@\n+\n+public:\n+  static size_t compute_size(size_t heap_size);\n@@ -84,1 +85,1 @@\n-                                        const HeapWord* limit) const;\n+                                        HeapWord* limit) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/markBitMap.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -35,2 +36,2 @@\n-inline HeapWord* MarkBitMap::get_next_marked_addr(const HeapWord* addr,\n-                                                const HeapWord* limit) const {\n+inline HeapWord* MarkBitMap::get_next_marked_addr(const HeapWord* const addr,\n+                                                  HeapWord* const limit) const {\n","filename":"src\/hotspot\/share\/gc\/shared\/markBitMap.inline.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,4 +34,3 @@\n-\n-  \/\/ Use \"prev\" mark bitmap information using pTAMS.\n-  G1UsePrevMarking = Default,\n-  \/\/ Use \"next\" mark bitmap information from full gc marking. This does not\n+  \/\/ Use mark bitmap information (from concurrent marking) using TAMS.\n+  G1UseConcMarking = Default,\n+  \/\/ Use mark bitmap information from full gc marking. This does not\n@@ -39,1 +38,1 @@\n-  G1UseFullMarking = G1UsePrevMarking + 1\n+  G1UseFullMarking = G1UseConcMarking + 1,\n","filename":"src\/hotspot\/share\/gc\/shared\/verifyOption.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,6 +78,6 @@\n-  G1CMBitMap* bitmap = heap->concurrent_mark()->next_mark_bitmap();\n-  bitmap->mark(region->bottom());\n-  bitmap->mark(region->bottom() + MARK_OFFSET_1);\n-  bitmap->mark(region->bottom() + MARK_OFFSET_2);\n-  bitmap->mark(region->bottom() + MARK_OFFSET_3);\n-  bitmap->mark(region->end());\n+  G1CMBitMap* bitmap = heap->concurrent_mark()->mark_bitmap();\n+  bitmap->par_mark(region->bottom());\n+  bitmap->par_mark(region->bottom() + MARK_OFFSET_1);\n+  bitmap->par_mark(region->bottom() + MARK_OFFSET_2);\n+  bitmap->par_mark(region->bottom() + MARK_OFFSET_3);\n+  bitmap->par_mark(region->end());\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_heapRegion.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -95,3 +95,3 @@\n-                              idx_t search_end,\n-                              idx_t left_bit,\n-                              idx_t right_bit) {\n+                                         idx_t search_end,\n+                                         idx_t left_bit,\n+                                         idx_t right_bit) {\n","filename":"test\/hotspot\/gtest\/utilities\/test_bitMap_search.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,1 @@\n-        \/\/ [0.048s][info ][pagesize     ] Next Bitmap: ... page_size=4K ...\n+        \/\/ [0.048s][info ][pagesize     ] Mark Bitmap: ... page_size=4K ...\n@@ -104,3 +104,2 @@\n-    static void checkBitmaps(OutputAnalyzer output, long expectedPageSize) throws Exception {\n-        checkSize(output, expectedPageSize, \"Prev Bitmap: .*page_size=([^ ]+)\");\n-        checkSize(output, expectedPageSize, \"Next Bitmap: .*page_size=([^ ]+)\");\n+    static void checkBitmap(OutputAnalyzer output, long expectedPageSize) throws Exception {\n+        checkSize(output, expectedPageSize, \"Mark Bitmap: .*page_size=([^ ]+)\");\n@@ -127,1 +126,1 @@\n-            checkBitmaps(output, (bitmapShouldUseLargePages ? largePageSize : smallPageSize));\n+            checkBitmap(output, (bitmapShouldUseLargePages ? largePageSize : smallPageSize));\n@@ -130,1 +129,1 @@\n-            checkBitmaps(output, smallPageSize);\n+            checkBitmap(output, smallPageSize);\n@@ -146,1 +145,1 @@\n-        checkBitmaps(output, smallPageSize);\n+        checkBitmap(output, smallPageSize);\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestLargePageUseForAuxMemory.java","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"}]}